2025-06-30 08:42:53.734505: I external/org_tensorflow/tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-06-30 08:42:53.779912: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-30 08:42:53.875287: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1521] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-30 08:42:53.875321: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:7704] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
======== BEGIN Original Module =========
module {
  func.func @main(%arg0: !torch.vtensor<[?,4],f32>, %arg1: !torch.vtensor<[?,4],f32>) -> !torch.vtensor<[],f32> {
    %0 = torch.aten.mul.Tensor %arg0, %arg1 : !torch.vtensor<[?,4],f32>, !torch.vtensor<[?,4],f32> -> !torch.vtensor<[?,4],f32>
    %none = torch.constant.none
    %1 = torch.aten.sum %0, %none : !torch.vtensor<[?,4],f32>, !torch.none -> !torch.vtensor<[],f32>
    return %1 : !torch.vtensor<[],f32>
  }
}

======= END Original Module ==========
[DISC] Load Input IR takes: 1.547000e-03 s.
[[ INFO ]] Running ConvertTorchToMhlo
======== After Torch2MHLO =========
module {
  func.func @main(%arg0: tensor<?x4xf32>, %arg1: tensor<?x4xf32>) -> tensor<f32> attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
    %0 = mhlo.constant dense<0.000000e+00> : tensor<f32>
    %1 = chlo.broadcast_multiply %arg0, %arg1 : (tensor<?x4xf32>, tensor<?x4xf32>) -> tensor<?x4xf32>
    %2 = mhlo.reduce(%1 init: %0) across dimensions = [0, 1] : (tensor<?x4xf32>, tensor<f32>) -> tensor<f32>
     reducer(%arg2: tensor<f32>, %arg3: tensor<f32>)  {
      %3 = mhlo.add %arg2, %arg3 : tensor<f32>
      mhlo.return %3 : tensor<f32>
    }
    return %2 : tensor<f32>
  }
}

==================================
[zj] Torch2MHLO takes: 1.534000e-02 s.
[[ INFO ]] Running TF2XLA
// -----// IR Dump After SCCP (sccp) //----- //
module {
  func.func @main(%arg0: tensor<?x4xf32>, %arg1: tensor<?x4xf32>) -> tensor<f32> attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
    %0 = mhlo.constant dense<0.000000e+00> : tensor<f32>
    %1 = chlo.broadcast_multiply %arg0, %arg1 : (tensor<?x4xf32>, tensor<?x4xf32>) -> tensor<?x4xf32>
    %2 = mhlo.reduce(%1 init: %0) across dimensions = [0, 1] : (tensor<?x4xf32>, tensor<f32>) -> tensor<f32>
     reducer(%arg2: tensor<f32>, %arg3: tensor<f32>)  {
      %3 = mhlo.add %arg2, %arg3 : tensor<f32>
      mhlo.return %3 : tensor<f32>
    }
    return %2 : tensor<f32>
  }
}


// -----// IR Dump After SCCP (sccp) //----- //
module {
  func.func @main(%arg0: tensor<?x4xf32>, %arg1: tensor<?x4xf32>) -> tensor<f32> attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
    %0 = mhlo.constant dense<0.000000e+00> : tensor<f32>
    %1 = chlo.broadcast_multiply %arg0, %arg1 : (tensor<?x4xf32>, tensor<?x4xf32>) -> tensor<?x4xf32>
    %2 = mhlo.reduce(%1 init: %0) across dimensions = [0, 1] : (tensor<?x4xf32>, tensor<f32>) -> tensor<f32>
     reducer(%arg2: tensor<f32>, %arg3: tensor<f32>)  {
      %3 = mhlo.add %arg2, %arg3 : tensor<f32>
      mhlo.return %3 : tensor<f32>
    }
    return %2 : tensor<f32>
  }
}


// -----// IR Dump After LegalizeTF (xla-legalize-tf) //----- //
module {
  func.func @main(%arg0: tensor<?x4xf32>, %arg1: tensor<?x4xf32>) -> tensor<f32> attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
    %0 = mhlo.constant dense<0.000000e+00> : tensor<f32>
    %1 = shape.shape_of %arg0 : tensor<?x4xf32> -> tensor<2xindex>
    %2 = shape.shape_of %arg1 : tensor<?x4xf32> -> tensor<2xindex>
    %3 = shape.cstr_broadcastable %1, %2 : tensor<2xindex>, tensor<2xindex>
    %4 = shape.assuming %3 -> (tensor<?x4xf32>) {
      %6 = shape.shape_of %arg0 : tensor<?x4xf32> -> tensor<2xindex>
      %7 = shape.shape_of %arg1 : tensor<?x4xf32> -> tensor<2xindex>
      %8 = shape.broadcast %6, %7 : tensor<2xindex>, tensor<2xindex> -> tensor<2xindex>
      %9 = "mhlo.dynamic_broadcast_in_dim"(%arg0, %8) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<?x4xf32>, tensor<2xindex>) -> tensor<?x4xf32>
      %10 = "mhlo.dynamic_broadcast_in_dim"(%arg1, %8) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<?x4xf32>, tensor<2xindex>) -> tensor<?x4xf32>
      %11 = mhlo.multiply %9, %10 : tensor<?x4xf32>
      shape.assuming_yield %11 : tensor<?x4xf32>
    }
    %5 = mhlo.reduce(%4 init: %0) across dimensions = [0, 1] : (tensor<?x4xf32>, tensor<f32>) -> tensor<f32>
     reducer(%arg2: tensor<f32>, %arg3: tensor<f32>)  {
      %6 = mhlo.add %arg2, %arg3 : tensor<f32>
      mhlo.return %6 : tensor<f32>
    }
    return %5 : tensor<f32>
  }
}


===-------------------------------------------------------------------------===
                         ... Execution time report ...
===-------------------------------------------------------------------------===
  Total Execution Time: 0.0063 seconds

  ----Wall Time----  ----Name----
    0.0001 (  1.4%)  ReviseArgumentsForStaticRankPass
    0.0000 (  0.7%)  FunctionalControlFlowToRegionsPass
    0.0004 (  6.7%)  Inliner
    0.0000 (  0.2%)    (A) CallGraph
    0.0001 (  1.9%)  'func.func' Pipeline
    0.0001 (  1.9%)    Canonicalizer
    0.0002 (  2.6%)  'func.func' Pipeline
    0.0000 (  0.5%)    DropWhileShapeInvariantPass
    0.0000 (  0.5%)    ReplicateTensorListInitOpsPass
    0.0001 (  1.5%)    Canonicalizer
    0.0004 (  6.3%)  SCCP
    0.0001 (  1.0%)  GuaranteeAllFuncsOneUsePass
    0.0000 (  0.0%)    (A) CallGraph
    0.0000 (  0.8%)  TensorFlowShapeInferencePass
    0.0003 (  5.3%)  SCCP
    0.0000 (  0.7%)  TensorListOpsDecompositionPass
    0.0000 (  0.6%)  StackOpsDecompositionPass
    0.0000 (  0.6%)  TensorArrayOpsDecompositionPass
    0.0001 (  2.0%)  'func.func' Pipeline
    0.0001 (  2.0%)    DecomposeResourceOpsPass
    0.0001 (  0.8%)  PromoteResourcesToArgsPass
    0.0000 (  0.7%)  SymbolDCE
    0.0000 (  0.6%)  'func.func' Pipeline
    0.0000 (  0.6%)    SinkConstantsToControlFlowPass
    0.0000 (  0.7%)  TensorFlowShapeInferencePass
    0.0002 (  3.5%)  StablehloLegalizeToHloPass
    0.0001 (  2.3%)  'func.func' Pipeline
    0.0001 (  1.4%)    DiscLowerTfPass
    0.0001 (  0.9%)    LowerQuantizedPass
    0.0001 (  0.9%)  LegalizeTfTypesPass
    0.0016 ( 25.2%)  LegalizeTF
    0.0002 (  3.5%)  'func.func' Pipeline
    0.0001 (  2.3%)    DiscLowerTfPass
    0.0001 (  1.1%)    InfeedOpsXlaAdjustLayout
    0.0001 (  1.8%)  LegalizeTFCollective
    0.0002 (  2.5%)  'func.func' Pipeline
    0.0002 (  2.5%)    Canonicalizer
    0.0001 (  1.3%)  TensorFlowShapeInferencePass
    0.0006 (  9.5%)  LegalizeTF
    0.0001 (  1.3%)  LegalizeTFCommunicationPass
    0.0002 (  3.0%)  'func.func' Pipeline
    0.0001 (  1.8%)    DiscDynamicSliceConverterPass
    0.0001 (  1.1%)    SinkConstantsToControlFlowPass
    0.0007 ( 11.1%)  Rest
    0.0063 (100.0%)  Total
======== BEGIN After TF2HLO =========
module {
  func.func @main(%arg0: tensor<?x4xf32>, %arg1: tensor<?x4xf32>) -> tensor<f32> attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
    %0 = mhlo.constant dense<0.000000e+00> : tensor<f32>
    %1 = shape.shape_of %arg0 : tensor<?x4xf32> -> tensor<2xindex>
    %2 = shape.shape_of %arg1 : tensor<?x4xf32> -> tensor<2xindex>
    %3 = shape.cstr_broadcastable %1, %2 : tensor<2xindex>, tensor<2xindex>
    %4 = shape.assuming %3 -> (tensor<?x4xf32>) {
      %6 = shape.shape_of %arg0 : tensor<?x4xf32> -> tensor<2xindex>
      %7 = shape.shape_of %arg1 : tensor<?x4xf32> -> tensor<2xindex>
      %8 = shape.broadcast %6, %7 : tensor<2xindex>, tensor<2xindex> -> tensor<2xindex>
      %9 = "mhlo.dynamic_broadcast_in_dim"(%arg0, %8) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<?x4xf32>, tensor<2xindex>) -> tensor<?x4xf32>
      %10 = "mhlo.dynamic_broadcast_in_dim"(%arg1, %8) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<?x4xf32>, tensor<2xindex>) -> tensor<?x4xf32>
      %11 = mhlo.multiply %9, %10 : tensor<?x4xf32>
      shape.assuming_yield %11 : tensor<?x4xf32>
    }
    %5 = mhlo.reduce(%4 init: %0) across dimensions = [0, 1] : (tensor<?x4xf32>, tensor<f32>) -> tensor<f32>
     reducer(%arg2: tensor<f32>, %arg3: tensor<f32>)  {
      %6 = mhlo.add %arg2, %arg3 : tensor<f32>
      mhlo.return %6 : tensor<f32>
    }
    return %5 : tensor<f32>
  }
}

======= END After TF2HLO ==========
[DISC] tf2hlo takes: 7.133000e-03 s.
// -----// IR Dump After RemoveShapeConstraintsPass (disc-remove-shape-constraint) //----- //
func.func @main(%arg0: tensor<?x4xf32>, %arg1: tensor<?x4xf32>) -> tensor<f32> attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
  %0 = shape.const_witness true
  %1 = mhlo.constant dense<0.000000e+00> : tensor<f32>
  %2 = shape.assuming %0 -> (tensor<?x4xf32>) {
    %4 = shape.shape_of %arg0 : tensor<?x4xf32> -> tensor<2xindex>
    %5 = shape.shape_of %arg1 : tensor<?x4xf32> -> tensor<2xindex>
    %6 = shape.broadcast %4, %5 : tensor<2xindex>, tensor<2xindex> -> tensor<2xindex>
    %7 = "mhlo.dynamic_broadcast_in_dim"(%arg0, %6) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<?x4xf32>, tensor<2xindex>) -> tensor<?x4xf32>
    %8 = "mhlo.dynamic_broadcast_in_dim"(%arg1, %6) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<?x4xf32>, tensor<2xindex>) -> tensor<?x4xf32>
    %9 = mhlo.multiply %7, %8 : tensor<?x4xf32>
    shape.assuming_yield %9 : tensor<?x4xf32>
  }
  %3 = mhlo.reduce(%2 init: %1) across dimensions = [0, 1] : (tensor<?x4xf32>, tensor<f32>) -> tensor<f32>
   reducer(%arg2: tensor<f32>, %arg3: tensor<f32>)  {
    %4 = mhlo.add %arg2, %arg3 : tensor<f32>
    mhlo.return %4 : tensor<f32>
  }
  return %3 : tensor<f32>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: tensor<?x4xf32>, %arg1: tensor<?x4xf32>) -> tensor<f32> attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
  %0 = mhlo.constant dense<0.000000e+00> : tensor<f32>
  %1 = shape.shape_of %arg0 : tensor<?x4xf32> -> tensor<2xindex>
  %2 = shape.shape_of %arg1 : tensor<?x4xf32> -> tensor<2xindex>
  %3 = shape.broadcast %1, %2 : tensor<2xindex>, tensor<2xindex> -> tensor<2xindex>
  %4 = "mhlo.dynamic_broadcast_in_dim"(%arg0, %3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<?x4xf32>, tensor<2xindex>) -> tensor<?x4xf32>
  %5 = "mhlo.dynamic_broadcast_in_dim"(%arg1, %3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<?x4xf32>, tensor<2xindex>) -> tensor<?x4xf32>
  %6 = mhlo.multiply %4, %5 : tensor<?x4xf32>
  %7 = mhlo.reduce(%6 init: %0) across dimensions = [0, 1] : (tensor<?x4xf32>, tensor<f32>) -> tensor<f32>
   reducer(%arg2: tensor<f32>, %arg3: tensor<f32>)  {
    %8 = mhlo.add %arg2, %arg3 : tensor<f32>
    mhlo.return %8 : tensor<f32>
  }
  return %7 : tensor<f32>
}

// -----// IR Dump After ConvertShapeToStandardPass (disc-convert-shape-to-std) //----- //
func.func @main(%arg0: tensor<?x4xf32>, %arg1: tensor<?x4xf32>) -> tensor<f32> attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
  %0 = mhlo.constant dense<0.000000e+00> : tensor<f32>
  %c0 = arith.constant 0 : index
  %dim = tensor.dim %arg0, %c0 : tensor<?x4xf32>
  %c4 = arith.constant 4 : index
  %from_elements = tensor.from_elements %dim, %c4 : tensor<2xindex>
  %c0_0 = arith.constant 0 : index
  %dim_1 = tensor.dim %arg1, %c0_0 : tensor<?x4xf32>
  %c4_2 = arith.constant 4 : index
  %from_elements_3 = tensor.from_elements %dim_1, %c4_2 : tensor<2xindex>
  %c0_4 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c0_5 = arith.constant 0 : index
  %extracted = tensor.extract %from_elements[%c0_5] : tensor<2xindex>
  %1 = arith.cmpi eq, %extracted, %c1 : index
  %2 = arith.select %1, %c1, %extracted : index
  %c0_6 = arith.constant 0 : index
  %extracted_7 = tensor.extract %from_elements_3[%c0_6] : tensor<2xindex>
  %3 = arith.cmpi eq, %extracted_7, %c1 : index
  %4 = arith.select %3, %2, %extracted_7 : index
  %c1_8 = arith.constant 1 : index
  %c1_9 = arith.constant 1 : index
  %extracted_10 = tensor.extract %from_elements[%c1_9] : tensor<2xindex>
  %5 = arith.cmpi eq, %extracted_10, %c1_8 : index
  %6 = arith.select %5, %c1_8, %extracted_10 : index
  %c1_11 = arith.constant 1 : index
  %extracted_12 = tensor.extract %from_elements_3[%c1_11] : tensor<2xindex>
  %7 = arith.cmpi eq, %extracted_12, %c1_8 : index
  %8 = arith.select %7, %6, %extracted_12 : index
  %from_elements_13 = tensor.from_elements %4, %8 : tensor<2xindex>
  %9 = "mhlo.dynamic_broadcast_in_dim"(%arg0, %from_elements_13) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<?x4xf32>, tensor<2xindex>) -> tensor<?x4xf32>
  %10 = "mhlo.dynamic_broadcast_in_dim"(%arg1, %from_elements_13) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<?x4xf32>, tensor<2xindex>) -> tensor<?x4xf32>
  %11 = mhlo.multiply %9, %10 : tensor<?x4xf32>
  %12 = mhlo.reduce(%11 init: %0) across dimensions = [0, 1] : (tensor<?x4xf32>, tensor<f32>) -> tensor<f32>
   reducer(%arg2: tensor<f32>, %arg3: tensor<f32>)  {
    %13 = mhlo.add %arg2, %arg3 : tensor<f32>
    mhlo.return %13 : tensor<f32>
  }
  return %12 : tensor<f32>
}

SymbolicDimMgr::save walkRankedTensorValue takes: 7 us
SymbolicDimMgr::save update attributes takes: 2 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 0 us
productSet.size() = 0
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 3 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 0 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 0 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 0 us
SymbolicDimMgr::save updateProductEqualityMap takes: 22 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
SymbolicDimMgr::save collect symbolicDim ops takes: 7 us
SymbolicDimMgr::save remove symbolicDim ops takes: 1 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 0 us
SymbolicDimMgr::save canonicalize the name takes: 8 us
SymbolicDimMgr::save replace the name takes: 7 us
SymbolicDimMgr::save updateFunctionType takes: 0 us
// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<?x4xf32, [@S0, @C4]>, %arg1: tensor<?x4xf32, [@S1, @C4]>) -> tensor<f32> attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
    %c4 = arith.constant 4 : index
    %c0 = arith.constant 0 : index
    %0 = mhlo.constant dense<0.000000e+00> : tensor<f32>
    %c1 = arith.constant 1 : index
    %dim = tensor.dim %arg0, %c0 : tensor<?x4xf32, [@S0, @C4]>
    %dim_0 = tensor.dim %arg1, %c0 : tensor<?x4xf32, [@S1, @C4]>
    %1 = arith.cmpi eq, %dim_0, %c1 : index
    %2 = arith.select %1, %dim, %dim_0 : index
    %from_elements = tensor.from_elements %2, %c4 : tensor<2xindex>
    %3 = "mhlo.dynamic_broadcast_in_dim"(%arg0, %from_elements) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<?x4xf32, [@S0, @C4]>, tensor<2xindex>) -> tensor<?x4xf32, [@S2, @C4]>
    %4 = "mhlo.dynamic_broadcast_in_dim"(%arg1, %from_elements) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<?x4xf32, [@S1, @C4]>, tensor<2xindex>) -> tensor<?x4xf32, [@S2, @C4]>
    %5 = mhlo.multiply %3, %4 : tensor<?x4xf32, [@S2, @C4]>
    %6 = mhlo.reduce(%5 init: %0) across dimensions = [0, 1] : (tensor<?x4xf32, [@S2, @C4]>, tensor<f32>) -> tensor<f32>
     reducer(%arg2: tensor<f32>, %arg3: tensor<f32>)  {
      %7 = mhlo.add %arg2, %arg3 : tensor<f32>
      mhlo.return %7 : tensor<f32>
    }
    return %6 : tensor<f32>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = true, sym_name = "C4", value = 4 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    return
  }
}


SymbolicDimMgr::save walkRankedTensorValue takes: 7 us
SymbolicDimMgr::save update attributes takes: 1 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 0 us
productSet.size() = 0
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 3 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 0 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 0 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 0 us
SymbolicDimMgr::save updateProductEqualityMap takes: 22 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
SymbolicDimMgr::save collect symbolicDim ops takes: 6 us
SymbolicDimMgr::save remove symbolicDim ops takes: 1 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 0 us
SymbolicDimMgr::save canonicalize the name takes: 6 us
SymbolicDimMgr::save replace the name takes: 6 us
SymbolicDimMgr::save updateFunctionType takes: 0 us
// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<?x4xf32, [@S0, @C4]>, %arg1: tensor<?x4xf32, [@S1, @C4]>) -> tensor<f32> attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
    %c4 = arith.constant 4 : index
    %c0 = arith.constant 0 : index
    %0 = mhlo.constant dense<0.000000e+00> : tensor<f32>
    %c1 = arith.constant 1 : index
    %dim = tensor.dim %arg0, %c0 : tensor<?x4xf32, [@S0, @C4]>
    %dim_0 = tensor.dim %arg1, %c0 : tensor<?x4xf32, [@S1, @C4]>
    %1 = arith.cmpi eq, %dim_0, %c1 : index
    %2 = arith.select %1, %dim, %dim_0 : index
    %from_elements = tensor.from_elements %2, %c4 : tensor<2xindex>
    %3 = "mhlo.dynamic_broadcast_in_dim"(%arg0, %from_elements) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<?x4xf32, [@S0, @C4]>, tensor<2xindex>) -> tensor<?x4xf32, [@S2, @C4]>
    %4 = "mhlo.dynamic_broadcast_in_dim"(%arg1, %from_elements) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<?x4xf32, [@S1, @C4]>, tensor<2xindex>) -> tensor<?x4xf32, [@S2, @C4]>
    %5 = mhlo.multiply %3, %4 : tensor<?x4xf32, [@S2, @C4]>
    %6 = mhlo.reduce(%5 init: %0) across dimensions = [0, 1] : (tensor<?x4xf32, [@S2, @C4]>, tensor<f32>) -> tensor<f32>
     reducer(%arg2: tensor<f32>, %arg3: tensor<f32>)  {
      %7 = mhlo.add %arg2, %arg3 : tensor<f32>
      mhlo.return %7 : tensor<f32>
    }
    return %6 : tensor<f32>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = true, sym_name = "C4", value = 4 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    return
  }
}


SymbolicDimMgr::save walkRankedTensorValue takes: 7 us
SymbolicDimMgr::save update attributes takes: 1 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 0 us
productSet.size() = 0
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 3 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 0 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 0 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 0 us
SymbolicDimMgr::save updateProductEqualityMap takes: 22 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
SymbolicDimMgr::save collect symbolicDim ops takes: 6 us
SymbolicDimMgr::save remove symbolicDim ops takes: 1 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 0 us
SymbolicDimMgr::save canonicalize the name takes: 6 us
SymbolicDimMgr::save replace the name takes: 5 us
SymbolicDimMgr::save updateFunctionType takes: 0 us
// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<?x4xf32, [@S0, @C4]>, %arg1: tensor<?x4xf32, [@S1, @C4]>) -> tensor<f32> attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
    %c4 = arith.constant 4 : index
    %c0 = arith.constant 0 : index
    %0 = mhlo.constant dense<0.000000e+00> : tensor<f32>
    %c1 = arith.constant 1 : index
    %dim = tensor.dim %arg0, %c0 : tensor<?x4xf32, [@S0, @C4]>
    %dim_0 = tensor.dim %arg1, %c0 : tensor<?x4xf32, [@S1, @C4]>
    %1 = arith.cmpi eq, %dim_0, %c1 : index
    %2 = arith.select %1, %dim, %dim_0 : index
    %from_elements = tensor.from_elements %2, %c4 : tensor<2xindex>
    %3 = "mhlo.dynamic_broadcast_in_dim"(%arg0, %from_elements) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<?x4xf32, [@S0, @C4]>, tensor<2xindex>) -> tensor<?x4xf32, [@S2, @C4]>
    %4 = "mhlo.dynamic_broadcast_in_dim"(%arg1, %from_elements) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<?x4xf32, [@S1, @C4]>, tensor<2xindex>) -> tensor<?x4xf32, [@S2, @C4]>
    %5 = mhlo.multiply %3, %4 : tensor<?x4xf32, [@S2, @C4]>
    %6 = mhlo.reduce(%5 init: %0) across dimensions = [0, 1] : (tensor<?x4xf32, [@S2, @C4]>, tensor<f32>) -> tensor<f32>
     reducer(%arg2: tensor<f32>, %arg3: tensor<f32>)  {
      %7 = mhlo.add %arg2, %arg3 : tensor<f32>
      mhlo.return %7 : tensor<f32>
    }
    return %6 : tensor<f32>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = true, sym_name = "C4", value = 4 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    return
  }
}


// -----// IR Dump After HloCanonicalizeReductionPass (hlo-canonicalize-reduction) //----- //
func.func @main(%arg0: tensor<?x4xf32, [@S0, @C4]>, %arg1: tensor<?x4xf32, [@S1, @C4]>) -> tensor<f32> attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %0 = mhlo.constant dense<0.000000e+00> : tensor<f32>
  %c1 = arith.constant 1 : index
  %dim = tensor.dim %arg0, %c0 : tensor<?x4xf32, [@S0, @C4]>
  %dim_0 = tensor.dim %arg1, %c0 : tensor<?x4xf32, [@S1, @C4]>
  %1 = arith.cmpi eq, %dim_0, %c1 : index
  %2 = arith.select %1, %dim, %dim_0 : index
  %from_elements = tensor.from_elements %2, %c4 : tensor<2xindex>
  %3 = "mhlo.dynamic_broadcast_in_dim"(%arg0, %from_elements) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<?x4xf32, [@S0, @C4]>, tensor<2xindex>) -> tensor<?x4xf32, [@S2, @C4]>
  %4 = "mhlo.dynamic_broadcast_in_dim"(%arg1, %from_elements) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<?x4xf32, [@S1, @C4]>, tensor<2xindex>) -> tensor<?x4xf32, [@S2, @C4]>
  %5 = mhlo.multiply %3, %4 : tensor<?x4xf32, [@S2, @C4]>
  %c1_i32 = arith.constant 1 : i32
  %c0_1 = arith.constant 0 : index
  %dim_2 = tensor.dim %5, %c0_1 : tensor<?x4xf32, [@S2, @C4]>
  %6 = arith.index_cast %dim_2 : index to i32
  %7 = arith.muli %c1_i32, %6 : i32
  %c1_3 = arith.constant 1 : index
  %dim_4 = tensor.dim %5, %c1_3 : tensor<?x4xf32, [@S2, @C4]>
  %8 = arith.index_cast %dim_4 : index to i32
  %9 = arith.muli %7, %8 : i32
  %from_elements_5 = tensor.from_elements %9, %c1_i32 : tensor<2xi32>
  %10 = mhlo.dynamic_reshape %5, %from_elements_5 : (tensor<?x4xf32, [@S2, @C4]>, tensor<2xi32>) -> tensor<?x?xf32>
  %11 = mhlo.reduce(%10 init: %0) across dimensions = [0] : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32>
   reducer(%arg2: tensor<f32>, %arg3: tensor<f32>)  {
    %13 = mhlo.add %arg2, %arg3 : tensor<f32>
    mhlo.return %13 : tensor<f32>
  }
  %12 = mhlo.reshape %11 : (tensor<?xf32>) -> tensor<f32>
  return %12 : tensor<f32>
}

SymbolicDimMgr::save walkRankedTensorValue takes: 8 us
SymbolicDimMgr::save update attributes takes: 2 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 8 us
productSet.size() = 2
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 5 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 11 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 0 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 0 us
SymbolicDimMgr::save updateProductEqualityMap takes: 45 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
SymbolicDimMgr::save collect symbolicDim ops takes: 8 us
SymbolicDimMgr::save remove symbolicDim ops takes: 1 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 1 us
SymbolicDimMgr::save canonicalize the name takes: 9 us
SymbolicDimMgr::save replace the name takes: 7 us
SymbolicDimMgr::save updateFunctionType takes: 0 us
SymbolicDimMgr::save walkRankedTensorValue takes: 8 us
SymbolicDimMgr::save update attributes takes: 2 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 7 us
productSet.size() = 2
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 4 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 10 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 0 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 0 us
SymbolicDimMgr::save updateProductEqualityMap takes: 42 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
SymbolicDimMgr::save collect symbolicDim ops takes: 8 us
SymbolicDimMgr::save remove symbolicDim ops takes: 1 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 0 us
SymbolicDimMgr::save canonicalize the name takes: 8 us
SymbolicDimMgr::save replace the name takes: 7 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<?x4xf32, [@S0, @C4]>, %arg1: tensor<?x4xf32, [@S1, @C4]>) -> tensor<f32> attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c0 = arith.constant 0 : index
    %c4_i32 = arith.constant 4 : i32
    %c1_i32 = arith.constant 1 : i32
    %0 = mhlo.constant dense<0.000000e+00> : tensor<f32>
    %dim = tensor.dim %arg0, %c0 : tensor<?x4xf32, [@S0, @C4]>
    %dim_0 = tensor.dim %arg1, %c0 : tensor<?x4xf32, [@S1, @C4]>
    %1 = arith.cmpi eq, %dim_0, %c1 : index
    %2 = arith.select %1, %dim, %dim_0 : index
    %from_elements = tensor.from_elements %2, %c4 : tensor<2xindex>
    %3 = "mhlo.dynamic_broadcast_in_dim"(%arg0, %from_elements) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<?x4xf32, [@S0, @C4]>, tensor<2xindex>) -> tensor<?x4xf32, [@S2, @C4]>
    %4 = "mhlo.dynamic_broadcast_in_dim"(%arg1, %from_elements) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<?x4xf32, [@S1, @C4]>, tensor<2xindex>) -> tensor<?x4xf32, [@S2, @C4]>
    %5 = mhlo.multiply %3, %4 : tensor<?x4xf32, [@S2, @C4]>
    %6 = arith.index_cast %2 : index to i32
    %7 = arith.muli %6, %c4_i32 : i32
    %from_elements_1 = tensor.from_elements %7, %c1_i32 : tensor<2xi32>
    %8 = mhlo.dynamic_reshape %5, %from_elements_1 : (tensor<?x4xf32, [@S2, @C4]>, tensor<2xi32>) -> tensor<?x1xf32, [@S3, @C1]>
    %9 = mhlo.reduce(%8 init: %0) across dimensions = [0] : (tensor<?x1xf32, [@S3, @C1]>, tensor<f32>) -> tensor<1xf32>
     reducer(%arg2: tensor<f32>, %arg3: tensor<f32>)  {
      %11 = mhlo.add %arg2, %arg3 : tensor<f32>
      mhlo.return %11 : tensor<f32>
    }
    %10 = mhlo.reshape %9 : (tensor<1xf32>) -> tensor<f32>
    return %10 : tensor<f32>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = true, sym_name = "C4", value = 4 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = true, sym_name = "C1", value = 1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %c4 = arith.constant 4 : index
    %0 = "disc_shape.dim"() {name = @S2} : () -> index
    %1 = "disc_shape.dim"() {name = @S3} : () -> index
    "disc_shape.tie_product_equal"(%c4, %0, %1) {operand_segment_sizes = array<i32: 2, 1>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After DiscMarkShapeCalculationPass (disc-mhlo-mark-shape-calc) //----- //
module {
  func.func @main(%arg0: tensor<?x4xf32, [@S0, @C4]>, %arg1: tensor<?x4xf32, [@S1, @C4]>) -> tensor<f32> attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c0 = arith.constant 0 : index
    %c4_i32 = arith.constant 4 : i32
    %c1_i32 = arith.constant 1 : i32
    %0 = mhlo.constant dense<0.000000e+00> : tensor<f32>
    %dim = tensor.dim %arg0, %c0 : tensor<?x4xf32, [@S0, @C4]>
    %dim_0 = tensor.dim %arg1, %c0 : tensor<?x4xf32, [@S1, @C4]>
    %1 = arith.cmpi eq, %dim_0, %c1 : index
    %2 = arith.select %1, %dim, %dim_0 : index
    %from_elements = tensor.from_elements %2, %c4 {disc.shape_op = true} : tensor<2xindex>
    %3 = "mhlo.dynamic_broadcast_in_dim"(%arg0, %from_elements) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<?x4xf32, [@S0, @C4]>, tensor<2xindex>) -> tensor<?x4xf32, [@S2, @C4]>
    %4 = "mhlo.dynamic_broadcast_in_dim"(%arg1, %from_elements) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<?x4xf32, [@S1, @C4]>, tensor<2xindex>) -> tensor<?x4xf32, [@S2, @C4]>
    %5 = mhlo.multiply %3, %4 : tensor<?x4xf32, [@S2, @C4]>
    %6 = arith.index_cast %2 : index to i32
    %7 = arith.muli %6, %c4_i32 : i32
    %from_elements_1 = tensor.from_elements %7, %c1_i32 {disc.shape_op = true} : tensor<2xi32>
    %8 = mhlo.dynamic_reshape %5, %from_elements_1 : (tensor<?x4xf32, [@S2, @C4]>, tensor<2xi32>) -> tensor<?x1xf32, [@S3, @C1]>
    %9 = mhlo.reduce(%8 init: %0) across dimensions = [0] : (tensor<?x1xf32, [@S3, @C1]>, tensor<f32>) -> tensor<1xf32>
     reducer(%arg2: tensor<f32>, %arg3: tensor<f32>)  {
      %11 = mhlo.add %arg2, %arg3 : tensor<f32>
      mhlo.return %11 : tensor<f32>
    }
    %10 = mhlo.reshape %9 : (tensor<1xf32>) -> tensor<f32>
    return %10 : tensor<f32>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = true, sym_name = "C4", value = 4 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = true, sym_name = "C1", value = 1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %c4 = arith.constant 4 : index
    %0 = "disc_shape.dim"() {name = @S2} : () -> index
    %1 = "disc_shape.dim"() {name = @S3} : () -> index
    "disc_shape.tie_product_equal"(%c4, %0, %1) {operand_segment_sizes = array<i32: 2, 1>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After PlaceOpsPass (mhlo-place-ops) //----- //
module {
  func.func @main(%arg0: tensor<?x4xf32, [@S0, @C4]>, %arg1: tensor<?x4xf32, [@S1, @C4]>) -> tensor<f32> attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c0 = arith.constant 0 : index
    %c4_i32 = arith.constant 4 : i32
    %c1_i32 = arith.constant 1 : i32
    %0 = mhlo.constant {disc.device = "gpu"} dense<0.000000e+00> : tensor<f32>
    %dim = tensor.dim %arg0, %c0 : tensor<?x4xf32, [@S0, @C4]>
    %dim_0 = tensor.dim %arg1, %c0 : tensor<?x4xf32, [@S1, @C4]>
    %1 = arith.cmpi eq, %dim_0, %c1 : index
    %2 = arith.select %1, %dim, %dim_0 : index
    %from_elements = tensor.from_elements %2, %c4 {disc.shape_op = true} : tensor<2xindex>
    %3 = "mhlo_disc.h2d"(%arg0) : (tensor<?x4xf32, [@S0, @C4]>) -> tensor<?x4xf32, [@S0, @C4]>
    %4 = "mhlo.dynamic_broadcast_in_dim"(%3, %from_elements) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x4xf32, [@S0, @C4]>, tensor<2xindex>) -> tensor<?x4xf32, [@S2, @C4]>
    %5 = "mhlo_disc.h2d"(%arg1) : (tensor<?x4xf32, [@S1, @C4]>) -> tensor<?x4xf32, [@S1, @C4]>
    %6 = "mhlo.dynamic_broadcast_in_dim"(%5, %from_elements) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x4xf32, [@S1, @C4]>, tensor<2xindex>) -> tensor<?x4xf32, [@S2, @C4]>
    %7 = mhlo.multiply %4, %6 {disc.device = "gpu"} : tensor<?x4xf32, [@S2, @C4]>
    %8 = arith.index_cast %2 : index to i32
    %9 = arith.muli %8, %c4_i32 : i32
    %from_elements_1 = tensor.from_elements %9, %c1_i32 {disc.shape_op = true} : tensor<2xi32>
    %10 = mhlo.dynamic_reshape %7, %from_elements_1 {disc.device = "gpu"} : (tensor<?x4xf32, [@S2, @C4]>, tensor<2xi32>) -> tensor<?x1xf32, [@S3, @C1]>
    %11 = mhlo.reduce(%10 init: %0) across dimensions = [0] {disc.device = "gpu"} : (tensor<?x1xf32, [@S3, @C1]>, tensor<f32>) -> tensor<1xf32>
     reducer(%arg2: tensor<f32>, %arg3: tensor<f32>)  {
      %14 = mhlo.add %arg2, %arg3 {disc.device = "gpu"} : tensor<f32>
      mhlo.return %14 : tensor<f32>
    }
    %12 = mhlo.reshape %11 {disc.device = "gpu"} : (tensor<1xf32>) -> tensor<f32>
    %13 = "mhlo_disc.d2h"(%12) {disc.device = "cpu"} : (tensor<f32>) -> tensor<f32>
    return %13 : tensor<f32>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = true, sym_name = "C4", value = 4 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = true, sym_name = "C1", value = 1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %c4 = arith.constant 4 : index
    %0 = "disc_shape.dim"() {name = @S2} : () -> index
    %1 = "disc_shape.dim"() {name = @S3} : () -> index
    "disc_shape.tie_product_equal"(%c4, %0, %1) {operand_segment_sizes = array<i32: 2, 1>} : (index, index, index) -> ()
    return
  }
}


SymbolicDimMgr::save walkRankedTensorValue takes: 10 us
SymbolicDimMgr::save update attributes takes: 2 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 8 us
productSet.size() = 2
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 4 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 10 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 0 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 0 us
SymbolicDimMgr::save updateProductEqualityMap takes: 44 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
SymbolicDimMgr::save collect symbolicDim ops takes: 9 us
SymbolicDimMgr::save remove symbolicDim ops takes: 1 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 0 us
SymbolicDimMgr::save canonicalize the name takes: 8 us
SymbolicDimMgr::save replace the name takes: 9 us
SymbolicDimMgr::save updateFunctionType takes: 0 us
// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<?x4xf32, [@S0, @C4]>, %arg1: tensor<?x4xf32, [@S1, @C4]>) -> tensor<f32> attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
    %c4 = arith.constant 4 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %0 = mhlo.constant {disc.device = "gpu"} dense<0.000000e+00> : tensor<f32>
    %c1_i32 = arith.constant 1 : i32
    %c4_i32 = arith.constant 4 : i32
    %dim = tensor.dim %arg0, %c0 : tensor<?x4xf32, [@S0, @C4]>
    %dim_0 = tensor.dim %arg1, %c0 : tensor<?x4xf32, [@S1, @C4]>
    %1 = arith.cmpi eq, %dim_0, %c1 : index
    %2 = arith.select %1, %dim, %dim_0 : index
    %from_elements = tensor.from_elements %2, %c4 {disc.shape_op = true} : tensor<2xindex>
    %3 = "mhlo_disc.h2d"(%arg0) : (tensor<?x4xf32, [@S0, @C4]>) -> tensor<?x4xf32, [@S0, @C4]>
    %4 = "mhlo.dynamic_broadcast_in_dim"(%3, %from_elements) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x4xf32, [@S0, @C4]>, tensor<2xindex>) -> tensor<?x4xf32, [@S2, @C4]>
    %5 = "mhlo_disc.h2d"(%arg1) : (tensor<?x4xf32, [@S1, @C4]>) -> tensor<?x4xf32, [@S1, @C4]>
    %6 = "mhlo.dynamic_broadcast_in_dim"(%5, %from_elements) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x4xf32, [@S1, @C4]>, tensor<2xindex>) -> tensor<?x4xf32, [@S2, @C4]>
    %7 = mhlo.multiply %4, %6 {disc.device = "gpu"} : tensor<?x4xf32, [@S2, @C4]>
    %8 = arith.index_cast %2 : index to i32
    %9 = arith.muli %8, %c4_i32 : i32
    %from_elements_1 = tensor.from_elements %9, %c1_i32 {disc.shape_op = true} : tensor<2xi32>
    %10 = mhlo.dynamic_reshape %7, %from_elements_1 {disc.device = "gpu"} : (tensor<?x4xf32, [@S2, @C4]>, tensor<2xi32>) -> tensor<?x1xf32, [@S3, @C1]>
    %11 = mhlo.reduce(%10 init: %0) across dimensions = [0] {disc.device = "gpu"} : (tensor<?x1xf32, [@S3, @C1]>, tensor<f32>) -> tensor<1xf32>
     reducer(%arg2: tensor<f32>, %arg3: tensor<f32>)  {
      %14 = mhlo.add %arg2, %arg3 {disc.device = "gpu"} : tensor<f32>
      mhlo.return %14 : tensor<f32>
    }
    %12 = mhlo.reshape %11 {disc.device = "gpu"} : (tensor<1xf32>) -> tensor<f32>
    %13 = "mhlo_disc.d2h"(%12) {disc.device = "cpu"} : (tensor<f32>) -> tensor<f32>
    return %13 : tensor<f32>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = true, sym_name = "C4", value = 4 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = true, sym_name = "C1", value = 1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %c4 = arith.constant 4 : index
    %0 = "disc_shape.dim"() {name = @S2} : () -> index
    %1 = "disc_shape.dim"() {name = @S3} : () -> index
    "disc_shape.tie_product_equal"(%c4, %0, %1) {operand_segment_sizes = array<i32: 2, 1>} : (index, index, index) -> ()
    return
  }
}


SymbolicDimMgr::save walkRankedTensorValue takes: 10 us
SymbolicDimMgr::save update attributes takes: 2 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 15 us
productSet.size() = 2
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 4 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 9 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 0 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 0 us
SymbolicDimMgr::save updateProductEqualityMap takes: 50 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
SymbolicDimMgr::save collect symbolicDim ops takes: 9 us
SymbolicDimMgr::save remove symbolicDim ops takes: 1 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 0 us
SymbolicDimMgr::save canonicalize the name takes: 8 us
SymbolicDimMgr::save replace the name takes: 8 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<?x4xf32, [@S0, @C4]>, %arg1: tensor<?x4xf32, [@S1, @C4]>) -> tensor<f32> attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
    %c4 = arith.constant 4 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c4_i32 = arith.constant 4 : i32
    %c1_i32 = arith.constant 1 : i32
    %0 = mhlo.constant {disc.device = "gpu"} dense<0.000000e+00> : tensor<f32>
    %dim = tensor.dim %arg0, %c0 : tensor<?x4xf32, [@S0, @C4]>
    %dim_0 = tensor.dim %arg1, %c0 : tensor<?x4xf32, [@S1, @C4]>
    %1 = arith.cmpi eq, %dim_0, %c1 : index
    %2 = arith.select %1, %dim, %dim_0 : index
    %from_elements = tensor.from_elements %2, %c4 {disc.shape_op = true} : tensor<2xindex>
    %3 = "mhlo_disc.h2d"(%arg0) : (tensor<?x4xf32, [@S0, @C4]>) -> tensor<?x4xf32, [@S0, @C4]>
    %4 = "mhlo.dynamic_broadcast_in_dim"(%3, %from_elements) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x4xf32, [@S0, @C4]>, tensor<2xindex>) -> tensor<?x4xf32, [@S2, @C4]>
    %5 = "mhlo_disc.h2d"(%arg1) : (tensor<?x4xf32, [@S1, @C4]>) -> tensor<?x4xf32, [@S1, @C4]>
    %6 = "mhlo.dynamic_broadcast_in_dim"(%5, %from_elements) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x4xf32, [@S1, @C4]>, tensor<2xindex>) -> tensor<?x4xf32, [@S2, @C4]>
    %7 = mhlo.multiply %4, %6 {disc.device = "gpu"} : tensor<?x4xf32, [@S2, @C4]>
    %8 = arith.index_cast %2 : index to i32
    %9 = arith.muli %8, %c4_i32 : i32
    %from_elements_1 = tensor.from_elements %9, %c1_i32 {disc.shape_op = true} : tensor<2xi32>
    %10 = mhlo.dynamic_reshape %7, %from_elements_1 {disc.device = "gpu"} : (tensor<?x4xf32, [@S2, @C4]>, tensor<2xi32>) -> tensor<?x1xf32, [@S3, @C1]>
    %11 = mhlo.reduce(%10 init: %0) across dimensions = [0] {disc.device = "gpu"} : (tensor<?x1xf32, [@S3, @C1]>, tensor<f32>) -> tensor<1xf32>
     reducer(%arg2: tensor<f32>, %arg3: tensor<f32>)  {
      %14 = mhlo.add %arg2, %arg3 {disc.device = "gpu"} : tensor<f32>
      mhlo.return %14 : tensor<f32>
    }
    %12 = mhlo.reshape %11 {disc.device = "gpu"} : (tensor<1xf32>) -> tensor<f32>
    %13 = "mhlo_disc.d2h"(%12) {disc.device = "cpu"} : (tensor<f32>) -> tensor<f32>
    return %13 : tensor<f32>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = true, sym_name = "C4", value = 4 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = true, sym_name = "C1", value = 1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %c4 = arith.constant 4 : index
    %0 = "disc_shape.dim"() {name = @S2} : () -> index
    %1 = "disc_shape.dim"() {name = @S3} : () -> index
    "disc_shape.tie_product_equal"(%c4, %0, %1) {operand_segment_sizes = array<i32: 2, 1>} : (index, index, index) -> ()
    return
  }
}


SymbolicDimMgr::save walkRankedTensorValue takes: 10 us
SymbolicDimMgr::save update attributes takes: 2 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 13 us
productSet.size() = 2
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 4 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 10 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 0 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 0 us
SymbolicDimMgr::save updateProductEqualityMap takes: 54 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
SymbolicDimMgr::save collect symbolicDim ops takes: 10 us
SymbolicDimMgr::save remove symbolicDim ops takes: 1 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 0 us
SymbolicDimMgr::save canonicalize the name takes: 8 us
SymbolicDimMgr::save replace the name takes: 9 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<?x4xf32, [@S0, @C4]>, %arg1: tensor<?x4xf32, [@S1, @C4]>) -> tensor<f32> attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
    %c4 = arith.constant 4 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %0 = mhlo.constant {disc.device = "gpu"} dense<0.000000e+00> : tensor<f32>
    %c1_i32 = arith.constant 1 : i32
    %c4_i32 = arith.constant 4 : i32
    %dim = tensor.dim %arg0, %c0 : tensor<?x4xf32, [@S0, @C4]>
    %dim_0 = tensor.dim %arg1, %c0 : tensor<?x4xf32, [@S1, @C4]>
    %1 = arith.cmpi eq, %dim_0, %c1 : index
    %2 = arith.select %1, %dim, %dim_0 : index
    %from_elements = tensor.from_elements %2, %c4 {disc.shape_op = true} : tensor<2xindex>
    %3 = "mhlo_disc.h2d"(%arg0) : (tensor<?x4xf32, [@S0, @C4]>) -> tensor<?x4xf32, [@S0, @C4]>
    %4 = "mhlo.dynamic_broadcast_in_dim"(%3, %from_elements) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x4xf32, [@S0, @C4]>, tensor<2xindex>) -> tensor<?x4xf32, [@S2, @C4]>
    %5 = "mhlo_disc.h2d"(%arg1) : (tensor<?x4xf32, [@S1, @C4]>) -> tensor<?x4xf32, [@S1, @C4]>
    %6 = "mhlo.dynamic_broadcast_in_dim"(%5, %from_elements) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x4xf32, [@S1, @C4]>, tensor<2xindex>) -> tensor<?x4xf32, [@S2, @C4]>
    %7 = mhlo.multiply %4, %6 {disc.device = "gpu"} : tensor<?x4xf32, [@S2, @C4]>
    %8 = arith.index_cast %2 : index to i32
    %9 = arith.muli %8, %c4_i32 : i32
    %from_elements_1 = tensor.from_elements %9, %c1_i32 {disc.shape_op = true} : tensor<2xi32>
    %10 = mhlo.dynamic_reshape %7, %from_elements_1 {disc.device = "gpu"} : (tensor<?x4xf32, [@S2, @C4]>, tensor<2xi32>) -> tensor<?x1xf32, [@S3, @C1]>
    %11 = mhlo.reduce(%10 init: %0) across dimensions = [0] {disc.device = "gpu"} : (tensor<?x1xf32, [@S3, @C1]>, tensor<f32>) -> tensor<1xf32>
     reducer(%arg2: tensor<f32>, %arg3: tensor<f32>)  {
      %14 = mhlo.add %arg2, %arg3 {disc.device = "gpu"} : tensor<f32>
      mhlo.return %14 : tensor<f32>
    }
    %12 = mhlo.reshape %11 {disc.device = "gpu"} : (tensor<1xf32>) -> tensor<f32>
    %13 = "mhlo_disc.d2h"(%12) {disc.device = "cpu"} : (tensor<f32>) -> tensor<f32>
    return %13 : tensor<f32>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = true, sym_name = "C4", value = 4 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = true, sym_name = "C1", value = 1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %c4 = arith.constant 4 : index
    %0 = "disc_shape.dim"() {name = @S2} : () -> index
    %1 = "disc_shape.dim"() {name = @S3} : () -> index
    "disc_shape.tie_product_equal"(%c4, %0, %1) {operand_segment_sizes = array<i32: 2, 1>} : (index, index, index) -> ()
    return
  }
}


SymbolicDimMgr::save walkRankedTensorValue takes: 10 us
SymbolicDimMgr::save update attributes takes: 2 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 6 us
productSet.size() = 2
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 4 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 10 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 0 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 0 us
SymbolicDimMgr::save updateProductEqualityMap takes: 41 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
SymbolicDimMgr::save collect symbolicDim ops takes: 9 us
SymbolicDimMgr::save remove symbolicDim ops takes: 1 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 0 us
SymbolicDimMgr::save canonicalize the name takes: 8 us
SymbolicDimMgr::save replace the name takes: 8 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<?x4xf32>, %arg1: tensor<?x4xf32>) -> tensor<f32> attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
    %c4 = arith.constant 4 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c4_i32 = arith.constant 4 : i32
    %c1_i32 = arith.constant 1 : i32
    %0 = mhlo.constant {disc.device = "gpu"} dense<0.000000e+00> : tensor<f32>
    %dim = tensor.dim %arg0, %c0 : tensor<?x4xf32>
    %1 = "disc_shape.tie_shape"(%arg0, %dim, %c4) {kDiscSymbolicDimAttr = [@S0, @C4]} : (tensor<?x4xf32>, index, index) -> tensor<?x4xf32>
    %dim_0 = tensor.dim %arg1, %c0 : tensor<?x4xf32>
    %2 = "disc_shape.tie_shape"(%arg1, %dim_0, %c4) {kDiscSymbolicDimAttr = [@S1, @C4]} : (tensor<?x4xf32>, index, index) -> tensor<?x4xf32>
    %3 = arith.cmpi eq, %dim_0, %c1 : index
    %4 = arith.select %3, %dim, %dim_0 : index
    %from_elements = tensor.from_elements %4, %c4 {disc.shape_op = true} : tensor<2xindex>
    %5 = "disc_shape.tie_shape"(%from_elements, %c2) : (tensor<2xindex>, index) -> tensor<2xindex>
    %6 = "mhlo_disc.h2d"(%1) : (tensor<?x4xf32>) -> tensor<?x4xf32>
    %7 = "disc_shape.tie_shape"(%6, %dim, %c4) {kDiscSymbolicDimAttr = [@S0, @C4]} : (tensor<?x4xf32>, index, index) -> tensor<?x4xf32>
    %8 = "mhlo.dynamic_broadcast_in_dim"(%7, %5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x4xf32>, tensor<2xindex>) -> tensor<?x4xf32>
    %9 = "disc_shape.tie_shape"(%8, %4, %c4) {kDiscSymbolicDimAttr = [@S2, @C4]} : (tensor<?x4xf32>, index, index) -> tensor<?x4xf32>
    %10 = "mhlo_disc.h2d"(%2) : (tensor<?x4xf32>) -> tensor<?x4xf32>
    %11 = "disc_shape.tie_shape"(%10, %dim_0, %c4) {kDiscSymbolicDimAttr = [@S1, @C4]} : (tensor<?x4xf32>, index, index) -> tensor<?x4xf32>
    %12 = "mhlo.dynamic_broadcast_in_dim"(%11, %5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x4xf32>, tensor<2xindex>) -> tensor<?x4xf32>
    %13 = "disc_shape.tie_shape"(%12, %4, %c4) {kDiscSymbolicDimAttr = [@S2, @C4]} : (tensor<?x4xf32>, index, index) -> tensor<?x4xf32>
    %14 = mhlo.multiply %9, %13 {disc.device = "gpu"} : tensor<?x4xf32>
    %15 = "disc_shape.tie_shape"(%14, %4, %c4) {kDiscSymbolicDimAttr = [@S2, @C4]} : (tensor<?x4xf32>, index, index) -> tensor<?x4xf32>
    %16 = arith.index_cast %4 : index to i32
    %17 = arith.muli %16, %c4_i32 : i32
    %from_elements_1 = tensor.from_elements %17, %c1_i32 {disc.shape_op = true} : tensor<2xi32>
    %18 = "disc_shape.tie_shape"(%from_elements_1, %c2) : (tensor<2xi32>, index) -> tensor<2xi32>
    %19 = arith.index_cast %17 : i32 to index
    %20 = mhlo.dynamic_reshape %15, %18 {disc.device = "gpu"} : (tensor<?x4xf32>, tensor<2xi32>) -> tensor<?x1xf32>
    %21 = "disc_shape.tie_shape"(%20, %19, %c1) {kDiscSymbolicDimAttr = [@S3, @C1]} : (tensor<?x1xf32>, index, index) -> tensor<?x1xf32>
    %22 = mhlo.reduce(%21 init: %0) across dimensions = [0] {disc.device = "gpu"} : (tensor<?x1xf32>, tensor<f32>) -> tensor<1xf32>
     reducer(%arg2: tensor<f32>, %arg3: tensor<f32>)  {
      %26 = mhlo.add %arg2, %arg3 {disc.device = "gpu"} : tensor<f32>
      mhlo.return %26 : tensor<f32>
    }
    %23 = "disc_shape.tie_shape"(%22, %c1) : (tensor<1xf32>, index) -> tensor<1xf32>
    %24 = mhlo.reshape %23 {disc.device = "gpu"} : (tensor<1xf32>) -> tensor<f32>
    %25 = "mhlo_disc.d2h"(%24) {disc.device = "cpu"} : (tensor<f32>) -> tensor<f32>
    return %25 : tensor<f32>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = true, sym_name = "C4", value = 4 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = true, sym_name = "C1", value = 1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %c4 = arith.constant 4 : index
    %0 = "disc_shape.dim"() {name = @S2} : () -> index
    %1 = "disc_shape.dim"() {name = @S3} : () -> index
    "disc_shape.tie_product_equal"(%c4, %0, %1) {operand_segment_sizes = array<i32: 2, 1>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: tensor<?x4xf32>, %arg1: tensor<?x4xf32>) -> tensor<f32> attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c4_i32 = arith.constant 4 : i32
  %c1_i32 = arith.constant 1 : i32
  %0 = mhlo.constant {disc.device = "gpu"} dense<0.000000e+00> : tensor<f32>
  %dim = tensor.dim %arg0, %c0 : tensor<?x4xf32>
  %1 = "disc_shape.tie_shape"(%arg0, %dim, %c4) {kDiscSymbolicDimAttr = [@S0, @C4]} : (tensor<?x4xf32>, index, index) -> tensor<?x4xf32>
  %dim_0 = tensor.dim %arg1, %c0 : tensor<?x4xf32>
  %2 = "disc_shape.tie_shape"(%arg1, %dim_0, %c4) {kDiscSymbolicDimAttr = [@S1, @C4]} : (tensor<?x4xf32>, index, index) -> tensor<?x4xf32>
  %3 = arith.cmpi eq, %dim_0, %c1 : index
  %4 = arith.select %3, %dim, %dim_0 : index
  %from_elements = tensor.from_elements %4, %c4 {disc.shape_op = true} : tensor<2xindex>
  %5 = "mhlo_disc.h2d"(%1) : (tensor<?x4xf32>) -> tensor<?x4xf32>
  %6 = "disc_shape.tie_shape"(%5, %dim, %c4) {kDiscSymbolicDimAttr = [@S0, @C4]} : (tensor<?x4xf32>, index, index) -> tensor<?x4xf32>
  %7 = "mhlo.dynamic_broadcast_in_dim"(%6, %from_elements) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x4xf32>, tensor<2xindex>) -> tensor<?x4xf32>
  %8 = "disc_shape.tie_shape"(%7, %4, %c4) {kDiscSymbolicDimAttr = [@S2, @C4]} : (tensor<?x4xf32>, index, index) -> tensor<?x4xf32>
  %9 = "mhlo_disc.h2d"(%2) : (tensor<?x4xf32>) -> tensor<?x4xf32>
  %10 = "disc_shape.tie_shape"(%9, %dim_0, %c4) {kDiscSymbolicDimAttr = [@S1, @C4]} : (tensor<?x4xf32>, index, index) -> tensor<?x4xf32>
  %11 = "mhlo.dynamic_broadcast_in_dim"(%10, %from_elements) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x4xf32>, tensor<2xindex>) -> tensor<?x4xf32>
  %12 = "disc_shape.tie_shape"(%11, %4, %c4) {kDiscSymbolicDimAttr = [@S2, @C4]} : (tensor<?x4xf32>, index, index) -> tensor<?x4xf32>
  %13 = mhlo.multiply %8, %12 {disc.device = "gpu"} : tensor<?x4xf32>
  %14 = "disc_shape.tie_shape"(%13, %4, %c4) {kDiscSymbolicDimAttr = [@S2, @C4]} : (tensor<?x4xf32>, index, index) -> tensor<?x4xf32>
  %15 = arith.index_cast %4 : index to i32
  %16 = arith.muli %15, %c4_i32 : i32
  %from_elements_1 = tensor.from_elements %16, %c1_i32 {disc.shape_op = true} : tensor<2xi32>
  %17 = arith.index_cast %16 : i32 to index
  %18 = mhlo.dynamic_reshape %14, %from_elements_1 {disc.device = "gpu"} : (tensor<?x4xf32>, tensor<2xi32>) -> tensor<?x1xf32>
  %19 = "disc_shape.tie_shape"(%18, %17, %c1) {kDiscSymbolicDimAttr = [@S3, @C1]} : (tensor<?x1xf32>, index, index) -> tensor<?x1xf32>
  %20 = mhlo.reduce(%19 init: %0) across dimensions = [0] {disc.device = "gpu"} : (tensor<?x1xf32>, tensor<f32>) -> tensor<1xf32>
   reducer(%arg2: tensor<f32>, %arg3: tensor<f32>)  {
    %23 = mhlo.add %arg2, %arg3 {disc.device = "gpu"} : tensor<f32>
    mhlo.return %23 : tensor<f32>
  }
  %21 = mhlo.reshape %20 {disc.device = "gpu"} : (tensor<1xf32>) -> tensor<f32>
  %22 = "mhlo_disc.d2h"(%21) {disc.device = "cpu"} : (tensor<f32>) -> tensor<f32>
  return %22 : tensor<f32>
}

// -----// IR Dump After FuncBufferize (func-bufferize) //----- //
module {
  func.func @main(%arg0: memref<?x4xf32>, %arg1: memref<?x4xf32>) -> memref<f32> attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
    %0 = bufferization.to_tensor %arg1 : memref<?x4xf32>
    %1 = bufferization.to_tensor %arg0 : memref<?x4xf32>
    %c4 = arith.constant 4 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c4_i32 = arith.constant 4 : i32
    %c1_i32 = arith.constant 1 : i32
    %2 = mhlo.constant {disc.device = "gpu"} dense<0.000000e+00> : tensor<f32>
    %dim = tensor.dim %1, %c0 : tensor<?x4xf32>
    %3 = "disc_shape.tie_shape"(%1, %dim, %c4) {kDiscSymbolicDimAttr = [@S0, @C4]} : (tensor<?x4xf32>, index, index) -> tensor<?x4xf32>
    %dim_0 = tensor.dim %0, %c0 : tensor<?x4xf32>
    %4 = "disc_shape.tie_shape"(%0, %dim_0, %c4) {kDiscSymbolicDimAttr = [@S1, @C4]} : (tensor<?x4xf32>, index, index) -> tensor<?x4xf32>
    %5 = arith.cmpi eq, %dim_0, %c1 : index
    %6 = arith.select %5, %dim, %dim_0 : index
    %from_elements = tensor.from_elements %6, %c4 {disc.shape_op = true} : tensor<2xindex>
    %7 = "mhlo_disc.h2d"(%3) : (tensor<?x4xf32>) -> tensor<?x4xf32>
    %8 = "disc_shape.tie_shape"(%7, %dim, %c4) {kDiscSymbolicDimAttr = [@S0, @C4]} : (tensor<?x4xf32>, index, index) -> tensor<?x4xf32>
    %9 = "mhlo.dynamic_broadcast_in_dim"(%8, %from_elements) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x4xf32>, tensor<2xindex>) -> tensor<?x4xf32>
    %10 = "disc_shape.tie_shape"(%9, %6, %c4) {kDiscSymbolicDimAttr = [@S2, @C4]} : (tensor<?x4xf32>, index, index) -> tensor<?x4xf32>
    %11 = "mhlo_disc.h2d"(%4) : (tensor<?x4xf32>) -> tensor<?x4xf32>
    %12 = "disc_shape.tie_shape"(%11, %dim_0, %c4) {kDiscSymbolicDimAttr = [@S1, @C4]} : (tensor<?x4xf32>, index, index) -> tensor<?x4xf32>
    %13 = "mhlo.dynamic_broadcast_in_dim"(%12, %from_elements) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x4xf32>, tensor<2xindex>) -> tensor<?x4xf32>
    %14 = "disc_shape.tie_shape"(%13, %6, %c4) {kDiscSymbolicDimAttr = [@S2, @C4]} : (tensor<?x4xf32>, index, index) -> tensor<?x4xf32>
    %15 = mhlo.multiply %10, %14 {disc.device = "gpu"} : tensor<?x4xf32>
    %16 = "disc_shape.tie_shape"(%15, %6, %c4) {kDiscSymbolicDimAttr = [@S2, @C4]} : (tensor<?x4xf32>, index, index) -> tensor<?x4xf32>
    %17 = arith.index_cast %6 : index to i32
    %18 = arith.muli %17, %c4_i32 : i32
    %from_elements_1 = tensor.from_elements %18, %c1_i32 {disc.shape_op = true} : tensor<2xi32>
    %19 = arith.index_cast %18 : i32 to index
    %20 = mhlo.dynamic_reshape %16, %from_elements_1 {disc.device = "gpu"} : (tensor<?x4xf32>, tensor<2xi32>) -> tensor<?x1xf32>
    %21 = "disc_shape.tie_shape"(%20, %19, %c1) {kDiscSymbolicDimAttr = [@S3, @C1]} : (tensor<?x1xf32>, index, index) -> tensor<?x1xf32>
    %22 = mhlo.reduce(%21 init: %2) across dimensions = [0] {disc.device = "gpu"} : (tensor<?x1xf32>, tensor<f32>) -> tensor<1xf32>
     reducer(%arg2: tensor<f32>, %arg3: tensor<f32>)  {
      %26 = mhlo.add %arg2, %arg3 {disc.device = "gpu"} : tensor<f32>
      mhlo.return %26 : tensor<f32>
    }
    %23 = mhlo.reshape %22 {disc.device = "gpu"} : (tensor<1xf32>) -> tensor<f32>
    %24 = "mhlo_disc.d2h"(%23) {disc.device = "cpu"} : (tensor<f32>) -> tensor<f32>
    %25 = bufferization.to_memref %24 : memref<f32>
    return %25 : memref<f32>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = true, sym_name = "C4", value = 4 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = true, sym_name = "C1", value = 1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %c4 = arith.constant 4 : index
    %0 = "disc_shape.dim"() {name = @S2} : () -> index
    %1 = "disc_shape.dim"() {name = @S3} : () -> index
    "disc_shape.tie_product_equal"(%c4, %0, %1) {operand_segment_sizes = array<i32: 2, 1>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After DiscHloLegalizeToLhloPass (disc-hlo-legalize-to-lhlo) //----- //
module {
  func.func @main(%arg0: memref<?x4xf32>, %arg1: memref<?x4xf32>) -> memref<f32> attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
    %0 = bufferization.to_tensor %arg1 : memref<?x4xf32>
    %1 = bufferization.to_memref %0 : memref<?x4xf32>
    %2 = bufferization.to_tensor %arg0 : memref<?x4xf32>
    %3 = bufferization.to_memref %2 : memref<?x4xf32>
    %c4 = arith.constant 4 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c4_i32 = arith.constant 4 : i32
    %c1_i32 = arith.constant 1 : i32
    %4 = mhlo.constant {disc.device = "gpu"} dense<0.000000e+00> : tensor<f32>
    %dim = tensor.dim %2, %c0 : tensor<?x4xf32>
    %c4_0 = arith.constant 4 : index
    %5 = arith.muli %c4_0, %dim : index
    %reinterpret_cast = memref.reinterpret_cast %3 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
    %dim_1 = tensor.dim %0, %c0 : tensor<?x4xf32>
    %c4_2 = arith.constant 4 : index
    %6 = arith.muli %c4_2, %dim_1 : index
    %reinterpret_cast_3 = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_1, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
    %7 = arith.cmpi eq, %dim_1, %c1 : index
    %8 = arith.select %7, %dim, %dim_1 : index
    %from_elements = tensor.from_elements %8, %c4 {disc.shape_op = true} : tensor<2xindex>
    %9 = bufferization.to_tensor %reinterpret_cast : memref<?x4xf32>
    %10 = shape.shape_of %9 : tensor<?x4xf32> -> tensor<2xindex>
    %c0_4 = arith.constant 0 : index
    %extracted = tensor.extract %10[%c0_4] : tensor<2xindex>
    %alloc = memref.alloc(%extracted) : memref<?x4xf32>
    "lmhlo_disc.h2d"(%reinterpret_cast, %alloc) : (memref<?x4xf32>, memref<?x4xf32>) -> ()
    %c4_5 = arith.constant 4 : index
    %11 = arith.muli %c4_5, %dim : index
    %reinterpret_cast_6 = memref.reinterpret_cast %alloc to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
    %12 = bufferization.to_tensor %reinterpret_cast_6 : memref<?x4xf32>
    %13 = "mhlo.dynamic_broadcast_in_dim"(%12, %from_elements) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x4xf32>, tensor<2xindex>) -> tensor<?x4xf32>
    %14 = bufferization.to_memref %13 : memref<?x4xf32>
    %c4_7 = arith.constant 4 : index
    %15 = arith.muli %c4_7, %8 : index
    %reinterpret_cast_8 = memref.reinterpret_cast %14 to offset: [0], sizes: [%8, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32> to memref<?x4xf32>
    %16 = bufferization.to_tensor %reinterpret_cast_8 : memref<?x4xf32>
    %17 = bufferization.to_tensor %reinterpret_cast_3 : memref<?x4xf32>
    %18 = shape.shape_of %17 : tensor<?x4xf32> -> tensor<2xindex>
    %c0_9 = arith.constant 0 : index
    %extracted_10 = tensor.extract %18[%c0_9] : tensor<2xindex>
    %alloc_11 = memref.alloc(%extracted_10) : memref<?x4xf32>
    "lmhlo_disc.h2d"(%reinterpret_cast_3, %alloc_11) : (memref<?x4xf32>, memref<?x4xf32>) -> ()
    %c4_12 = arith.constant 4 : index
    %19 = arith.muli %c4_12, %dim_1 : index
    %reinterpret_cast_13 = memref.reinterpret_cast %alloc_11 to offset: [0], sizes: [%dim_1, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
    %20 = bufferization.to_tensor %reinterpret_cast_13 : memref<?x4xf32>
    %21 = "mhlo.dynamic_broadcast_in_dim"(%20, %from_elements) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (tensor<?x4xf32>, tensor<2xindex>) -> tensor<?x4xf32>
    %22 = bufferization.to_memref %21 : memref<?x4xf32>
    %c4_14 = arith.constant 4 : index
    %23 = arith.muli %c4_14, %8 : index
    %reinterpret_cast_15 = memref.reinterpret_cast %22 to offset: [0], sizes: [%8, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32> to memref<?x4xf32>
    %24 = bufferization.to_tensor %reinterpret_cast_15 : memref<?x4xf32>
    %25 = mhlo.multiply %16, %24 {disc.device = "gpu"} : tensor<?x4xf32>
    %26 = bufferization.to_memref %25 : memref<?x4xf32>
    %c4_16 = arith.constant 4 : index
    %27 = arith.muli %c4_16, %8 : index
    %reinterpret_cast_17 = memref.reinterpret_cast %26 to offset: [0], sizes: [%8, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32> to memref<?x4xf32>
    %28 = bufferization.to_tensor %reinterpret_cast_17 : memref<?x4xf32>
    %29 = arith.index_cast %8 : index to i32
    %30 = arith.muli %29, %c4_i32 : i32
    %from_elements_18 = tensor.from_elements %30, %c1_i32 {disc.shape_op = true} : tensor<2xi32>
    %31 = arith.index_cast %30 : i32 to index
    %32 = mhlo.dynamic_reshape %28, %from_elements_18 {disc.device = "gpu"} : (tensor<?x4xf32>, tensor<2xi32>) -> tensor<?x1xf32>
    %33 = bufferization.to_memref %32 : memref<?x1xf32>
    %c1_19 = arith.constant 1 : index
    %34 = arith.muli %c1_19, %31 : index
    %reinterpret_cast_20 = memref.reinterpret_cast %33 to offset: [0], sizes: [%31, 1], strides: [1, 1] {kDiscSymbolicDimAttr = [@S3, @C1]} : memref<?x1xf32> to memref<?x1xf32>
    %35 = bufferization.to_tensor %reinterpret_cast_20 : memref<?x1xf32>
    %36 = mhlo.reduce(%35 init: %4) across dimensions = [0] {disc.device = "gpu"} : (tensor<?x1xf32>, tensor<f32>) -> tensor<1xf32>
     reducer(%arg2: tensor<f32>, %arg3: tensor<f32>)  {
      %41 = mhlo.add %arg2, %arg3 {disc.device = "gpu"} : tensor<f32>
      mhlo.return %41 : tensor<f32>
    }
    %37 = mhlo.reshape %36 {disc.device = "gpu"} : (tensor<1xf32>) -> tensor<f32>
    %38 = bufferization.to_memref %37 : memref<f32>
    %alloc_21 = memref.alloc() : memref<f32>
    "lmhlo_disc.d2h"(%38, %alloc_21) {disc.device = "cpu"} : (memref<f32>, memref<f32>) -> ()
    %39 = bufferization.to_tensor %alloc_21 : memref<f32>
    %40 = bufferization.to_memref %39 : memref<f32>
    return %40 : memref<f32>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = true, sym_name = "C4", value = 4 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = true, sym_name = "C1", value = 1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %c4 = arith.constant 4 : index
    %0 = "disc_shape.dim"() {name = @S2} : () -> index
    %1 = "disc_shape.dim"() {name = @S3} : () -> index
    "disc_shape.tie_product_equal"(%c4, %0, %1) {operand_segment_sizes = array<i32: 2, 1>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After HloLegalizeToLhloPass (hlo-legalize-to-lhlo) //----- //
module {
  func.func @main(%arg0: memref<?x4xf32>, %arg1: memref<?x4xf32>) -> memref<f32> attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
    %0 = bufferization.to_tensor %arg1 : memref<?x4xf32>
    %1 = bufferization.to_tensor %arg0 : memref<?x4xf32>
    %c4 = arith.constant 4 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c4_i32 = arith.constant 4 : i32
    %c1_i32 = arith.constant 1 : i32
    %alloc = memref.alloc() : memref<f32>
    "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0.000000e+00> : tensor<f32>} : (memref<f32>) -> ()
    %dim = tensor.dim %1, %c0 : tensor<?x4xf32>
    %c4_0 = arith.constant 4 : index
    %2 = arith.muli %c4_0, %dim : index
    %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
    %dim_1 = tensor.dim %0, %c0 : tensor<?x4xf32>
    %c4_2 = arith.constant 4 : index
    %3 = arith.muli %c4_2, %dim_1 : index
    %reinterpret_cast_3 = memref.reinterpret_cast %arg1 to offset: [0], sizes: [%dim_1, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
    %4 = arith.cmpi eq, %dim_1, %c1 : index
    %5 = arith.select %4, %dim, %dim_1 : index
    %from_elements = tensor.from_elements %5, %c4 {disc.shape_op = true} : tensor<2xindex>
    %6 = bufferization.to_memref %from_elements : memref<2xindex>
    %7 = bufferization.to_tensor %reinterpret_cast : memref<?x4xf32>
    %8 = shape.shape_of %7 : tensor<?x4xf32> -> tensor<2xindex>
    %c0_4 = arith.constant 0 : index
    %extracted = tensor.extract %8[%c0_4] : tensor<2xindex>
    %alloc_5 = memref.alloc(%extracted) : memref<?x4xf32>
    "lmhlo_disc.h2d"(%reinterpret_cast, %alloc_5) : (memref<?x4xf32>, memref<?x4xf32>) -> ()
    %c4_6 = arith.constant 4 : index
    %9 = arith.muli %c4_6, %dim : index
    %reinterpret_cast_7 = memref.reinterpret_cast %alloc_5 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
    %10 = bufferization.to_tensor %reinterpret_cast_7 : memref<?x4xf32>
    %11 = bufferization.to_memref %10 : memref<?x4xf32>
    %12 = bufferization.to_tensor %11 : memref<?x4xf32>
    %13 = bufferization.to_tensor %6 : memref<2xindex>
    %c0_8 = arith.constant 0 : index
    %extracted_9 = tensor.extract %13[%c0_8] : tensor<2xindex>
    %alloc_10 = memref.alloc(%extracted_9) : memref<?x4xf32>
    "lmhlo.dynamic_broadcast_in_dim"(%11, %6, %alloc_10) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32>, memref<2xindex>, memref<?x4xf32>) -> ()
    %14 = bufferization.to_tensor %alloc_10 : memref<?x4xf32>
    %15 = bufferization.to_memref %14 : memref<?x4xf32>
    %c4_11 = arith.constant 4 : index
    %16 = arith.muli %c4_11, %5 : index
    %reinterpret_cast_12 = memref.reinterpret_cast %15 to offset: [0], sizes: [%5, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32> to memref<?x4xf32>
    %17 = bufferization.to_tensor %reinterpret_cast_12 : memref<?x4xf32>
    %18 = bufferization.to_memref %17 : memref<?x4xf32>
    %19 = bufferization.to_tensor %reinterpret_cast_3 : memref<?x4xf32>
    %20 = shape.shape_of %19 : tensor<?x4xf32> -> tensor<2xindex>
    %c0_13 = arith.constant 0 : index
    %extracted_14 = tensor.extract %20[%c0_13] : tensor<2xindex>
    %alloc_15 = memref.alloc(%extracted_14) : memref<?x4xf32>
    "lmhlo_disc.h2d"(%reinterpret_cast_3, %alloc_15) : (memref<?x4xf32>, memref<?x4xf32>) -> ()
    %c4_16 = arith.constant 4 : index
    %21 = arith.muli %c4_16, %dim_1 : index
    %reinterpret_cast_17 = memref.reinterpret_cast %alloc_15 to offset: [0], sizes: [%dim_1, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
    %22 = bufferization.to_tensor %reinterpret_cast_17 : memref<?x4xf32>
    %23 = bufferization.to_memref %22 : memref<?x4xf32>
    %24 = bufferization.to_tensor %23 : memref<?x4xf32>
    %25 = bufferization.to_tensor %6 : memref<2xindex>
    %c0_18 = arith.constant 0 : index
    %extracted_19 = tensor.extract %25[%c0_18] : tensor<2xindex>
    %alloc_20 = memref.alloc(%extracted_19) : memref<?x4xf32>
    "lmhlo.dynamic_broadcast_in_dim"(%23, %6, %alloc_20) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32>, memref<2xindex>, memref<?x4xf32>) -> ()
    %26 = bufferization.to_tensor %alloc_20 : memref<?x4xf32>
    %27 = bufferization.to_memref %26 : memref<?x4xf32>
    %c4_21 = arith.constant 4 : index
    %28 = arith.muli %c4_21, %5 : index
    %reinterpret_cast_22 = memref.reinterpret_cast %27 to offset: [0], sizes: [%5, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32> to memref<?x4xf32>
    %29 = bufferization.to_tensor %reinterpret_cast_22 : memref<?x4xf32>
    %30 = bufferization.to_memref %29 : memref<?x4xf32>
    %31 = bufferization.to_tensor %18 : memref<?x4xf32>
    %32 = bufferization.to_tensor %30 : memref<?x4xf32>
    %33 = shape.shape_of %31 : tensor<?x4xf32> -> tensor<2xindex>
    %c0_23 = arith.constant 0 : index
    %extracted_24 = tensor.extract %33[%c0_23] : tensor<2xindex>
    %alloc_25 = memref.alloc(%extracted_24) : memref<?x4xf32>
    "lmhlo.multiply"(%18, %30, %alloc_25) {disc.device = "gpu"} : (memref<?x4xf32>, memref<?x4xf32>, memref<?x4xf32>) -> ()
    %34 = bufferization.to_tensor %alloc_25 : memref<?x4xf32>
    %35 = bufferization.to_memref %34 : memref<?x4xf32>
    %c4_26 = arith.constant 4 : index
    %36 = arith.muli %c4_26, %5 : index
    %reinterpret_cast_27 = memref.reinterpret_cast %35 to offset: [0], sizes: [%5, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32> to memref<?x4xf32>
    %37 = bufferization.to_tensor %reinterpret_cast_27 : memref<?x4xf32>
    %38 = bufferization.to_memref %37 : memref<?x4xf32>
    %39 = arith.index_cast %5 : index to i32
    %40 = arith.muli %39, %c4_i32 : i32
    %from_elements_28 = tensor.from_elements %40, %c1_i32 {disc.shape_op = true} : tensor<2xi32>
    %41 = bufferization.to_memref %from_elements_28 : memref<2xi32>
    %42 = arith.index_cast %40 : i32 to index
    %43 = bufferization.to_tensor %38 : memref<?x4xf32>
    %44 = bufferization.to_tensor %41 : memref<2xi32>
    %45 = arith.index_cast %44 : tensor<2xi32> to tensor<2xindex>
    %c0_29 = arith.constant 0 : index
    %extracted_30 = tensor.extract %45[%c0_29] : tensor<2xindex>
    %alloc_31 = memref.alloc(%extracted_30) : memref<?x1xf32>
    "lmhlo.dynamic_reshape"(%38, %41, %alloc_31) {disc.device = "gpu"} : (memref<?x4xf32>, memref<2xi32>, memref<?x1xf32>) -> ()
    %46 = bufferization.to_tensor %alloc_31 : memref<?x1xf32>
    %47 = bufferization.to_memref %46 : memref<?x1xf32>
    %c1_32 = arith.constant 1 : index
    %48 = arith.muli %c1_32, %42 : index
    %reinterpret_cast_33 = memref.reinterpret_cast %47 to offset: [0], sizes: [%42, 1], strides: [1, 1] {kDiscSymbolicDimAttr = [@S3, @C1]} : memref<?x1xf32> to memref<?x1xf32>
    %49 = bufferization.to_tensor %reinterpret_cast_33 : memref<?x1xf32>
    %50 = bufferization.to_memref %49 : memref<?x1xf32>
    %alloc_34 = memref.alloc() : memref<1xf32>
    "lmhlo.reduce"(%50, %alloc, %alloc_34) ({
    ^bb0(%arg2: memref<f32>, %arg3: memref<f32>, %arg4: memref<f32>):
      %alloc_37 = memref.alloc() : memref<f32>
      "lmhlo.add"(%arg2, %arg3, %alloc_37) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
      "lmhlo.copy"(%alloc_37, %arg4) : (memref<f32>, memref<f32>) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x1xf32>, memref<f32>, memref<1xf32>) -> ()
    %alloc_35 = memref.alloc() : memref<f32>
    "lmhlo.reshape"(%alloc_34, %alloc_35) {disc.device = "gpu"} : (memref<1xf32>, memref<f32>) -> ()
    %51 = bufferization.to_tensor %alloc_35 : memref<f32>
    %52 = bufferization.to_memref %51 : memref<f32>
    %alloc_36 = memref.alloc() : memref<f32>
    "lmhlo_disc.d2h"(%52, %alloc_36) {disc.device = "cpu"} : (memref<f32>, memref<f32>) -> ()
    return %alloc_36 : memref<f32>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = true, sym_name = "C4", value = 4 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = true, sym_name = "C1", value = 1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %c4 = arith.constant 4 : index
    %0 = "disc_shape.dim"() {name = @S2} : () -> index
    %1 = "disc_shape.dim"() {name = @S3} : () -> index
    "disc_shape.tie_product_equal"(%c4, %0, %1) {operand_segment_sizes = array<i32: 2, 1>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: memref<?x4xf32>, %arg1: memref<?x4xf32>) -> memref<f32> attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
  %c1_i32 = arith.constant 1 : i32
  %c4_i32 = arith.constant 4 : i32
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c4 = arith.constant 4 : index
  %alloc = memref.alloc() : memref<f32>
  "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0.000000e+00> : tensor<f32>} : (memref<f32>) -> ()
  %dim = memref.dim %arg0, %c0 : memref<?x4xf32>
  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %dim_0 = memref.dim %arg1, %c0 : memref<?x4xf32>
  %reinterpret_cast_1 = memref.reinterpret_cast %arg1 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %0 = arith.cmpi eq, %dim_0, %c1 : index
  %1 = arith.select %0, %dim, %dim_0 : index
  %from_elements = tensor.from_elements %1, %c4 {disc.shape_op = true} : tensor<2xindex>
  %2 = bufferization.to_memref %from_elements : memref<2xindex>
  %alloc_2 = memref.alloc(%dim) : memref<?x4xf32>
  "lmhlo_disc.h2d"(%reinterpret_cast, %alloc_2) : (memref<?x4xf32>, memref<?x4xf32>) -> ()
  %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %3 = bufferization.to_tensor %2 : memref<2xindex>
  %extracted = tensor.extract %3[%c0] : tensor<2xindex>
  %alloc_4 = memref.alloc(%extracted) : memref<?x4xf32>
  "lmhlo.dynamic_broadcast_in_dim"(%reinterpret_cast_3, %2, %alloc_4) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32>, memref<2xindex>, memref<?x4xf32>) -> ()
  %reinterpret_cast_5 = memref.reinterpret_cast %alloc_4 to offset: [0], sizes: [%1, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %alloc_6 = memref.alloc(%dim_0) : memref<?x4xf32>
  "lmhlo_disc.h2d"(%reinterpret_cast_1, %alloc_6) : (memref<?x4xf32>, memref<?x4xf32>) -> ()
  %reinterpret_cast_7 = memref.reinterpret_cast %alloc_6 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %4 = bufferization.to_tensor %2 : memref<2xindex>
  %extracted_8 = tensor.extract %4[%c0] : tensor<2xindex>
  %alloc_9 = memref.alloc(%extracted_8) : memref<?x4xf32>
  "lmhlo.dynamic_broadcast_in_dim"(%reinterpret_cast_7, %2, %alloc_9) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32>, memref<2xindex>, memref<?x4xf32>) -> ()
  %reinterpret_cast_10 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [%1, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %alloc_11 = memref.alloc(%1) : memref<?x4xf32>
  "lmhlo.multiply"(%reinterpret_cast_5, %reinterpret_cast_10, %alloc_11) {disc.device = "gpu"} : (memref<?x4xf32>, memref<?x4xf32>, memref<?x4xf32>) -> ()
  %reinterpret_cast_12 = memref.reinterpret_cast %alloc_11 to offset: [0], sizes: [%1, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %5 = arith.index_cast %1 : index to i32
  %6 = arith.muli %5, %c4_i32 : i32
  %from_elements_13 = tensor.from_elements %6, %c1_i32 {disc.shape_op = true} : tensor<2xi32>
  %7 = bufferization.to_memref %from_elements_13 : memref<2xi32>
  %8 = arith.index_cast %6 : i32 to index
  %9 = bufferization.to_tensor %7 : memref<2xi32>
  %extracted_14 = tensor.extract %9[%c0] : tensor<2xi32>
  %10 = arith.index_cast %extracted_14 : i32 to index
  %alloc_15 = memref.alloc(%10) : memref<?x1xf32>
  "lmhlo.dynamic_reshape"(%reinterpret_cast_12, %7, %alloc_15) {disc.device = "gpu"} : (memref<?x4xf32>, memref<2xi32>, memref<?x1xf32>) -> ()
  %reinterpret_cast_16 = memref.reinterpret_cast %alloc_15 to offset: [0], sizes: [%8, 1], strides: [1, 1] {kDiscSymbolicDimAttr = [@S3, @C1]} : memref<?x1xf32> to memref<?x1xf32>
  %alloc_17 = memref.alloc() : memref<1xf32>
  "lmhlo.reduce"(%reinterpret_cast_16, %alloc, %alloc_17) ({
  ^bb0(%arg2: memref<f32>, %arg3: memref<f32>, %arg4: memref<f32>):
    "lmhlo.add"(%arg2, %arg3, %arg4) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x1xf32>, memref<f32>, memref<1xf32>) -> ()
  %alloc_18 = memref.alloc() : memref<f32>
  "lmhlo.reshape"(%alloc_17, %alloc_18) {disc.device = "gpu"} : (memref<1xf32>, memref<f32>) -> ()
  %alloc_19 = memref.alloc() : memref<f32>
  "lmhlo_disc.d2h"(%alloc_18, %alloc_19) {disc.device = "cpu"} : (memref<f32>, memref<f32>) -> ()
  return %alloc_19 : memref<f32>
}

// -----// IR Dump After LegalizeToTensorOpPass (lhlo-legalize-to-tensor-op) //----- //
func.func @main(%arg0: memref<?x4xf32>, %arg1: memref<?x4xf32>) -> memref<f32> attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
  %c1_i32 = arith.constant 1 : i32
  %c4_i32 = arith.constant 4 : i32
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c4 = arith.constant 4 : index
  %alloc = memref.alloc() : memref<f32>
  "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0.000000e+00> : tensor<f32>} : (memref<f32>) -> ()
  %dim = memref.dim %arg0, %c0 : memref<?x4xf32>
  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %dim_0 = memref.dim %arg1, %c0 : memref<?x4xf32>
  %reinterpret_cast_1 = memref.reinterpret_cast %arg1 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %0 = arith.cmpi eq, %dim_0, %c1 : index
  %1 = arith.select %0, %dim, %dim_0 : index
  %from_elements = tensor.from_elements %1, %c4 {disc.shape_op = true} : tensor<2xindex>
  %2 = bufferization.to_memref %from_elements : memref<2xindex>
  %alloc_2 = memref.alloc(%dim) : memref<?x4xf32>
  "lmhlo_disc.h2d"(%reinterpret_cast, %alloc_2) : (memref<?x4xf32>, memref<?x4xf32>) -> ()
  %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %3 = memref.load %2[%c0] : memref<2xindex>
  %alloc_4 = memref.alloc(%3) : memref<?x4xf32>
  "lmhlo.dynamic_broadcast_in_dim"(%reinterpret_cast_3, %2, %alloc_4) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32>, memref<2xindex>, memref<?x4xf32>) -> ()
  %reinterpret_cast_5 = memref.reinterpret_cast %alloc_4 to offset: [0], sizes: [%1, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %alloc_6 = memref.alloc(%dim_0) : memref<?x4xf32>
  "lmhlo_disc.h2d"(%reinterpret_cast_1, %alloc_6) : (memref<?x4xf32>, memref<?x4xf32>) -> ()
  %reinterpret_cast_7 = memref.reinterpret_cast %alloc_6 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %4 = memref.load %2[%c0] : memref<2xindex>
  %alloc_8 = memref.alloc(%4) : memref<?x4xf32>
  "lmhlo.dynamic_broadcast_in_dim"(%reinterpret_cast_7, %2, %alloc_8) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32>, memref<2xindex>, memref<?x4xf32>) -> ()
  %reinterpret_cast_9 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [%1, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %alloc_10 = memref.alloc(%1) : memref<?x4xf32>
  "lmhlo.multiply"(%reinterpret_cast_5, %reinterpret_cast_9, %alloc_10) {disc.device = "gpu"} : (memref<?x4xf32>, memref<?x4xf32>, memref<?x4xf32>) -> ()
  %reinterpret_cast_11 = memref.reinterpret_cast %alloc_10 to offset: [0], sizes: [%1, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %5 = arith.index_cast %1 : index to i32
  %6 = arith.muli %5, %c4_i32 : i32
  %from_elements_12 = tensor.from_elements %6, %c1_i32 {disc.shape_op = true} : tensor<2xi32>
  %7 = bufferization.to_memref %from_elements_12 : memref<2xi32>
  %8 = arith.index_cast %6 : i32 to index
  %9 = memref.load %7[%c0] : memref<2xi32>
  %10 = arith.index_cast %9 : i32 to index
  %alloc_13 = memref.alloc(%10) : memref<?x1xf32>
  "lmhlo.dynamic_reshape"(%reinterpret_cast_11, %7, %alloc_13) {disc.device = "gpu"} : (memref<?x4xf32>, memref<2xi32>, memref<?x1xf32>) -> ()
  %reinterpret_cast_14 = memref.reinterpret_cast %alloc_13 to offset: [0], sizes: [%8, 1], strides: [1, 1] {kDiscSymbolicDimAttr = [@S3, @C1]} : memref<?x1xf32> to memref<?x1xf32>
  %alloc_15 = memref.alloc() : memref<1xf32>
  "lmhlo.reduce"(%reinterpret_cast_14, %alloc, %alloc_15) ({
  ^bb0(%arg2: memref<f32>, %arg3: memref<f32>, %arg4: memref<f32>):
    "lmhlo.add"(%arg2, %arg3, %arg4) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x1xf32>, memref<f32>, memref<1xf32>) -> ()
  %alloc_16 = memref.alloc() : memref<f32>
  "lmhlo.reshape"(%alloc_15, %alloc_16) {disc.device = "gpu"} : (memref<1xf32>, memref<f32>) -> ()
  %alloc_17 = memref.alloc() : memref<f32>
  "lmhlo_disc.d2h"(%alloc_16, %alloc_17) {disc.device = "cpu"} : (memref<f32>, memref<f32>) -> ()
  return %alloc_17 : memref<f32>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: memref<?x4xf32>, %arg1: memref<?x4xf32>) -> memref<f32> attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
  %c1_i32 = arith.constant 1 : i32
  %c4_i32 = arith.constant 4 : i32
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c4 = arith.constant 4 : index
  %alloc = memref.alloc() : memref<f32>
  "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0.000000e+00> : tensor<f32>} : (memref<f32>) -> ()
  %dim = memref.dim %arg0, %c0 : memref<?x4xf32>
  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %dim_0 = memref.dim %arg1, %c0 : memref<?x4xf32>
  %reinterpret_cast_1 = memref.reinterpret_cast %arg1 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %0 = arith.cmpi eq, %dim_0, %c1 : index
  %1 = arith.select %0, %dim, %dim_0 : index
  %from_elements = tensor.from_elements %1, %c4 {disc.shape_op = true} : tensor<2xindex>
  %2 = bufferization.to_memref %from_elements : memref<2xindex>
  %alloc_2 = memref.alloc(%dim) : memref<?x4xf32>
  "lmhlo_disc.h2d"(%reinterpret_cast, %alloc_2) : (memref<?x4xf32>, memref<?x4xf32>) -> ()
  %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %alloc_4 = memref.alloc(%1) : memref<?x4xf32>
  "lmhlo.dynamic_broadcast_in_dim"(%reinterpret_cast_3, %2, %alloc_4) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32>, memref<2xindex>, memref<?x4xf32>) -> ()
  %reinterpret_cast_5 = memref.reinterpret_cast %alloc_4 to offset: [0], sizes: [%1, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %alloc_6 = memref.alloc(%dim_0) : memref<?x4xf32>
  "lmhlo_disc.h2d"(%reinterpret_cast_1, %alloc_6) : (memref<?x4xf32>, memref<?x4xf32>) -> ()
  %reinterpret_cast_7 = memref.reinterpret_cast %alloc_6 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %alloc_8 = memref.alloc(%1) : memref<?x4xf32>
  "lmhlo.dynamic_broadcast_in_dim"(%reinterpret_cast_7, %2, %alloc_8) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32>, memref<2xindex>, memref<?x4xf32>) -> ()
  %reinterpret_cast_9 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [%1, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %alloc_10 = memref.alloc(%1) : memref<?x4xf32>
  "lmhlo.multiply"(%reinterpret_cast_5, %reinterpret_cast_9, %alloc_10) {disc.device = "gpu"} : (memref<?x4xf32>, memref<?x4xf32>, memref<?x4xf32>) -> ()
  %reinterpret_cast_11 = memref.reinterpret_cast %alloc_10 to offset: [0], sizes: [%1, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %3 = arith.index_cast %1 : index to i32
  %4 = arith.muli %3, %c4_i32 : i32
  %from_elements_12 = tensor.from_elements %4, %c1_i32 {disc.shape_op = true} : tensor<2xi32>
  %5 = bufferization.to_memref %from_elements_12 : memref<2xi32>
  %6 = arith.index_cast %4 : i32 to index
  %7 = arith.index_cast %4 : i32 to index
  %alloc_13 = memref.alloc(%7) : memref<?x1xf32>
  "lmhlo.dynamic_reshape"(%reinterpret_cast_11, %5, %alloc_13) {disc.device = "gpu"} : (memref<?x4xf32>, memref<2xi32>, memref<?x1xf32>) -> ()
  %reinterpret_cast_14 = memref.reinterpret_cast %alloc_13 to offset: [0], sizes: [%6, 1], strides: [1, 1] {kDiscSymbolicDimAttr = [@S3, @C1]} : memref<?x1xf32> to memref<?x1xf32>
  %alloc_15 = memref.alloc() : memref<1xf32>
  "lmhlo.reduce"(%reinterpret_cast_14, %alloc, %alloc_15) ({
  ^bb0(%arg2: memref<f32>, %arg3: memref<f32>, %arg4: memref<f32>):
    "lmhlo.add"(%arg2, %arg3, %arg4) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x1xf32>, memref<f32>, memref<1xf32>) -> ()
  %alloc_16 = memref.alloc() : memref<f32>
  "lmhlo.reshape"(%alloc_15, %alloc_16) {disc.device = "gpu"} : (memref<1xf32>, memref<f32>) -> ()
  %alloc_17 = memref.alloc() : memref<f32>
  "lmhlo_disc.d2h"(%alloc_16, %alloc_17) {disc.device = "cpu"} : (memref<f32>, memref<f32>) -> ()
  return %alloc_17 : memref<f32>
}

// -----// IR Dump After TensorBufferize (tensor-bufferize) //----- //
func.func @main(%arg0: memref<?x4xf32>, %arg1: memref<?x4xf32>) -> memref<f32> attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
  %c1_i32 = arith.constant 1 : i32
  %c4_i32 = arith.constant 4 : i32
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c4 = arith.constant 4 : index
  %alloc = memref.alloc() : memref<f32>
  "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0.000000e+00> : tensor<f32>} : (memref<f32>) -> ()
  %dim = memref.dim %arg0, %c0 : memref<?x4xf32>
  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %dim_0 = memref.dim %arg1, %c0 : memref<?x4xf32>
  %reinterpret_cast_1 = memref.reinterpret_cast %arg1 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %0 = arith.cmpi eq, %dim_0, %c1 : index
  %1 = arith.select %0, %dim, %dim_0 : index
  %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<2xindex>
  %c0_3 = arith.constant 0 : index
  %c1_4 = arith.constant 1 : index
  memref.store %1, %alloc_2[%c0_3] : memref<2xindex>
  memref.store %c4, %alloc_2[%c1_4] : memref<2xindex>
  %alloc_5 = memref.alloc(%dim) : memref<?x4xf32>
  "lmhlo_disc.h2d"(%reinterpret_cast, %alloc_5) : (memref<?x4xf32>, memref<?x4xf32>) -> ()
  %reinterpret_cast_6 = memref.reinterpret_cast %alloc_5 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %alloc_7 = memref.alloc(%1) : memref<?x4xf32>
  "lmhlo.dynamic_broadcast_in_dim"(%reinterpret_cast_6, %alloc_2, %alloc_7) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32>, memref<2xindex>, memref<?x4xf32>) -> ()
  %reinterpret_cast_8 = memref.reinterpret_cast %alloc_7 to offset: [0], sizes: [%1, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %alloc_9 = memref.alloc(%dim_0) : memref<?x4xf32>
  "lmhlo_disc.h2d"(%reinterpret_cast_1, %alloc_9) : (memref<?x4xf32>, memref<?x4xf32>) -> ()
  %reinterpret_cast_10 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %alloc_11 = memref.alloc(%1) : memref<?x4xf32>
  "lmhlo.dynamic_broadcast_in_dim"(%reinterpret_cast_10, %alloc_2, %alloc_11) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32>, memref<2xindex>, memref<?x4xf32>) -> ()
  %reinterpret_cast_12 = memref.reinterpret_cast %alloc_11 to offset: [0], sizes: [%1, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %alloc_13 = memref.alloc(%1) : memref<?x4xf32>
  "lmhlo.multiply"(%reinterpret_cast_8, %reinterpret_cast_12, %alloc_13) {disc.device = "gpu"} : (memref<?x4xf32>, memref<?x4xf32>, memref<?x4xf32>) -> ()
  %reinterpret_cast_14 = memref.reinterpret_cast %alloc_13 to offset: [0], sizes: [%1, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %2 = arith.index_cast %1 : index to i32
  %3 = arith.muli %2, %c4_i32 : i32
  %alloc_15 = memref.alloc() {alignment = 64 : i64} : memref<2xi32>
  %c0_16 = arith.constant 0 : index
  %c1_17 = arith.constant 1 : index
  memref.store %3, %alloc_15[%c0_16] : memref<2xi32>
  memref.store %c1_i32, %alloc_15[%c1_17] : memref<2xi32>
  %4 = arith.index_cast %3 : i32 to index
  %5 = arith.index_cast %3 : i32 to index
  %alloc_18 = memref.alloc(%5) : memref<?x1xf32>
  "lmhlo.dynamic_reshape"(%reinterpret_cast_14, %alloc_15, %alloc_18) {disc.device = "gpu"} : (memref<?x4xf32>, memref<2xi32>, memref<?x1xf32>) -> ()
  %reinterpret_cast_19 = memref.reinterpret_cast %alloc_18 to offset: [0], sizes: [%4, 1], strides: [1, 1] {kDiscSymbolicDimAttr = [@S3, @C1]} : memref<?x1xf32> to memref<?x1xf32>
  %alloc_20 = memref.alloc() : memref<1xf32>
  "lmhlo.reduce"(%reinterpret_cast_19, %alloc, %alloc_20) ({
  ^bb0(%arg2: memref<f32>, %arg3: memref<f32>, %arg4: memref<f32>):
    "lmhlo.add"(%arg2, %arg3, %arg4) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x1xf32>, memref<f32>, memref<1xf32>) -> ()
  %alloc_21 = memref.alloc() : memref<f32>
  "lmhlo.reshape"(%alloc_20, %alloc_21) {disc.device = "gpu"} : (memref<1xf32>, memref<f32>) -> ()
  %alloc_22 = memref.alloc() : memref<f32>
  "lmhlo_disc.d2h"(%alloc_21, %alloc_22) {disc.device = "cpu"} : (memref<f32>, memref<f32>) -> ()
  return %alloc_22 : memref<f32>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: memref<?x4xf32>, %arg1: memref<?x4xf32>) -> memref<f32> attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
  %c1_i32 = arith.constant 1 : i32
  %c4_i32 = arith.constant 4 : i32
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c4 = arith.constant 4 : index
  %alloc = memref.alloc() : memref<f32>
  "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0.000000e+00> : tensor<f32>} : (memref<f32>) -> ()
  %dim = memref.dim %arg0, %c0 : memref<?x4xf32>
  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %dim_0 = memref.dim %arg1, %c0 : memref<?x4xf32>
  %reinterpret_cast_1 = memref.reinterpret_cast %arg1 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %0 = arith.cmpi eq, %dim_0, %c1 : index
  %1 = arith.select %0, %dim, %dim_0 : index
  %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<2xindex>
  memref.store %1, %alloc_2[%c0] : memref<2xindex>
  memref.store %c4, %alloc_2[%c1] : memref<2xindex>
  %alloc_3 = memref.alloc(%dim) : memref<?x4xf32>
  "lmhlo_disc.h2d"(%reinterpret_cast, %alloc_3) : (memref<?x4xf32>, memref<?x4xf32>) -> ()
  %reinterpret_cast_4 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %alloc_5 = memref.alloc(%1) : memref<?x4xf32>
  "lmhlo.dynamic_broadcast_in_dim"(%reinterpret_cast_4, %alloc_2, %alloc_5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32>, memref<2xindex>, memref<?x4xf32>) -> ()
  %reinterpret_cast_6 = memref.reinterpret_cast %alloc_5 to offset: [0], sizes: [%1, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %alloc_7 = memref.alloc(%dim_0) : memref<?x4xf32>
  "lmhlo_disc.h2d"(%reinterpret_cast_1, %alloc_7) : (memref<?x4xf32>, memref<?x4xf32>) -> ()
  %reinterpret_cast_8 = memref.reinterpret_cast %alloc_7 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %alloc_9 = memref.alloc(%1) : memref<?x4xf32>
  "lmhlo.dynamic_broadcast_in_dim"(%reinterpret_cast_8, %alloc_2, %alloc_9) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32>, memref<2xindex>, memref<?x4xf32>) -> ()
  %reinterpret_cast_10 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [%1, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %alloc_11 = memref.alloc(%1) : memref<?x4xf32>
  "lmhlo.multiply"(%reinterpret_cast_6, %reinterpret_cast_10, %alloc_11) {disc.device = "gpu"} : (memref<?x4xf32>, memref<?x4xf32>, memref<?x4xf32>) -> ()
  %reinterpret_cast_12 = memref.reinterpret_cast %alloc_11 to offset: [0], sizes: [%1, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %2 = arith.index_cast %1 : index to i32
  %3 = arith.muli %2, %c4_i32 : i32
  %alloc_13 = memref.alloc() {alignment = 64 : i64} : memref<2xi32>
  memref.store %3, %alloc_13[%c0] : memref<2xi32>
  memref.store %c1_i32, %alloc_13[%c1] : memref<2xi32>
  %4 = arith.index_cast %3 : i32 to index
  %5 = arith.index_cast %3 : i32 to index
  %alloc_14 = memref.alloc(%5) : memref<?x1xf32>
  "lmhlo.dynamic_reshape"(%reinterpret_cast_12, %alloc_13, %alloc_14) {disc.device = "gpu"} : (memref<?x4xf32>, memref<2xi32>, memref<?x1xf32>) -> ()
  %reinterpret_cast_15 = memref.reinterpret_cast %alloc_14 to offset: [0], sizes: [%4, 1], strides: [1, 1] {kDiscSymbolicDimAttr = [@S3, @C1]} : memref<?x1xf32> to memref<?x1xf32>
  %alloc_16 = memref.alloc() : memref<1xf32>
  "lmhlo.reduce"(%reinterpret_cast_15, %alloc, %alloc_16) ({
  ^bb0(%arg2: memref<f32>, %arg3: memref<f32>, %arg4: memref<f32>):
    "lmhlo.add"(%arg2, %arg3, %arg4) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x1xf32>, memref<f32>, memref<1xf32>) -> ()
  %alloc_17 = memref.alloc() : memref<f32>
  "lmhlo.reshape"(%alloc_16, %alloc_17) {disc.device = "gpu"} : (memref<1xf32>, memref<f32>) -> ()
  %alloc_18 = memref.alloc() : memref<f32>
  "lmhlo_disc.d2h"(%alloc_17, %alloc_18) {disc.device = "cpu"} : (memref<f32>, memref<f32>) -> ()
  return %alloc_18 : memref<f32>
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main(%arg0: memref<?x4xf32>, %arg1: memref<?x4xf32>) -> memref<f32> attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
  %c1_i32 = arith.constant 1 : i32
  %c4_i32 = arith.constant 4 : i32
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c4 = arith.constant 4 : index
  %alloc = memref.alloc() : memref<f32>
  "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0.000000e+00> : tensor<f32>} : (memref<f32>) -> ()
  %dim = memref.dim %arg0, %c0 : memref<?x4xf32>
  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %dim_0 = memref.dim %arg1, %c0 : memref<?x4xf32>
  %reinterpret_cast_1 = memref.reinterpret_cast %arg1 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %0 = arith.cmpi eq, %dim_0, %c1 : index
  %1 = arith.select %0, %dim, %dim_0 : index
  %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<2xindex>
  memref.store %1, %alloc_2[%c0] : memref<2xindex>
  memref.store %c4, %alloc_2[%c1] : memref<2xindex>
  %alloc_3 = memref.alloc(%dim) : memref<?x4xf32>
  "lmhlo_disc.h2d"(%reinterpret_cast, %alloc_3) : (memref<?x4xf32>, memref<?x4xf32>) -> ()
  %reinterpret_cast_4 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %alloc_5 = memref.alloc(%1) : memref<?x4xf32>
  "lmhlo.dynamic_broadcast_in_dim"(%reinterpret_cast_4, %alloc_2, %alloc_5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32>, memref<2xindex>, memref<?x4xf32>) -> ()
  %reinterpret_cast_6 = memref.reinterpret_cast %alloc_5 to offset: [0], sizes: [%1, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %alloc_7 = memref.alloc(%dim_0) : memref<?x4xf32>
  "lmhlo_disc.h2d"(%reinterpret_cast_1, %alloc_7) : (memref<?x4xf32>, memref<?x4xf32>) -> ()
  %reinterpret_cast_8 = memref.reinterpret_cast %alloc_7 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %alloc_9 = memref.alloc(%1) : memref<?x4xf32>
  "lmhlo.dynamic_broadcast_in_dim"(%reinterpret_cast_8, %alloc_2, %alloc_9) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32>, memref<2xindex>, memref<?x4xf32>) -> ()
  %reinterpret_cast_10 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [%1, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %alloc_11 = memref.alloc(%1) : memref<?x4xf32>
  "lmhlo.multiply"(%reinterpret_cast_6, %reinterpret_cast_10, %alloc_11) {disc.device = "gpu"} : (memref<?x4xf32>, memref<?x4xf32>, memref<?x4xf32>) -> ()
  %reinterpret_cast_12 = memref.reinterpret_cast %alloc_11 to offset: [0], sizes: [%1, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %2 = arith.index_cast %1 : index to i32
  %3 = arith.muli %2, %c4_i32 : i32
  %alloc_13 = memref.alloc() {alignment = 64 : i64} : memref<2xi32>
  memref.store %3, %alloc_13[%c0] : memref<2xi32>
  memref.store %c1_i32, %alloc_13[%c1] : memref<2xi32>
  %4 = arith.index_cast %3 : i32 to index
  %alloc_14 = memref.alloc(%4) : memref<?x1xf32>
  "lmhlo.dynamic_reshape"(%reinterpret_cast_12, %alloc_13, %alloc_14) {disc.device = "gpu"} : (memref<?x4xf32>, memref<2xi32>, memref<?x1xf32>) -> ()
  %reinterpret_cast_15 = memref.reinterpret_cast %alloc_14 to offset: [0], sizes: [%4, 1], strides: [1, 1] {kDiscSymbolicDimAttr = [@S3, @C1]} : memref<?x1xf32> to memref<?x1xf32>
  %alloc_16 = memref.alloc() : memref<1xf32>
  "lmhlo.reduce"(%reinterpret_cast_15, %alloc, %alloc_16) ({
  ^bb0(%arg2: memref<f32>, %arg3: memref<f32>, %arg4: memref<f32>):
    "lmhlo.add"(%arg2, %arg3, %arg4) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x1xf32>, memref<f32>, memref<1xf32>) -> ()
  %alloc_17 = memref.alloc() : memref<f32>
  "lmhlo.reshape"(%alloc_16, %alloc_17) {disc.device = "gpu"} : (memref<1xf32>, memref<f32>) -> ()
  %alloc_18 = memref.alloc() : memref<f32>
  "lmhlo_disc.d2h"(%alloc_17, %alloc_18) {disc.device = "cpu"} : (memref<f32>, memref<f32>) -> ()
  return %alloc_18 : memref<f32>
}

// -----// IR Dump After DiscMemrefCanonicalizer (disc-memref-canonicalize) //----- //
func.func @main(%arg0: memref<?x4xf32>, %arg1: memref<?x4xf32>) -> memref<f32> attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
  %c1_i32 = arith.constant 1 : i32
  %c4_i32 = arith.constant 4 : i32
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c4 = arith.constant 4 : index
  %alloc = memref.alloc() : memref<f32>
  "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0.000000e+00> : tensor<f32>} : (memref<f32>) -> ()
  %dim = memref.dim %arg0, %c0 : memref<?x4xf32>
  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %dim_0 = memref.dim %arg1, %c0 : memref<?x4xf32>
  %reinterpret_cast_1 = memref.reinterpret_cast %arg1 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %0 = arith.cmpi eq, %dim_0, %c1 : index
  %1 = arith.select %0, %dim, %dim_0 : index
  %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<2xindex>
  memref.store %1, %alloc_2[%c0] : memref<2xindex>
  memref.store %c4, %alloc_2[%c1] : memref<2xindex>
  %alloc_3 = memref.alloc(%dim) {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32>
  "lmhlo_disc.h2d"(%reinterpret_cast, %alloc_3) : (memref<?x4xf32>, memref<?x4xf32>) -> ()
  %alloc_4 = memref.alloc(%1) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32>
  "lmhlo.dynamic_broadcast_in_dim"(%alloc_3, %alloc_2, %alloc_4) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32>, memref<2xindex>, memref<?x4xf32>) -> ()
  %alloc_5 = memref.alloc(%dim_0) {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32>
  "lmhlo_disc.h2d"(%reinterpret_cast_1, %alloc_5) : (memref<?x4xf32>, memref<?x4xf32>) -> ()
  %alloc_6 = memref.alloc(%1) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32>
  "lmhlo.dynamic_broadcast_in_dim"(%alloc_5, %alloc_2, %alloc_6) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32>, memref<2xindex>, memref<?x4xf32>) -> ()
  %alloc_7 = memref.alloc(%1) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32>
  "lmhlo.multiply"(%alloc_4, %alloc_6, %alloc_7) {disc.device = "gpu"} : (memref<?x4xf32>, memref<?x4xf32>, memref<?x4xf32>) -> ()
  %2 = arith.index_cast %1 : index to i32
  %3 = arith.muli %2, %c4_i32 : i32
  %alloc_8 = memref.alloc() {alignment = 64 : i64} : memref<2xi32>
  memref.store %3, %alloc_8[%c0] : memref<2xi32>
  memref.store %c1_i32, %alloc_8[%c1] : memref<2xi32>
  %4 = arith.index_cast %3 : i32 to index
  %alloc_9 = memref.alloc(%4) {kDiscSymbolicDimAttr = [@S3, @C1]} : memref<?x1xf32>
  "lmhlo.dynamic_reshape"(%alloc_7, %alloc_8, %alloc_9) {disc.device = "gpu"} : (memref<?x4xf32>, memref<2xi32>, memref<?x1xf32>) -> ()
  %alloc_10 = memref.alloc() : memref<1xf32>
  "lmhlo.reduce"(%alloc_9, %alloc, %alloc_10) ({
  ^bb0(%arg2: memref<f32>, %arg3: memref<f32>, %arg4: memref<f32>):
    "lmhlo.add"(%arg2, %arg3, %arg4) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x1xf32>, memref<f32>, memref<1xf32>) -> ()
  %alloc_11 = memref.alloc() : memref<f32>
  "lmhlo.reshape"(%alloc_10, %alloc_11) {disc.device = "gpu"} : (memref<1xf32>, memref<f32>) -> ()
  %alloc_12 = memref.alloc() : memref<f32>
  "lmhlo_disc.d2h"(%alloc_11, %alloc_12) {disc.device = "cpu"} : (memref<f32>, memref<f32>) -> ()
  return %alloc_12 : memref<f32>
}

// -----// IR Dump After DiscAssignMemorySpacePass (disc-assign-memory-space) //----- //
module {
  func.func @main(%arg0: memref<?x4xf32>, %arg1: memref<?x4xf32>) -> memref<f32> attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
    %c1_i32 = arith.constant 1 : i32
    %c4_i32 = arith.constant 4 : i32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %c4 = arith.constant 4 : index
    %alloc = memref.alloc() : memref<f32, #gpu.address_space<global>>
    "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0.000000e+00> : tensor<f32>} : (memref<f32, #gpu.address_space<global>>) -> ()
    %dim = memref.dim %arg0, %c0 : memref<?x4xf32>
    %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
    %dim_0 = memref.dim %arg1, %c0 : memref<?x4xf32>
    %reinterpret_cast_1 = memref.reinterpret_cast %arg1 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
    %0 = arith.cmpi eq, %dim_0, %c1 : index
    %1 = arith.select %0, %dim, %dim_0 : index
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<2xindex>
    memref.store %1, %alloc_2[%c0] : memref<2xindex>
    memref.store %c4, %alloc_2[%c1] : memref<2xindex>
    %alloc_3 = memref.alloc(%dim) {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
    "lmhlo_disc.h2d"(%reinterpret_cast, %alloc_3) : (memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
    %alloc_4 = memref.alloc(%1) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
    "lmhlo.dynamic_broadcast_in_dim"(%alloc_3, %alloc_2, %alloc_4) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xindex>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
    %alloc_5 = memref.alloc(%dim_0) {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
    "lmhlo_disc.h2d"(%reinterpret_cast_1, %alloc_5) : (memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
    %alloc_6 = memref.alloc(%1) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
    "lmhlo.dynamic_broadcast_in_dim"(%alloc_5, %alloc_2, %alloc_6) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xindex>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
    %alloc_7 = memref.alloc(%1) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
    "lmhlo.multiply"(%alloc_4, %alloc_6, %alloc_7) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
    %2 = arith.index_cast %1 : index to i32
    %3 = arith.muli %2, %c4_i32 : i32
    %alloc_8 = memref.alloc() {alignment = 64 : i64} : memref<2xi32>
    memref.store %3, %alloc_8[%c0] : memref<2xi32>
    memref.store %c1_i32, %alloc_8[%c1] : memref<2xi32>
    %4 = arith.index_cast %3 : i32 to index
    %alloc_9 = memref.alloc(%4) {kDiscSymbolicDimAttr = [@S3, @C1]} : memref<?x1xf32, #gpu.address_space<global>>
    "lmhlo.dynamic_reshape"(%alloc_7, %alloc_8, %alloc_9) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xi32>, memref<?x1xf32, #gpu.address_space<global>>) -> ()
    %alloc_10 = memref.alloc() : memref<1xf32, #gpu.address_space<global>>
    "lmhlo.reduce"(%alloc_9, %alloc, %alloc_10) ({
    ^bb0(%arg2: memref<f32>, %arg3: memref<f32>, %arg4: memref<f32>):
      "lmhlo.add"(%arg2, %arg3, %arg4) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x1xf32, #gpu.address_space<global>>, memref<f32, #gpu.address_space<global>>, memref<1xf32, #gpu.address_space<global>>) -> ()
    %alloc_11 = memref.alloc() : memref<f32, #gpu.address_space<global>>
    "lmhlo.reshape"(%alloc_10, %alloc_11) {disc.device = "gpu"} : (memref<1xf32, #gpu.address_space<global>>, memref<f32, #gpu.address_space<global>>) -> ()
    %alloc_12 = memref.alloc() : memref<f32>
    "lmhlo_disc.d2h"(%alloc_11, %alloc_12) {disc.device = "cpu"} : (memref<f32, #gpu.address_space<global>>, memref<f32>) -> ()
    return %alloc_12 : memref<f32>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = true, sym_name = "C4", value = 4 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = true, sym_name = "C1", value = 1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %c4 = arith.constant 4 : index
    %0 = "disc_shape.dim"() {name = @S2} : () -> index
    %1 = "disc_shape.dim"() {name = @S3} : () -> index
    "disc_shape.tie_product_equal"(%c4, %0, %1) {operand_segment_sizes = array<i32: 2, 1>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After PromoteBuffersToStack (promote-buffers-to-stack) //----- //
func.func @main(%arg0: memref<?x4xf32>, %arg1: memref<?x4xf32>) -> memref<f32> attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
  %c1_i32 = arith.constant 1 : i32
  %c4_i32 = arith.constant 4 : i32
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c4 = arith.constant 4 : index
  %alloc = memref.alloc() : memref<f32, #gpu.address_space<global>>
  "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0.000000e+00> : tensor<f32>} : (memref<f32, #gpu.address_space<global>>) -> ()
  %dim = memref.dim %arg0, %c0 : memref<?x4xf32>
  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %dim_0 = memref.dim %arg1, %c0 : memref<?x4xf32>
  %reinterpret_cast_1 = memref.reinterpret_cast %arg1 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %0 = arith.cmpi eq, %dim_0, %c1 : index
  %1 = arith.select %0, %dim, %dim_0 : index
  %alloca = memref.alloca() {alignment = 64 : i64} : memref<2xindex>
  memref.store %1, %alloca[%c0] : memref<2xindex>
  memref.store %c4, %alloca[%c1] : memref<2xindex>
  %alloc_2 = memref.alloc(%dim) {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  "lmhlo_disc.h2d"(%reinterpret_cast, %alloc_2) : (memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  %alloc_3 = memref.alloc(%1) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  "lmhlo.dynamic_broadcast_in_dim"(%alloc_2, %alloca, %alloc_3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xindex>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  %alloc_4 = memref.alloc(%dim_0) {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  "lmhlo_disc.h2d"(%reinterpret_cast_1, %alloc_4) : (memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  %alloc_5 = memref.alloc(%1) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  "lmhlo.dynamic_broadcast_in_dim"(%alloc_4, %alloca, %alloc_5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xindex>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  %alloc_6 = memref.alloc(%1) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  "lmhlo.multiply"(%alloc_3, %alloc_5, %alloc_6) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  %2 = arith.index_cast %1 : index to i32
  %3 = arith.muli %2, %c4_i32 : i32
  %alloca_7 = memref.alloca() {alignment = 64 : i64} : memref<2xi32>
  memref.store %3, %alloca_7[%c0] : memref<2xi32>
  memref.store %c1_i32, %alloca_7[%c1] : memref<2xi32>
  %4 = arith.index_cast %3 : i32 to index
  %alloc_8 = memref.alloc(%4) {kDiscSymbolicDimAttr = [@S3, @C1]} : memref<?x1xf32, #gpu.address_space<global>>
  "lmhlo.dynamic_reshape"(%alloc_6, %alloca_7, %alloc_8) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xi32>, memref<?x1xf32, #gpu.address_space<global>>) -> ()
  %alloc_9 = memref.alloc() : memref<1xf32, #gpu.address_space<global>>
  "lmhlo.reduce"(%alloc_8, %alloc, %alloc_9) ({
  ^bb0(%arg2: memref<f32>, %arg3: memref<f32>, %arg4: memref<f32>):
    "lmhlo.add"(%arg2, %arg3, %arg4) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x1xf32, #gpu.address_space<global>>, memref<f32, #gpu.address_space<global>>, memref<1xf32, #gpu.address_space<global>>) -> ()
  %alloc_10 = memref.alloc() : memref<f32, #gpu.address_space<global>>
  "lmhlo.reshape"(%alloc_9, %alloc_10) {disc.device = "gpu"} : (memref<1xf32, #gpu.address_space<global>>, memref<f32, #gpu.address_space<global>>) -> ()
  %alloc_11 = memref.alloc() : memref<f32>
  "lmhlo_disc.d2h"(%alloc_10, %alloc_11) {disc.device = "cpu"} : (memref<f32, #gpu.address_space<global>>, memref<f32>) -> ()
  return %alloc_11 : memref<f32>
}

SymbolicDimMgr::save walkRankedTensorValue takes: 2 us
SymbolicDimMgr::save update attributes takes: 10 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 6 us
productSet.size() = 2
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 4 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 10 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 0 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 0 us
SymbolicDimMgr::save updateProductEqualityMap takes: 43 us
SymbolicDimMgr::save updateFunctionType takes: 2 us
SymbolicDimMgr::save collect symbolicDim ops takes: 8 us
SymbolicDimMgr::save remove symbolicDim ops takes: 0 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 0 us
SymbolicDimMgr::save canonicalize the name takes: 9 us
SymbolicDimMgr::save replace the name takes: 7 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
// -----// IR Dump After DiscFusionPass (disc-fusion) //----- //
func.func @main(%arg0: memref<?x4xf32>, %arg1: memref<?x4xf32>) -> memref<f32> attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
  %c1_i32 = arith.constant 1 : i32
  %c4_i32 = arith.constant 4 : i32
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %dim = memref.dim %arg1, %c0 : memref<?x4xf32>
  %dim_0 = memref.dim %arg0, %c0 : memref<?x4xf32>
  %c4 = arith.constant 4 : index
  %alloc = memref.alloc() : memref<f32, #gpu.address_space<global>>
  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %reinterpret_cast_1 = memref.reinterpret_cast %arg1 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %0 = arith.cmpi eq, %dim, %c1 : index
  %1 = arith.select %0, %dim_0, %dim : index
  %alloca = memref.alloca() {alignment = 64 : i64} : memref<2xindex>
  memref.store %1, %alloca[%c0] : memref<2xindex>
  memref.store %c4, %alloca[%c1] : memref<2xindex>
  %alloc_2 = memref.alloc(%dim_0) {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  "lmhlo_disc.h2d"(%reinterpret_cast, %alloc_2) : (memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  %alloc_3 = memref.alloc(%1) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %alloc_4 = memref.alloc(%dim) {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  "lmhlo_disc.h2d"(%reinterpret_cast_1, %alloc_4) : (memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  %alloc_5 = memref.alloc(%1) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %alloc_6 = memref.alloc(%1) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %2 = arith.index_cast %1 : index to i32
  %3 = arith.muli %2, %c4_i32 : i32
  %alloca_7 = memref.alloca() {alignment = 64 : i64} : memref<2xi32>
  memref.store %3, %alloca_7[%c0] : memref<2xi32>
  memref.store %c1_i32, %alloca_7[%c1] : memref<2xi32>
  %4 = arith.index_cast %3 : i32 to index
  %alloc_8 = memref.alloc(%4) {kDiscSymbolicDimAttr = [@S3, @C1]} : memref<?x1xf32, #gpu.address_space<global>>
  %alloc_9 = memref.alloc() : memref<1xf32, #gpu.address_space<global>>
  "lmhlo.fusion"() ({
    "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0.000000e+00> : tensor<f32>} : (memref<f32, #gpu.address_space<global>>) -> ()
    "lmhlo.dynamic_broadcast_in_dim"(%alloc_2, %alloca, %alloc_3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xindex>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
    "lmhlo.dynamic_broadcast_in_dim"(%alloc_4, %alloca, %alloc_5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xindex>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
    "lmhlo.multiply"(%alloc_3, %alloc_5, %alloc_6) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
    "lmhlo.dynamic_reshape"(%alloc_6, %alloca_7, %alloc_8) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xi32>, memref<?x1xf32, #gpu.address_space<global>>) -> ()
    "lmhlo.reduce"(%alloc_8, %alloc, %alloc_9) ({
    ^bb0(%arg2: memref<f32>, %arg3: memref<f32>, %arg4: memref<f32>):
      "lmhlo.add"(%arg2, %arg3, %arg4) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x1xf32, #gpu.address_space<global>>, memref<f32, #gpu.address_space<global>>, memref<1xf32, #gpu.address_space<global>>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion_type = "kColReduction"} : () -> ()
  %alloc_10 = memref.alloc() : memref<f32, #gpu.address_space<global>>
  "lmhlo.reshape"(%alloc_9, %alloc_10) {disc.device = "gpu"} : (memref<1xf32, #gpu.address_space<global>>, memref<f32, #gpu.address_space<global>>) -> ()
  %alloc_11 = memref.alloc() : memref<f32>
  "lmhlo_disc.d2h"(%alloc_10, %alloc_11) {disc.device = "cpu"} : (memref<f32, #gpu.address_space<global>>, memref<f32>) -> ()
  return %alloc_11 : memref<f32>
}

SymbolicDimMgr::save walkRankedTensorValue takes: 4 us
SymbolicDimMgr::save update attributes takes: 17 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 10 us
productSet.size() = 4
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 6 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 25 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 1 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 1 us
SymbolicDimMgr::save updateProductEqualityMap takes: 63 us
SymbolicDimMgr::save updateFunctionType takes: 3 us
SymbolicDimMgr::save collect symbolicDim ops takes: 15 us
SymbolicDimMgr::save remove symbolicDim ops takes: 0 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 1 us
SymbolicDimMgr::save canonicalize the name takes: 10 us
SymbolicDimMgr::save replace the name takes: 17 us
SymbolicDimMgr::save updateFunctionType takes: 2 us
// -----// IR Dump After DiscSpecializeFusionWithSpeculationPass (disc-specialize-fusion-with-speculation) //----- //
func.func @main(%arg0: memref<?x4xf32>, %arg1: memref<?x4xf32>) -> memref<f32> attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
  %c1_i32 = arith.constant 1 : i32
  %c4_i32 = arith.constant 4 : i32
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %dim = memref.dim %arg1, %c0 : memref<?x4xf32>
  %dim_0 = memref.dim %arg0, %c0 : memref<?x4xf32>
  %c4 = arith.constant 4 : index
  %alloc = memref.alloc() : memref<f32, #gpu.address_space<global>>
  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %reinterpret_cast_1 = memref.reinterpret_cast %arg1 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %0 = arith.cmpi eq, %dim, %c1 : index
  %1 = arith.select %0, %dim_0, %dim : index
  %alloca = memref.alloca() {alignment = 64 : i64} : memref<2xindex>
  memref.store %1, %alloca[%c0] : memref<2xindex>
  memref.store %c4, %alloca[%c1] : memref<2xindex>
  %alloc_2 = memref.alloc(%dim_0) {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  "lmhlo_disc.h2d"(%reinterpret_cast, %alloc_2) : (memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  %alloc_3 = memref.alloc(%1) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %alloc_4 = memref.alloc(%dim) {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  "lmhlo_disc.h2d"(%reinterpret_cast_1, %alloc_4) : (memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  %alloc_5 = memref.alloc(%1) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %alloc_6 = memref.alloc(%1) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %2 = arith.index_cast %1 : index to i32
  %3 = arith.muli %2, %c4_i32 : i32
  %alloca_7 = memref.alloca() {alignment = 64 : i64} : memref<2xi32>
  memref.store %3, %alloca_7[%c0] : memref<2xi32>
  memref.store %c1_i32, %alloca_7[%c1] : memref<2xi32>
  %4 = arith.index_cast %3 : i32 to index
  %alloc_8 = memref.alloc(%4) {kDiscSymbolicDimAttr = [@S3, @C1]} : memref<?x1xf32, #gpu.address_space<global>>
  %alloc_9 = memref.alloc() : memref<1xf32, #gpu.address_space<global>>
  %c0_10 = arith.constant 0 : index
  %dim_11 = memref.dim %alloc_2, %c0_10 : memref<?x4xf32, #gpu.address_space<global>>
  %c4_12 = arith.constant 4 : index
  %c4_13 = arith.constant 4 : index
  %5 = arith.muli %c4_13, %dim_11 : index
  %reinterpret_cast_14 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [%dim_11, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S4, @C4]} : memref<?x4xf32, #gpu.address_space<global>> to memref<?x4xf32, #gpu.address_space<global>>
  %c0_15 = arith.constant 0 : index
  %dim_16 = memref.dim %alloc_3, %c0_15 : memref<?x4xf32, #gpu.address_space<global>>
  %c4_17 = arith.constant 4 : index
  %c4_18 = arith.constant 4 : index
  %6 = arith.muli %c4_18, %dim_16 : index
  %reinterpret_cast_19 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [%dim_16, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S4, @C4]} : memref<?x4xf32, #gpu.address_space<global>> to memref<?x4xf32, #gpu.address_space<global>>
  %c0_20 = arith.constant 0 : index
  %dim_21 = memref.dim %alloc_4, %c0_20 : memref<?x4xf32, #gpu.address_space<global>>
  %c4_22 = arith.constant 4 : index
  %c4_23 = arith.constant 4 : index
  %7 = arith.muli %c4_23, %dim_21 : index
  %reinterpret_cast_24 = memref.reinterpret_cast %alloc_4 to offset: [0], sizes: [%dim_21, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S4, @C4]} : memref<?x4xf32, #gpu.address_space<global>> to memref<?x4xf32, #gpu.address_space<global>>
  %c0_25 = arith.constant 0 : index
  %dim_26 = memref.dim %alloc_5, %c0_25 : memref<?x4xf32, #gpu.address_space<global>>
  %c4_27 = arith.constant 4 : index
  %c4_28 = arith.constant 4 : index
  %8 = arith.muli %c4_28, %dim_26 : index
  %reinterpret_cast_29 = memref.reinterpret_cast %alloc_5 to offset: [0], sizes: [%dim_26, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S4, @C4]} : memref<?x4xf32, #gpu.address_space<global>> to memref<?x4xf32, #gpu.address_space<global>>
  %c0_30 = arith.constant 0 : index
  %dim_31 = memref.dim %alloc_6, %c0_30 : memref<?x4xf32, #gpu.address_space<global>>
  %c4_32 = arith.constant 4 : index
  %c4_33 = arith.constant 4 : index
  %9 = arith.muli %c4_33, %dim_31 : index
  %reinterpret_cast_34 = memref.reinterpret_cast %alloc_6 to offset: [0], sizes: [%dim_31, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S4, @C4]} : memref<?x4xf32, #gpu.address_space<global>> to memref<?x4xf32, #gpu.address_space<global>>
  %c0_35 = arith.constant 0 : index
  %dim_36 = memref.dim %alloc_8, %c0_35 : memref<?x1xf32, #gpu.address_space<global>>
  %c1_37 = arith.constant 1 : index
  %c1_38 = arith.constant 1 : index
  %10 = arith.muli %c1_38, %dim_36 : index
  %reinterpret_cast_39 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [%dim_36, 1], strides: [1, 1] {kDiscSymbolicDimAttr = [@S5, @C1]} : memref<?x1xf32, #gpu.address_space<global>> to memref<?x1xf32, #gpu.address_space<global>>
  %c0_40 = arith.constant 0 : index
  %dim_41 = memref.dim %reinterpret_cast_14, %c0_40 : memref<?x4xf32, #gpu.address_space<global>>
  %c0_42 = arith.constant 0 : index
  %dim_43 = memref.dim %reinterpret_cast_19, %c0_42 : memref<?x4xf32, #gpu.address_space<global>>
  %11 = arith.cmpi eq, %dim_41, %dim_43 : index
  %c1_44 = arith.constant 1 : index
  %dim_45 = memref.dim %reinterpret_cast_14, %c1_44 : memref<?x4xf32, #gpu.address_space<global>>
  %c1_46 = arith.constant 1 : index
  %dim_47 = memref.dim %reinterpret_cast_19, %c1_46 : memref<?x4xf32, #gpu.address_space<global>>
  %12 = arith.cmpi eq, %dim_45, %dim_47 : index
  %13 = arith.andi %12, %11 : i1
  %c0_48 = arith.constant 0 : index
  %dim_49 = memref.dim %reinterpret_cast_24, %c0_48 : memref<?x4xf32, #gpu.address_space<global>>
  %c0_50 = arith.constant 0 : index
  %dim_51 = memref.dim %reinterpret_cast_29, %c0_50 : memref<?x4xf32, #gpu.address_space<global>>
  %14 = arith.cmpi eq, %dim_49, %dim_51 : index
  %15 = arith.andi %14, %13 : i1
  %c1_52 = arith.constant 1 : index
  %dim_53 = memref.dim %reinterpret_cast_24, %c1_52 : memref<?x4xf32, #gpu.address_space<global>>
  %c1_54 = arith.constant 1 : index
  %dim_55 = memref.dim %reinterpret_cast_29, %c1_54 : memref<?x4xf32, #gpu.address_space<global>>
  %16 = arith.cmpi eq, %dim_53, %dim_55 : index
  %17 = arith.andi %16, %15 : i1
  scf.if %17 {
    %c4_58 = arith.constant 4 : index
    %c4_59 = arith.constant 4 : index
    %c0_60 = arith.constant 0 : index
    %dim_61 = memref.dim %reinterpret_cast_34, %c0_60 : memref<?x4xf32, #gpu.address_space<global>>
    %c4_62 = arith.constant 4 : index
    %c0_63 = arith.constant 0 : index
    %dim_64 = memref.dim %reinterpret_cast_34, %c0_63 : memref<?x4xf32, #gpu.address_space<global>>
    %c4_65 = arith.constant 4 : index
    %c0_66 = arith.constant 0 : index
    %dim_67 = memref.dim %reinterpret_cast_39, %c0_66 : memref<?x1xf32, #gpu.address_space<global>>
    %c1_68 = arith.constant 1 : index
    %c0_69 = arith.constant 0 : index
    %dim_70 = memref.dim %reinterpret_cast_39, %c0_69 : memref<?x1xf32, #gpu.address_space<global>>
    %c1_71 = arith.constant 1 : index
    %c4_72 = arith.constant 4 : index
    %18 = arith.muli %c4_72, %dim_64 : index
    %reinterpret_cast_73 = memref.reinterpret_cast %reinterpret_cast_14 to offset: [0], sizes: [%dim_64, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S4, @C4]} : memref<?x4xf32, #gpu.address_space<global>> to memref<?x4xf32, #gpu.address_space<global>>
    %c4_74 = arith.constant 4 : index
    %19 = arith.muli %c4_74, %dim_64 : index
    %reinterpret_cast_75 = memref.reinterpret_cast %reinterpret_cast_24 to offset: [0], sizes: [%dim_64, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S4, @C4]} : memref<?x4xf32, #gpu.address_space<global>> to memref<?x4xf32, #gpu.address_space<global>>
    %c4_76 = arith.constant 4 : index
    %20 = arith.muli %c4_76, %dim_64 : index
    %reinterpret_cast_77 = memref.reinterpret_cast %reinterpret_cast_34 to offset: [0], sizes: [%dim_64, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S4, @C4]} : memref<?x4xf32, #gpu.address_space<global>> to memref<?x4xf32, #gpu.address_space<global>>
    %c1_78 = arith.constant 1 : index
    %21 = arith.muli %c1_78, %dim_70 : index
    %reinterpret_cast_79 = memref.reinterpret_cast %reinterpret_cast_39 to offset: [0], sizes: [%dim_70, 1], strides: [1, 1] {kDiscSymbolicDimAttr = [@S5, @C1]} : memref<?x1xf32, #gpu.address_space<global>> to memref<?x1xf32, #gpu.address_space<global>>
    %c0_80 = arith.constant 0 : index
    %dim_81 = memref.dim %reinterpret_cast_79, %c0_80 : memref<?x1xf32, #gpu.address_space<global>>
    %c1_82 = arith.constant 1 : index
    %dim_83 = memref.dim %reinterpret_cast_79, %c1_82 : memref<?x1xf32, #gpu.address_space<global>>
    %22 = arith.muli %dim_81, %dim_83 : index
    %c256 = arith.constant 256 : index
    %23 = arith.ceildivsi %22, %c256 : index
    %c72 = arith.constant 72 : index
    %24 = arith.cmpi slt, %dim_81, %dim_83 : index
    scf.if %24 {
      "lmhlo.fusion"() ({
        "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0.000000e+00> : tensor<f32>} : (memref<f32, #gpu.address_space<global>>) -> ()
        "lmhlo.multiply"(%reinterpret_cast_73, %reinterpret_cast_75, %reinterpret_cast_77) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_reshape"(%reinterpret_cast_77, %alloca_7, %reinterpret_cast_79) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xi32>, memref<?x1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.reduce"(%reinterpret_cast_79, %alloc, %alloc_9) ({
        ^bb0(%arg2: memref<f32>, %arg3: memref<f32>, %arg4: memref<f32>):
          "lmhlo.add"(%arg2, %arg3, %arg4) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
          "lmhlo.terminator"() : () -> ()
        }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x1xf32, #gpu.address_space<global>>, memref<f32, #gpu.address_space<global>>, memref<1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXthread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0.000000e+00> : tensor<f32>} : (memref<f32, #gpu.address_space<global>>) -> ()
        "lmhlo.multiply"(%reinterpret_cast_73, %reinterpret_cast_75, %reinterpret_cast_77) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_reshape"(%reinterpret_cast_77, %alloca_7, %reinterpret_cast_79) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xi32>, memref<?x1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.reduce"(%reinterpret_cast_79, %alloc, %alloc_9) ({
        ^bb0(%arg2: memref<f32>, %arg3: memref<f32>, %arg4: memref<f32>):
          "lmhlo.add"(%arg2, %arg3, %arg4) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
          "lmhlo.terminator"() : () -> ()
        }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x1xf32, #gpu.address_space<global>>, memref<f32, #gpu.address_space<global>>, memref<1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXblock_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
    }
  } else {
    %c0_58 = arith.constant 0 : index
    %dim_59 = memref.dim %alloc_8, %c0_58 : memref<?x1xf32, #gpu.address_space<global>>
    %c1_60 = arith.constant 1 : index
    %dim_61 = memref.dim %alloc_8, %c1_60 : memref<?x1xf32, #gpu.address_space<global>>
    %18 = arith.muli %dim_59, %dim_61 : index
    %c256 = arith.constant 256 : index
    %19 = arith.ceildivsi %18, %c256 : index
    %c72 = arith.constant 72 : index
    %20 = arith.cmpi slt, %dim_59, %dim_61 : index
    scf.if %20 {
      "lmhlo.fusion"() ({
        "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0.000000e+00> : tensor<f32>} : (memref<f32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_broadcast_in_dim"(%alloc_2, %alloca, %alloc_3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xindex>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_broadcast_in_dim"(%alloc_4, %alloca, %alloc_5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xindex>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.multiply"(%alloc_3, %alloc_5, %alloc_6) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_reshape"(%alloc_6, %alloca_7, %alloc_8) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xi32>, memref<?x1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.reduce"(%alloc_8, %alloc, %alloc_9) ({
        ^bb0(%arg2: memref<f32>, %arg3: memref<f32>, %arg4: memref<f32>):
          "lmhlo.add"(%arg2, %arg3, %arg4) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
          "lmhlo.terminator"() : () -> ()
        }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x1xf32, #gpu.address_space<global>>, memref<f32, #gpu.address_space<global>>, memref<1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "thread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0.000000e+00> : tensor<f32>} : (memref<f32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_broadcast_in_dim"(%alloc_2, %alloca, %alloc_3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xindex>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_broadcast_in_dim"(%alloc_4, %alloca, %alloc_5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xindex>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.multiply"(%alloc_3, %alloc_5, %alloc_6) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_reshape"(%alloc_6, %alloca_7, %alloc_8) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xi32>, memref<?x1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.reduce"(%alloc_8, %alloc, %alloc_9) ({
        ^bb0(%arg2: memref<f32>, %arg3: memref<f32>, %arg4: memref<f32>):
          "lmhlo.add"(%arg2, %arg3, %arg4) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
          "lmhlo.terminator"() : () -> ()
        }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x1xf32, #gpu.address_space<global>>, memref<f32, #gpu.address_space<global>>, memref<1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "block_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
    }
  }
  %alloc_56 = memref.alloc() : memref<f32, #gpu.address_space<global>>
  "lmhlo.reshape"(%alloc_9, %alloc_56) {disc.device = "gpu"} : (memref<1xf32, #gpu.address_space<global>>, memref<f32, #gpu.address_space<global>>) -> ()
  %alloc_57 = memref.alloc() : memref<f32>
  "lmhlo_disc.d2h"(%alloc_56, %alloc_57) {disc.device = "cpu"} : (memref<f32, #gpu.address_space<global>>, memref<f32>) -> ()
  return %alloc_57 : memref<f32>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: memref<?x4xf32>, %arg1: memref<?x4xf32>) -> memref<f32> attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
  %c4 = arith.constant 4 : index
  %c1_i32 = arith.constant 1 : i32
  %c4_i32 = arith.constant 4 : i32
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %dim = memref.dim %arg1, %c0 : memref<?x4xf32>
  %dim_0 = memref.dim %arg0, %c0 : memref<?x4xf32>
  %alloc = memref.alloc() : memref<f32, #gpu.address_space<global>>
  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %reinterpret_cast_1 = memref.reinterpret_cast %arg1 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %0 = arith.cmpi eq, %dim, %c1 : index
  %1 = arith.select %0, %dim_0, %dim : index
  %alloca = memref.alloca() {alignment = 64 : i64} : memref<2xindex>
  memref.store %1, %alloca[%c0] : memref<2xindex>
  memref.store %c4, %alloca[%c1] : memref<2xindex>
  %alloc_2 = memref.alloc(%dim_0) {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  "lmhlo_disc.h2d"(%reinterpret_cast, %alloc_2) : (memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  %alloc_3 = memref.alloc(%1) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %alloc_4 = memref.alloc(%dim) {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  "lmhlo_disc.h2d"(%reinterpret_cast_1, %alloc_4) : (memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  %alloc_5 = memref.alloc(%1) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %alloc_6 = memref.alloc(%1) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %2 = arith.index_cast %1 : index to i32
  %3 = arith.muli %2, %c4_i32 : i32
  %alloca_7 = memref.alloca() {alignment = 64 : i64} : memref<2xi32>
  memref.store %3, %alloca_7[%c0] : memref<2xi32>
  memref.store %c1_i32, %alloca_7[%c1] : memref<2xi32>
  %4 = arith.index_cast %3 : i32 to index
  %alloc_8 = memref.alloc(%4) {kDiscSymbolicDimAttr = [@S3, @C1]} : memref<?x1xf32, #gpu.address_space<global>>
  %alloc_9 = memref.alloc() : memref<1xf32, #gpu.address_space<global>>
  %5 = arith.cmpi eq, %dim_0, %1 : index
  %6 = arith.cmpi eq, %dim, %1 : index
  %7 = arith.andi %6, %5 : i1
  scf.if %7 {
    %reinterpret_cast_12 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [%1, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S4, @C4]} : memref<?x4xf32, #gpu.address_space<global>> to memref<?x4xf32, #gpu.address_space<global>>
    %reinterpret_cast_13 = memref.reinterpret_cast %alloc_4 to offset: [0], sizes: [%1, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S4, @C4]} : memref<?x4xf32, #gpu.address_space<global>> to memref<?x4xf32, #gpu.address_space<global>>
    %reinterpret_cast_14 = memref.reinterpret_cast %alloc_6 to offset: [0], sizes: [%1, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S4, @C4]} : memref<?x4xf32, #gpu.address_space<global>> to memref<?x4xf32, #gpu.address_space<global>>
    %reinterpret_cast_15 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [%4, 1], strides: [1, 1] {kDiscSymbolicDimAttr = [@S5, @C1]} : memref<?x1xf32, #gpu.address_space<global>> to memref<?x1xf32, #gpu.address_space<global>>
    %8 = arith.cmpi slt, %4, %c1 : index
    scf.if %8 {
      "lmhlo.fusion"() ({
        "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0.000000e+00> : tensor<f32>} : (memref<f32, #gpu.address_space<global>>) -> ()
        "lmhlo.multiply"(%reinterpret_cast_12, %reinterpret_cast_13, %reinterpret_cast_14) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_reshape"(%reinterpret_cast_14, %alloca_7, %reinterpret_cast_15) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xi32>, memref<?x1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.reduce"(%reinterpret_cast_15, %alloc, %alloc_9) ({
        ^bb0(%arg2: memref<f32>, %arg3: memref<f32>, %arg4: memref<f32>):
          "lmhlo.add"(%arg2, %arg3, %arg4) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
          "lmhlo.terminator"() : () -> ()
        }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x1xf32, #gpu.address_space<global>>, memref<f32, #gpu.address_space<global>>, memref<1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXthread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0.000000e+00> : tensor<f32>} : (memref<f32, #gpu.address_space<global>>) -> ()
        "lmhlo.multiply"(%reinterpret_cast_12, %reinterpret_cast_13, %reinterpret_cast_14) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_reshape"(%reinterpret_cast_14, %alloca_7, %reinterpret_cast_15) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xi32>, memref<?x1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.reduce"(%reinterpret_cast_15, %alloc, %alloc_9) ({
        ^bb0(%arg2: memref<f32>, %arg3: memref<f32>, %arg4: memref<f32>):
          "lmhlo.add"(%arg2, %arg3, %arg4) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
          "lmhlo.terminator"() : () -> ()
        }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x1xf32, #gpu.address_space<global>>, memref<f32, #gpu.address_space<global>>, memref<1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXblock_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
    }
  } else {
    %8 = arith.cmpi slt, %4, %c1 : index
    scf.if %8 {
      "lmhlo.fusion"() ({
        "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0.000000e+00> : tensor<f32>} : (memref<f32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_broadcast_in_dim"(%alloc_2, %alloca, %alloc_3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xindex>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_broadcast_in_dim"(%alloc_4, %alloca, %alloc_5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xindex>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.multiply"(%alloc_3, %alloc_5, %alloc_6) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_reshape"(%alloc_6, %alloca_7, %alloc_8) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xi32>, memref<?x1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.reduce"(%alloc_8, %alloc, %alloc_9) ({
        ^bb0(%arg2: memref<f32>, %arg3: memref<f32>, %arg4: memref<f32>):
          "lmhlo.add"(%arg2, %arg3, %arg4) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
          "lmhlo.terminator"() : () -> ()
        }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x1xf32, #gpu.address_space<global>>, memref<f32, #gpu.address_space<global>>, memref<1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "thread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0.000000e+00> : tensor<f32>} : (memref<f32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_broadcast_in_dim"(%alloc_2, %alloca, %alloc_3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xindex>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_broadcast_in_dim"(%alloc_4, %alloca, %alloc_5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xindex>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.multiply"(%alloc_3, %alloc_5, %alloc_6) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_reshape"(%alloc_6, %alloca_7, %alloc_8) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xi32>, memref<?x1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.reduce"(%alloc_8, %alloc, %alloc_9) ({
        ^bb0(%arg2: memref<f32>, %arg3: memref<f32>, %arg4: memref<f32>):
          "lmhlo.add"(%arg2, %arg3, %arg4) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
          "lmhlo.terminator"() : () -> ()
        }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x1xf32, #gpu.address_space<global>>, memref<f32, #gpu.address_space<global>>, memref<1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "block_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
    }
  }
  %alloc_10 = memref.alloc() : memref<f32, #gpu.address_space<global>>
  "lmhlo.reshape"(%alloc_9, %alloc_10) {disc.device = "gpu"} : (memref<1xf32, #gpu.address_space<global>>, memref<f32, #gpu.address_space<global>>) -> ()
  %alloc_11 = memref.alloc() : memref<f32>
  "lmhlo_disc.d2h"(%alloc_10, %alloc_11) {disc.device = "cpu"} : (memref<f32, #gpu.address_space<global>>, memref<f32>) -> ()
  return %alloc_11 : memref<f32>
}

// -----// IR Dump After DiscMemRefLoadStoreSimplifierPass (disc-memref-load-store-simplifier) //----- //
func.func @shape_constraint_graph() {
  %c4 = arith.constant 4 : index
  %0 = "disc_shape.dim"() {name = @S2} : () -> index
  %1 = "disc_shape.dim"() {name = @S3} : () -> index
  "disc_shape.tie_product_equal"(%c4, %0, %1) {operand_segment_sizes = array<i32: 2, 1>} : (index, index, index) -> ()
  %2 = "disc_shape.dim"() {name = @S4} : () -> index
  %3 = "disc_shape.dim"() {name = @S5} : () -> index
  "disc_shape.tie_product_equal"(%c4, %2, %3) {operand_segment_sizes = array<i32: 2, 1>} : (index, index, index) -> ()
  return
}

// -----// IR Dump After DiscReduceBufferLiveRangePass (disc-reduce-buffer-live-range) //----- //
func.func @main(%arg0: memref<?x4xf32>, %arg1: memref<?x4xf32>) -> memref<f32> attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
  %c4 = arith.constant 4 : index
  %c1_i32 = arith.constant 1 : i32
  %c4_i32 = arith.constant 4 : i32
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %dim = memref.dim %arg1, %c0 : memref<?x4xf32>
  %dim_0 = memref.dim %arg0, %c0 : memref<?x4xf32>
  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %reinterpret_cast_1 = memref.reinterpret_cast %arg1 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %0 = arith.cmpi eq, %dim, %c1 : index
  %1 = arith.select %0, %dim_0, %dim : index
  %alloca = memref.alloca() {alignment = 64 : i64} : memref<2xindex>
  memref.store %1, %alloca[%c0] : memref<2xindex>
  memref.store %c4, %alloca[%c1] : memref<2xindex>
  %alloc = memref.alloc(%dim_0) {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  "lmhlo_disc.h2d"(%reinterpret_cast, %alloc) : (memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  %alloc_2 = memref.alloc(%dim) {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  "lmhlo_disc.h2d"(%reinterpret_cast_1, %alloc_2) : (memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  %2 = arith.index_cast %1 : index to i32
  %3 = arith.muli %2, %c4_i32 : i32
  %alloca_3 = memref.alloca() {alignment = 64 : i64} : memref<2xi32>
  memref.store %3, %alloca_3[%c0] : memref<2xi32>
  memref.store %c1_i32, %alloca_3[%c1] : memref<2xi32>
  %4 = arith.index_cast %3 : i32 to index
  %5 = arith.cmpi eq, %dim_0, %1 : index
  %6 = arith.cmpi eq, %dim, %1 : index
  %7 = arith.andi %6, %5 : i1
  %alloc_4 = memref.alloc() : memref<f32, #gpu.address_space<global>>
  %alloc_5 = memref.alloc(%1) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %alloc_6 = memref.alloc(%1) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %alloc_7 = memref.alloc(%1) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %alloc_8 = memref.alloc(%4) {kDiscSymbolicDimAttr = [@S3, @C1]} : memref<?x1xf32, #gpu.address_space<global>>
  %alloc_9 = memref.alloc() : memref<1xf32, #gpu.address_space<global>>
  scf.if %7 {
    %reinterpret_cast_12 = memref.reinterpret_cast %alloc to offset: [0], sizes: [%1, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S4, @C4]} : memref<?x4xf32, #gpu.address_space<global>> to memref<?x4xf32, #gpu.address_space<global>>
    %reinterpret_cast_13 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [%1, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S4, @C4]} : memref<?x4xf32, #gpu.address_space<global>> to memref<?x4xf32, #gpu.address_space<global>>
    %reinterpret_cast_14 = memref.reinterpret_cast %alloc_7 to offset: [0], sizes: [%1, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S4, @C4]} : memref<?x4xf32, #gpu.address_space<global>> to memref<?x4xf32, #gpu.address_space<global>>
    %reinterpret_cast_15 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [%4, 1], strides: [1, 1] {kDiscSymbolicDimAttr = [@S5, @C1]} : memref<?x1xf32, #gpu.address_space<global>> to memref<?x1xf32, #gpu.address_space<global>>
    %8 = arith.cmpi slt, %4, %c1 : index
    scf.if %8 {
      "lmhlo.fusion"() ({
        "lmhlo.constant"(%alloc_4) {disc.device = "gpu", value = dense<0.000000e+00> : tensor<f32>} : (memref<f32, #gpu.address_space<global>>) -> ()
        "lmhlo.multiply"(%reinterpret_cast_12, %reinterpret_cast_13, %reinterpret_cast_14) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_reshape"(%reinterpret_cast_14, %alloca_3, %reinterpret_cast_15) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xi32>, memref<?x1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.reduce"(%reinterpret_cast_15, %alloc_4, %alloc_9) ({
        ^bb0(%arg2: memref<f32>, %arg3: memref<f32>, %arg4: memref<f32>):
          "lmhlo.add"(%arg2, %arg3, %arg4) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
          "lmhlo.terminator"() : () -> ()
        }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x1xf32, #gpu.address_space<global>>, memref<f32, #gpu.address_space<global>>, memref<1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXthread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        "lmhlo.constant"(%alloc_4) {disc.device = "gpu", value = dense<0.000000e+00> : tensor<f32>} : (memref<f32, #gpu.address_space<global>>) -> ()
        "lmhlo.multiply"(%reinterpret_cast_12, %reinterpret_cast_13, %reinterpret_cast_14) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_reshape"(%reinterpret_cast_14, %alloca_3, %reinterpret_cast_15) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xi32>, memref<?x1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.reduce"(%reinterpret_cast_15, %alloc_4, %alloc_9) ({
        ^bb0(%arg2: memref<f32>, %arg3: memref<f32>, %arg4: memref<f32>):
          "lmhlo.add"(%arg2, %arg3, %arg4) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
          "lmhlo.terminator"() : () -> ()
        }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x1xf32, #gpu.address_space<global>>, memref<f32, #gpu.address_space<global>>, memref<1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXblock_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
    }
  } else {
    %8 = arith.cmpi slt, %4, %c1 : index
    scf.if %8 {
      "lmhlo.fusion"() ({
        "lmhlo.constant"(%alloc_4) {disc.device = "gpu", value = dense<0.000000e+00> : tensor<f32>} : (memref<f32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_broadcast_in_dim"(%alloc, %alloca, %alloc_5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xindex>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_broadcast_in_dim"(%alloc_2, %alloca, %alloc_6) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xindex>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.multiply"(%alloc_5, %alloc_6, %alloc_7) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_reshape"(%alloc_7, %alloca_3, %alloc_8) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xi32>, memref<?x1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.reduce"(%alloc_8, %alloc_4, %alloc_9) ({
        ^bb0(%arg2: memref<f32>, %arg3: memref<f32>, %arg4: memref<f32>):
          "lmhlo.add"(%arg2, %arg3, %arg4) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
          "lmhlo.terminator"() : () -> ()
        }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x1xf32, #gpu.address_space<global>>, memref<f32, #gpu.address_space<global>>, memref<1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "thread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        "lmhlo.constant"(%alloc_4) {disc.device = "gpu", value = dense<0.000000e+00> : tensor<f32>} : (memref<f32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_broadcast_in_dim"(%alloc, %alloca, %alloc_5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xindex>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_broadcast_in_dim"(%alloc_2, %alloca, %alloc_6) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xindex>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.multiply"(%alloc_5, %alloc_6, %alloc_7) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_reshape"(%alloc_7, %alloca_3, %alloc_8) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xi32>, memref<?x1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.reduce"(%alloc_8, %alloc_4, %alloc_9) ({
        ^bb0(%arg2: memref<f32>, %arg3: memref<f32>, %arg4: memref<f32>):
          "lmhlo.add"(%arg2, %arg3, %arg4) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
          "lmhlo.terminator"() : () -> ()
        }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x1xf32, #gpu.address_space<global>>, memref<f32, #gpu.address_space<global>>, memref<1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "block_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
    }
  }
  %alloc_10 = memref.alloc() : memref<f32, #gpu.address_space<global>>
  "lmhlo.reshape"(%alloc_9, %alloc_10) {disc.device = "gpu"} : (memref<1xf32, #gpu.address_space<global>>, memref<f32, #gpu.address_space<global>>) -> ()
  %alloc_11 = memref.alloc() : memref<f32>
  "lmhlo_disc.d2h"(%alloc_10, %alloc_11) {disc.device = "cpu"} : (memref<f32, #gpu.address_space<global>>, memref<f32>) -> ()
  return %alloc_11 : memref<f32>
}

// -----// IR Dump After BufferDeallocation (buffer-deallocation) //----- //
func.func @main(%arg0: memref<?x4xf32>, %arg1: memref<?x4xf32>) -> memref<f32> attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
  %c4 = arith.constant 4 : index
  %c1_i32 = arith.constant 1 : i32
  %c4_i32 = arith.constant 4 : i32
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %dim = memref.dim %arg1, %c0 : memref<?x4xf32>
  %dim_0 = memref.dim %arg0, %c0 : memref<?x4xf32>
  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %reinterpret_cast_1 = memref.reinterpret_cast %arg1 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %0 = arith.cmpi eq, %dim, %c1 : index
  %1 = arith.select %0, %dim_0, %dim : index
  %alloca = memref.alloca() {alignment = 64 : i64} : memref<2xindex>
  memref.store %1, %alloca[%c0] : memref<2xindex>
  memref.store %c4, %alloca[%c1] : memref<2xindex>
  %alloc = memref.alloc(%dim_0) {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  "lmhlo_disc.h2d"(%reinterpret_cast, %alloc) : (memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  %alloc_2 = memref.alloc(%dim) {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  "lmhlo_disc.h2d"(%reinterpret_cast_1, %alloc_2) : (memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  %2 = arith.index_cast %1 : index to i32
  %3 = arith.muli %2, %c4_i32 : i32
  %alloca_3 = memref.alloca() {alignment = 64 : i64} : memref<2xi32>
  memref.store %3, %alloca_3[%c0] : memref<2xi32>
  memref.store %c1_i32, %alloca_3[%c1] : memref<2xi32>
  %4 = arith.index_cast %3 : i32 to index
  %5 = arith.cmpi eq, %dim_0, %1 : index
  %6 = arith.cmpi eq, %dim, %1 : index
  %7 = arith.andi %6, %5 : i1
  %alloc_4 = memref.alloc() : memref<f32, #gpu.address_space<global>>
  %alloc_5 = memref.alloc(%1) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %alloc_6 = memref.alloc(%1) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %alloc_7 = memref.alloc(%1) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %alloc_8 = memref.alloc(%4) {kDiscSymbolicDimAttr = [@S3, @C1]} : memref<?x1xf32, #gpu.address_space<global>>
  %alloc_9 = memref.alloc() : memref<1xf32, #gpu.address_space<global>>
  scf.if %7 {
    %reinterpret_cast_12 = memref.reinterpret_cast %alloc to offset: [0], sizes: [%1, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S4, @C4]} : memref<?x4xf32, #gpu.address_space<global>> to memref<?x4xf32, #gpu.address_space<global>>
    %reinterpret_cast_13 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [%1, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S4, @C4]} : memref<?x4xf32, #gpu.address_space<global>> to memref<?x4xf32, #gpu.address_space<global>>
    %reinterpret_cast_14 = memref.reinterpret_cast %alloc_7 to offset: [0], sizes: [%1, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S4, @C4]} : memref<?x4xf32, #gpu.address_space<global>> to memref<?x4xf32, #gpu.address_space<global>>
    %reinterpret_cast_15 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [%4, 1], strides: [1, 1] {kDiscSymbolicDimAttr = [@S5, @C1]} : memref<?x1xf32, #gpu.address_space<global>> to memref<?x1xf32, #gpu.address_space<global>>
    %8 = arith.cmpi slt, %4, %c1 : index
    scf.if %8 {
      "lmhlo.fusion"() ({
        "lmhlo.constant"(%alloc_4) {disc.device = "gpu", value = dense<0.000000e+00> : tensor<f32>} : (memref<f32, #gpu.address_space<global>>) -> ()
        "lmhlo.multiply"(%reinterpret_cast_12, %reinterpret_cast_13, %reinterpret_cast_14) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_reshape"(%reinterpret_cast_14, %alloca_3, %reinterpret_cast_15) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xi32>, memref<?x1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.reduce"(%reinterpret_cast_15, %alloc_4, %alloc_9) ({
        ^bb0(%arg2: memref<f32>, %arg3: memref<f32>, %arg4: memref<f32>):
          "lmhlo.add"(%arg2, %arg3, %arg4) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
          "lmhlo.terminator"() : () -> ()
        }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x1xf32, #gpu.address_space<global>>, memref<f32, #gpu.address_space<global>>, memref<1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXthread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        "lmhlo.constant"(%alloc_4) {disc.device = "gpu", value = dense<0.000000e+00> : tensor<f32>} : (memref<f32, #gpu.address_space<global>>) -> ()
        "lmhlo.multiply"(%reinterpret_cast_12, %reinterpret_cast_13, %reinterpret_cast_14) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_reshape"(%reinterpret_cast_14, %alloca_3, %reinterpret_cast_15) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xi32>, memref<?x1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.reduce"(%reinterpret_cast_15, %alloc_4, %alloc_9) ({
        ^bb0(%arg2: memref<f32>, %arg3: memref<f32>, %arg4: memref<f32>):
          "lmhlo.add"(%arg2, %arg3, %arg4) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
          "lmhlo.terminator"() : () -> ()
        }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x1xf32, #gpu.address_space<global>>, memref<f32, #gpu.address_space<global>>, memref<1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXblock_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
    }
  } else {
    %8 = arith.cmpi slt, %4, %c1 : index
    scf.if %8 {
      "lmhlo.fusion"() ({
        "lmhlo.constant"(%alloc_4) {disc.device = "gpu", value = dense<0.000000e+00> : tensor<f32>} : (memref<f32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_broadcast_in_dim"(%alloc, %alloca, %alloc_5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xindex>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_broadcast_in_dim"(%alloc_2, %alloca, %alloc_6) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xindex>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.multiply"(%alloc_5, %alloc_6, %alloc_7) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_reshape"(%alloc_7, %alloca_3, %alloc_8) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xi32>, memref<?x1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.reduce"(%alloc_8, %alloc_4, %alloc_9) ({
        ^bb0(%arg2: memref<f32>, %arg3: memref<f32>, %arg4: memref<f32>):
          "lmhlo.add"(%arg2, %arg3, %arg4) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
          "lmhlo.terminator"() : () -> ()
        }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x1xf32, #gpu.address_space<global>>, memref<f32, #gpu.address_space<global>>, memref<1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "thread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        "lmhlo.constant"(%alloc_4) {disc.device = "gpu", value = dense<0.000000e+00> : tensor<f32>} : (memref<f32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_broadcast_in_dim"(%alloc, %alloca, %alloc_5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xindex>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_broadcast_in_dim"(%alloc_2, %alloca, %alloc_6) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xindex>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.multiply"(%alloc_5, %alloc_6, %alloc_7) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_reshape"(%alloc_7, %alloca_3, %alloc_8) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xi32>, memref<?x1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.reduce"(%alloc_8, %alloc_4, %alloc_9) ({
        ^bb0(%arg2: memref<f32>, %arg3: memref<f32>, %arg4: memref<f32>):
          "lmhlo.add"(%arg2, %arg3, %arg4) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
          "lmhlo.terminator"() : () -> ()
        }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x1xf32, #gpu.address_space<global>>, memref<f32, #gpu.address_space<global>>, memref<1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "block_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
    }
  }
  memref.dealloc %alloc_8 : memref<?x1xf32, #gpu.address_space<global>>
  memref.dealloc %alloc_7 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc_6 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc_5 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc_4 : memref<f32, #gpu.address_space<global>>
  memref.dealloc %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc : memref<?x4xf32, #gpu.address_space<global>>
  %alloc_10 = memref.alloc() : memref<f32, #gpu.address_space<global>>
  "lmhlo.reshape"(%alloc_9, %alloc_10) {disc.device = "gpu"} : (memref<1xf32, #gpu.address_space<global>>, memref<f32, #gpu.address_space<global>>) -> ()
  memref.dealloc %alloc_9 : memref<1xf32, #gpu.address_space<global>>
  %alloc_11 = memref.alloc() : memref<f32>
  "lmhlo_disc.d2h"(%alloc_10, %alloc_11) {disc.device = "cpu"} : (memref<f32, #gpu.address_space<global>>, memref<f32>) -> ()
  memref.dealloc %alloc_10 : memref<f32, #gpu.address_space<global>>
  return %alloc_11 : memref<f32>
}

// -----// IR Dump After RalInjectExecutionContextPass (disc-ral-inject-execution-context) //----- //
module {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
    %c0 = arith.constant 0 : index
    %0 = "disc_ral.recv_input"(%arg0, %c0) : (!disc_ral.context, index) -> memref<?x4xf32>
    %c1 = arith.constant 1 : index
    %1 = "disc_ral.recv_input"(%arg0, %c1) : (!disc_ral.context, index) -> memref<?x4xf32>
    %c4 = arith.constant 4 : index
    %c1_i32 = arith.constant 1 : i32
    %c4_i32 = arith.constant 4 : i32
    %c1_0 = arith.constant 1 : index
    %c0_1 = arith.constant 0 : index
    %dim = memref.dim %1, %c0_1 : memref<?x4xf32>
    %dim_2 = memref.dim %0, %c0_1 : memref<?x4xf32>
    %reinterpret_cast = memref.reinterpret_cast %0 to offset: [0], sizes: [%dim_2, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
    %reinterpret_cast_3 = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
    %2 = arith.cmpi eq, %dim, %c1_0 : index
    %3 = arith.select %2, %dim_2, %dim : index
    %alloca = memref.alloca() {alignment = 64 : i64} : memref<2xindex>
    memref.store %3, %alloca[%c0_1] : memref<2xindex>
    memref.store %c4, %alloca[%c1_0] : memref<2xindex>
    %alloc = memref.alloc(%dim_2) {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
    "lmhlo_disc.h2d"(%reinterpret_cast, %alloc) : (memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
    %alloc_4 = memref.alloc(%dim) {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
    "lmhlo_disc.h2d"(%reinterpret_cast_3, %alloc_4) : (memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
    %4 = arith.index_cast %3 : index to i32
    %5 = arith.muli %4, %c4_i32 : i32
    %alloca_5 = memref.alloca() {alignment = 64 : i64} : memref<2xi32>
    memref.store %5, %alloca_5[%c0_1] : memref<2xi32>
    memref.store %c1_i32, %alloca_5[%c1_0] : memref<2xi32>
    %6 = arith.index_cast %5 : i32 to index
    %7 = arith.cmpi eq, %dim_2, %3 : index
    %8 = arith.cmpi eq, %dim, %3 : index
    %9 = arith.andi %8, %7 : i1
    %alloc_6 = memref.alloc() : memref<f32, #gpu.address_space<global>>
    %alloc_7 = memref.alloc(%3) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
    %alloc_8 = memref.alloc(%3) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
    %alloc_9 = memref.alloc(%3) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
    %alloc_10 = memref.alloc(%6) {kDiscSymbolicDimAttr = [@S3, @C1]} : memref<?x1xf32, #gpu.address_space<global>>
    %alloc_11 = memref.alloc() : memref<1xf32, #gpu.address_space<global>>
    scf.if %9 {
      %reinterpret_cast_15 = memref.reinterpret_cast %alloc to offset: [0], sizes: [%3, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S4, @C4]} : memref<?x4xf32, #gpu.address_space<global>> to memref<?x4xf32, #gpu.address_space<global>>
      %reinterpret_cast_16 = memref.reinterpret_cast %alloc_4 to offset: [0], sizes: [%3, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S4, @C4]} : memref<?x4xf32, #gpu.address_space<global>> to memref<?x4xf32, #gpu.address_space<global>>
      %reinterpret_cast_17 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [%3, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S4, @C4]} : memref<?x4xf32, #gpu.address_space<global>> to memref<?x4xf32, #gpu.address_space<global>>
      %reinterpret_cast_18 = memref.reinterpret_cast %alloc_10 to offset: [0], sizes: [%6, 1], strides: [1, 1] {kDiscSymbolicDimAttr = [@S5, @C1]} : memref<?x1xf32, #gpu.address_space<global>> to memref<?x1xf32, #gpu.address_space<global>>
      %10 = arith.cmpi slt, %6, %c1_0 : index
      scf.if %10 {
        "lmhlo.fusion"() ({
          "lmhlo.constant"(%alloc_6) {disc.device = "gpu", value = dense<0.000000e+00> : tensor<f32>} : (memref<f32, #gpu.address_space<global>>) -> ()
          "lmhlo.multiply"(%reinterpret_cast_15, %reinterpret_cast_16, %reinterpret_cast_17) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
          "lmhlo.dynamic_reshape"(%reinterpret_cast_17, %alloca_5, %reinterpret_cast_18) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xi32>, memref<?x1xf32, #gpu.address_space<global>>) -> ()
          "lmhlo.reduce"(%reinterpret_cast_18, %alloc_6, %alloc_11) ({
          ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
            "lmhlo.add"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
            "lmhlo.terminator"() : () -> ()
          }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x1xf32, #gpu.address_space<global>>, memref<f32, #gpu.address_space<global>>, memref<1xf32, #gpu.address_space<global>>) -> ()
          "lmhlo.terminator"() : () -> ()
        }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXthread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
      } else {
        "lmhlo.fusion"() ({
          "lmhlo.constant"(%alloc_6) {disc.device = "gpu", value = dense<0.000000e+00> : tensor<f32>} : (memref<f32, #gpu.address_space<global>>) -> ()
          "lmhlo.multiply"(%reinterpret_cast_15, %reinterpret_cast_16, %reinterpret_cast_17) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
          "lmhlo.dynamic_reshape"(%reinterpret_cast_17, %alloca_5, %reinterpret_cast_18) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xi32>, memref<?x1xf32, #gpu.address_space<global>>) -> ()
          "lmhlo.reduce"(%reinterpret_cast_18, %alloc_6, %alloc_11) ({
          ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
            "lmhlo.add"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
            "lmhlo.terminator"() : () -> ()
          }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x1xf32, #gpu.address_space<global>>, memref<f32, #gpu.address_space<global>>, memref<1xf32, #gpu.address_space<global>>) -> ()
          "lmhlo.terminator"() : () -> ()
        }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXblock_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
      }
    } else {
      %10 = arith.cmpi slt, %6, %c1_0 : index
      scf.if %10 {
        "lmhlo.fusion"() ({
          "lmhlo.constant"(%alloc_6) {disc.device = "gpu", value = dense<0.000000e+00> : tensor<f32>} : (memref<f32, #gpu.address_space<global>>) -> ()
          "lmhlo.dynamic_broadcast_in_dim"(%alloc, %alloca, %alloc_7) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xindex>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
          "lmhlo.dynamic_broadcast_in_dim"(%alloc_4, %alloca, %alloc_8) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xindex>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
          "lmhlo.multiply"(%alloc_7, %alloc_8, %alloc_9) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
          "lmhlo.dynamic_reshape"(%alloc_9, %alloca_5, %alloc_10) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xi32>, memref<?x1xf32, #gpu.address_space<global>>) -> ()
          "lmhlo.reduce"(%alloc_10, %alloc_6, %alloc_11) ({
          ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
            "lmhlo.add"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
            "lmhlo.terminator"() : () -> ()
          }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x1xf32, #gpu.address_space<global>>, memref<f32, #gpu.address_space<global>>, memref<1xf32, #gpu.address_space<global>>) -> ()
          "lmhlo.terminator"() : () -> ()
        }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "thread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
      } else {
        "lmhlo.fusion"() ({
          "lmhlo.constant"(%alloc_6) {disc.device = "gpu", value = dense<0.000000e+00> : tensor<f32>} : (memref<f32, #gpu.address_space<global>>) -> ()
          "lmhlo.dynamic_broadcast_in_dim"(%alloc, %alloca, %alloc_7) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xindex>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
          "lmhlo.dynamic_broadcast_in_dim"(%alloc_4, %alloca, %alloc_8) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xindex>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
          "lmhlo.multiply"(%alloc_7, %alloc_8, %alloc_9) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
          "lmhlo.dynamic_reshape"(%alloc_9, %alloca_5, %alloc_10) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xi32>, memref<?x1xf32, #gpu.address_space<global>>) -> ()
          "lmhlo.reduce"(%alloc_10, %alloc_6, %alloc_11) ({
          ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
            "lmhlo.add"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
            "lmhlo.terminator"() : () -> ()
          }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x1xf32, #gpu.address_space<global>>, memref<f32, #gpu.address_space<global>>, memref<1xf32, #gpu.address_space<global>>) -> ()
          "lmhlo.terminator"() : () -> ()
        }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "block_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
      }
    }
    memref.dealloc %alloc_10 : memref<?x1xf32, #gpu.address_space<global>>
    memref.dealloc %alloc_9 : memref<?x4xf32, #gpu.address_space<global>>
    memref.dealloc %alloc_8 : memref<?x4xf32, #gpu.address_space<global>>
    memref.dealloc %alloc_7 : memref<?x4xf32, #gpu.address_space<global>>
    memref.dealloc %alloc_6 : memref<f32, #gpu.address_space<global>>
    memref.dealloc %alloc_4 : memref<?x4xf32, #gpu.address_space<global>>
    memref.dealloc %alloc : memref<?x4xf32, #gpu.address_space<global>>
    %alloc_12 = memref.alloc() : memref<f32, #gpu.address_space<global>>
    "lmhlo.reshape"(%alloc_11, %alloc_12) {disc.device = "gpu"} : (memref<1xf32, #gpu.address_space<global>>, memref<f32, #gpu.address_space<global>>) -> ()
    memref.dealloc %alloc_11 : memref<1xf32, #gpu.address_space<global>>
    %alloc_13 = memref.alloc() : memref<f32>
    "lmhlo_disc.d2h"(%alloc_12, %alloc_13) {disc.device = "cpu"} : (memref<f32, #gpu.address_space<global>>, memref<f32>) -> ()
    memref.dealloc %alloc_12 : memref<f32, #gpu.address_space<global>>
    %c0_14 = arith.constant 0 : index
    "disc_ral.send_output"(%arg0, %c0_14, %alloc_13) : (!disc_ral.context, index, memref<f32>) -> ()
    return
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = true, sym_name = "C4", value = 4 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = true, sym_name = "C1", value = 1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S4", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = false, sym_name = "S5", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %c4 = arith.constant 4 : index
    %0 = "disc_shape.dim"() {name = @S2} : () -> index
    %1 = "disc_shape.dim"() {name = @S3} : () -> index
    "disc_shape.tie_product_equal"(%c4, %0, %1) {operand_segment_sizes = array<i32: 2, 1>} : (index, index, index) -> ()
    %2 = "disc_shape.dim"() {name = @S4} : () -> index
    %3 = "disc_shape.dim"() {name = @S5} : () -> index
    "disc_shape.tie_product_equal"(%c4, %2, %3) {operand_segment_sizes = array<i32: 2, 1>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After DiscLowerToLibraryCallPass (disc-lower-to-library-call) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c4_i32 = arith.constant 4 : i32
  %c1_i32 = arith.constant 1 : i32
  %c4 = arith.constant 4 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
  %2 = "disc_ral.dispatch"(%arg0, %c1) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
  %dim = memref.dim %2, %c0 : memref<?x4xf32>
  %dim_0 = memref.dim %1, %c0 : memref<?x4xf32>
  %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %reinterpret_cast_1 = memref.reinterpret_cast %2 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %3 = arith.cmpi eq, %dim, %c1 : index
  %4 = arith.select %3, %dim_0, %dim : index
  %alloca = memref.alloca() {alignment = 64 : i64} : memref<2xindex>
  memref.store %4, %alloca[%c0] : memref<2xindex>
  memref.store %c4, %alloca[%c1] : memref<2xindex>
  %alloc = memref.alloc(%dim_0) {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %5 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast, %alloc) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  %alloc_2 = memref.alloc(%dim) {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %6 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  "disc_ral.dispatch"(%arg0, %6, %reinterpret_cast_1, %alloc_2) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  "disc_ral.dispatch"(%arg0, %6) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  %7 = arith.index_cast %4 : index to i32
  %8 = arith.muli %7, %c4_i32 : i32
  %alloca_3 = memref.alloca() {alignment = 64 : i64} : memref<2xi32>
  memref.store %8, %alloca_3[%c0] : memref<2xi32>
  memref.store %c1_i32, %alloca_3[%c1] : memref<2xi32>
  %9 = arith.index_cast %8 : i32 to index
  %10 = arith.cmpi eq, %dim_0, %4 : index
  %11 = arith.cmpi eq, %dim, %4 : index
  %12 = arith.andi %11, %10 : i1
  %alloc_4 = memref.alloc() : memref<f32, #gpu.address_space<global>>
  %alloc_5 = memref.alloc(%4) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %alloc_6 = memref.alloc(%4) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %alloc_7 = memref.alloc(%4) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %alloc_8 = memref.alloc(%9) {kDiscSymbolicDimAttr = [@S3, @C1]} : memref<?x1xf32, #gpu.address_space<global>>
  %alloc_9 = memref.alloc() : memref<1xf32, #gpu.address_space<global>>
  scf.if %12 {
    %reinterpret_cast_13 = memref.reinterpret_cast %alloc to offset: [0], sizes: [%4, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S4, @C4]} : memref<?x4xf32, #gpu.address_space<global>> to memref<?x4xf32, #gpu.address_space<global>>
    %reinterpret_cast_14 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [%4, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S4, @C4]} : memref<?x4xf32, #gpu.address_space<global>> to memref<?x4xf32, #gpu.address_space<global>>
    %reinterpret_cast_15 = memref.reinterpret_cast %alloc_7 to offset: [0], sizes: [%4, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S4, @C4]} : memref<?x4xf32, #gpu.address_space<global>> to memref<?x4xf32, #gpu.address_space<global>>
    %reinterpret_cast_16 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [%9, 1], strides: [1, 1] {kDiscSymbolicDimAttr = [@S5, @C1]} : memref<?x1xf32, #gpu.address_space<global>> to memref<?x1xf32, #gpu.address_space<global>>
    %16 = arith.cmpi slt, %9, %c1 : index
    scf.if %16 {
      "lmhlo.fusion"() ({
        "lmhlo.constant"(%alloc_4) {disc.device = "gpu", value = dense<0.000000e+00> : tensor<f32>} : (memref<f32, #gpu.address_space<global>>) -> ()
        "lmhlo.multiply"(%reinterpret_cast_13, %reinterpret_cast_14, %reinterpret_cast_15) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_reshape"(%reinterpret_cast_15, %alloca_3, %reinterpret_cast_16) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xi32>, memref<?x1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.reduce"(%reinterpret_cast_16, %alloc_4, %alloc_9) ({
        ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
          "lmhlo.add"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
          "lmhlo.terminator"() : () -> ()
        }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x1xf32, #gpu.address_space<global>>, memref<f32, #gpu.address_space<global>>, memref<1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXthread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        "lmhlo.constant"(%alloc_4) {disc.device = "gpu", value = dense<0.000000e+00> : tensor<f32>} : (memref<f32, #gpu.address_space<global>>) -> ()
        "lmhlo.multiply"(%reinterpret_cast_13, %reinterpret_cast_14, %reinterpret_cast_15) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_reshape"(%reinterpret_cast_15, %alloca_3, %reinterpret_cast_16) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xi32>, memref<?x1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.reduce"(%reinterpret_cast_16, %alloc_4, %alloc_9) ({
        ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
          "lmhlo.add"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
          "lmhlo.terminator"() : () -> ()
        }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x1xf32, #gpu.address_space<global>>, memref<f32, #gpu.address_space<global>>, memref<1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXblock_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
    }
  } else {
    %16 = arith.cmpi slt, %9, %c1 : index
    scf.if %16 {
      "lmhlo.fusion"() ({
        "lmhlo.constant"(%alloc_4) {disc.device = "gpu", value = dense<0.000000e+00> : tensor<f32>} : (memref<f32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_broadcast_in_dim"(%alloc, %alloca, %alloc_5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xindex>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_broadcast_in_dim"(%alloc_2, %alloca, %alloc_6) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xindex>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.multiply"(%alloc_5, %alloc_6, %alloc_7) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_reshape"(%alloc_7, %alloca_3, %alloc_8) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xi32>, memref<?x1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.reduce"(%alloc_8, %alloc_4, %alloc_9) ({
        ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
          "lmhlo.add"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
          "lmhlo.terminator"() : () -> ()
        }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x1xf32, #gpu.address_space<global>>, memref<f32, #gpu.address_space<global>>, memref<1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "thread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        "lmhlo.constant"(%alloc_4) {disc.device = "gpu", value = dense<0.000000e+00> : tensor<f32>} : (memref<f32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_broadcast_in_dim"(%alloc, %alloca, %alloc_5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xindex>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_broadcast_in_dim"(%alloc_2, %alloca, %alloc_6) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xindex>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.multiply"(%alloc_5, %alloc_6, %alloc_7) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_reshape"(%alloc_7, %alloca_3, %alloc_8) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xi32>, memref<?x1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.reduce"(%alloc_8, %alloc_4, %alloc_9) ({
        ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
          "lmhlo.add"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
          "lmhlo.terminator"() : () -> ()
        }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x1xf32, #gpu.address_space<global>>, memref<f32, #gpu.address_space<global>>, memref<1xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "block_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
    }
  }
  memref.dealloc %alloc_8 : memref<?x1xf32, #gpu.address_space<global>>
  memref.dealloc %alloc_7 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc_6 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc_5 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc_4 : memref<f32, #gpu.address_space<global>>
  memref.dealloc %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc : memref<?x4xf32, #gpu.address_space<global>>
  %13 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca_10 = memref.alloca() : memref<0xindex>
  %14 = "disc_ral.dispatch"(%arg0, %13, %alloc_9, %alloca_10) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1xf32, #gpu.address_space<global>>, memref<0xindex>) -> memref<f32, #gpu.address_space<global>>
  %reinterpret_cast_11 = memref.reinterpret_cast %14 to offset: [0], sizes: [], strides: [] : memref<f32, #gpu.address_space<global>> to memref<f32, #gpu.address_space<global>>
  memref.dealloc %alloc_9 : memref<1xf32, #gpu.address_space<global>>
  %alloc_12 = memref.alloc() : memref<f32>
  %15 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  "disc_ral.dispatch"(%arg0, %15, %reinterpret_cast_11, %alloc_12) {backend_config = "", call_target_name = "d2h", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<f32, #gpu.address_space<global>>, memref<f32>) -> ()
  "disc_ral.dispatch"(%arg0, %15) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  memref.dealloc %reinterpret_cast_11 : memref<f32, #gpu.address_space<global>>
  "disc_ral.dispatch"(%arg0, %c0, %alloc_12) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<f32>) -> ()
  return
}

kColReduction <main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32>, use_new: 0 schedule_hint: 7
kColReduction <main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64>, use_new: 0 schedule_hint: 8
kColReduction <main_kColReduction_reduce__6_1_0___thread_tile_h32>, use_new: 0 schedule_hint: 7
kColReduction <main_kColReduction_reduce__6_1_0___block_tile_h64>, use_new: 0 schedule_hint: 8
SymbolicDimMgr::save walkRankedTensorValue takes: 8 us
SymbolicDimMgr::save update attributes takes: 19 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 9 us
productSet.size() = 4
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 6 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 26 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 1 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 1 us
SymbolicDimMgr::save updateProductEqualityMap takes: 68 us
SymbolicDimMgr::save updateFunctionType takes: 5 us
SymbolicDimMgr::save collect symbolicDim ops takes: 20 us
SymbolicDimMgr::save remove symbolicDim ops takes: 0 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 1 us
SymbolicDimMgr::save canonicalize the name takes: 13 us
SymbolicDimMgr::save replace the name takes: 20 us
SymbolicDimMgr::save updateFunctionType takes: 3 us
// -----// IR Dump After DiscLhloLegalizeRootsToParallelLoopsPass (disc-lhlo-legalize-roots-to-parallel-loops) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
  %c2 = arith.constant 2 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c512 = arith.constant 512 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c4_i32 = arith.constant 4 : i32
  %c1_i32 = arith.constant 1 : i32
  %c4 = arith.constant 4 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
  %2 = "disc_ral.dispatch"(%arg0, %c1) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
  %dim = memref.dim %2, %c0 : memref<?x4xf32>
  %dim_0 = memref.dim %1, %c0 : memref<?x4xf32>
  %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %reinterpret_cast_1 = memref.reinterpret_cast %2 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %3 = arith.cmpi eq, %dim, %c1 : index
  %4 = arith.select %3, %dim_0, %dim : index
  %alloca = memref.alloca() {alignment = 64 : i64} : memref<2xindex>
  memref.store %4, %alloca[%c0] : memref<2xindex>
  memref.store %c4, %alloca[%c1] : memref<2xindex>
  %alloc = memref.alloc(%dim_0) {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %5 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast, %alloc) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  %alloc_2 = memref.alloc(%dim) {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %6 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  "disc_ral.dispatch"(%arg0, %6, %reinterpret_cast_1, %alloc_2) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  "disc_ral.dispatch"(%arg0, %6) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  %7 = arith.index_cast %4 : index to i32
  %8 = arith.muli %7, %c4_i32 : i32
  %alloca_3 = memref.alloca() {alignment = 64 : i64} : memref<2xi32>
  memref.store %8, %alloca_3[%c0] : memref<2xi32>
  memref.store %c1_i32, %alloca_3[%c1] : memref<2xi32>
  %9 = arith.index_cast %8 : i32 to index
  %10 = arith.cmpi eq, %dim_0, %4 : index
  %11 = arith.cmpi eq, %dim, %4 : index
  %12 = arith.andi %11, %10 : i1
  %alloc_4 = memref.alloc() : memref<f32, #gpu.address_space<global>>
  %alloc_5 = memref.alloc(%4) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %alloc_6 = memref.alloc(%4) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %alloc_7 = memref.alloc(%4) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %alloc_8 = memref.alloc(%9) {kDiscSymbolicDimAttr = [@S3, @C1]} : memref<?x1xf32, #gpu.address_space<global>>
  %alloc_9 = memref.alloc() : memref<1xf32, #gpu.address_space<global>>
  scf.if %12 {
    %reinterpret_cast_13 = memref.reinterpret_cast %alloc to offset: [0], sizes: [%4, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S4, @C4]} : memref<?x4xf32, #gpu.address_space<global>> to memref<?x4xf32, #gpu.address_space<global>>
    %reinterpret_cast_14 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [%4, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S4, @C4]} : memref<?x4xf32, #gpu.address_space<global>> to memref<?x4xf32, #gpu.address_space<global>>
    %reinterpret_cast_15 = memref.reinterpret_cast %alloc_7 to offset: [0], sizes: [%4, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S4, @C4]} : memref<?x4xf32, #gpu.address_space<global>> to memref<?x4xf32, #gpu.address_space<global>>
    %reinterpret_cast_16 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [%9, 1], strides: [1, 1] {kDiscSymbolicDimAttr = [@S5, @C1]} : memref<?x1xf32, #gpu.address_space<global>> to memref<?x1xf32, #gpu.address_space<global>>
    %16 = arith.cmpi slt, %9, %c1 : index
    scf.if %16 {
      "lmhlo.fusion"() ({
        "lmhlo.constant"(%alloc_4) {disc.device = "gpu", value = dense<0.000000e+00> : tensor<f32>} : (memref<f32, #gpu.address_space<global>>) -> ()
        "lmhlo.multiply"(%reinterpret_cast_13, %reinterpret_cast_14, %reinterpret_cast_15) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_reshape"(%reinterpret_cast_15, %alloca_3, %reinterpret_cast_16) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xi32>, memref<?x1xf32, #gpu.address_space<global>>) -> ()
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %18 = "disc_shape.delinearize"(%arg1, %c1) : (index, index) -> index
          %19 = memref.load %alloc_4[] : memref<f32, #gpu.address_space<global>>
          memref.store %19, %alloc_9[%18] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %17 = arith.ceildivui %9, %c32 : index
        scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%17, %c512) step (%c1, %c1) {
          %18 = memref.load %alloc_4[] : memref<f32, #gpu.address_space<global>>
          %19 = arith.cmpi ult, %arg2, %c1 : index
          %20 = scf.if %19 -> (f32) {
            %21 = scf.for %arg3 = %c0 to %c32 step %c1 iter_args(%arg4 = %18) -> (f32) {
              %22 = arith.muli %arg1, %c32 : index
              %23 = arith.addi %22, %arg3 : index
              %24 = arith.cmpi slt, %23, %9 : index
              %25 = scf.if %24 -> (f32) {
                %26 = memref.load %reinterpret_cast_16[%23, %arg2] : memref<?x1xf32, #gpu.address_space<global>>
                %27 = arith.addf %arg4, %26 : f32
                scf.yield %27 : f32
              } else {
                scf.yield %arg4 : f32
              }
              scf.yield %25 : f32
            }
            scf.yield %21 : f32
          } else {
            scf.yield %18 : f32
          }
          scf.if %19 {
            %21 = memref.atomic_rmw addf %20, %alloc_9[%arg2] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXthread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        "lmhlo.constant"(%alloc_4) {disc.device = "gpu", value = dense<0.000000e+00> : tensor<f32>} : (memref<f32, #gpu.address_space<global>>) -> ()
        "lmhlo.multiply"(%reinterpret_cast_13, %reinterpret_cast_14, %reinterpret_cast_15) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_reshape"(%reinterpret_cast_15, %alloca_3, %reinterpret_cast_16) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xi32>, memref<?x1xf32, #gpu.address_space<global>>) -> ()
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %18 = "disc_shape.delinearize"(%arg1, %c1) : (index, index) -> index
          %19 = memref.load %alloc_4[] : memref<f32, #gpu.address_space<global>>
          memref.store %19, %alloc_9[%18] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %17 = arith.ceildivui %9, %c512 : index
        scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%17, %c256) step (%c1, %c1) {
          %18 = memref.load %alloc_4[] : memref<f32, #gpu.address_space<global>>
          %19 = arith.divui %arg2, %c32 : index
          %20 = arith.remui %arg2, %c32 : index
          %21 = arith.muli %20, %c8 : index
          %22 = arith.addi %19, %21 : index
          %alloc_17 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
          %23 = arith.cmpi ult, %20, %c1 : index
          %24 = scf.if %23 -> (f32) {
            %30 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %18) -> (f32) {
              %31 = arith.muli %arg1, %c8 : index
              %32 = arith.addi %19, %31 : index
              %33 = arith.muli %32, %c64 : index
              %34 = arith.addi %arg3, %33 : index
              %35 = arith.cmpi slt, %34, %9 : index
              %36 = scf.if %35 -> (f32) {
                %37 = memref.load %reinterpret_cast_16[%34, %20] : memref<?x1xf32, #gpu.address_space<global>>
                %38 = arith.addf %arg4, %37 : f32
                scf.yield %38 : f32
              } else {
                scf.yield %arg4 : f32
              }
              scf.yield %36 : f32
            }
            scf.yield %30 : f32
          } else {
            scf.yield %18 : f32
          }
          memref.store %24, %alloc_17[%22] : memref<256xf32, #gpu.address_space<workgroup>>
          gpu.barrier
          %25 = arith.cmpi slt, %19, %c4 : index
          scf.if %25 {
            %30 = arith.addi %22, %c4 : index
            %31 = memref.load %alloc_17[%22] : memref<256xf32, #gpu.address_space<workgroup>>
            %32 = memref.load %alloc_17[%30] : memref<256xf32, #gpu.address_space<workgroup>>
            %33 = arith.addf %31, %32 : f32
            memref.store %33, %alloc_17[%22] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %26 = arith.cmpi slt, %19, %c2 : index
          scf.if %26 {
            %30 = arith.addi %22, %c2 : index
            %31 = memref.load %alloc_17[%22] : memref<256xf32, #gpu.address_space<workgroup>>
            %32 = memref.load %alloc_17[%30] : memref<256xf32, #gpu.address_space<workgroup>>
            %33 = arith.addf %31, %32 : f32
            memref.store %33, %alloc_17[%22] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %27 = arith.cmpi slt, %19, %c1 : index
          scf.if %27 {
            %30 = arith.addi %22, %c1 : index
            %31 = memref.load %alloc_17[%22] : memref<256xf32, #gpu.address_space<workgroup>>
            %32 = memref.load %alloc_17[%30] : memref<256xf32, #gpu.address_space<workgroup>>
            %33 = arith.addf %31, %32 : f32
            memref.store %33, %alloc_17[%22] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %28 = arith.cmpi eq, %19, %c0 : index
          %29 = arith.andi %28, %23 : i1
          scf.if %29 {
            %30 = memref.load %alloc_17[%22] : memref<256xf32, #gpu.address_space<workgroup>>
            %31 = memref.atomic_rmw addf %30, %alloc_9[%20] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXblock_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
    }
  } else {
    %16 = arith.cmpi slt, %9, %c1 : index
    scf.if %16 {
      "lmhlo.fusion"() ({
        "lmhlo.constant"(%alloc_4) {disc.device = "gpu", value = dense<0.000000e+00> : tensor<f32>} : (memref<f32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_broadcast_in_dim"(%alloc, %alloca, %alloc_5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xindex>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_broadcast_in_dim"(%alloc_2, %alloca, %alloc_6) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xindex>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.multiply"(%alloc_5, %alloc_6, %alloc_7) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_reshape"(%alloc_7, %alloca_3, %alloc_8) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xi32>, memref<?x1xf32, #gpu.address_space<global>>) -> ()
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %18 = "disc_shape.delinearize"(%arg1, %c1) : (index, index) -> index
          %19 = memref.load %alloc_4[] : memref<f32, #gpu.address_space<global>>
          memref.store %19, %alloc_9[%18] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %17 = arith.ceildivui %9, %c32 : index
        scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%17, %c512) step (%c1, %c1) {
          %18 = memref.load %alloc_4[] : memref<f32, #gpu.address_space<global>>
          %19 = arith.cmpi ult, %arg2, %c1 : index
          %20 = scf.if %19 -> (f32) {
            %21 = scf.for %arg3 = %c0 to %c32 step %c1 iter_args(%arg4 = %18) -> (f32) {
              %22 = arith.muli %arg1, %c32 : index
              %23 = arith.addi %22, %arg3 : index
              %24 = arith.cmpi slt, %23, %9 : index
              %25 = scf.if %24 -> (f32) {
                %26 = memref.load %alloc_8[%23, %arg2] : memref<?x1xf32, #gpu.address_space<global>>
                %27 = arith.addf %arg4, %26 : f32
                scf.yield %27 : f32
              } else {
                scf.yield %arg4 : f32
              }
              scf.yield %25 : f32
            }
            scf.yield %21 : f32
          } else {
            scf.yield %18 : f32
          }
          scf.if %19 {
            %21 = memref.atomic_rmw addf %20, %alloc_9[%arg2] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "thread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        "lmhlo.constant"(%alloc_4) {disc.device = "gpu", value = dense<0.000000e+00> : tensor<f32>} : (memref<f32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_broadcast_in_dim"(%alloc, %alloca, %alloc_5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xindex>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_broadcast_in_dim"(%alloc_2, %alloca, %alloc_6) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xindex>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.multiply"(%alloc_5, %alloc_6, %alloc_7) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
        "lmhlo.dynamic_reshape"(%alloc_7, %alloca_3, %alloc_8) {disc.device = "gpu"} : (memref<?x4xf32, #gpu.address_space<global>>, memref<2xi32>, memref<?x1xf32, #gpu.address_space<global>>) -> ()
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %18 = "disc_shape.delinearize"(%arg1, %c1) : (index, index) -> index
          %19 = memref.load %alloc_4[] : memref<f32, #gpu.address_space<global>>
          memref.store %19, %alloc_9[%18] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %17 = arith.ceildivui %9, %c512 : index
        scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%17, %c256) step (%c1, %c1) {
          %18 = memref.load %alloc_4[] : memref<f32, #gpu.address_space<global>>
          %19 = arith.divui %arg2, %c32 : index
          %20 = arith.remui %arg2, %c32 : index
          %21 = arith.muli %20, %c8 : index
          %22 = arith.addi %19, %21 : index
          %alloc_13 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
          %23 = arith.cmpi ult, %20, %c1 : index
          %24 = scf.if %23 -> (f32) {
            %30 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %18) -> (f32) {
              %31 = arith.muli %arg1, %c8 : index
              %32 = arith.addi %19, %31 : index
              %33 = arith.muli %32, %c64 : index
              %34 = arith.addi %arg3, %33 : index
              %35 = arith.cmpi slt, %34, %9 : index
              %36 = scf.if %35 -> (f32) {
                %37 = memref.load %alloc_8[%34, %20] : memref<?x1xf32, #gpu.address_space<global>>
                %38 = arith.addf %arg4, %37 : f32
                scf.yield %38 : f32
              } else {
                scf.yield %arg4 : f32
              }
              scf.yield %36 : f32
            }
            scf.yield %30 : f32
          } else {
            scf.yield %18 : f32
          }
          memref.store %24, %alloc_13[%22] : memref<256xf32, #gpu.address_space<workgroup>>
          gpu.barrier
          %25 = arith.cmpi slt, %19, %c4 : index
          scf.if %25 {
            %30 = arith.addi %22, %c4 : index
            %31 = memref.load %alloc_13[%22] : memref<256xf32, #gpu.address_space<workgroup>>
            %32 = memref.load %alloc_13[%30] : memref<256xf32, #gpu.address_space<workgroup>>
            %33 = arith.addf %31, %32 : f32
            memref.store %33, %alloc_13[%22] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %26 = arith.cmpi slt, %19, %c2 : index
          scf.if %26 {
            %30 = arith.addi %22, %c2 : index
            %31 = memref.load %alloc_13[%22] : memref<256xf32, #gpu.address_space<workgroup>>
            %32 = memref.load %alloc_13[%30] : memref<256xf32, #gpu.address_space<workgroup>>
            %33 = arith.addf %31, %32 : f32
            memref.store %33, %alloc_13[%22] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %27 = arith.cmpi slt, %19, %c1 : index
          scf.if %27 {
            %30 = arith.addi %22, %c1 : index
            %31 = memref.load %alloc_13[%22] : memref<256xf32, #gpu.address_space<workgroup>>
            %32 = memref.load %alloc_13[%30] : memref<256xf32, #gpu.address_space<workgroup>>
            %33 = arith.addf %31, %32 : f32
            memref.store %33, %alloc_13[%22] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %28 = arith.cmpi eq, %19, %c0 : index
          %29 = arith.andi %28, %23 : i1
          scf.if %29 {
            %30 = memref.load %alloc_13[%22] : memref<256xf32, #gpu.address_space<workgroup>>
            %31 = memref.atomic_rmw addf %30, %alloc_9[%20] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "block_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
    }
  }
  memref.dealloc %alloc_8 : memref<?x1xf32, #gpu.address_space<global>>
  memref.dealloc %alloc_7 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc_6 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc_5 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc_4 : memref<f32, #gpu.address_space<global>>
  memref.dealloc %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc : memref<?x4xf32, #gpu.address_space<global>>
  %13 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca_10 = memref.alloca() : memref<0xindex>
  %14 = "disc_ral.dispatch"(%arg0, %13, %alloc_9, %alloca_10) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1xf32, #gpu.address_space<global>>, memref<0xindex>) -> memref<f32, #gpu.address_space<global>>
  %reinterpret_cast_11 = memref.reinterpret_cast %14 to offset: [0], sizes: [], strides: [] : memref<f32, #gpu.address_space<global>> to memref<f32, #gpu.address_space<global>>
  memref.dealloc %alloc_9 : memref<1xf32, #gpu.address_space<global>>
  %alloc_12 = memref.alloc() : memref<f32>
  %15 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  "disc_ral.dispatch"(%arg0, %15, %reinterpret_cast_11, %alloc_12) {backend_config = "", call_target_name = "d2h", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<f32, #gpu.address_space<global>>, memref<f32>) -> ()
  "disc_ral.dispatch"(%arg0, %15) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  memref.dealloc %reinterpret_cast_11 : memref<f32, #gpu.address_space<global>>
  "disc_ral.dispatch"(%arg0, %c0, %alloc_12) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<f32>) -> ()
  return
}

// -----// IR Dump After InputInlineFusionPass (disc-input-inline-fusion) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c512 = arith.constant 512 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c4_i32 = arith.constant 4 : i32
  %c1_i32 = arith.constant 1 : i32
  %c4 = arith.constant 4 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
  %2 = "disc_ral.dispatch"(%arg0, %c1) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
  %dim = memref.dim %2, %c0 : memref<?x4xf32>
  %dim_0 = memref.dim %1, %c0 : memref<?x4xf32>
  %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %reinterpret_cast_1 = memref.reinterpret_cast %2 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %3 = arith.cmpi eq, %dim, %c1 : index
  %4 = arith.select %3, %dim_0, %dim : index
  %alloca = memref.alloca() {alignment = 64 : i64} : memref<2xindex>
  memref.store %4, %alloca[%c0] : memref<2xindex>
  memref.store %c4, %alloca[%c1] : memref<2xindex>
  %alloc = memref.alloc(%dim_0) {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %5 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast, %alloc) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  %alloc_2 = memref.alloc(%dim) {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %6 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  "disc_ral.dispatch"(%arg0, %6, %reinterpret_cast_1, %alloc_2) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  "disc_ral.dispatch"(%arg0, %6) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  %7 = arith.index_cast %4 : index to i32
  %8 = arith.muli %7, %c4_i32 : i32
  %alloca_3 = memref.alloca() {alignment = 64 : i64} : memref<2xi32>
  memref.store %8, %alloca_3[%c0] : memref<2xi32>
  memref.store %c1_i32, %alloca_3[%c1] : memref<2xi32>
  %9 = arith.index_cast %8 : i32 to index
  %10 = arith.cmpi eq, %dim_0, %4 : index
  %11 = arith.cmpi eq, %dim, %4 : index
  %12 = arith.andi %11, %10 : i1
  %alloc_4 = memref.alloc() : memref<f32, #gpu.address_space<global>>
  %alloc_5 = memref.alloc(%4) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %alloc_6 = memref.alloc(%4) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %alloc_7 = memref.alloc(%4) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %alloc_8 = memref.alloc(%9) {kDiscSymbolicDimAttr = [@S3, @C1]} : memref<?x1xf32, #gpu.address_space<global>>
  %alloc_9 = memref.alloc() : memref<1xf32, #gpu.address_space<global>>
  scf.if %12 {
    %reinterpret_cast_13 = memref.reinterpret_cast %alloc to offset: [0], sizes: [%4, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S4, @C4]} : memref<?x4xf32, #gpu.address_space<global>> to memref<?x4xf32, #gpu.address_space<global>>
    %reinterpret_cast_14 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [%4, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S4, @C4]} : memref<?x4xf32, #gpu.address_space<global>> to memref<?x4xf32, #gpu.address_space<global>>
    %16 = arith.cmpi slt, %9, %c1 : index
    scf.if %16 {
      "lmhlo.fusion"() ({
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %18 = "disc_shape.delinearize"(%arg1, %c1) : (index, index) -> index
          memref.store %cst, %alloc_9[%18] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %17 = arith.ceildivui %9, %c32 : index
        scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%17, %c512) step (%c1, %c1) {
          %18 = arith.cmpi ult, %arg2, %c1 : index
          %19 = scf.if %18 -> (f32) {
            %20 = scf.for %arg3 = %c0 to %c32 step %c1 iter_args(%arg4 = %cst) -> (f32) {
              %21 = arith.muli %arg1, %c32 : index
              %22 = arith.addi %21, %arg3 : index
              %23 = arith.cmpi slt, %22, %9 : index
              %24 = scf.if %23 -> (f32) {
                %25 = "disc_shape.linearize"(%22, %arg2, %9, %c1) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
                %26:2 = "disc_shape.delinearize"(%25, %4, %c4) : (index, index, index) -> (index, index)
                %27 = memref.load %reinterpret_cast_13[%26#0, %26#1] : memref<?x4xf32, #gpu.address_space<global>>
                %28 = memref.load %reinterpret_cast_14[%26#0, %26#1] : memref<?x4xf32, #gpu.address_space<global>>
                %29 = arith.mulf %27, %28 : f32
                %30 = arith.addf %arg4, %29 : f32
                scf.yield %30 : f32
              } else {
                scf.yield %arg4 : f32
              }
              scf.yield %24 : f32
            }
            scf.yield %20 : f32
          } else {
            scf.yield %cst : f32
          }
          scf.if %18 {
            %20 = memref.atomic_rmw addf %19, %alloc_9[%arg2] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXthread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %18 = "disc_shape.delinearize"(%arg1, %c1) : (index, index) -> index
          memref.store %cst, %alloc_9[%18] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %17 = arith.ceildivui %9, %c512 : index
        scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%17, %c256) step (%c1, %c1) {
          %18 = arith.divui %arg2, %c32 : index
          %19 = arith.remui %arg2, %c32 : index
          %20 = arith.muli %19, %c8 : index
          %21 = arith.addi %18, %20 : index
          %alloc_15 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
          %22 = arith.cmpi ult, %19, %c1 : index
          %23 = scf.if %22 -> (f32) {
            %29 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %cst) -> (f32) {
              %30 = arith.muli %arg1, %c8 : index
              %31 = arith.addi %18, %30 : index
              %32 = arith.muli %31, %c64 : index
              %33 = arith.addi %arg3, %32 : index
              %34 = arith.cmpi slt, %33, %9 : index
              %35 = scf.if %34 -> (f32) {
                %36 = "disc_shape.linearize"(%33, %19, %9, %c1) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
                %37:2 = "disc_shape.delinearize"(%36, %4, %c4) : (index, index, index) -> (index, index)
                %38 = memref.load %reinterpret_cast_13[%37#0, %37#1] : memref<?x4xf32, #gpu.address_space<global>>
                %39 = memref.load %reinterpret_cast_14[%37#0, %37#1] : memref<?x4xf32, #gpu.address_space<global>>
                %40 = arith.mulf %38, %39 : f32
                %41 = arith.addf %arg4, %40 : f32
                scf.yield %41 : f32
              } else {
                scf.yield %arg4 : f32
              }
              scf.yield %35 : f32
            }
            scf.yield %29 : f32
          } else {
            scf.yield %cst : f32
          }
          memref.store %23, %alloc_15[%21] : memref<256xf32, #gpu.address_space<workgroup>>
          gpu.barrier
          %24 = arith.cmpi slt, %18, %c4 : index
          scf.if %24 {
            %29 = arith.addi %21, %c4 : index
            %30 = memref.load %alloc_15[%21] : memref<256xf32, #gpu.address_space<workgroup>>
            %31 = memref.load %alloc_15[%29] : memref<256xf32, #gpu.address_space<workgroup>>
            %32 = arith.addf %30, %31 : f32
            memref.store %32, %alloc_15[%21] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %25 = arith.cmpi slt, %18, %c2 : index
          scf.if %25 {
            %29 = arith.addi %21, %c2 : index
            %30 = memref.load %alloc_15[%21] : memref<256xf32, #gpu.address_space<workgroup>>
            %31 = memref.load %alloc_15[%29] : memref<256xf32, #gpu.address_space<workgroup>>
            %32 = arith.addf %30, %31 : f32
            memref.store %32, %alloc_15[%21] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %26 = arith.cmpi slt, %18, %c1 : index
          scf.if %26 {
            %29 = arith.addi %21, %c1 : index
            %30 = memref.load %alloc_15[%21] : memref<256xf32, #gpu.address_space<workgroup>>
            %31 = memref.load %alloc_15[%29] : memref<256xf32, #gpu.address_space<workgroup>>
            %32 = arith.addf %30, %31 : f32
            memref.store %32, %alloc_15[%21] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %27 = arith.cmpi eq, %18, %c0 : index
          %28 = arith.andi %27, %22 : i1
          scf.if %28 {
            %29 = memref.load %alloc_15[%21] : memref<256xf32, #gpu.address_space<workgroup>>
            %30 = memref.atomic_rmw addf %29, %alloc_9[%19] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXblock_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
    }
  } else {
    %16 = arith.cmpi slt, %9, %c1 : index
    scf.if %16 {
      "lmhlo.fusion"() ({
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %18 = "disc_shape.delinearize"(%arg1, %c1) : (index, index) -> index
          memref.store %cst, %alloc_9[%18] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %17 = arith.ceildivui %9, %c32 : index
        scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%17, %c512) step (%c1, %c1) {
          %18 = arith.cmpi ult, %arg2, %c1 : index
          %19 = scf.if %18 -> (f32) {
            %20 = scf.for %arg3 = %c0 to %c32 step %c1 iter_args(%arg4 = %cst) -> (f32) {
              %21 = arith.muli %arg1, %c32 : index
              %22 = arith.addi %21, %arg3 : index
              %23 = arith.cmpi slt, %22, %9 : index
              %24 = scf.if %23 -> (f32) {
                %25 = "disc_shape.linearize"(%22, %arg2, %9, %c1) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
                %26:2 = "disc_shape.delinearize"(%25, %4, %c4) : (index, index, index) -> (index, index)
                %27 = arith.cmpi eq, %dim_0, %4 : index
                %28 = arith.select %27, %26#0, %c0 : index
                %29 = memref.load %alloc[%28, %26#1] : memref<?x4xf32, #gpu.address_space<global>>
                %30 = arith.cmpi eq, %dim, %4 : index
                %31 = arith.select %30, %26#0, %c0 : index
                %32 = memref.load %alloc_2[%31, %26#1] : memref<?x4xf32, #gpu.address_space<global>>
                %33 = arith.mulf %29, %32 : f32
                %34 = arith.addf %arg4, %33 : f32
                scf.yield %34 : f32
              } else {
                scf.yield %arg4 : f32
              }
              scf.yield %24 : f32
            }
            scf.yield %20 : f32
          } else {
            scf.yield %cst : f32
          }
          scf.if %18 {
            %20 = memref.atomic_rmw addf %19, %alloc_9[%arg2] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "thread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %18 = "disc_shape.delinearize"(%arg1, %c1) : (index, index) -> index
          memref.store %cst, %alloc_9[%18] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %17 = arith.ceildivui %9, %c512 : index
        scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%17, %c256) step (%c1, %c1) {
          %18 = arith.divui %arg2, %c32 : index
          %19 = arith.remui %arg2, %c32 : index
          %20 = arith.muli %19, %c8 : index
          %21 = arith.addi %18, %20 : index
          %alloc_13 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
          %22 = arith.cmpi ult, %19, %c1 : index
          %23 = scf.if %22 -> (f32) {
            %29 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %cst) -> (f32) {
              %30 = arith.muli %arg1, %c8 : index
              %31 = arith.addi %18, %30 : index
              %32 = arith.muli %31, %c64 : index
              %33 = arith.addi %arg3, %32 : index
              %34 = arith.cmpi slt, %33, %9 : index
              %35 = scf.if %34 -> (f32) {
                %36 = "disc_shape.linearize"(%33, %19, %9, %c1) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
                %37:2 = "disc_shape.delinearize"(%36, %4, %c4) : (index, index, index) -> (index, index)
                %38 = arith.cmpi eq, %dim_0, %4 : index
                %39 = arith.select %38, %37#0, %c0 : index
                %40 = memref.load %alloc[%39, %37#1] : memref<?x4xf32, #gpu.address_space<global>>
                %41 = arith.cmpi eq, %dim, %4 : index
                %42 = arith.select %41, %37#0, %c0 : index
                %43 = memref.load %alloc_2[%42, %37#1] : memref<?x4xf32, #gpu.address_space<global>>
                %44 = arith.mulf %40, %43 : f32
                %45 = arith.addf %arg4, %44 : f32
                scf.yield %45 : f32
              } else {
                scf.yield %arg4 : f32
              }
              scf.yield %35 : f32
            }
            scf.yield %29 : f32
          } else {
            scf.yield %cst : f32
          }
          memref.store %23, %alloc_13[%21] : memref<256xf32, #gpu.address_space<workgroup>>
          gpu.barrier
          %24 = arith.cmpi slt, %18, %c4 : index
          scf.if %24 {
            %29 = arith.addi %21, %c4 : index
            %30 = memref.load %alloc_13[%21] : memref<256xf32, #gpu.address_space<workgroup>>
            %31 = memref.load %alloc_13[%29] : memref<256xf32, #gpu.address_space<workgroup>>
            %32 = arith.addf %30, %31 : f32
            memref.store %32, %alloc_13[%21] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %25 = arith.cmpi slt, %18, %c2 : index
          scf.if %25 {
            %29 = arith.addi %21, %c2 : index
            %30 = memref.load %alloc_13[%21] : memref<256xf32, #gpu.address_space<workgroup>>
            %31 = memref.load %alloc_13[%29] : memref<256xf32, #gpu.address_space<workgroup>>
            %32 = arith.addf %30, %31 : f32
            memref.store %32, %alloc_13[%21] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %26 = arith.cmpi slt, %18, %c1 : index
          scf.if %26 {
            %29 = arith.addi %21, %c1 : index
            %30 = memref.load %alloc_13[%21] : memref<256xf32, #gpu.address_space<workgroup>>
            %31 = memref.load %alloc_13[%29] : memref<256xf32, #gpu.address_space<workgroup>>
            %32 = arith.addf %30, %31 : f32
            memref.store %32, %alloc_13[%21] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %27 = arith.cmpi eq, %18, %c0 : index
          %28 = arith.andi %27, %22 : i1
          scf.if %28 {
            %29 = memref.load %alloc_13[%21] : memref<256xf32, #gpu.address_space<workgroup>>
            %30 = memref.atomic_rmw addf %29, %alloc_9[%19] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "block_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
    }
  }
  memref.dealloc %alloc_8 : memref<?x1xf32, #gpu.address_space<global>>
  memref.dealloc %alloc_7 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc_6 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc_5 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc_4 : memref<f32, #gpu.address_space<global>>
  memref.dealloc %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc : memref<?x4xf32, #gpu.address_space<global>>
  %13 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca_10 = memref.alloca() : memref<0xindex>
  %14 = "disc_ral.dispatch"(%arg0, %13, %alloc_9, %alloca_10) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1xf32, #gpu.address_space<global>>, memref<0xindex>) -> memref<f32, #gpu.address_space<global>>
  %reinterpret_cast_11 = memref.reinterpret_cast %14 to offset: [0], sizes: [], strides: [] : memref<f32, #gpu.address_space<global>> to memref<f32, #gpu.address_space<global>>
  memref.dealloc %alloc_9 : memref<1xf32, #gpu.address_space<global>>
  %alloc_12 = memref.alloc() : memref<f32>
  %15 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  "disc_ral.dispatch"(%arg0, %15, %reinterpret_cast_11, %alloc_12) {backend_config = "", call_target_name = "d2h", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<f32, #gpu.address_space<global>>, memref<f32>) -> ()
  "disc_ral.dispatch"(%arg0, %15) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  memref.dealloc %reinterpret_cast_11 : memref<f32, #gpu.address_space<global>>
  "disc_ral.dispatch"(%arg0, %c0, %alloc_12) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<f32>) -> ()
  return
}

// -----// IR Dump After ArithExpandOps (arith-expand) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c512 = arith.constant 512 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c4_i32 = arith.constant 4 : i32
  %c1_i32 = arith.constant 1 : i32
  %c4 = arith.constant 4 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
  %2 = "disc_ral.dispatch"(%arg0, %c1) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
  %dim = memref.dim %2, %c0 : memref<?x4xf32>
  %dim_0 = memref.dim %1, %c0 : memref<?x4xf32>
  %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %reinterpret_cast_1 = memref.reinterpret_cast %2 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %3 = arith.cmpi eq, %dim, %c1 : index
  %4 = arith.select %3, %dim_0, %dim : index
  %alloca = memref.alloca() {alignment = 64 : i64} : memref<2xindex>
  memref.store %4, %alloca[%c0] : memref<2xindex>
  memref.store %c4, %alloca[%c1] : memref<2xindex>
  %alloc = memref.alloc(%dim_0) {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %5 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast, %alloc) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  %alloc_2 = memref.alloc(%dim) {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %6 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  "disc_ral.dispatch"(%arg0, %6, %reinterpret_cast_1, %alloc_2) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  "disc_ral.dispatch"(%arg0, %6) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  %7 = arith.index_cast %4 : index to i32
  %8 = arith.muli %7, %c4_i32 : i32
  %alloca_3 = memref.alloca() {alignment = 64 : i64} : memref<2xi32>
  memref.store %8, %alloca_3[%c0] : memref<2xi32>
  memref.store %c1_i32, %alloca_3[%c1] : memref<2xi32>
  %9 = arith.index_cast %8 : i32 to index
  %10 = arith.cmpi eq, %dim_0, %4 : index
  %11 = arith.cmpi eq, %dim, %4 : index
  %12 = arith.andi %11, %10 : i1
  %alloc_4 = memref.alloc() : memref<f32, #gpu.address_space<global>>
  %alloc_5 = memref.alloc(%4) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %alloc_6 = memref.alloc(%4) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %alloc_7 = memref.alloc(%4) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %alloc_8 = memref.alloc(%9) {kDiscSymbolicDimAttr = [@S3, @C1]} : memref<?x1xf32, #gpu.address_space<global>>
  %alloc_9 = memref.alloc() : memref<1xf32, #gpu.address_space<global>>
  scf.if %12 {
    %reinterpret_cast_13 = memref.reinterpret_cast %alloc to offset: [0], sizes: [%4, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S4, @C4]} : memref<?x4xf32, #gpu.address_space<global>> to memref<?x4xf32, #gpu.address_space<global>>
    %reinterpret_cast_14 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [%4, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S4, @C4]} : memref<?x4xf32, #gpu.address_space<global>> to memref<?x4xf32, #gpu.address_space<global>>
    %16 = arith.cmpi slt, %9, %c1 : index
    scf.if %16 {
      "lmhlo.fusion"() ({
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %22 = "disc_shape.delinearize"(%arg1, %c1) : (index, index) -> index
          memref.store %cst, %alloc_9[%22] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %c0_15 = arith.constant 0 : index
        %17 = arith.cmpi eq, %9, %c0_15 : index
        %c1_16 = arith.constant 1 : index
        %18 = arith.subi %9, %c1_16 : index
        %19 = arith.divui %18, %c32 : index
        %20 = arith.addi %19, %c1_16 : index
        %21 = arith.select %17, %c0_15, %20 : index
        scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%21, %c512) step (%c1, %c1) {
          %22 = arith.cmpi ult, %arg2, %c1 : index
          %23 = scf.if %22 -> (f32) {
            %24 = scf.for %arg3 = %c0 to %c32 step %c1 iter_args(%arg4 = %cst) -> (f32) {
              %25 = arith.muli %arg1, %c32 : index
              %26 = arith.addi %25, %arg3 : index
              %27 = arith.cmpi slt, %26, %9 : index
              %28 = scf.if %27 -> (f32) {
                %29 = "disc_shape.linearize"(%26, %arg2, %9, %c1) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
                %30:2 = "disc_shape.delinearize"(%29, %4, %c4) : (index, index, index) -> (index, index)
                %31 = memref.load %reinterpret_cast_13[%30#0, %30#1] : memref<?x4xf32, #gpu.address_space<global>>
                %32 = memref.load %reinterpret_cast_14[%30#0, %30#1] : memref<?x4xf32, #gpu.address_space<global>>
                %33 = arith.mulf %31, %32 : f32
                %34 = arith.addf %arg4, %33 : f32
                scf.yield %34 : f32
              } else {
                scf.yield %arg4 : f32
              }
              scf.yield %28 : f32
            }
            scf.yield %24 : f32
          } else {
            scf.yield %cst : f32
          }
          scf.if %22 {
            %24 = memref.atomic_rmw addf %23, %alloc_9[%arg2] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXthread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %22 = "disc_shape.delinearize"(%arg1, %c1) : (index, index) -> index
          memref.store %cst, %alloc_9[%22] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %c0_15 = arith.constant 0 : index
        %17 = arith.cmpi eq, %9, %c0_15 : index
        %c1_16 = arith.constant 1 : index
        %18 = arith.subi %9, %c1_16 : index
        %19 = arith.divui %18, %c512 : index
        %20 = arith.addi %19, %c1_16 : index
        %21 = arith.select %17, %c0_15, %20 : index
        scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%21, %c256) step (%c1, %c1) {
          %22 = arith.divui %arg2, %c32 : index
          %23 = arith.remui %arg2, %c32 : index
          %24 = arith.muli %23, %c8 : index
          %25 = arith.addi %22, %24 : index
          %alloc_17 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
          %26 = arith.cmpi ult, %23, %c1 : index
          %27 = scf.if %26 -> (f32) {
            %33 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %cst) -> (f32) {
              %34 = arith.muli %arg1, %c8 : index
              %35 = arith.addi %22, %34 : index
              %36 = arith.muli %35, %c64 : index
              %37 = arith.addi %arg3, %36 : index
              %38 = arith.cmpi slt, %37, %9 : index
              %39 = scf.if %38 -> (f32) {
                %40 = "disc_shape.linearize"(%37, %23, %9, %c1) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
                %41:2 = "disc_shape.delinearize"(%40, %4, %c4) : (index, index, index) -> (index, index)
                %42 = memref.load %reinterpret_cast_13[%41#0, %41#1] : memref<?x4xf32, #gpu.address_space<global>>
                %43 = memref.load %reinterpret_cast_14[%41#0, %41#1] : memref<?x4xf32, #gpu.address_space<global>>
                %44 = arith.mulf %42, %43 : f32
                %45 = arith.addf %arg4, %44 : f32
                scf.yield %45 : f32
              } else {
                scf.yield %arg4 : f32
              }
              scf.yield %39 : f32
            }
            scf.yield %33 : f32
          } else {
            scf.yield %cst : f32
          }
          memref.store %27, %alloc_17[%25] : memref<256xf32, #gpu.address_space<workgroup>>
          gpu.barrier
          %28 = arith.cmpi slt, %22, %c4 : index
          scf.if %28 {
            %33 = arith.addi %25, %c4 : index
            %34 = memref.load %alloc_17[%25] : memref<256xf32, #gpu.address_space<workgroup>>
            %35 = memref.load %alloc_17[%33] : memref<256xf32, #gpu.address_space<workgroup>>
            %36 = arith.addf %34, %35 : f32
            memref.store %36, %alloc_17[%25] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %29 = arith.cmpi slt, %22, %c2 : index
          scf.if %29 {
            %33 = arith.addi %25, %c2 : index
            %34 = memref.load %alloc_17[%25] : memref<256xf32, #gpu.address_space<workgroup>>
            %35 = memref.load %alloc_17[%33] : memref<256xf32, #gpu.address_space<workgroup>>
            %36 = arith.addf %34, %35 : f32
            memref.store %36, %alloc_17[%25] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %30 = arith.cmpi slt, %22, %c1 : index
          scf.if %30 {
            %33 = arith.addi %25, %c1 : index
            %34 = memref.load %alloc_17[%25] : memref<256xf32, #gpu.address_space<workgroup>>
            %35 = memref.load %alloc_17[%33] : memref<256xf32, #gpu.address_space<workgroup>>
            %36 = arith.addf %34, %35 : f32
            memref.store %36, %alloc_17[%25] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %31 = arith.cmpi eq, %22, %c0 : index
          %32 = arith.andi %31, %26 : i1
          scf.if %32 {
            %33 = memref.load %alloc_17[%25] : memref<256xf32, #gpu.address_space<workgroup>>
            %34 = memref.atomic_rmw addf %33, %alloc_9[%23] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXblock_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
    }
  } else {
    %16 = arith.cmpi slt, %9, %c1 : index
    scf.if %16 {
      "lmhlo.fusion"() ({
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %22 = "disc_shape.delinearize"(%arg1, %c1) : (index, index) -> index
          memref.store %cst, %alloc_9[%22] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %c0_13 = arith.constant 0 : index
        %17 = arith.cmpi eq, %9, %c0_13 : index
        %c1_14 = arith.constant 1 : index
        %18 = arith.subi %9, %c1_14 : index
        %19 = arith.divui %18, %c32 : index
        %20 = arith.addi %19, %c1_14 : index
        %21 = arith.select %17, %c0_13, %20 : index
        scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%21, %c512) step (%c1, %c1) {
          %22 = arith.cmpi ult, %arg2, %c1 : index
          %23 = scf.if %22 -> (f32) {
            %24 = scf.for %arg3 = %c0 to %c32 step %c1 iter_args(%arg4 = %cst) -> (f32) {
              %25 = arith.muli %arg1, %c32 : index
              %26 = arith.addi %25, %arg3 : index
              %27 = arith.cmpi slt, %26, %9 : index
              %28 = scf.if %27 -> (f32) {
                %29 = "disc_shape.linearize"(%26, %arg2, %9, %c1) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
                %30:2 = "disc_shape.delinearize"(%29, %4, %c4) : (index, index, index) -> (index, index)
                %31 = arith.cmpi eq, %dim_0, %4 : index
                %32 = arith.select %31, %30#0, %c0 : index
                %33 = memref.load %alloc[%32, %30#1] : memref<?x4xf32, #gpu.address_space<global>>
                %34 = arith.cmpi eq, %dim, %4 : index
                %35 = arith.select %34, %30#0, %c0 : index
                %36 = memref.load %alloc_2[%35, %30#1] : memref<?x4xf32, #gpu.address_space<global>>
                %37 = arith.mulf %33, %36 : f32
                %38 = arith.addf %arg4, %37 : f32
                scf.yield %38 : f32
              } else {
                scf.yield %arg4 : f32
              }
              scf.yield %28 : f32
            }
            scf.yield %24 : f32
          } else {
            scf.yield %cst : f32
          }
          scf.if %22 {
            %24 = memref.atomic_rmw addf %23, %alloc_9[%arg2] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "thread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %22 = "disc_shape.delinearize"(%arg1, %c1) : (index, index) -> index
          memref.store %cst, %alloc_9[%22] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %c0_13 = arith.constant 0 : index
        %17 = arith.cmpi eq, %9, %c0_13 : index
        %c1_14 = arith.constant 1 : index
        %18 = arith.subi %9, %c1_14 : index
        %19 = arith.divui %18, %c512 : index
        %20 = arith.addi %19, %c1_14 : index
        %21 = arith.select %17, %c0_13, %20 : index
        scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%21, %c256) step (%c1, %c1) {
          %22 = arith.divui %arg2, %c32 : index
          %23 = arith.remui %arg2, %c32 : index
          %24 = arith.muli %23, %c8 : index
          %25 = arith.addi %22, %24 : index
          %alloc_15 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
          %26 = arith.cmpi ult, %23, %c1 : index
          %27 = scf.if %26 -> (f32) {
            %33 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %cst) -> (f32) {
              %34 = arith.muli %arg1, %c8 : index
              %35 = arith.addi %22, %34 : index
              %36 = arith.muli %35, %c64 : index
              %37 = arith.addi %arg3, %36 : index
              %38 = arith.cmpi slt, %37, %9 : index
              %39 = scf.if %38 -> (f32) {
                %40 = "disc_shape.linearize"(%37, %23, %9, %c1) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
                %41:2 = "disc_shape.delinearize"(%40, %4, %c4) : (index, index, index) -> (index, index)
                %42 = arith.cmpi eq, %dim_0, %4 : index
                %43 = arith.select %42, %41#0, %c0 : index
                %44 = memref.load %alloc[%43, %41#1] : memref<?x4xf32, #gpu.address_space<global>>
                %45 = arith.cmpi eq, %dim, %4 : index
                %46 = arith.select %45, %41#0, %c0 : index
                %47 = memref.load %alloc_2[%46, %41#1] : memref<?x4xf32, #gpu.address_space<global>>
                %48 = arith.mulf %44, %47 : f32
                %49 = arith.addf %arg4, %48 : f32
                scf.yield %49 : f32
              } else {
                scf.yield %arg4 : f32
              }
              scf.yield %39 : f32
            }
            scf.yield %33 : f32
          } else {
            scf.yield %cst : f32
          }
          memref.store %27, %alloc_15[%25] : memref<256xf32, #gpu.address_space<workgroup>>
          gpu.barrier
          %28 = arith.cmpi slt, %22, %c4 : index
          scf.if %28 {
            %33 = arith.addi %25, %c4 : index
            %34 = memref.load %alloc_15[%25] : memref<256xf32, #gpu.address_space<workgroup>>
            %35 = memref.load %alloc_15[%33] : memref<256xf32, #gpu.address_space<workgroup>>
            %36 = arith.addf %34, %35 : f32
            memref.store %36, %alloc_15[%25] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %29 = arith.cmpi slt, %22, %c2 : index
          scf.if %29 {
            %33 = arith.addi %25, %c2 : index
            %34 = memref.load %alloc_15[%25] : memref<256xf32, #gpu.address_space<workgroup>>
            %35 = memref.load %alloc_15[%33] : memref<256xf32, #gpu.address_space<workgroup>>
            %36 = arith.addf %34, %35 : f32
            memref.store %36, %alloc_15[%25] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %30 = arith.cmpi slt, %22, %c1 : index
          scf.if %30 {
            %33 = arith.addi %25, %c1 : index
            %34 = memref.load %alloc_15[%25] : memref<256xf32, #gpu.address_space<workgroup>>
            %35 = memref.load %alloc_15[%33] : memref<256xf32, #gpu.address_space<workgroup>>
            %36 = arith.addf %34, %35 : f32
            memref.store %36, %alloc_15[%25] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %31 = arith.cmpi eq, %22, %c0 : index
          %32 = arith.andi %31, %26 : i1
          scf.if %32 {
            %33 = memref.load %alloc_15[%25] : memref<256xf32, #gpu.address_space<workgroup>>
            %34 = memref.atomic_rmw addf %33, %alloc_9[%23] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "block_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
    }
  }
  memref.dealloc %alloc_8 : memref<?x1xf32, #gpu.address_space<global>>
  memref.dealloc %alloc_7 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc_6 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc_5 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc_4 : memref<f32, #gpu.address_space<global>>
  memref.dealloc %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc : memref<?x4xf32, #gpu.address_space<global>>
  %13 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca_10 = memref.alloca() : memref<0xindex>
  %14 = "disc_ral.dispatch"(%arg0, %13, %alloc_9, %alloca_10) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1xf32, #gpu.address_space<global>>, memref<0xindex>) -> memref<f32, #gpu.address_space<global>>
  %reinterpret_cast_11 = memref.reinterpret_cast %14 to offset: [0], sizes: [], strides: [] : memref<f32, #gpu.address_space<global>> to memref<f32, #gpu.address_space<global>>
  memref.dealloc %alloc_9 : memref<1xf32, #gpu.address_space<global>>
  %alloc_12 = memref.alloc() : memref<f32>
  %15 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  "disc_ral.dispatch"(%arg0, %15, %reinterpret_cast_11, %alloc_12) {backend_config = "", call_target_name = "d2h", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<f32, #gpu.address_space<global>>, memref<f32>) -> ()
  "disc_ral.dispatch"(%arg0, %15) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  memref.dealloc %reinterpret_cast_11 : memref<f32, #gpu.address_space<global>>
  "disc_ral.dispatch"(%arg0, %c0, %alloc_12) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<f32>) -> ()
  return
}

// -----// IR Dump After FoldMemRefAliasOps (fold-memref-alias-ops) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c512 = arith.constant 512 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c4_i32 = arith.constant 4 : i32
  %c1_i32 = arith.constant 1 : i32
  %c4 = arith.constant 4 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
  %2 = "disc_ral.dispatch"(%arg0, %c1) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
  %dim = memref.dim %2, %c0 : memref<?x4xf32>
  %dim_0 = memref.dim %1, %c0 : memref<?x4xf32>
  %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %reinterpret_cast_1 = memref.reinterpret_cast %2 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %3 = arith.cmpi eq, %dim, %c1 : index
  %4 = arith.select %3, %dim_0, %dim : index
  %alloca = memref.alloca() {alignment = 64 : i64} : memref<2xindex>
  memref.store %4, %alloca[%c0] : memref<2xindex>
  memref.store %c4, %alloca[%c1] : memref<2xindex>
  %alloc = memref.alloc(%dim_0) {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %5 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast, %alloc) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  %alloc_2 = memref.alloc(%dim) {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %6 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  "disc_ral.dispatch"(%arg0, %6, %reinterpret_cast_1, %alloc_2) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  "disc_ral.dispatch"(%arg0, %6) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  %7 = arith.index_cast %4 : index to i32
  %8 = arith.muli %7, %c4_i32 : i32
  %alloca_3 = memref.alloca() {alignment = 64 : i64} : memref<2xi32>
  memref.store %8, %alloca_3[%c0] : memref<2xi32>
  memref.store %c1_i32, %alloca_3[%c1] : memref<2xi32>
  %9 = arith.index_cast %8 : i32 to index
  %10 = arith.cmpi eq, %dim_0, %4 : index
  %11 = arith.cmpi eq, %dim, %4 : index
  %12 = arith.andi %11, %10 : i1
  %alloc_4 = memref.alloc() : memref<f32, #gpu.address_space<global>>
  %alloc_5 = memref.alloc(%4) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %alloc_6 = memref.alloc(%4) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %alloc_7 = memref.alloc(%4) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %alloc_8 = memref.alloc(%9) {kDiscSymbolicDimAttr = [@S3, @C1]} : memref<?x1xf32, #gpu.address_space<global>>
  %alloc_9 = memref.alloc() : memref<1xf32, #gpu.address_space<global>>
  scf.if %12 {
    %reinterpret_cast_13 = memref.reinterpret_cast %alloc to offset: [0], sizes: [%4, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S4, @C4]} : memref<?x4xf32, #gpu.address_space<global>> to memref<?x4xf32, #gpu.address_space<global>>
    %reinterpret_cast_14 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [%4, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S4, @C4]} : memref<?x4xf32, #gpu.address_space<global>> to memref<?x4xf32, #gpu.address_space<global>>
    %16 = arith.cmpi slt, %9, %c1 : index
    scf.if %16 {
      "lmhlo.fusion"() ({
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %22 = "disc_shape.delinearize"(%arg1, %c1) : (index, index) -> index
          memref.store %cst, %alloc_9[%22] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %17 = arith.cmpi eq, %9, %c0 : index
        %18 = arith.subi %9, %c1 : index
        %19 = arith.divui %18, %c32 : index
        %20 = arith.addi %19, %c1 : index
        %21 = arith.select %17, %c0, %20 : index
        scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%21, %c512) step (%c1, %c1) {
          %22 = arith.cmpi ult, %arg2, %c1 : index
          %23 = scf.if %22 -> (f32) {
            %24 = scf.for %arg3 = %c0 to %c32 step %c1 iter_args(%arg4 = %cst) -> (f32) {
              %25 = arith.muli %arg1, %c32 : index
              %26 = arith.addi %25, %arg3 : index
              %27 = arith.cmpi slt, %26, %9 : index
              %28 = scf.if %27 -> (f32) {
                %29 = "disc_shape.linearize"(%26, %arg2, %9, %c1) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
                %30:2 = "disc_shape.delinearize"(%29, %4, %c4) : (index, index, index) -> (index, index)
                %31 = memref.load %reinterpret_cast_13[%30#0, %30#1] : memref<?x4xf32, #gpu.address_space<global>>
                %32 = memref.load %reinterpret_cast_14[%30#0, %30#1] : memref<?x4xf32, #gpu.address_space<global>>
                %33 = arith.mulf %31, %32 : f32
                %34 = arith.addf %arg4, %33 : f32
                scf.yield %34 : f32
              } else {
                scf.yield %arg4 : f32
              }
              scf.yield %28 : f32
            }
            scf.yield %24 : f32
          } else {
            scf.yield %cst : f32
          }
          scf.if %22 {
            %24 = memref.atomic_rmw addf %23, %alloc_9[%arg2] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXthread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %22 = "disc_shape.delinearize"(%arg1, %c1) : (index, index) -> index
          memref.store %cst, %alloc_9[%22] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %17 = arith.cmpi eq, %9, %c0 : index
        %18 = arith.subi %9, %c1 : index
        %19 = arith.divui %18, %c512 : index
        %20 = arith.addi %19, %c1 : index
        %21 = arith.select %17, %c0, %20 : index
        scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%21, %c256) step (%c1, %c1) {
          %22 = arith.divui %arg2, %c32 : index
          %23 = arith.remui %arg2, %c32 : index
          %24 = arith.muli %23, %c8 : index
          %25 = arith.addi %22, %24 : index
          %alloc_15 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
          %26 = arith.cmpi ult, %23, %c1 : index
          %27 = scf.if %26 -> (f32) {
            %33 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %cst) -> (f32) {
              %34 = arith.muli %arg1, %c8 : index
              %35 = arith.addi %22, %34 : index
              %36 = arith.muli %35, %c64 : index
              %37 = arith.addi %arg3, %36 : index
              %38 = arith.cmpi slt, %37, %9 : index
              %39 = scf.if %38 -> (f32) {
                %40 = "disc_shape.linearize"(%37, %23, %9, %c1) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
                %41:2 = "disc_shape.delinearize"(%40, %4, %c4) : (index, index, index) -> (index, index)
                %42 = memref.load %reinterpret_cast_13[%41#0, %41#1] : memref<?x4xf32, #gpu.address_space<global>>
                %43 = memref.load %reinterpret_cast_14[%41#0, %41#1] : memref<?x4xf32, #gpu.address_space<global>>
                %44 = arith.mulf %42, %43 : f32
                %45 = arith.addf %arg4, %44 : f32
                scf.yield %45 : f32
              } else {
                scf.yield %arg4 : f32
              }
              scf.yield %39 : f32
            }
            scf.yield %33 : f32
          } else {
            scf.yield %cst : f32
          }
          memref.store %27, %alloc_15[%25] : memref<256xf32, #gpu.address_space<workgroup>>
          gpu.barrier
          %28 = arith.cmpi slt, %22, %c4 : index
          scf.if %28 {
            %33 = arith.addi %25, %c4 : index
            %34 = memref.load %alloc_15[%25] : memref<256xf32, #gpu.address_space<workgroup>>
            %35 = memref.load %alloc_15[%33] : memref<256xf32, #gpu.address_space<workgroup>>
            %36 = arith.addf %34, %35 : f32
            memref.store %36, %alloc_15[%25] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %29 = arith.cmpi slt, %22, %c2 : index
          scf.if %29 {
            %33 = arith.addi %25, %c2 : index
            %34 = memref.load %alloc_15[%25] : memref<256xf32, #gpu.address_space<workgroup>>
            %35 = memref.load %alloc_15[%33] : memref<256xf32, #gpu.address_space<workgroup>>
            %36 = arith.addf %34, %35 : f32
            memref.store %36, %alloc_15[%25] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %30 = arith.cmpi slt, %22, %c1 : index
          scf.if %30 {
            %33 = arith.addi %25, %c1 : index
            %34 = memref.load %alloc_15[%25] : memref<256xf32, #gpu.address_space<workgroup>>
            %35 = memref.load %alloc_15[%33] : memref<256xf32, #gpu.address_space<workgroup>>
            %36 = arith.addf %34, %35 : f32
            memref.store %36, %alloc_15[%25] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %31 = arith.cmpi eq, %22, %c0 : index
          %32 = arith.andi %31, %26 : i1
          scf.if %32 {
            %33 = memref.load %alloc_15[%25] : memref<256xf32, #gpu.address_space<workgroup>>
            %34 = memref.atomic_rmw addf %33, %alloc_9[%23] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXblock_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
    }
  } else {
    %16 = arith.cmpi slt, %9, %c1 : index
    scf.if %16 {
      "lmhlo.fusion"() ({
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %22 = "disc_shape.delinearize"(%arg1, %c1) : (index, index) -> index
          memref.store %cst, %alloc_9[%22] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %17 = arith.cmpi eq, %9, %c0 : index
        %18 = arith.subi %9, %c1 : index
        %19 = arith.divui %18, %c32 : index
        %20 = arith.addi %19, %c1 : index
        %21 = arith.select %17, %c0, %20 : index
        scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%21, %c512) step (%c1, %c1) {
          %22 = arith.cmpi ult, %arg2, %c1 : index
          %23 = scf.if %22 -> (f32) {
            %24 = scf.for %arg3 = %c0 to %c32 step %c1 iter_args(%arg4 = %cst) -> (f32) {
              %25 = arith.muli %arg1, %c32 : index
              %26 = arith.addi %25, %arg3 : index
              %27 = arith.cmpi slt, %26, %9 : index
              %28 = scf.if %27 -> (f32) {
                %29 = "disc_shape.linearize"(%26, %arg2, %9, %c1) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
                %30:2 = "disc_shape.delinearize"(%29, %4, %c4) : (index, index, index) -> (index, index)
                %31 = arith.cmpi eq, %dim_0, %4 : index
                %32 = arith.select %31, %30#0, %c0 : index
                %33 = memref.load %alloc[%32, %30#1] : memref<?x4xf32, #gpu.address_space<global>>
                %34 = arith.cmpi eq, %dim, %4 : index
                %35 = arith.select %34, %30#0, %c0 : index
                %36 = memref.load %alloc_2[%35, %30#1] : memref<?x4xf32, #gpu.address_space<global>>
                %37 = arith.mulf %33, %36 : f32
                %38 = arith.addf %arg4, %37 : f32
                scf.yield %38 : f32
              } else {
                scf.yield %arg4 : f32
              }
              scf.yield %28 : f32
            }
            scf.yield %24 : f32
          } else {
            scf.yield %cst : f32
          }
          scf.if %22 {
            %24 = memref.atomic_rmw addf %23, %alloc_9[%arg2] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "thread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %22 = "disc_shape.delinearize"(%arg1, %c1) : (index, index) -> index
          memref.store %cst, %alloc_9[%22] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %17 = arith.cmpi eq, %9, %c0 : index
        %18 = arith.subi %9, %c1 : index
        %19 = arith.divui %18, %c512 : index
        %20 = arith.addi %19, %c1 : index
        %21 = arith.select %17, %c0, %20 : index
        scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%21, %c256) step (%c1, %c1) {
          %22 = arith.divui %arg2, %c32 : index
          %23 = arith.remui %arg2, %c32 : index
          %24 = arith.muli %23, %c8 : index
          %25 = arith.addi %22, %24 : index
          %alloc_13 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
          %26 = arith.cmpi ult, %23, %c1 : index
          %27 = scf.if %26 -> (f32) {
            %33 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %cst) -> (f32) {
              %34 = arith.muli %arg1, %c8 : index
              %35 = arith.addi %22, %34 : index
              %36 = arith.muli %35, %c64 : index
              %37 = arith.addi %arg3, %36 : index
              %38 = arith.cmpi slt, %37, %9 : index
              %39 = scf.if %38 -> (f32) {
                %40 = "disc_shape.linearize"(%37, %23, %9, %c1) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
                %41:2 = "disc_shape.delinearize"(%40, %4, %c4) : (index, index, index) -> (index, index)
                %42 = arith.cmpi eq, %dim_0, %4 : index
                %43 = arith.select %42, %41#0, %c0 : index
                %44 = memref.load %alloc[%43, %41#1] : memref<?x4xf32, #gpu.address_space<global>>
                %45 = arith.cmpi eq, %dim, %4 : index
                %46 = arith.select %45, %41#0, %c0 : index
                %47 = memref.load %alloc_2[%46, %41#1] : memref<?x4xf32, #gpu.address_space<global>>
                %48 = arith.mulf %44, %47 : f32
                %49 = arith.addf %arg4, %48 : f32
                scf.yield %49 : f32
              } else {
                scf.yield %arg4 : f32
              }
              scf.yield %39 : f32
            }
            scf.yield %33 : f32
          } else {
            scf.yield %cst : f32
          }
          memref.store %27, %alloc_13[%25] : memref<256xf32, #gpu.address_space<workgroup>>
          gpu.barrier
          %28 = arith.cmpi slt, %22, %c4 : index
          scf.if %28 {
            %33 = arith.addi %25, %c4 : index
            %34 = memref.load %alloc_13[%25] : memref<256xf32, #gpu.address_space<workgroup>>
            %35 = memref.load %alloc_13[%33] : memref<256xf32, #gpu.address_space<workgroup>>
            %36 = arith.addf %34, %35 : f32
            memref.store %36, %alloc_13[%25] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %29 = arith.cmpi slt, %22, %c2 : index
          scf.if %29 {
            %33 = arith.addi %25, %c2 : index
            %34 = memref.load %alloc_13[%25] : memref<256xf32, #gpu.address_space<workgroup>>
            %35 = memref.load %alloc_13[%33] : memref<256xf32, #gpu.address_space<workgroup>>
            %36 = arith.addf %34, %35 : f32
            memref.store %36, %alloc_13[%25] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %30 = arith.cmpi slt, %22, %c1 : index
          scf.if %30 {
            %33 = arith.addi %25, %c1 : index
            %34 = memref.load %alloc_13[%25] : memref<256xf32, #gpu.address_space<workgroup>>
            %35 = memref.load %alloc_13[%33] : memref<256xf32, #gpu.address_space<workgroup>>
            %36 = arith.addf %34, %35 : f32
            memref.store %36, %alloc_13[%25] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %31 = arith.cmpi eq, %22, %c0 : index
          %32 = arith.andi %31, %26 : i1
          scf.if %32 {
            %33 = memref.load %alloc_13[%25] : memref<256xf32, #gpu.address_space<workgroup>>
            %34 = memref.atomic_rmw addf %33, %alloc_9[%23] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "block_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
    }
  }
  memref.dealloc %alloc_8 : memref<?x1xf32, #gpu.address_space<global>>
  memref.dealloc %alloc_7 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc_6 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc_5 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc_4 : memref<f32, #gpu.address_space<global>>
  memref.dealloc %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc : memref<?x4xf32, #gpu.address_space<global>>
  %13 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca_10 = memref.alloca() : memref<0xindex>
  %14 = "disc_ral.dispatch"(%arg0, %13, %alloc_9, %alloca_10) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1xf32, #gpu.address_space<global>>, memref<0xindex>) -> memref<f32, #gpu.address_space<global>>
  %reinterpret_cast_11 = memref.reinterpret_cast %14 to offset: [0], sizes: [], strides: [] : memref<f32, #gpu.address_space<global>> to memref<f32, #gpu.address_space<global>>
  memref.dealloc %alloc_9 : memref<1xf32, #gpu.address_space<global>>
  %alloc_12 = memref.alloc() : memref<f32>
  %15 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  "disc_ral.dispatch"(%arg0, %15, %reinterpret_cast_11, %alloc_12) {backend_config = "", call_target_name = "d2h", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<f32, #gpu.address_space<global>>, memref<f32>) -> ()
  "disc_ral.dispatch"(%arg0, %15) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  memref.dealloc %reinterpret_cast_11 : memref<f32, #gpu.address_space<global>>
  "disc_ral.dispatch"(%arg0, %c0, %alloc_12) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<f32>) -> ()
  return
}

// -----// IR Dump After DiscFlattenMemrefAccessPass (disc-flatten-memref-access) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c512 = arith.constant 512 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c4_i32 = arith.constant 4 : i32
  %c1_i32 = arith.constant 1 : i32
  %c4 = arith.constant 4 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
  %2 = "disc_ral.dispatch"(%arg0, %c1) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
  %dim = memref.dim %2, %c0 : memref<?x4xf32>
  %dim_0 = memref.dim %1, %c0 : memref<?x4xf32>
  %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %reinterpret_cast_1 = memref.reinterpret_cast %2 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %3 = arith.cmpi eq, %dim, %c1 : index
  %4 = arith.select %3, %dim_0, %dim : index
  %alloca = memref.alloca() {alignment = 64 : i64} : memref<2xindex>
  memref.store %4, %alloca[%c0] : memref<2xindex>
  memref.store %c4, %alloca[%c1] : memref<2xindex>
  %alloc = memref.alloc(%dim_0) {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %5 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast, %alloc) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  %alloc_2 = memref.alloc(%dim) {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %6 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  "disc_ral.dispatch"(%arg0, %6, %reinterpret_cast_1, %alloc_2) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  "disc_ral.dispatch"(%arg0, %6) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  %7 = arith.index_cast %4 : index to i32
  %8 = arith.muli %7, %c4_i32 : i32
  %alloca_3 = memref.alloca() {alignment = 64 : i64} : memref<2xi32>
  memref.store %8, %alloca_3[%c0] : memref<2xi32>
  memref.store %c1_i32, %alloca_3[%c1] : memref<2xi32>
  %9 = arith.index_cast %8 : i32 to index
  %10 = arith.cmpi eq, %dim_0, %4 : index
  %11 = arith.cmpi eq, %dim, %4 : index
  %12 = arith.andi %11, %10 : i1
  %alloc_4 = memref.alloc() : memref<f32, #gpu.address_space<global>>
  %alloc_5 = memref.alloc(%4) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %alloc_6 = memref.alloc(%4) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %alloc_7 = memref.alloc(%4) {kDiscSymbolicDimAttr = [@S2, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %alloc_8 = memref.alloc(%9) {kDiscSymbolicDimAttr = [@S3, @C1]} : memref<?x1xf32, #gpu.address_space<global>>
  %alloc_9 = memref.alloc() : memref<1xf32, #gpu.address_space<global>>
  scf.if %12 {
    %reinterpret_cast_13 = memref.reinterpret_cast %alloc to offset: [0], sizes: [%4, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S4, @C4]} : memref<?x4xf32, #gpu.address_space<global>> to memref<?x4xf32, #gpu.address_space<global>>
    %reinterpret_cast_14 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [%4, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S4, @C4]} : memref<?x4xf32, #gpu.address_space<global>> to memref<?x4xf32, #gpu.address_space<global>>
    %16 = arith.cmpi slt, %9, %c1 : index
    scf.if %16 {
      "lmhlo.fusion"() ({
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %22 = "disc_shape.delinearize"(%arg1, %c1) : (index, index) -> index
          %c1_15 = arith.constant 1 : index
          %23 = "disc_shape.linearize"(%22, %c1_15) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_16 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
          memref.store %cst, %reinterpret_cast_16[%23] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %17 = arith.cmpi eq, %9, %c0 : index
        %18 = arith.subi %9, %c1 : index
        %19 = arith.divui %18, %c32 : index
        %20 = arith.addi %19, %c1 : index
        %21 = arith.select %17, %c0, %20 : index
        scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%21, %c512) step (%c1, %c1) {
          %22 = arith.cmpi ult, %arg2, %c1 : index
          %23 = scf.if %22 -> (f32) {
            %24 = scf.for %arg3 = %c0 to %c32 step %c1 iter_args(%arg4 = %cst) -> (f32) {
              %25 = arith.muli %arg1, %c32 : index
              %26 = arith.addi %25, %arg3 : index
              %27 = arith.cmpi slt, %26, %9 : index
              %28 = scf.if %27 -> (f32) {
                %29 = "disc_shape.linearize"(%26, %arg2, %9, %c1) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
                %30:2 = "disc_shape.delinearize"(%29, %4, %c4) : (index, index, index) -> (index, index)
                %c0_15 = arith.constant 0 : index
                %dim_16 = memref.dim %reinterpret_cast_13, %c0_15 : memref<?x4xf32, #gpu.address_space<global>>
                %c4_17 = arith.constant 4 : index
                %31 = "disc_shape.linearize"(%30#0, %30#1, %dim_16, %c4_17) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
                %c1_18 = arith.constant 1 : index
                %c0_19 = arith.constant 0 : index
                %dim_20 = memref.dim %reinterpret_cast_13, %c0_19 : memref<?x4xf32, #gpu.address_space<global>>
                %32 = arith.muli %c1_18, %dim_20 : index
                %c1_21 = arith.constant 1 : index
                %dim_22 = memref.dim %reinterpret_cast_13, %c1_21 : memref<?x4xf32, #gpu.address_space<global>>
                %33 = arith.muli %32, %dim_22 : index
                %c1_23 = arith.constant 1 : index
                %c0_24 = arith.constant 0 : index
                %reinterpret_cast_25 = memref.reinterpret_cast %reinterpret_cast_13 to offset: [%c0_24], sizes: [%33], strides: [%c1_23] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %34 = memref.load %reinterpret_cast_25[%31] : memref<?xf32, #gpu.address_space<global>>
                %c0_26 = arith.constant 0 : index
                %dim_27 = memref.dim %reinterpret_cast_14, %c0_26 : memref<?x4xf32, #gpu.address_space<global>>
                %c4_28 = arith.constant 4 : index
                %35 = "disc_shape.linearize"(%30#0, %30#1, %dim_27, %c4_28) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
                %c1_29 = arith.constant 1 : index
                %c0_30 = arith.constant 0 : index
                %dim_31 = memref.dim %reinterpret_cast_14, %c0_30 : memref<?x4xf32, #gpu.address_space<global>>
                %36 = arith.muli %c1_29, %dim_31 : index
                %c1_32 = arith.constant 1 : index
                %dim_33 = memref.dim %reinterpret_cast_14, %c1_32 : memref<?x4xf32, #gpu.address_space<global>>
                %37 = arith.muli %36, %dim_33 : index
                %c1_34 = arith.constant 1 : index
                %c0_35 = arith.constant 0 : index
                %reinterpret_cast_36 = memref.reinterpret_cast %reinterpret_cast_14 to offset: [%c0_35], sizes: [%37], strides: [%c1_34] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %38 = memref.load %reinterpret_cast_36[%35] : memref<?xf32, #gpu.address_space<global>>
                %39 = arith.mulf %34, %38 : f32
                %40 = arith.addf %arg4, %39 : f32
                scf.yield %40 : f32
              } else {
                scf.yield %arg4 : f32
              }
              scf.yield %28 : f32
            }
            scf.yield %24 : f32
          } else {
            scf.yield %cst : f32
          }
          scf.if %22 {
            %24 = memref.atomic_rmw addf %23, %alloc_9[%arg2] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXthread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %22 = "disc_shape.delinearize"(%arg1, %c1) : (index, index) -> index
          %c1_15 = arith.constant 1 : index
          %23 = "disc_shape.linearize"(%22, %c1_15) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_16 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
          memref.store %cst, %reinterpret_cast_16[%23] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %17 = arith.cmpi eq, %9, %c0 : index
        %18 = arith.subi %9, %c1 : index
        %19 = arith.divui %18, %c512 : index
        %20 = arith.addi %19, %c1 : index
        %21 = arith.select %17, %c0, %20 : index
        scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%21, %c256) step (%c1, %c1) {
          %22 = arith.divui %arg2, %c32 : index
          %23 = arith.remui %arg2, %c32 : index
          %24 = arith.muli %23, %c8 : index
          %25 = arith.addi %22, %24 : index
          %alloc_15 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
          %26 = arith.cmpi ult, %23, %c1 : index
          %27 = scf.if %26 -> (f32) {
            %34 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %cst) -> (f32) {
              %35 = arith.muli %arg1, %c8 : index
              %36 = arith.addi %22, %35 : index
              %37 = arith.muli %36, %c64 : index
              %38 = arith.addi %arg3, %37 : index
              %39 = arith.cmpi slt, %38, %9 : index
              %40 = scf.if %39 -> (f32) {
                %41 = "disc_shape.linearize"(%38, %23, %9, %c1) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
                %42:2 = "disc_shape.delinearize"(%41, %4, %c4) : (index, index, index) -> (index, index)
                %c0_18 = arith.constant 0 : index
                %dim_19 = memref.dim %reinterpret_cast_13, %c0_18 : memref<?x4xf32, #gpu.address_space<global>>
                %c4_20 = arith.constant 4 : index
                %43 = "disc_shape.linearize"(%42#0, %42#1, %dim_19, %c4_20) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
                %c1_21 = arith.constant 1 : index
                %c0_22 = arith.constant 0 : index
                %dim_23 = memref.dim %reinterpret_cast_13, %c0_22 : memref<?x4xf32, #gpu.address_space<global>>
                %44 = arith.muli %c1_21, %dim_23 : index
                %c1_24 = arith.constant 1 : index
                %dim_25 = memref.dim %reinterpret_cast_13, %c1_24 : memref<?x4xf32, #gpu.address_space<global>>
                %45 = arith.muli %44, %dim_25 : index
                %c1_26 = arith.constant 1 : index
                %c0_27 = arith.constant 0 : index
                %reinterpret_cast_28 = memref.reinterpret_cast %reinterpret_cast_13 to offset: [%c0_27], sizes: [%45], strides: [%c1_26] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %46 = memref.load %reinterpret_cast_28[%43] : memref<?xf32, #gpu.address_space<global>>
                %c0_29 = arith.constant 0 : index
                %dim_30 = memref.dim %reinterpret_cast_14, %c0_29 : memref<?x4xf32, #gpu.address_space<global>>
                %c4_31 = arith.constant 4 : index
                %47 = "disc_shape.linearize"(%42#0, %42#1, %dim_30, %c4_31) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
                %c1_32 = arith.constant 1 : index
                %c0_33 = arith.constant 0 : index
                %dim_34 = memref.dim %reinterpret_cast_14, %c0_33 : memref<?x4xf32, #gpu.address_space<global>>
                %48 = arith.muli %c1_32, %dim_34 : index
                %c1_35 = arith.constant 1 : index
                %dim_36 = memref.dim %reinterpret_cast_14, %c1_35 : memref<?x4xf32, #gpu.address_space<global>>
                %49 = arith.muli %48, %dim_36 : index
                %c1_37 = arith.constant 1 : index
                %c0_38 = arith.constant 0 : index
                %reinterpret_cast_39 = memref.reinterpret_cast %reinterpret_cast_14 to offset: [%c0_38], sizes: [%49], strides: [%c1_37] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %50 = memref.load %reinterpret_cast_39[%47] : memref<?xf32, #gpu.address_space<global>>
                %51 = arith.mulf %46, %50 : f32
                %52 = arith.addf %arg4, %51 : f32
                scf.yield %52 : f32
              } else {
                scf.yield %arg4 : f32
              }
              scf.yield %40 : f32
            }
            scf.yield %34 : f32
          } else {
            scf.yield %cst : f32
          }
          %c256_16 = arith.constant 256 : index
          %28 = "disc_shape.linearize"(%25, %c256_16) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_17 = memref.reinterpret_cast %alloc_15 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %27, %reinterpret_cast_17[%28] : memref<256xf32, #gpu.address_space<workgroup>>
          gpu.barrier
          %29 = arith.cmpi slt, %22, %c4 : index
          scf.if %29 {
            %34 = arith.addi %25, %c4 : index
            %c256_18 = arith.constant 256 : index
            %35 = "disc_shape.linearize"(%25, %c256_18) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_19 = memref.reinterpret_cast %alloc_15 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            %36 = memref.load %reinterpret_cast_19[%35] : memref<256xf32, #gpu.address_space<workgroup>>
            %c256_20 = arith.constant 256 : index
            %37 = "disc_shape.linearize"(%34, %c256_20) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_21 = memref.reinterpret_cast %alloc_15 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            %38 = memref.load %reinterpret_cast_21[%37] : memref<256xf32, #gpu.address_space<workgroup>>
            %39 = arith.addf %36, %38 : f32
            %c256_22 = arith.constant 256 : index
            %40 = "disc_shape.linearize"(%25, %c256_22) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_23 = memref.reinterpret_cast %alloc_15 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            memref.store %39, %reinterpret_cast_23[%40] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %30 = arith.cmpi slt, %22, %c2 : index
          scf.if %30 {
            %34 = arith.addi %25, %c2 : index
            %c256_18 = arith.constant 256 : index
            %35 = "disc_shape.linearize"(%25, %c256_18) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_19 = memref.reinterpret_cast %alloc_15 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            %36 = memref.load %reinterpret_cast_19[%35] : memref<256xf32, #gpu.address_space<workgroup>>
            %c256_20 = arith.constant 256 : index
            %37 = "disc_shape.linearize"(%34, %c256_20) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_21 = memref.reinterpret_cast %alloc_15 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            %38 = memref.load %reinterpret_cast_21[%37] : memref<256xf32, #gpu.address_space<workgroup>>
            %39 = arith.addf %36, %38 : f32
            %c256_22 = arith.constant 256 : index
            %40 = "disc_shape.linearize"(%25, %c256_22) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_23 = memref.reinterpret_cast %alloc_15 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            memref.store %39, %reinterpret_cast_23[%40] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %31 = arith.cmpi slt, %22, %c1 : index
          scf.if %31 {
            %34 = arith.addi %25, %c1 : index
            %c256_18 = arith.constant 256 : index
            %35 = "disc_shape.linearize"(%25, %c256_18) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_19 = memref.reinterpret_cast %alloc_15 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            %36 = memref.load %reinterpret_cast_19[%35] : memref<256xf32, #gpu.address_space<workgroup>>
            %c256_20 = arith.constant 256 : index
            %37 = "disc_shape.linearize"(%34, %c256_20) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_21 = memref.reinterpret_cast %alloc_15 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            %38 = memref.load %reinterpret_cast_21[%37] : memref<256xf32, #gpu.address_space<workgroup>>
            %39 = arith.addf %36, %38 : f32
            %c256_22 = arith.constant 256 : index
            %40 = "disc_shape.linearize"(%25, %c256_22) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_23 = memref.reinterpret_cast %alloc_15 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            memref.store %39, %reinterpret_cast_23[%40] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %32 = arith.cmpi eq, %22, %c0 : index
          %33 = arith.andi %32, %26 : i1
          scf.if %33 {
            %c256_18 = arith.constant 256 : index
            %34 = "disc_shape.linearize"(%25, %c256_18) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_19 = memref.reinterpret_cast %alloc_15 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            %35 = memref.load %reinterpret_cast_19[%34] : memref<256xf32, #gpu.address_space<workgroup>>
            %36 = memref.atomic_rmw addf %35, %alloc_9[%23] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXblock_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
    }
  } else {
    %16 = arith.cmpi slt, %9, %c1 : index
    scf.if %16 {
      "lmhlo.fusion"() ({
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %22 = "disc_shape.delinearize"(%arg1, %c1) : (index, index) -> index
          %c1_13 = arith.constant 1 : index
          %23 = "disc_shape.linearize"(%22, %c1_13) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_14 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
          memref.store %cst, %reinterpret_cast_14[%23] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %17 = arith.cmpi eq, %9, %c0 : index
        %18 = arith.subi %9, %c1 : index
        %19 = arith.divui %18, %c32 : index
        %20 = arith.addi %19, %c1 : index
        %21 = arith.select %17, %c0, %20 : index
        scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%21, %c512) step (%c1, %c1) {
          %22 = arith.cmpi ult, %arg2, %c1 : index
          %23 = scf.if %22 -> (f32) {
            %24 = scf.for %arg3 = %c0 to %c32 step %c1 iter_args(%arg4 = %cst) -> (f32) {
              %25 = arith.muli %arg1, %c32 : index
              %26 = arith.addi %25, %arg3 : index
              %27 = arith.cmpi slt, %26, %9 : index
              %28 = scf.if %27 -> (f32) {
                %29 = "disc_shape.linearize"(%26, %arg2, %9, %c1) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
                %30:2 = "disc_shape.delinearize"(%29, %4, %c4) : (index, index, index) -> (index, index)
                %31 = arith.cmpi eq, %dim_0, %4 : index
                %32 = arith.select %31, %30#0, %c0 : index
                %c0_13 = arith.constant 0 : index
                %dim_14 = memref.dim %alloc, %c0_13 : memref<?x4xf32, #gpu.address_space<global>>
                %c4_15 = arith.constant 4 : index
                %33 = "disc_shape.linearize"(%32, %30#1, %dim_14, %c4_15) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
                %c1_16 = arith.constant 1 : index
                %c0_17 = arith.constant 0 : index
                %dim_18 = memref.dim %alloc, %c0_17 : memref<?x4xf32, #gpu.address_space<global>>
                %34 = arith.muli %c1_16, %dim_18 : index
                %c1_19 = arith.constant 1 : index
                %dim_20 = memref.dim %alloc, %c1_19 : memref<?x4xf32, #gpu.address_space<global>>
                %35 = arith.muli %34, %dim_20 : index
                %c1_21 = arith.constant 1 : index
                %c0_22 = arith.constant 0 : index
                %reinterpret_cast_23 = memref.reinterpret_cast %alloc to offset: [%c0_22], sizes: [%35], strides: [%c1_21] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %36 = memref.load %reinterpret_cast_23[%33] : memref<?xf32, #gpu.address_space<global>>
                %37 = arith.cmpi eq, %dim, %4 : index
                %38 = arith.select %37, %30#0, %c0 : index
                %c0_24 = arith.constant 0 : index
                %dim_25 = memref.dim %alloc_2, %c0_24 : memref<?x4xf32, #gpu.address_space<global>>
                %c4_26 = arith.constant 4 : index
                %39 = "disc_shape.linearize"(%38, %30#1, %dim_25, %c4_26) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
                %c1_27 = arith.constant 1 : index
                %c0_28 = arith.constant 0 : index
                %dim_29 = memref.dim %alloc_2, %c0_28 : memref<?x4xf32, #gpu.address_space<global>>
                %40 = arith.muli %c1_27, %dim_29 : index
                %c1_30 = arith.constant 1 : index
                %dim_31 = memref.dim %alloc_2, %c1_30 : memref<?x4xf32, #gpu.address_space<global>>
                %41 = arith.muli %40, %dim_31 : index
                %c1_32 = arith.constant 1 : index
                %c0_33 = arith.constant 0 : index
                %reinterpret_cast_34 = memref.reinterpret_cast %alloc_2 to offset: [%c0_33], sizes: [%41], strides: [%c1_32] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %42 = memref.load %reinterpret_cast_34[%39] : memref<?xf32, #gpu.address_space<global>>
                %43 = arith.mulf %36, %42 : f32
                %44 = arith.addf %arg4, %43 : f32
                scf.yield %44 : f32
              } else {
                scf.yield %arg4 : f32
              }
              scf.yield %28 : f32
            }
            scf.yield %24 : f32
          } else {
            scf.yield %cst : f32
          }
          scf.if %22 {
            %24 = memref.atomic_rmw addf %23, %alloc_9[%arg2] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "thread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %22 = "disc_shape.delinearize"(%arg1, %c1) : (index, index) -> index
          %c1_13 = arith.constant 1 : index
          %23 = "disc_shape.linearize"(%22, %c1_13) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_14 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
          memref.store %cst, %reinterpret_cast_14[%23] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %17 = arith.cmpi eq, %9, %c0 : index
        %18 = arith.subi %9, %c1 : index
        %19 = arith.divui %18, %c512 : index
        %20 = arith.addi %19, %c1 : index
        %21 = arith.select %17, %c0, %20 : index
        scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%21, %c256) step (%c1, %c1) {
          %22 = arith.divui %arg2, %c32 : index
          %23 = arith.remui %arg2, %c32 : index
          %24 = arith.muli %23, %c8 : index
          %25 = arith.addi %22, %24 : index
          %alloc_13 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
          %26 = arith.cmpi ult, %23, %c1 : index
          %27 = scf.if %26 -> (f32) {
            %34 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %cst) -> (f32) {
              %35 = arith.muli %arg1, %c8 : index
              %36 = arith.addi %22, %35 : index
              %37 = arith.muli %36, %c64 : index
              %38 = arith.addi %arg3, %37 : index
              %39 = arith.cmpi slt, %38, %9 : index
              %40 = scf.if %39 -> (f32) {
                %41 = "disc_shape.linearize"(%38, %23, %9, %c1) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
                %42:2 = "disc_shape.delinearize"(%41, %4, %c4) : (index, index, index) -> (index, index)
                %43 = arith.cmpi eq, %dim_0, %4 : index
                %44 = arith.select %43, %42#0, %c0 : index
                %c0_16 = arith.constant 0 : index
                %dim_17 = memref.dim %alloc, %c0_16 : memref<?x4xf32, #gpu.address_space<global>>
                %c4_18 = arith.constant 4 : index
                %45 = "disc_shape.linearize"(%44, %42#1, %dim_17, %c4_18) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
                %c1_19 = arith.constant 1 : index
                %c0_20 = arith.constant 0 : index
                %dim_21 = memref.dim %alloc, %c0_20 : memref<?x4xf32, #gpu.address_space<global>>
                %46 = arith.muli %c1_19, %dim_21 : index
                %c1_22 = arith.constant 1 : index
                %dim_23 = memref.dim %alloc, %c1_22 : memref<?x4xf32, #gpu.address_space<global>>
                %47 = arith.muli %46, %dim_23 : index
                %c1_24 = arith.constant 1 : index
                %c0_25 = arith.constant 0 : index
                %reinterpret_cast_26 = memref.reinterpret_cast %alloc to offset: [%c0_25], sizes: [%47], strides: [%c1_24] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %48 = memref.load %reinterpret_cast_26[%45] : memref<?xf32, #gpu.address_space<global>>
                %49 = arith.cmpi eq, %dim, %4 : index
                %50 = arith.select %49, %42#0, %c0 : index
                %c0_27 = arith.constant 0 : index
                %dim_28 = memref.dim %alloc_2, %c0_27 : memref<?x4xf32, #gpu.address_space<global>>
                %c4_29 = arith.constant 4 : index
                %51 = "disc_shape.linearize"(%50, %42#1, %dim_28, %c4_29) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
                %c1_30 = arith.constant 1 : index
                %c0_31 = arith.constant 0 : index
                %dim_32 = memref.dim %alloc_2, %c0_31 : memref<?x4xf32, #gpu.address_space<global>>
                %52 = arith.muli %c1_30, %dim_32 : index
                %c1_33 = arith.constant 1 : index
                %dim_34 = memref.dim %alloc_2, %c1_33 : memref<?x4xf32, #gpu.address_space<global>>
                %53 = arith.muli %52, %dim_34 : index
                %c1_35 = arith.constant 1 : index
                %c0_36 = arith.constant 0 : index
                %reinterpret_cast_37 = memref.reinterpret_cast %alloc_2 to offset: [%c0_36], sizes: [%53], strides: [%c1_35] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %54 = memref.load %reinterpret_cast_37[%51] : memref<?xf32, #gpu.address_space<global>>
                %55 = arith.mulf %48, %54 : f32
                %56 = arith.addf %arg4, %55 : f32
                scf.yield %56 : f32
              } else {
                scf.yield %arg4 : f32
              }
              scf.yield %40 : f32
            }
            scf.yield %34 : f32
          } else {
            scf.yield %cst : f32
          }
          %c256_14 = arith.constant 256 : index
          %28 = "disc_shape.linearize"(%25, %c256_14) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_15 = memref.reinterpret_cast %alloc_13 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %27, %reinterpret_cast_15[%28] : memref<256xf32, #gpu.address_space<workgroup>>
          gpu.barrier
          %29 = arith.cmpi slt, %22, %c4 : index
          scf.if %29 {
            %34 = arith.addi %25, %c4 : index
            %c256_16 = arith.constant 256 : index
            %35 = "disc_shape.linearize"(%25, %c256_16) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_17 = memref.reinterpret_cast %alloc_13 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            %36 = memref.load %reinterpret_cast_17[%35] : memref<256xf32, #gpu.address_space<workgroup>>
            %c256_18 = arith.constant 256 : index
            %37 = "disc_shape.linearize"(%34, %c256_18) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_19 = memref.reinterpret_cast %alloc_13 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            %38 = memref.load %reinterpret_cast_19[%37] : memref<256xf32, #gpu.address_space<workgroup>>
            %39 = arith.addf %36, %38 : f32
            %c256_20 = arith.constant 256 : index
            %40 = "disc_shape.linearize"(%25, %c256_20) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_21 = memref.reinterpret_cast %alloc_13 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            memref.store %39, %reinterpret_cast_21[%40] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %30 = arith.cmpi slt, %22, %c2 : index
          scf.if %30 {
            %34 = arith.addi %25, %c2 : index
            %c256_16 = arith.constant 256 : index
            %35 = "disc_shape.linearize"(%25, %c256_16) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_17 = memref.reinterpret_cast %alloc_13 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            %36 = memref.load %reinterpret_cast_17[%35] : memref<256xf32, #gpu.address_space<workgroup>>
            %c256_18 = arith.constant 256 : index
            %37 = "disc_shape.linearize"(%34, %c256_18) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_19 = memref.reinterpret_cast %alloc_13 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            %38 = memref.load %reinterpret_cast_19[%37] : memref<256xf32, #gpu.address_space<workgroup>>
            %39 = arith.addf %36, %38 : f32
            %c256_20 = arith.constant 256 : index
            %40 = "disc_shape.linearize"(%25, %c256_20) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_21 = memref.reinterpret_cast %alloc_13 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            memref.store %39, %reinterpret_cast_21[%40] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %31 = arith.cmpi slt, %22, %c1 : index
          scf.if %31 {
            %34 = arith.addi %25, %c1 : index
            %c256_16 = arith.constant 256 : index
            %35 = "disc_shape.linearize"(%25, %c256_16) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_17 = memref.reinterpret_cast %alloc_13 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            %36 = memref.load %reinterpret_cast_17[%35] : memref<256xf32, #gpu.address_space<workgroup>>
            %c256_18 = arith.constant 256 : index
            %37 = "disc_shape.linearize"(%34, %c256_18) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_19 = memref.reinterpret_cast %alloc_13 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            %38 = memref.load %reinterpret_cast_19[%37] : memref<256xf32, #gpu.address_space<workgroup>>
            %39 = arith.addf %36, %38 : f32
            %c256_20 = arith.constant 256 : index
            %40 = "disc_shape.linearize"(%25, %c256_20) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_21 = memref.reinterpret_cast %alloc_13 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            memref.store %39, %reinterpret_cast_21[%40] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %32 = arith.cmpi eq, %22, %c0 : index
          %33 = arith.andi %32, %26 : i1
          scf.if %33 {
            %c256_16 = arith.constant 256 : index
            %34 = "disc_shape.linearize"(%25, %c256_16) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_17 = memref.reinterpret_cast %alloc_13 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            %35 = memref.load %reinterpret_cast_17[%34] : memref<256xf32, #gpu.address_space<workgroup>>
            %36 = memref.atomic_rmw addf %35, %alloc_9[%23] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "block_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
    }
  }
  memref.dealloc %alloc_8 : memref<?x1xf32, #gpu.address_space<global>>
  memref.dealloc %alloc_7 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc_6 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc_5 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc_4 : memref<f32, #gpu.address_space<global>>
  memref.dealloc %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc : memref<?x4xf32, #gpu.address_space<global>>
  %13 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca_10 = memref.alloca() : memref<0xindex>
  %14 = "disc_ral.dispatch"(%arg0, %13, %alloc_9, %alloca_10) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1xf32, #gpu.address_space<global>>, memref<0xindex>) -> memref<f32, #gpu.address_space<global>>
  %reinterpret_cast_11 = memref.reinterpret_cast %14 to offset: [0], sizes: [], strides: [] : memref<f32, #gpu.address_space<global>> to memref<f32, #gpu.address_space<global>>
  memref.dealloc %alloc_9 : memref<1xf32, #gpu.address_space<global>>
  %alloc_12 = memref.alloc() : memref<f32>
  %15 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  "disc_ral.dispatch"(%arg0, %15, %reinterpret_cast_11, %alloc_12) {backend_config = "", call_target_name = "d2h", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<f32, #gpu.address_space<global>>, memref<f32>) -> ()
  "disc_ral.dispatch"(%arg0, %15) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  memref.dealloc %reinterpret_cast_11 : memref<f32, #gpu.address_space<global>>
  "disc_ral.dispatch"(%arg0, %c0, %alloc_12) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<f32>) -> ()
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c512 = arith.constant 512 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c4_i32 = arith.constant 4 : i32
  %c4 = arith.constant 4 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
  %2 = "disc_ral.dispatch"(%arg0, %c1) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
  %dim = memref.dim %2, %c0 : memref<?x4xf32>
  %dim_0 = memref.dim %1, %c0 : memref<?x4xf32>
  %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %reinterpret_cast_1 = memref.reinterpret_cast %2 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %3 = arith.cmpi eq, %dim, %c1 : index
  %4 = arith.select %3, %dim_0, %dim : index
  %alloc = memref.alloc(%dim_0) {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %5 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast, %alloc) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  %alloc_2 = memref.alloc(%dim) {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %6 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  "disc_ral.dispatch"(%arg0, %6, %reinterpret_cast_1, %alloc_2) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  "disc_ral.dispatch"(%arg0, %6) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  %7 = arith.index_cast %4 : index to i32
  %8 = arith.muli %7, %c4_i32 : i32
  %9 = arith.index_cast %8 : i32 to index
  %10 = arith.cmpi eq, %dim_0, %4 : index
  %11 = arith.cmpi eq, %dim, %4 : index
  %12 = arith.andi %11, %10 : i1
  %alloc_3 = memref.alloc() : memref<1xf32, #gpu.address_space<global>>
  scf.if %12 {
    %16 = arith.cmpi slt, %9, %c1 : index
    scf.if %16 {
      "lmhlo.fusion"() ({
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %reinterpret_cast_6 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
          memref.store %cst, %reinterpret_cast_6[%arg1] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %17 = arith.cmpi eq, %9, %c0 : index
        %18 = arith.subi %9, %c1 : index
        %19 = arith.divui %18, %c32 : index
        %20 = arith.addi %19, %c1 : index
        %21 = arith.select %17, %c0, %20 : index
        scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%21, %c512) step (%c1, %c1) {
          %22 = arith.cmpi ult, %arg2, %c1 : index
          scf.if %22 {
            %23 = scf.for %arg3 = %c0 to %c32 step %c1 iter_args(%arg4 = %cst) -> (f32) {
              %25 = arith.muli %arg1, %c32 : index
              %26 = arith.addi %25, %arg3 : index
              %27 = arith.cmpi slt, %26, %9 : index
              %28 = scf.if %27 -> (f32) {
                %29 = "disc_shape.linearize"(%26, %9) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
                %30 = arith.muli %4, %c4 : index
                %reinterpret_cast_6 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%30], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %31 = memref.load %reinterpret_cast_6[%29] : memref<?xf32, #gpu.address_space<global>>
                %32 = arith.muli %4, %c4 : index
                %reinterpret_cast_7 = memref.reinterpret_cast %alloc_2 to offset: [%c0], sizes: [%32], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %33 = memref.load %reinterpret_cast_7[%29] : memref<?xf32, #gpu.address_space<global>>
                %34 = arith.mulf %31, %33 : f32
                %35 = arith.addf %arg4, %34 : f32
                scf.yield %35 : f32
              } else {
                scf.yield %arg4 : f32
              }
              scf.yield %28 : f32
            }
            %24 = memref.atomic_rmw addf %23, %alloc_3[%arg2] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXthread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %reinterpret_cast_6 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
          memref.store %cst, %reinterpret_cast_6[%arg1] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %17 = arith.cmpi eq, %9, %c0 : index
        %18 = arith.subi %9, %c1 : index
        %19 = arith.divui %18, %c512 : index
        %20 = arith.addi %19, %c1 : index
        %21 = arith.select %17, %c0, %20 : index
        scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%21, %c256) step (%c1, %c1) {
          %22 = arith.divui %arg2, %c32 : index
          %23 = arith.remui %arg2, %c32 : index
          %24 = arith.muli %23, %c8 : index
          %25 = arith.addi %22, %24 : index
          %alloc_6 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
          %26 = arith.cmpi ult, %23, %c1 : index
          %27 = scf.if %26 -> (f32) {
            %34 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %cst) -> (f32) {
              %35 = arith.muli %arg1, %c8 : index
              %36 = arith.addi %22, %35 : index
              %37 = arith.muli %36, %c64 : index
              %38 = arith.addi %arg3, %37 : index
              %39 = arith.cmpi slt, %38, %9 : index
              %40 = scf.if %39 -> (f32) {
                %41 = "disc_shape.linearize"(%38, %9) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
                %42 = arith.muli %4, %c4 : index
                %reinterpret_cast_8 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%42], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %43 = memref.load %reinterpret_cast_8[%41] : memref<?xf32, #gpu.address_space<global>>
                %44 = arith.muli %4, %c4 : index
                %reinterpret_cast_9 = memref.reinterpret_cast %alloc_2 to offset: [%c0], sizes: [%44], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %45 = memref.load %reinterpret_cast_9[%41] : memref<?xf32, #gpu.address_space<global>>
                %46 = arith.mulf %43, %45 : f32
                %47 = arith.addf %arg4, %46 : f32
                scf.yield %47 : f32
              } else {
                scf.yield %arg4 : f32
              }
              scf.yield %40 : f32
            }
            scf.yield %34 : f32
          } else {
            scf.yield %cst : f32
          }
          %28 = "disc_shape.linearize"(%25, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_7 = memref.reinterpret_cast %alloc_6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %27, %reinterpret_cast_7[%28] : memref<256xf32, #gpu.address_space<workgroup>>
          gpu.barrier
          %29 = arith.cmpi slt, %22, %c4 : index
          scf.if %29 {
            %34 = arith.addi %25, %c4 : index
            %35 = "disc_shape.linearize"(%25, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_8 = memref.reinterpret_cast %alloc_6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            %36 = memref.load %reinterpret_cast_8[%35] : memref<256xf32, #gpu.address_space<workgroup>>
            %37 = "disc_shape.linearize"(%34, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_9 = memref.reinterpret_cast %alloc_6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            %38 = memref.load %reinterpret_cast_9[%37] : memref<256xf32, #gpu.address_space<workgroup>>
            %39 = arith.addf %36, %38 : f32
            %40 = "disc_shape.linearize"(%25, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_10 = memref.reinterpret_cast %alloc_6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            memref.store %39, %reinterpret_cast_10[%40] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %30 = arith.cmpi slt, %22, %c2 : index
          scf.if %30 {
            %34 = arith.addi %25, %c2 : index
            %35 = "disc_shape.linearize"(%25, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_8 = memref.reinterpret_cast %alloc_6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            %36 = memref.load %reinterpret_cast_8[%35] : memref<256xf32, #gpu.address_space<workgroup>>
            %37 = "disc_shape.linearize"(%34, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_9 = memref.reinterpret_cast %alloc_6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            %38 = memref.load %reinterpret_cast_9[%37] : memref<256xf32, #gpu.address_space<workgroup>>
            %39 = arith.addf %36, %38 : f32
            %40 = "disc_shape.linearize"(%25, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_10 = memref.reinterpret_cast %alloc_6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            memref.store %39, %reinterpret_cast_10[%40] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %31 = arith.cmpi slt, %22, %c1 : index
          scf.if %31 {
            %34 = arith.addi %25, %c1 : index
            %35 = "disc_shape.linearize"(%25, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_8 = memref.reinterpret_cast %alloc_6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            %36 = memref.load %reinterpret_cast_8[%35] : memref<256xf32, #gpu.address_space<workgroup>>
            %37 = "disc_shape.linearize"(%34, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_9 = memref.reinterpret_cast %alloc_6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            %38 = memref.load %reinterpret_cast_9[%37] : memref<256xf32, #gpu.address_space<workgroup>>
            %39 = arith.addf %36, %38 : f32
            %40 = "disc_shape.linearize"(%25, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_10 = memref.reinterpret_cast %alloc_6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            memref.store %39, %reinterpret_cast_10[%40] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %32 = arith.cmpi eq, %22, %c0 : index
          %33 = arith.andi %32, %26 : i1
          scf.if %33 {
            %34 = "disc_shape.linearize"(%25, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_8 = memref.reinterpret_cast %alloc_6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            %35 = memref.load %reinterpret_cast_8[%34] : memref<256xf32, #gpu.address_space<workgroup>>
            %36 = memref.atomic_rmw addf %35, %alloc_3[%23] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXblock_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
    }
  } else {
    %16 = arith.cmpi slt, %9, %c1 : index
    scf.if %16 {
      "lmhlo.fusion"() ({
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %reinterpret_cast_6 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
          memref.store %cst, %reinterpret_cast_6[%arg1] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %17 = arith.cmpi eq, %9, %c0 : index
        %18 = arith.subi %9, %c1 : index
        %19 = arith.divui %18, %c32 : index
        %20 = arith.addi %19, %c1 : index
        %21 = arith.select %17, %c0, %20 : index
        scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%21, %c512) step (%c1, %c1) {
          %22 = arith.cmpi ult, %arg2, %c1 : index
          scf.if %22 {
            %23 = scf.for %arg3 = %c0 to %c32 step %c1 iter_args(%arg4 = %cst) -> (f32) {
              %25 = arith.muli %arg1, %c32 : index
              %26 = arith.addi %25, %arg3 : index
              %27 = arith.cmpi slt, %26, %9 : index
              %28 = scf.if %27 -> (f32) {
                %29 = "disc_shape.linearize"(%26, %9) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
                %30:2 = "disc_shape.delinearize"(%29, %4, %c4) : (index, index, index) -> (index, index)
                %31 = arith.cmpi eq, %dim_0, %4 : index
                %32 = arith.select %31, %30#0, %c0 : index
                %33 = "disc_shape.linearize"(%32, %30#1, %dim_0, %c4) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
                %34 = arith.muli %dim_0, %c4 : index
                %reinterpret_cast_6 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%34], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %35 = memref.load %reinterpret_cast_6[%33] : memref<?xf32, #gpu.address_space<global>>
                %36 = arith.cmpi eq, %dim, %4 : index
                %37 = arith.select %36, %30#0, %c0 : index
                %38 = "disc_shape.linearize"(%37, %30#1, %dim, %c4) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
                %39 = arith.muli %dim, %c4 : index
                %reinterpret_cast_7 = memref.reinterpret_cast %alloc_2 to offset: [%c0], sizes: [%39], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %40 = memref.load %reinterpret_cast_7[%38] : memref<?xf32, #gpu.address_space<global>>
                %41 = arith.mulf %35, %40 : f32
                %42 = arith.addf %arg4, %41 : f32
                scf.yield %42 : f32
              } else {
                scf.yield %arg4 : f32
              }
              scf.yield %28 : f32
            }
            %24 = memref.atomic_rmw addf %23, %alloc_3[%arg2] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "thread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %reinterpret_cast_6 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
          memref.store %cst, %reinterpret_cast_6[%arg1] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %17 = arith.cmpi eq, %9, %c0 : index
        %18 = arith.subi %9, %c1 : index
        %19 = arith.divui %18, %c512 : index
        %20 = arith.addi %19, %c1 : index
        %21 = arith.select %17, %c0, %20 : index
        scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%21, %c256) step (%c1, %c1) {
          %22 = arith.divui %arg2, %c32 : index
          %23 = arith.remui %arg2, %c32 : index
          %24 = arith.muli %23, %c8 : index
          %25 = arith.addi %22, %24 : index
          %alloc_6 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
          %26 = arith.cmpi ult, %23, %c1 : index
          %27 = scf.if %26 -> (f32) {
            %34 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %cst) -> (f32) {
              %35 = arith.muli %arg1, %c8 : index
              %36 = arith.addi %22, %35 : index
              %37 = arith.muli %36, %c64 : index
              %38 = arith.addi %arg3, %37 : index
              %39 = arith.cmpi slt, %38, %9 : index
              %40 = scf.if %39 -> (f32) {
                %41 = "disc_shape.linearize"(%38, %9) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
                %42:2 = "disc_shape.delinearize"(%41, %4, %c4) : (index, index, index) -> (index, index)
                %43 = arith.cmpi eq, %dim_0, %4 : index
                %44 = arith.select %43, %42#0, %c0 : index
                %45 = "disc_shape.linearize"(%44, %42#1, %dim_0, %c4) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
                %46 = arith.muli %dim_0, %c4 : index
                %reinterpret_cast_8 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%46], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %47 = memref.load %reinterpret_cast_8[%45] : memref<?xf32, #gpu.address_space<global>>
                %48 = arith.cmpi eq, %dim, %4 : index
                %49 = arith.select %48, %42#0, %c0 : index
                %50 = "disc_shape.linearize"(%49, %42#1, %dim, %c4) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
                %51 = arith.muli %dim, %c4 : index
                %reinterpret_cast_9 = memref.reinterpret_cast %alloc_2 to offset: [%c0], sizes: [%51], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %52 = memref.load %reinterpret_cast_9[%50] : memref<?xf32, #gpu.address_space<global>>
                %53 = arith.mulf %47, %52 : f32
                %54 = arith.addf %arg4, %53 : f32
                scf.yield %54 : f32
              } else {
                scf.yield %arg4 : f32
              }
              scf.yield %40 : f32
            }
            scf.yield %34 : f32
          } else {
            scf.yield %cst : f32
          }
          %28 = "disc_shape.linearize"(%25, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_7 = memref.reinterpret_cast %alloc_6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %27, %reinterpret_cast_7[%28] : memref<256xf32, #gpu.address_space<workgroup>>
          gpu.barrier
          %29 = arith.cmpi slt, %22, %c4 : index
          scf.if %29 {
            %34 = arith.addi %25, %c4 : index
            %35 = "disc_shape.linearize"(%25, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_8 = memref.reinterpret_cast %alloc_6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            %36 = memref.load %reinterpret_cast_8[%35] : memref<256xf32, #gpu.address_space<workgroup>>
            %37 = "disc_shape.linearize"(%34, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_9 = memref.reinterpret_cast %alloc_6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            %38 = memref.load %reinterpret_cast_9[%37] : memref<256xf32, #gpu.address_space<workgroup>>
            %39 = arith.addf %36, %38 : f32
            %40 = "disc_shape.linearize"(%25, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_10 = memref.reinterpret_cast %alloc_6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            memref.store %39, %reinterpret_cast_10[%40] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %30 = arith.cmpi slt, %22, %c2 : index
          scf.if %30 {
            %34 = arith.addi %25, %c2 : index
            %35 = "disc_shape.linearize"(%25, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_8 = memref.reinterpret_cast %alloc_6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            %36 = memref.load %reinterpret_cast_8[%35] : memref<256xf32, #gpu.address_space<workgroup>>
            %37 = "disc_shape.linearize"(%34, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_9 = memref.reinterpret_cast %alloc_6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            %38 = memref.load %reinterpret_cast_9[%37] : memref<256xf32, #gpu.address_space<workgroup>>
            %39 = arith.addf %36, %38 : f32
            %40 = "disc_shape.linearize"(%25, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_10 = memref.reinterpret_cast %alloc_6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            memref.store %39, %reinterpret_cast_10[%40] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %31 = arith.cmpi slt, %22, %c1 : index
          scf.if %31 {
            %34 = arith.addi %25, %c1 : index
            %35 = "disc_shape.linearize"(%25, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_8 = memref.reinterpret_cast %alloc_6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            %36 = memref.load %reinterpret_cast_8[%35] : memref<256xf32, #gpu.address_space<workgroup>>
            %37 = "disc_shape.linearize"(%34, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_9 = memref.reinterpret_cast %alloc_6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            %38 = memref.load %reinterpret_cast_9[%37] : memref<256xf32, #gpu.address_space<workgroup>>
            %39 = arith.addf %36, %38 : f32
            %40 = "disc_shape.linearize"(%25, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_10 = memref.reinterpret_cast %alloc_6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            memref.store %39, %reinterpret_cast_10[%40] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %32 = arith.cmpi eq, %22, %c0 : index
          %33 = arith.andi %32, %26 : i1
          scf.if %33 {
            %34 = "disc_shape.linearize"(%25, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %reinterpret_cast_8 = memref.reinterpret_cast %alloc_6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            %35 = memref.load %reinterpret_cast_8[%34] : memref<256xf32, #gpu.address_space<workgroup>>
            %36 = memref.atomic_rmw addf %35, %alloc_3[%23] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "block_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
    }
  }
  memref.dealloc %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc : memref<?x4xf32, #gpu.address_space<global>>
  %13 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca = memref.alloca() : memref<0xindex>
  %14 = "disc_ral.dispatch"(%arg0, %13, %alloc_3, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1xf32, #gpu.address_space<global>>, memref<0xindex>) -> memref<f32, #gpu.address_space<global>>
  %reinterpret_cast_4 = memref.reinterpret_cast %14 to offset: [0], sizes: [], strides: [] : memref<f32, #gpu.address_space<global>> to memref<f32, #gpu.address_space<global>>
  memref.dealloc %alloc_3 : memref<1xf32, #gpu.address_space<global>>
  %alloc_5 = memref.alloc() : memref<f32>
  %15 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  "disc_ral.dispatch"(%arg0, %15, %reinterpret_cast_4, %alloc_5) {backend_config = "", call_target_name = "d2h", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<f32, #gpu.address_space<global>>, memref<f32>) -> ()
  "disc_ral.dispatch"(%arg0, %15) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  memref.dealloc %reinterpret_cast_4 : memref<f32, #gpu.address_space<global>>
  "disc_ral.dispatch"(%arg0, %c0, %alloc_5) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<f32>) -> ()
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c512 = arith.constant 512 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c4_i32 = arith.constant 4 : i32
  %c4 = arith.constant 4 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
  %2 = "disc_ral.dispatch"(%arg0, %c1) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
  %dim = memref.dim %2, %c0 : memref<?x4xf32>
  %dim_0 = memref.dim %1, %c0 : memref<?x4xf32>
  %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %reinterpret_cast_1 = memref.reinterpret_cast %2 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %3 = arith.cmpi eq, %dim, %c1 : index
  %4 = arith.select %3, %dim_0, %dim : index
  %alloc = memref.alloc(%dim_0) {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %5 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast, %alloc) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  %alloc_2 = memref.alloc(%dim) {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_1, %alloc_2) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  %6 = arith.index_cast %4 : index to i32
  %7 = arith.muli %6, %c4_i32 : i32
  %8 = arith.index_cast %7 : i32 to index
  %9 = arith.cmpi eq, %dim_0, %4 : index
  %10 = arith.cmpi eq, %dim, %4 : index
  %11 = arith.andi %10, %9 : i1
  %alloc_3 = memref.alloc() : memref<1xf32, #gpu.address_space<global>>
  scf.if %11 {
    %13 = arith.cmpi slt, %8, %c1 : index
    scf.if %13 {
      "lmhlo.fusion"() ({
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %reinterpret_cast_6 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
          memref.store %cst, %reinterpret_cast_6[%arg1] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %14 = arith.cmpi eq, %8, %c0 : index
        %15 = arith.subi %8, %c1 : index
        %16 = arith.divui %15, %c32 : index
        %17 = arith.addi %16, %c1 : index
        %18 = arith.select %14, %c0, %17 : index
        scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%18, %c512) step (%c1, %c1) {
          %19 = arith.cmpi ult, %arg2, %c1 : index
          scf.if %19 {
            %20 = scf.for %arg3 = %c0 to %c32 step %c1 iter_args(%arg4 = %cst) -> (f32) {
              %22 = arith.muli %arg1, %c32 : index
              %23 = arith.addi %22, %arg3 : index
              %24 = arith.cmpi slt, %23, %8 : index
              %25 = scf.if %24 -> (f32) {
                %26 = "disc_shape.linearize"(%23, %8) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
                %27 = arith.muli %4, %c4 : index
                %reinterpret_cast_6 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%27], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %28 = memref.load %reinterpret_cast_6[%26] : memref<?xf32, #gpu.address_space<global>>
                %reinterpret_cast_7 = memref.reinterpret_cast %alloc_2 to offset: [%c0], sizes: [%27], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %29 = memref.load %reinterpret_cast_7[%26] : memref<?xf32, #gpu.address_space<global>>
                %30 = arith.mulf %28, %29 : f32
                %31 = arith.addf %arg4, %30 : f32
                scf.yield %31 : f32
              } else {
                scf.yield %arg4 : f32
              }
              scf.yield %25 : f32
            }
            %21 = memref.atomic_rmw addf %20, %alloc_3[%arg2] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXthread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %reinterpret_cast_6 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
          memref.store %cst, %reinterpret_cast_6[%arg1] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %14 = arith.cmpi eq, %8, %c0 : index
        %15 = arith.subi %8, %c1 : index
        %16 = arith.divui %15, %c512 : index
        %17 = arith.addi %16, %c1 : index
        %18 = arith.select %14, %c0, %17 : index
        scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%18, %c256) step (%c1, %c1) {
          %19 = arith.divui %arg2, %c32 : index
          %20 = arith.remui %arg2, %c32 : index
          %21 = arith.muli %20, %c8 : index
          %22 = arith.addi %19, %21 : index
          %alloc_6 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
          %23 = arith.cmpi ult, %20, %c1 : index
          %24 = scf.if %23 -> (f32) {
            %31 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %cst) -> (f32) {
              %32 = arith.muli %arg1, %c8 : index
              %33 = arith.addi %19, %32 : index
              %34 = arith.muli %33, %c64 : index
              %35 = arith.addi %arg3, %34 : index
              %36 = arith.cmpi slt, %35, %8 : index
              %37 = scf.if %36 -> (f32) {
                %38 = "disc_shape.linearize"(%35, %8) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
                %39 = arith.muli %4, %c4 : index
                %reinterpret_cast_8 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%39], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %40 = memref.load %reinterpret_cast_8[%38] : memref<?xf32, #gpu.address_space<global>>
                %reinterpret_cast_9 = memref.reinterpret_cast %alloc_2 to offset: [%c0], sizes: [%39], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %41 = memref.load %reinterpret_cast_9[%38] : memref<?xf32, #gpu.address_space<global>>
                %42 = arith.mulf %40, %41 : f32
                %43 = arith.addf %arg4, %42 : f32
                scf.yield %43 : f32
              } else {
                scf.yield %arg4 : f32
              }
              scf.yield %37 : f32
            }
            scf.yield %31 : f32
          } else {
            scf.yield %cst : f32
          }
          %25 = "disc_shape.linearize"(%22, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_7 = memref.reinterpret_cast %alloc_6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %24, %reinterpret_cast_7[%25] : memref<256xf32, #gpu.address_space<workgroup>>
          gpu.barrier
          %26 = arith.cmpi slt, %19, %c4 : index
          scf.if %26 {
            %31 = arith.addi %22, %c4 : index
            %32 = memref.load %reinterpret_cast_7[%25] : memref<256xf32, #gpu.address_space<workgroup>>
            %33 = "disc_shape.linearize"(%31, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %34 = memref.load %reinterpret_cast_7[%33] : memref<256xf32, #gpu.address_space<workgroup>>
            %35 = arith.addf %32, %34 : f32
            memref.store %35, %reinterpret_cast_7[%25] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %27 = arith.cmpi slt, %19, %c2 : index
          scf.if %27 {
            %31 = arith.addi %22, %c2 : index
            %32 = memref.load %reinterpret_cast_7[%25] : memref<256xf32, #gpu.address_space<workgroup>>
            %33 = "disc_shape.linearize"(%31, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %34 = memref.load %reinterpret_cast_7[%33] : memref<256xf32, #gpu.address_space<workgroup>>
            %35 = arith.addf %32, %34 : f32
            memref.store %35, %reinterpret_cast_7[%25] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %28 = arith.cmpi slt, %19, %c1 : index
          scf.if %28 {
            %31 = arith.addi %22, %c1 : index
            %32 = memref.load %reinterpret_cast_7[%25] : memref<256xf32, #gpu.address_space<workgroup>>
            %33 = "disc_shape.linearize"(%31, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %34 = memref.load %reinterpret_cast_7[%33] : memref<256xf32, #gpu.address_space<workgroup>>
            %35 = arith.addf %32, %34 : f32
            memref.store %35, %reinterpret_cast_7[%25] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %29 = arith.cmpi eq, %19, %c0 : index
          %30 = arith.andi %29, %23 : i1
          scf.if %30 {
            %31 = memref.load %reinterpret_cast_7[%25] : memref<256xf32, #gpu.address_space<workgroup>>
            %32 = memref.atomic_rmw addf %31, %alloc_3[%20] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXblock_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
    }
  } else {
    %13 = arith.cmpi slt, %8, %c1 : index
    scf.if %13 {
      "lmhlo.fusion"() ({
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %reinterpret_cast_6 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
          memref.store %cst, %reinterpret_cast_6[%arg1] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %14 = arith.cmpi eq, %8, %c0 : index
        %15 = arith.subi %8, %c1 : index
        %16 = arith.divui %15, %c32 : index
        %17 = arith.addi %16, %c1 : index
        %18 = arith.select %14, %c0, %17 : index
        scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%18, %c512) step (%c1, %c1) {
          %19 = arith.cmpi ult, %arg2, %c1 : index
          scf.if %19 {
            %20 = scf.for %arg3 = %c0 to %c32 step %c1 iter_args(%arg4 = %cst) -> (f32) {
              %22 = arith.muli %arg1, %c32 : index
              %23 = arith.addi %22, %arg3 : index
              %24 = arith.cmpi slt, %23, %8 : index
              %25 = scf.if %24 -> (f32) {
                %26 = "disc_shape.linearize"(%23, %8) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
                %27:2 = "disc_shape.delinearize"(%26, %4, %c4) : (index, index, index) -> (index, index)
                %28 = arith.select %9, %27#0, %c0 : index
                %29 = "disc_shape.linearize"(%28, %27#1, %dim_0, %c4) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
                %30 = arith.muli %dim_0, %c4 : index
                %reinterpret_cast_6 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%30], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %31 = memref.load %reinterpret_cast_6[%29] : memref<?xf32, #gpu.address_space<global>>
                %32 = arith.select %10, %27#0, %c0 : index
                %33 = "disc_shape.linearize"(%32, %27#1, %dim, %c4) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
                %34 = arith.muli %dim, %c4 : index
                %reinterpret_cast_7 = memref.reinterpret_cast %alloc_2 to offset: [%c0], sizes: [%34], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %35 = memref.load %reinterpret_cast_7[%33] : memref<?xf32, #gpu.address_space<global>>
                %36 = arith.mulf %31, %35 : f32
                %37 = arith.addf %arg4, %36 : f32
                scf.yield %37 : f32
              } else {
                scf.yield %arg4 : f32
              }
              scf.yield %25 : f32
            }
            %21 = memref.atomic_rmw addf %20, %alloc_3[%arg2] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "thread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %reinterpret_cast_6 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
          memref.store %cst, %reinterpret_cast_6[%arg1] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %14 = arith.cmpi eq, %8, %c0 : index
        %15 = arith.subi %8, %c1 : index
        %16 = arith.divui %15, %c512 : index
        %17 = arith.addi %16, %c1 : index
        %18 = arith.select %14, %c0, %17 : index
        scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%18, %c256) step (%c1, %c1) {
          %19 = arith.divui %arg2, %c32 : index
          %20 = arith.remui %arg2, %c32 : index
          %21 = arith.muli %20, %c8 : index
          %22 = arith.addi %19, %21 : index
          %alloc_6 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
          %23 = arith.cmpi ult, %20, %c1 : index
          %24 = scf.if %23 -> (f32) {
            %31 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %cst) -> (f32) {
              %32 = arith.muli %arg1, %c8 : index
              %33 = arith.addi %19, %32 : index
              %34 = arith.muli %33, %c64 : index
              %35 = arith.addi %arg3, %34 : index
              %36 = arith.cmpi slt, %35, %8 : index
              %37 = scf.if %36 -> (f32) {
                %38 = "disc_shape.linearize"(%35, %8) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
                %39:2 = "disc_shape.delinearize"(%38, %4, %c4) : (index, index, index) -> (index, index)
                %40 = arith.select %9, %39#0, %c0 : index
                %41 = "disc_shape.linearize"(%40, %39#1, %dim_0, %c4) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
                %42 = arith.muli %dim_0, %c4 : index
                %reinterpret_cast_8 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%42], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %43 = memref.load %reinterpret_cast_8[%41] : memref<?xf32, #gpu.address_space<global>>
                %44 = arith.select %10, %39#0, %c0 : index
                %45 = "disc_shape.linearize"(%44, %39#1, %dim, %c4) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
                %46 = arith.muli %dim, %c4 : index
                %reinterpret_cast_9 = memref.reinterpret_cast %alloc_2 to offset: [%c0], sizes: [%46], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %47 = memref.load %reinterpret_cast_9[%45] : memref<?xf32, #gpu.address_space<global>>
                %48 = arith.mulf %43, %47 : f32
                %49 = arith.addf %arg4, %48 : f32
                scf.yield %49 : f32
              } else {
                scf.yield %arg4 : f32
              }
              scf.yield %37 : f32
            }
            scf.yield %31 : f32
          } else {
            scf.yield %cst : f32
          }
          %25 = "disc_shape.linearize"(%22, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_7 = memref.reinterpret_cast %alloc_6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %24, %reinterpret_cast_7[%25] : memref<256xf32, #gpu.address_space<workgroup>>
          gpu.barrier
          %26 = arith.cmpi slt, %19, %c4 : index
          scf.if %26 {
            %31 = arith.addi %22, %c4 : index
            %32 = memref.load %reinterpret_cast_7[%25] : memref<256xf32, #gpu.address_space<workgroup>>
            %33 = "disc_shape.linearize"(%31, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %34 = memref.load %reinterpret_cast_7[%33] : memref<256xf32, #gpu.address_space<workgroup>>
            %35 = arith.addf %32, %34 : f32
            memref.store %35, %reinterpret_cast_7[%25] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %27 = arith.cmpi slt, %19, %c2 : index
          scf.if %27 {
            %31 = arith.addi %22, %c2 : index
            %32 = memref.load %reinterpret_cast_7[%25] : memref<256xf32, #gpu.address_space<workgroup>>
            %33 = "disc_shape.linearize"(%31, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %34 = memref.load %reinterpret_cast_7[%33] : memref<256xf32, #gpu.address_space<workgroup>>
            %35 = arith.addf %32, %34 : f32
            memref.store %35, %reinterpret_cast_7[%25] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %28 = arith.cmpi slt, %19, %c1 : index
          scf.if %28 {
            %31 = arith.addi %22, %c1 : index
            %32 = memref.load %reinterpret_cast_7[%25] : memref<256xf32, #gpu.address_space<workgroup>>
            %33 = "disc_shape.linearize"(%31, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
            %34 = memref.load %reinterpret_cast_7[%33] : memref<256xf32, #gpu.address_space<workgroup>>
            %35 = arith.addf %32, %34 : f32
            memref.store %35, %reinterpret_cast_7[%25] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %29 = arith.cmpi eq, %19, %c0 : index
          %30 = arith.andi %29, %23 : i1
          scf.if %30 {
            %31 = memref.load %reinterpret_cast_7[%25] : memref<256xf32, #gpu.address_space<workgroup>>
            %32 = memref.atomic_rmw addf %31, %alloc_3[%20] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "block_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
    }
  }
  memref.dealloc %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc : memref<?x4xf32, #gpu.address_space<global>>
  %alloca = memref.alloca() : memref<0xindex>
  %12 = "disc_ral.dispatch"(%arg0, %5, %alloc_3, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1xf32, #gpu.address_space<global>>, memref<0xindex>) -> memref<f32, #gpu.address_space<global>>
  %reinterpret_cast_4 = memref.reinterpret_cast %12 to offset: [0], sizes: [], strides: [] : memref<f32, #gpu.address_space<global>> to memref<f32, #gpu.address_space<global>>
  memref.dealloc %alloc_3 : memref<1xf32, #gpu.address_space<global>>
  %alloc_5 = memref.alloc() : memref<f32>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_4, %alloc_5) {backend_config = "", call_target_name = "d2h", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<f32, #gpu.address_space<global>>, memref<f32>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  memref.dealloc %reinterpret_cast_4 : memref<f32, #gpu.address_space<global>>
  "disc_ral.dispatch"(%arg0, %c0, %alloc_5) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<f32>) -> ()
  return
}

// -----// IR Dump After ConvertShapeToStandardPass (disc-convert-shape-to-std) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c512 = arith.constant 512 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c4_i32 = arith.constant 4 : i32
  %c4 = arith.constant 4 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
  %2 = "disc_ral.dispatch"(%arg0, %c1) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
  %dim = memref.dim %2, %c0 : memref<?x4xf32>
  %dim_0 = memref.dim %1, %c0 : memref<?x4xf32>
  %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %reinterpret_cast_1 = memref.reinterpret_cast %2 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %3 = arith.cmpi eq, %dim, %c1 : index
  %4 = arith.select %3, %dim_0, %dim : index
  %alloc = memref.alloc(%dim_0) {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %5 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast, %alloc) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  %alloc_2 = memref.alloc(%dim) {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_1, %alloc_2) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  %6 = arith.index_cast %4 : index to i32
  %7 = arith.muli %6, %c4_i32 : i32
  %8 = arith.index_cast %7 : i32 to index
  %9 = arith.cmpi eq, %dim_0, %4 : index
  %10 = arith.cmpi eq, %dim, %4 : index
  %11 = arith.andi %10, %9 : i1
  %alloc_3 = memref.alloc() : memref<1xf32, #gpu.address_space<global>>
  scf.if %11 {
    %13 = arith.cmpi slt, %8, %c1 : index
    scf.if %13 {
      "lmhlo.fusion"() ({
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %reinterpret_cast_6 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
          memref.store %cst, %reinterpret_cast_6[%arg1] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %14 = arith.cmpi eq, %8, %c0 : index
        %15 = arith.subi %8, %c1 : index
        %16 = arith.divui %15, %c32 : index
        %17 = arith.addi %16, %c1 : index
        %18 = arith.select %14, %c0, %17 : index
        scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%18, %c512) step (%c1, %c1) {
          %19 = arith.cmpi ult, %arg2, %c1 : index
          scf.if %19 {
            %20 = scf.for %arg3 = %c0 to %c32 step %c1 iter_args(%arg4 = %cst) -> (f32) {
              %22 = arith.muli %arg1, %c32 : index
              %23 = arith.addi %22, %arg3 : index
              %24 = arith.cmpi slt, %23, %8 : index
              %25 = scf.if %24 -> (f32) {
                %c0_6 = arith.constant 0 : index
                %26 = arith.muli %4, %c4 : index
                %reinterpret_cast_7 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%26], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %27 = memref.load %reinterpret_cast_7[%23] : memref<?xf32, #gpu.address_space<global>>
                %reinterpret_cast_8 = memref.reinterpret_cast %alloc_2 to offset: [%c0], sizes: [%26], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %28 = memref.load %reinterpret_cast_8[%23] : memref<?xf32, #gpu.address_space<global>>
                %29 = arith.mulf %27, %28 : f32
                %30 = arith.addf %arg4, %29 : f32
                scf.yield %30 : f32
              } else {
                scf.yield %arg4 : f32
              }
              scf.yield %25 : f32
            }
            %21 = memref.atomic_rmw addf %20, %alloc_3[%arg2] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXthread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %reinterpret_cast_6 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
          memref.store %cst, %reinterpret_cast_6[%arg1] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %14 = arith.cmpi eq, %8, %c0 : index
        %15 = arith.subi %8, %c1 : index
        %16 = arith.divui %15, %c512 : index
        %17 = arith.addi %16, %c1 : index
        %18 = arith.select %14, %c0, %17 : index
        scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%18, %c256) step (%c1, %c1) {
          %19 = arith.divui %arg2, %c32 : index
          %20 = arith.remui %arg2, %c32 : index
          %21 = arith.muli %20, %c8 : index
          %22 = arith.addi %19, %21 : index
          %alloc_6 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
          %23 = arith.cmpi ult, %20, %c1 : index
          %24 = scf.if %23 -> (f32) {
            %30 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %cst) -> (f32) {
              %31 = arith.muli %arg1, %c8 : index
              %32 = arith.addi %19, %31 : index
              %33 = arith.muli %32, %c64 : index
              %34 = arith.addi %arg3, %33 : index
              %35 = arith.cmpi slt, %34, %8 : index
              %36 = scf.if %35 -> (f32) {
                %c0_9 = arith.constant 0 : index
                %37 = arith.muli %4, %c4 : index
                %reinterpret_cast_10 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%37], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %38 = memref.load %reinterpret_cast_10[%34] : memref<?xf32, #gpu.address_space<global>>
                %reinterpret_cast_11 = memref.reinterpret_cast %alloc_2 to offset: [%c0], sizes: [%37], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %39 = memref.load %reinterpret_cast_11[%34] : memref<?xf32, #gpu.address_space<global>>
                %40 = arith.mulf %38, %39 : f32
                %41 = arith.addf %arg4, %40 : f32
                scf.yield %41 : f32
              } else {
                scf.yield %arg4 : f32
              }
              scf.yield %36 : f32
            }
            scf.yield %30 : f32
          } else {
            scf.yield %cst : f32
          }
          %c0_7 = arith.constant 0 : index
          %reinterpret_cast_8 = memref.reinterpret_cast %alloc_6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %24, %reinterpret_cast_8[%22] : memref<256xf32, #gpu.address_space<workgroup>>
          gpu.barrier
          %25 = arith.cmpi slt, %19, %c4 : index
          scf.if %25 {
            %30 = arith.addi %22, %c4 : index
            %31 = memref.load %reinterpret_cast_8[%22] : memref<256xf32, #gpu.address_space<workgroup>>
            %c0_9 = arith.constant 0 : index
            %32 = memref.load %reinterpret_cast_8[%30] : memref<256xf32, #gpu.address_space<workgroup>>
            %33 = arith.addf %31, %32 : f32
            memref.store %33, %reinterpret_cast_8[%22] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %26 = arith.cmpi slt, %19, %c2 : index
          scf.if %26 {
            %30 = arith.addi %22, %c2 : index
            %31 = memref.load %reinterpret_cast_8[%22] : memref<256xf32, #gpu.address_space<workgroup>>
            %c0_9 = arith.constant 0 : index
            %32 = memref.load %reinterpret_cast_8[%30] : memref<256xf32, #gpu.address_space<workgroup>>
            %33 = arith.addf %31, %32 : f32
            memref.store %33, %reinterpret_cast_8[%22] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %27 = arith.cmpi slt, %19, %c1 : index
          scf.if %27 {
            %30 = arith.addi %22, %c1 : index
            %31 = memref.load %reinterpret_cast_8[%22] : memref<256xf32, #gpu.address_space<workgroup>>
            %c0_9 = arith.constant 0 : index
            %32 = memref.load %reinterpret_cast_8[%30] : memref<256xf32, #gpu.address_space<workgroup>>
            %33 = arith.addf %31, %32 : f32
            memref.store %33, %reinterpret_cast_8[%22] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %28 = arith.cmpi eq, %19, %c0 : index
          %29 = arith.andi %28, %23 : i1
          scf.if %29 {
            %30 = memref.load %reinterpret_cast_8[%22] : memref<256xf32, #gpu.address_space<workgroup>>
            %31 = memref.atomic_rmw addf %30, %alloc_3[%20] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXblock_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
    }
  } else {
    %13 = arith.cmpi slt, %8, %c1 : index
    scf.if %13 {
      "lmhlo.fusion"() ({
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %reinterpret_cast_6 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
          memref.store %cst, %reinterpret_cast_6[%arg1] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %14 = arith.cmpi eq, %8, %c0 : index
        %15 = arith.subi %8, %c1 : index
        %16 = arith.divui %15, %c32 : index
        %17 = arith.addi %16, %c1 : index
        %18 = arith.select %14, %c0, %17 : index
        scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%18, %c512) step (%c1, %c1) {
          %19 = arith.cmpi ult, %arg2, %c1 : index
          scf.if %19 {
            %20 = scf.for %arg3 = %c0 to %c32 step %c1 iter_args(%arg4 = %cst) -> (f32) {
              %22 = arith.muli %arg1, %c32 : index
              %23 = arith.addi %22, %arg3 : index
              %24 = arith.cmpi slt, %23, %8 : index
              %25 = scf.if %24 -> (f32) {
                %c0_6 = arith.constant 0 : index
                %26 = arith.remui %23, %c4 : index
                %27 = arith.divui %23, %c4 : index
                %28 = arith.select %9, %27, %c0 : index
                %c0_7 = arith.constant 0 : index
                %29 = arith.muli %28, %c4 : index
                %30 = arith.addi %29, %26 : index
                %31 = arith.muli %dim_0, %c4 : index
                %reinterpret_cast_8 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%31], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %32 = memref.load %reinterpret_cast_8[%30] : memref<?xf32, #gpu.address_space<global>>
                %33 = arith.select %10, %27, %c0 : index
                %c0_9 = arith.constant 0 : index
                %34 = arith.muli %33, %c4 : index
                %35 = arith.addi %34, %26 : index
                %36 = arith.muli %dim, %c4 : index
                %reinterpret_cast_10 = memref.reinterpret_cast %alloc_2 to offset: [%c0], sizes: [%36], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %37 = memref.load %reinterpret_cast_10[%35] : memref<?xf32, #gpu.address_space<global>>
                %38 = arith.mulf %32, %37 : f32
                %39 = arith.addf %arg4, %38 : f32
                scf.yield %39 : f32
              } else {
                scf.yield %arg4 : f32
              }
              scf.yield %25 : f32
            }
            %21 = memref.atomic_rmw addf %20, %alloc_3[%arg2] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "thread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %reinterpret_cast_6 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
          memref.store %cst, %reinterpret_cast_6[%arg1] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %14 = arith.cmpi eq, %8, %c0 : index
        %15 = arith.subi %8, %c1 : index
        %16 = arith.divui %15, %c512 : index
        %17 = arith.addi %16, %c1 : index
        %18 = arith.select %14, %c0, %17 : index
        scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%18, %c256) step (%c1, %c1) {
          %19 = arith.divui %arg2, %c32 : index
          %20 = arith.remui %arg2, %c32 : index
          %21 = arith.muli %20, %c8 : index
          %22 = arith.addi %19, %21 : index
          %alloc_6 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
          %23 = arith.cmpi ult, %20, %c1 : index
          %24 = scf.if %23 -> (f32) {
            %30 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %cst) -> (f32) {
              %31 = arith.muli %arg1, %c8 : index
              %32 = arith.addi %19, %31 : index
              %33 = arith.muli %32, %c64 : index
              %34 = arith.addi %arg3, %33 : index
              %35 = arith.cmpi slt, %34, %8 : index
              %36 = scf.if %35 -> (f32) {
                %c0_9 = arith.constant 0 : index
                %37 = arith.remui %34, %c4 : index
                %38 = arith.divui %34, %c4 : index
                %39 = arith.select %9, %38, %c0 : index
                %c0_10 = arith.constant 0 : index
                %40 = arith.muli %39, %c4 : index
                %41 = arith.addi %40, %37 : index
                %42 = arith.muli %dim_0, %c4 : index
                %reinterpret_cast_11 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%42], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %43 = memref.load %reinterpret_cast_11[%41] : memref<?xf32, #gpu.address_space<global>>
                %44 = arith.select %10, %38, %c0 : index
                %c0_12 = arith.constant 0 : index
                %45 = arith.muli %44, %c4 : index
                %46 = arith.addi %45, %37 : index
                %47 = arith.muli %dim, %c4 : index
                %reinterpret_cast_13 = memref.reinterpret_cast %alloc_2 to offset: [%c0], sizes: [%47], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %48 = memref.load %reinterpret_cast_13[%46] : memref<?xf32, #gpu.address_space<global>>
                %49 = arith.mulf %43, %48 : f32
                %50 = arith.addf %arg4, %49 : f32
                scf.yield %50 : f32
              } else {
                scf.yield %arg4 : f32
              }
              scf.yield %36 : f32
            }
            scf.yield %30 : f32
          } else {
            scf.yield %cst : f32
          }
          %c0_7 = arith.constant 0 : index
          %reinterpret_cast_8 = memref.reinterpret_cast %alloc_6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %24, %reinterpret_cast_8[%22] : memref<256xf32, #gpu.address_space<workgroup>>
          gpu.barrier
          %25 = arith.cmpi slt, %19, %c4 : index
          scf.if %25 {
            %30 = arith.addi %22, %c4 : index
            %31 = memref.load %reinterpret_cast_8[%22] : memref<256xf32, #gpu.address_space<workgroup>>
            %c0_9 = arith.constant 0 : index
            %32 = memref.load %reinterpret_cast_8[%30] : memref<256xf32, #gpu.address_space<workgroup>>
            %33 = arith.addf %31, %32 : f32
            memref.store %33, %reinterpret_cast_8[%22] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %26 = arith.cmpi slt, %19, %c2 : index
          scf.if %26 {
            %30 = arith.addi %22, %c2 : index
            %31 = memref.load %reinterpret_cast_8[%22] : memref<256xf32, #gpu.address_space<workgroup>>
            %c0_9 = arith.constant 0 : index
            %32 = memref.load %reinterpret_cast_8[%30] : memref<256xf32, #gpu.address_space<workgroup>>
            %33 = arith.addf %31, %32 : f32
            memref.store %33, %reinterpret_cast_8[%22] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %27 = arith.cmpi slt, %19, %c1 : index
          scf.if %27 {
            %30 = arith.addi %22, %c1 : index
            %31 = memref.load %reinterpret_cast_8[%22] : memref<256xf32, #gpu.address_space<workgroup>>
            %c0_9 = arith.constant 0 : index
            %32 = memref.load %reinterpret_cast_8[%30] : memref<256xf32, #gpu.address_space<workgroup>>
            %33 = arith.addf %31, %32 : f32
            memref.store %33, %reinterpret_cast_8[%22] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %28 = arith.cmpi eq, %19, %c0 : index
          %29 = arith.andi %28, %23 : i1
          scf.if %29 {
            %30 = memref.load %reinterpret_cast_8[%22] : memref<256xf32, #gpu.address_space<workgroup>>
            %31 = memref.atomic_rmw addf %30, %alloc_3[%20] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "block_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
    }
  }
  memref.dealloc %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc : memref<?x4xf32, #gpu.address_space<global>>
  %alloca = memref.alloca() : memref<0xindex>
  %12 = "disc_ral.dispatch"(%arg0, %5, %alloc_3, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1xf32, #gpu.address_space<global>>, memref<0xindex>) -> memref<f32, #gpu.address_space<global>>
  %reinterpret_cast_4 = memref.reinterpret_cast %12 to offset: [0], sizes: [], strides: [] : memref<f32, #gpu.address_space<global>> to memref<f32, #gpu.address_space<global>>
  memref.dealloc %alloc_3 : memref<1xf32, #gpu.address_space<global>>
  %alloc_5 = memref.alloc() : memref<f32>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_4, %alloc_5) {backend_config = "", call_target_name = "d2h", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<f32, #gpu.address_space<global>>, memref<f32>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  memref.dealloc %reinterpret_cast_4 : memref<f32, #gpu.address_space<global>>
  "disc_ral.dispatch"(%arg0, %c0, %alloc_5) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<f32>) -> ()
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c512 = arith.constant 512 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c4_i32 = arith.constant 4 : i32
  %c4 = arith.constant 4 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
  %2 = "disc_ral.dispatch"(%arg0, %c1) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
  %dim = memref.dim %2, %c0 : memref<?x4xf32>
  %dim_0 = memref.dim %1, %c0 : memref<?x4xf32>
  %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %reinterpret_cast_1 = memref.reinterpret_cast %2 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %3 = arith.cmpi eq, %dim, %c1 : index
  %4 = arith.select %3, %dim_0, %dim : index
  %alloc = memref.alloc(%dim_0) {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %5 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast, %alloc) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  %alloc_2 = memref.alloc(%dim) {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_1, %alloc_2) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  %6 = arith.index_cast %4 : index to i32
  %7 = arith.muli %6, %c4_i32 : i32
  %8 = arith.index_cast %7 : i32 to index
  %9 = arith.cmpi eq, %dim_0, %4 : index
  %10 = arith.cmpi eq, %dim, %4 : index
  %11 = arith.andi %10, %9 : i1
  %alloc_3 = memref.alloc() : memref<1xf32, #gpu.address_space<global>>
  scf.if %11 {
    %13 = arith.cmpi slt, %8, %c1 : index
    scf.if %13 {
      "lmhlo.fusion"() ({
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %reinterpret_cast_6 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
          memref.store %cst, %reinterpret_cast_6[%arg1] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %14 = arith.cmpi eq, %8, %c0 : index
        %15 = arith.subi %8, %c1 : index
        %16 = arith.divui %15, %c32 : index
        %17 = arith.addi %16, %c1 : index
        %18 = arith.select %14, %c0, %17 : index
        scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%18, %c512) step (%c1, %c1) {
          %19 = arith.cmpi ult, %arg2, %c1 : index
          scf.if %19 {
            %20 = scf.for %arg3 = %c0 to %c32 step %c1 iter_args(%arg4 = %cst) -> (f32) {
              %22 = arith.muli %arg1, %c32 : index
              %23 = arith.addi %22, %arg3 : index
              %24 = arith.cmpi slt, %23, %8 : index
              %25 = scf.if %24 -> (f32) {
                %26 = arith.muli %4, %c4 : index
                %reinterpret_cast_6 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%26], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %27 = memref.load %reinterpret_cast_6[%23] : memref<?xf32, #gpu.address_space<global>>
                %reinterpret_cast_7 = memref.reinterpret_cast %alloc_2 to offset: [%c0], sizes: [%26], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %28 = memref.load %reinterpret_cast_7[%23] : memref<?xf32, #gpu.address_space<global>>
                %29 = arith.mulf %27, %28 : f32
                %30 = arith.addf %arg4, %29 : f32
                scf.yield %30 : f32
              } else {
                scf.yield %arg4 : f32
              }
              scf.yield %25 : f32
            }
            %21 = memref.atomic_rmw addf %20, %alloc_3[%arg2] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXthread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %reinterpret_cast_6 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
          memref.store %cst, %reinterpret_cast_6[%arg1] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %14 = arith.cmpi eq, %8, %c0 : index
        %15 = arith.subi %8, %c1 : index
        %16 = arith.divui %15, %c512 : index
        %17 = arith.addi %16, %c1 : index
        %18 = arith.select %14, %c0, %17 : index
        scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%18, %c256) step (%c1, %c1) {
          %19 = arith.divui %arg2, %c32 : index
          %20 = arith.remui %arg2, %c32 : index
          %21 = arith.muli %20, %c8 : index
          %22 = arith.addi %19, %21 : index
          %alloc_6 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
          %23 = arith.cmpi ult, %20, %c1 : index
          %24 = scf.if %23 -> (f32) {
            %30 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %cst) -> (f32) {
              %31 = arith.muli %arg1, %c8 : index
              %32 = arith.addi %19, %31 : index
              %33 = arith.muli %32, %c64 : index
              %34 = arith.addi %arg3, %33 : index
              %35 = arith.cmpi slt, %34, %8 : index
              %36 = scf.if %35 -> (f32) {
                %37 = arith.muli %4, %c4 : index
                %reinterpret_cast_8 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%37], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %38 = memref.load %reinterpret_cast_8[%34] : memref<?xf32, #gpu.address_space<global>>
                %reinterpret_cast_9 = memref.reinterpret_cast %alloc_2 to offset: [%c0], sizes: [%37], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %39 = memref.load %reinterpret_cast_9[%34] : memref<?xf32, #gpu.address_space<global>>
                %40 = arith.mulf %38, %39 : f32
                %41 = arith.addf %arg4, %40 : f32
                scf.yield %41 : f32
              } else {
                scf.yield %arg4 : f32
              }
              scf.yield %36 : f32
            }
            scf.yield %30 : f32
          } else {
            scf.yield %cst : f32
          }
          %reinterpret_cast_7 = memref.reinterpret_cast %alloc_6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %24, %reinterpret_cast_7[%22] : memref<256xf32, #gpu.address_space<workgroup>>
          gpu.barrier
          %25 = arith.cmpi slt, %19, %c4 : index
          scf.if %25 {
            %30 = arith.addi %22, %c4 : index
            %31 = memref.load %reinterpret_cast_7[%22] : memref<256xf32, #gpu.address_space<workgroup>>
            %32 = memref.load %reinterpret_cast_7[%30] : memref<256xf32, #gpu.address_space<workgroup>>
            %33 = arith.addf %31, %32 : f32
            memref.store %33, %reinterpret_cast_7[%22] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %26 = arith.cmpi slt, %19, %c2 : index
          scf.if %26 {
            %30 = arith.addi %22, %c2 : index
            %31 = memref.load %reinterpret_cast_7[%22] : memref<256xf32, #gpu.address_space<workgroup>>
            %32 = memref.load %reinterpret_cast_7[%30] : memref<256xf32, #gpu.address_space<workgroup>>
            %33 = arith.addf %31, %32 : f32
            memref.store %33, %reinterpret_cast_7[%22] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %27 = arith.cmpi slt, %19, %c1 : index
          scf.if %27 {
            %30 = arith.addi %22, %c1 : index
            %31 = memref.load %reinterpret_cast_7[%22] : memref<256xf32, #gpu.address_space<workgroup>>
            %32 = memref.load %reinterpret_cast_7[%30] : memref<256xf32, #gpu.address_space<workgroup>>
            %33 = arith.addf %31, %32 : f32
            memref.store %33, %reinterpret_cast_7[%22] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %28 = arith.cmpi eq, %19, %c0 : index
          %29 = arith.andi %28, %23 : i1
          scf.if %29 {
            %30 = memref.load %reinterpret_cast_7[%22] : memref<256xf32, #gpu.address_space<workgroup>>
            %31 = memref.atomic_rmw addf %30, %alloc_3[%20] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXblock_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
    }
  } else {
    %13 = arith.cmpi slt, %8, %c1 : index
    scf.if %13 {
      "lmhlo.fusion"() ({
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %reinterpret_cast_6 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
          memref.store %cst, %reinterpret_cast_6[%arg1] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %14 = arith.cmpi eq, %8, %c0 : index
        %15 = arith.subi %8, %c1 : index
        %16 = arith.divui %15, %c32 : index
        %17 = arith.addi %16, %c1 : index
        %18 = arith.select %14, %c0, %17 : index
        scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%18, %c512) step (%c1, %c1) {
          %19 = arith.cmpi ult, %arg2, %c1 : index
          scf.if %19 {
            %20 = scf.for %arg3 = %c0 to %c32 step %c1 iter_args(%arg4 = %cst) -> (f32) {
              %22 = arith.muli %arg1, %c32 : index
              %23 = arith.addi %22, %arg3 : index
              %24 = arith.cmpi slt, %23, %8 : index
              %25 = scf.if %24 -> (f32) {
                %26 = arith.remui %23, %c4 : index
                %27 = arith.divui %23, %c4 : index
                %28 = arith.select %9, %27, %c0 : index
                %29 = arith.muli %28, %c4 : index
                %30 = arith.addi %29, %26 : index
                %31 = arith.muli %dim_0, %c4 : index
                %reinterpret_cast_6 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%31], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %32 = memref.load %reinterpret_cast_6[%30] : memref<?xf32, #gpu.address_space<global>>
                %33 = arith.select %10, %27, %c0 : index
                %34 = arith.muli %33, %c4 : index
                %35 = arith.addi %34, %26 : index
                %36 = arith.muli %dim, %c4 : index
                %reinterpret_cast_7 = memref.reinterpret_cast %alloc_2 to offset: [%c0], sizes: [%36], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %37 = memref.load %reinterpret_cast_7[%35] : memref<?xf32, #gpu.address_space<global>>
                %38 = arith.mulf %32, %37 : f32
                %39 = arith.addf %arg4, %38 : f32
                scf.yield %39 : f32
              } else {
                scf.yield %arg4 : f32
              }
              scf.yield %25 : f32
            }
            %21 = memref.atomic_rmw addf %20, %alloc_3[%arg2] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "thread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %reinterpret_cast_6 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
          memref.store %cst, %reinterpret_cast_6[%arg1] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %14 = arith.cmpi eq, %8, %c0 : index
        %15 = arith.subi %8, %c1 : index
        %16 = arith.divui %15, %c512 : index
        %17 = arith.addi %16, %c1 : index
        %18 = arith.select %14, %c0, %17 : index
        scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%18, %c256) step (%c1, %c1) {
          %19 = arith.divui %arg2, %c32 : index
          %20 = arith.remui %arg2, %c32 : index
          %21 = arith.muli %20, %c8 : index
          %22 = arith.addi %19, %21 : index
          %alloc_6 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
          %23 = arith.cmpi ult, %20, %c1 : index
          %24 = scf.if %23 -> (f32) {
            %30 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %cst) -> (f32) {
              %31 = arith.muli %arg1, %c8 : index
              %32 = arith.addi %19, %31 : index
              %33 = arith.muli %32, %c64 : index
              %34 = arith.addi %arg3, %33 : index
              %35 = arith.cmpi slt, %34, %8 : index
              %36 = scf.if %35 -> (f32) {
                %37 = arith.remui %34, %c4 : index
                %38 = arith.divui %34, %c4 : index
                %39 = arith.select %9, %38, %c0 : index
                %40 = arith.muli %39, %c4 : index
                %41 = arith.addi %40, %37 : index
                %42 = arith.muli %dim_0, %c4 : index
                %reinterpret_cast_8 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%42], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %43 = memref.load %reinterpret_cast_8[%41] : memref<?xf32, #gpu.address_space<global>>
                %44 = arith.select %10, %38, %c0 : index
                %45 = arith.muli %44, %c4 : index
                %46 = arith.addi %45, %37 : index
                %47 = arith.muli %dim, %c4 : index
                %reinterpret_cast_9 = memref.reinterpret_cast %alloc_2 to offset: [%c0], sizes: [%47], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %48 = memref.load %reinterpret_cast_9[%46] : memref<?xf32, #gpu.address_space<global>>
                %49 = arith.mulf %43, %48 : f32
                %50 = arith.addf %arg4, %49 : f32
                scf.yield %50 : f32
              } else {
                scf.yield %arg4 : f32
              }
              scf.yield %36 : f32
            }
            scf.yield %30 : f32
          } else {
            scf.yield %cst : f32
          }
          %reinterpret_cast_7 = memref.reinterpret_cast %alloc_6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %24, %reinterpret_cast_7[%22] : memref<256xf32, #gpu.address_space<workgroup>>
          gpu.barrier
          %25 = arith.cmpi slt, %19, %c4 : index
          scf.if %25 {
            %30 = arith.addi %22, %c4 : index
            %31 = memref.load %reinterpret_cast_7[%22] : memref<256xf32, #gpu.address_space<workgroup>>
            %32 = memref.load %reinterpret_cast_7[%30] : memref<256xf32, #gpu.address_space<workgroup>>
            %33 = arith.addf %31, %32 : f32
            memref.store %33, %reinterpret_cast_7[%22] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %26 = arith.cmpi slt, %19, %c2 : index
          scf.if %26 {
            %30 = arith.addi %22, %c2 : index
            %31 = memref.load %reinterpret_cast_7[%22] : memref<256xf32, #gpu.address_space<workgroup>>
            %32 = memref.load %reinterpret_cast_7[%30] : memref<256xf32, #gpu.address_space<workgroup>>
            %33 = arith.addf %31, %32 : f32
            memref.store %33, %reinterpret_cast_7[%22] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %27 = arith.cmpi slt, %19, %c1 : index
          scf.if %27 {
            %30 = arith.addi %22, %c1 : index
            %31 = memref.load %reinterpret_cast_7[%22] : memref<256xf32, #gpu.address_space<workgroup>>
            %32 = memref.load %reinterpret_cast_7[%30] : memref<256xf32, #gpu.address_space<workgroup>>
            %33 = arith.addf %31, %32 : f32
            memref.store %33, %reinterpret_cast_7[%22] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %28 = arith.cmpi eq, %19, %c0 : index
          %29 = arith.andi %28, %23 : i1
          scf.if %29 {
            %30 = memref.load %reinterpret_cast_7[%22] : memref<256xf32, #gpu.address_space<workgroup>>
            %31 = memref.atomic_rmw addf %30, %alloc_3[%20] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "block_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
    }
  }
  memref.dealloc %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc : memref<?x4xf32, #gpu.address_space<global>>
  %alloca = memref.alloca() : memref<0xindex>
  %12 = "disc_ral.dispatch"(%arg0, %5, %alloc_3, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1xf32, #gpu.address_space<global>>, memref<0xindex>) -> memref<f32, #gpu.address_space<global>>
  %reinterpret_cast_4 = memref.reinterpret_cast %12 to offset: [0], sizes: [], strides: [] : memref<f32, #gpu.address_space<global>> to memref<f32, #gpu.address_space<global>>
  memref.dealloc %alloc_3 : memref<1xf32, #gpu.address_space<global>>
  %alloc_5 = memref.alloc() : memref<f32>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_4, %alloc_5) {backend_config = "", call_target_name = "d2h", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<f32, #gpu.address_space<global>>, memref<f32>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  memref.dealloc %reinterpret_cast_4 : memref<f32, #gpu.address_space<global>>
  "disc_ral.dispatch"(%arg0, %c0, %alloc_5) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<f32>) -> ()
  return
}

// -----// IR Dump After ParallelLoopCollapsing (disc-parallel-loop-collapsing) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c512 = arith.constant 512 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c4_i32 = arith.constant 4 : i32
  %c4 = arith.constant 4 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
  %2 = "disc_ral.dispatch"(%arg0, %c1) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
  %dim = memref.dim %2, %c0 : memref<?x4xf32>
  %dim_0 = memref.dim %1, %c0 : memref<?x4xf32>
  %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %reinterpret_cast_1 = memref.reinterpret_cast %2 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %3 = arith.cmpi eq, %dim, %c1 : index
  %4 = arith.select %3, %dim_0, %dim : index
  %alloc = memref.alloc(%dim_0) {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %5 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast, %alloc) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  %alloc_2 = memref.alloc(%dim) {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_1, %alloc_2) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  %6 = arith.index_cast %4 : index to i32
  %7 = arith.muli %6, %c4_i32 : i32
  %8 = arith.index_cast %7 : i32 to index
  %9 = arith.cmpi eq, %dim_0, %4 : index
  %10 = arith.cmpi eq, %dim, %4 : index
  %11 = arith.andi %10, %9 : i1
  %alloc_3 = memref.alloc() : memref<1xf32, #gpu.address_space<global>>
  scf.if %11 {
    %13 = arith.cmpi slt, %8, %c1 : index
    scf.if %13 {
      "lmhlo.fusion"() ({
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %reinterpret_cast_9 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
          memref.store %cst, %reinterpret_cast_9[%arg1] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %14 = arith.cmpi eq, %8, %c0 : index
        %15 = arith.subi %8, %c1 : index
        %16 = arith.divui %15, %c32 : index
        %17 = arith.addi %16, %c1 : index
        %18 = arith.select %14, %c0, %17 : index
        %c0_6 = arith.constant 0 : index
        %c1_7 = arith.constant 1 : index
        %c1_8 = arith.constant 1 : index
        %19 = arith.muli %c1_8, %18 : index
        %20 = arith.muli %19, %c512 : index
        scf.parallel (%arg1) = (%c0_6) to (%20) step (%c1_7) {
          %21 = arith.remsi %arg1, %c512 : index
          %22 = arith.divsi %arg1, %c512 : index
          %23 = arith.cmpi ult, %21, %c1 : index
          scf.if %23 {
            %24 = scf.for %arg2 = %c0 to %c32 step %c1 iter_args(%arg3 = %cst) -> (f32) {
              %26 = arith.muli %22, %c32 : index
              %27 = arith.addi %26, %arg2 : index
              %28 = arith.cmpi slt, %27, %8 : index
              %29 = scf.if %28 -> (f32) {
                %30 = arith.muli %4, %c4 : index
                %reinterpret_cast_9 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%30], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %31 = memref.load %reinterpret_cast_9[%27] : memref<?xf32, #gpu.address_space<global>>
                %reinterpret_cast_10 = memref.reinterpret_cast %alloc_2 to offset: [%c0], sizes: [%30], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %32 = memref.load %reinterpret_cast_10[%27] : memref<?xf32, #gpu.address_space<global>>
                %33 = arith.mulf %31, %32 : f32
                %34 = arith.addf %arg3, %33 : f32
                scf.yield %34 : f32
              } else {
                scf.yield %arg3 : f32
              }
              scf.yield %29 : f32
            }
            %25 = memref.atomic_rmw addf %24, %alloc_3[%21] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXthread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %reinterpret_cast_9 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
          memref.store %cst, %reinterpret_cast_9[%arg1] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %14 = arith.cmpi eq, %8, %c0 : index
        %15 = arith.subi %8, %c1 : index
        %16 = arith.divui %15, %c512 : index
        %17 = arith.addi %16, %c1 : index
        %18 = arith.select %14, %c0, %17 : index
        %c0_6 = arith.constant 0 : index
        %c1_7 = arith.constant 1 : index
        %c1_8 = arith.constant 1 : index
        %19 = arith.muli %c1_8, %18 : index
        %20 = arith.muli %19, %c256 : index
        scf.parallel (%arg1) = (%c0_6) to (%20) step (%c1_7) {
          %21 = arith.remsi %arg1, %c256 : index
          %22 = arith.divsi %arg1, %c256 : index
          %23 = arith.divui %21, %c32 : index
          %24 = arith.remui %21, %c32 : index
          %25 = arith.muli %24, %c8 : index
          %26 = arith.addi %23, %25 : index
          %alloc_9 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
          %27 = arith.cmpi ult, %24, %c1 : index
          %28 = scf.if %27 -> (f32) {
            %34 = scf.for %arg2 = %c0 to %c64 step %c1 iter_args(%arg3 = %cst) -> (f32) {
              %35 = arith.muli %22, %c8 : index
              %36 = arith.addi %23, %35 : index
              %37 = arith.muli %36, %c64 : index
              %38 = arith.addi %arg2, %37 : index
              %39 = arith.cmpi slt, %38, %8 : index
              %40 = scf.if %39 -> (f32) {
                %41 = arith.muli %4, %c4 : index
                %reinterpret_cast_11 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%41], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %42 = memref.load %reinterpret_cast_11[%38] : memref<?xf32, #gpu.address_space<global>>
                %reinterpret_cast_12 = memref.reinterpret_cast %alloc_2 to offset: [%c0], sizes: [%41], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %43 = memref.load %reinterpret_cast_12[%38] : memref<?xf32, #gpu.address_space<global>>
                %44 = arith.mulf %42, %43 : f32
                %45 = arith.addf %arg3, %44 : f32
                scf.yield %45 : f32
              } else {
                scf.yield %arg3 : f32
              }
              scf.yield %40 : f32
            }
            scf.yield %34 : f32
          } else {
            scf.yield %cst : f32
          }
          %reinterpret_cast_10 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %28, %reinterpret_cast_10[%26] : memref<256xf32, #gpu.address_space<workgroup>>
          gpu.barrier
          %29 = arith.cmpi slt, %23, %c4 : index
          scf.if %29 {
            %34 = arith.addi %26, %c4 : index
            %35 = memref.load %reinterpret_cast_10[%26] : memref<256xf32, #gpu.address_space<workgroup>>
            %36 = memref.load %reinterpret_cast_10[%34] : memref<256xf32, #gpu.address_space<workgroup>>
            %37 = arith.addf %35, %36 : f32
            memref.store %37, %reinterpret_cast_10[%26] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %30 = arith.cmpi slt, %23, %c2 : index
          scf.if %30 {
            %34 = arith.addi %26, %c2 : index
            %35 = memref.load %reinterpret_cast_10[%26] : memref<256xf32, #gpu.address_space<workgroup>>
            %36 = memref.load %reinterpret_cast_10[%34] : memref<256xf32, #gpu.address_space<workgroup>>
            %37 = arith.addf %35, %36 : f32
            memref.store %37, %reinterpret_cast_10[%26] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %31 = arith.cmpi slt, %23, %c1 : index
          scf.if %31 {
            %34 = arith.addi %26, %c1 : index
            %35 = memref.load %reinterpret_cast_10[%26] : memref<256xf32, #gpu.address_space<workgroup>>
            %36 = memref.load %reinterpret_cast_10[%34] : memref<256xf32, #gpu.address_space<workgroup>>
            %37 = arith.addf %35, %36 : f32
            memref.store %37, %reinterpret_cast_10[%26] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %32 = arith.cmpi eq, %23, %c0 : index
          %33 = arith.andi %32, %27 : i1
          scf.if %33 {
            %34 = memref.load %reinterpret_cast_10[%26] : memref<256xf32, #gpu.address_space<workgroup>>
            %35 = memref.atomic_rmw addf %34, %alloc_3[%24] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXblock_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
    }
  } else {
    %13 = arith.cmpi slt, %8, %c1 : index
    scf.if %13 {
      "lmhlo.fusion"() ({
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %reinterpret_cast_9 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
          memref.store %cst, %reinterpret_cast_9[%arg1] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %14 = arith.cmpi eq, %8, %c0 : index
        %15 = arith.subi %8, %c1 : index
        %16 = arith.divui %15, %c32 : index
        %17 = arith.addi %16, %c1 : index
        %18 = arith.select %14, %c0, %17 : index
        %c0_6 = arith.constant 0 : index
        %c1_7 = arith.constant 1 : index
        %c1_8 = arith.constant 1 : index
        %19 = arith.muli %c1_8, %18 : index
        %20 = arith.muli %19, %c512 : index
        scf.parallel (%arg1) = (%c0_6) to (%20) step (%c1_7) {
          %21 = arith.remsi %arg1, %c512 : index
          %22 = arith.divsi %arg1, %c512 : index
          %23 = arith.cmpi ult, %21, %c1 : index
          scf.if %23 {
            %24 = scf.for %arg2 = %c0 to %c32 step %c1 iter_args(%arg3 = %cst) -> (f32) {
              %26 = arith.muli %22, %c32 : index
              %27 = arith.addi %26, %arg2 : index
              %28 = arith.cmpi slt, %27, %8 : index
              %29 = scf.if %28 -> (f32) {
                %30 = arith.remui %27, %c4 : index
                %31 = arith.divui %27, %c4 : index
                %32 = arith.select %9, %31, %c0 : index
                %33 = arith.muli %32, %c4 : index
                %34 = arith.addi %33, %30 : index
                %35 = arith.muli %dim_0, %c4 : index
                %reinterpret_cast_9 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%35], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %36 = memref.load %reinterpret_cast_9[%34] : memref<?xf32, #gpu.address_space<global>>
                %37 = arith.select %10, %31, %c0 : index
                %38 = arith.muli %37, %c4 : index
                %39 = arith.addi %38, %30 : index
                %40 = arith.muli %dim, %c4 : index
                %reinterpret_cast_10 = memref.reinterpret_cast %alloc_2 to offset: [%c0], sizes: [%40], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %41 = memref.load %reinterpret_cast_10[%39] : memref<?xf32, #gpu.address_space<global>>
                %42 = arith.mulf %36, %41 : f32
                %43 = arith.addf %arg3, %42 : f32
                scf.yield %43 : f32
              } else {
                scf.yield %arg3 : f32
              }
              scf.yield %29 : f32
            }
            %25 = memref.atomic_rmw addf %24, %alloc_3[%21] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "thread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        scf.parallel (%arg1) = (%c0) to (%c1) step (%c1) {
          %reinterpret_cast_9 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
          memref.store %cst, %reinterpret_cast_9[%arg1] : memref<1xf32, #gpu.address_space<global>>
          scf.yield
        }
        %14 = arith.cmpi eq, %8, %c0 : index
        %15 = arith.subi %8, %c1 : index
        %16 = arith.divui %15, %c512 : index
        %17 = arith.addi %16, %c1 : index
        %18 = arith.select %14, %c0, %17 : index
        %c0_6 = arith.constant 0 : index
        %c1_7 = arith.constant 1 : index
        %c1_8 = arith.constant 1 : index
        %19 = arith.muli %c1_8, %18 : index
        %20 = arith.muli %19, %c256 : index
        scf.parallel (%arg1) = (%c0_6) to (%20) step (%c1_7) {
          %21 = arith.remsi %arg1, %c256 : index
          %22 = arith.divsi %arg1, %c256 : index
          %23 = arith.divui %21, %c32 : index
          %24 = arith.remui %21, %c32 : index
          %25 = arith.muli %24, %c8 : index
          %26 = arith.addi %23, %25 : index
          %alloc_9 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
          %27 = arith.cmpi ult, %24, %c1 : index
          %28 = scf.if %27 -> (f32) {
            %34 = scf.for %arg2 = %c0 to %c64 step %c1 iter_args(%arg3 = %cst) -> (f32) {
              %35 = arith.muli %22, %c8 : index
              %36 = arith.addi %23, %35 : index
              %37 = arith.muli %36, %c64 : index
              %38 = arith.addi %arg2, %37 : index
              %39 = arith.cmpi slt, %38, %8 : index
              %40 = scf.if %39 -> (f32) {
                %41 = arith.remui %38, %c4 : index
                %42 = arith.divui %38, %c4 : index
                %43 = arith.select %9, %42, %c0 : index
                %44 = arith.muli %43, %c4 : index
                %45 = arith.addi %44, %41 : index
                %46 = arith.muli %dim_0, %c4 : index
                %reinterpret_cast_11 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%46], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %47 = memref.load %reinterpret_cast_11[%45] : memref<?xf32, #gpu.address_space<global>>
                %48 = arith.select %10, %42, %c0 : index
                %49 = arith.muli %48, %c4 : index
                %50 = arith.addi %49, %41 : index
                %51 = arith.muli %dim, %c4 : index
                %reinterpret_cast_12 = memref.reinterpret_cast %alloc_2 to offset: [%c0], sizes: [%51], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                %52 = memref.load %reinterpret_cast_12[%50] : memref<?xf32, #gpu.address_space<global>>
                %53 = arith.mulf %47, %52 : f32
                %54 = arith.addf %arg3, %53 : f32
                scf.yield %54 : f32
              } else {
                scf.yield %arg3 : f32
              }
              scf.yield %40 : f32
            }
            scf.yield %34 : f32
          } else {
            scf.yield %cst : f32
          }
          %reinterpret_cast_10 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %28, %reinterpret_cast_10[%26] : memref<256xf32, #gpu.address_space<workgroup>>
          gpu.barrier
          %29 = arith.cmpi slt, %23, %c4 : index
          scf.if %29 {
            %34 = arith.addi %26, %c4 : index
            %35 = memref.load %reinterpret_cast_10[%26] : memref<256xf32, #gpu.address_space<workgroup>>
            %36 = memref.load %reinterpret_cast_10[%34] : memref<256xf32, #gpu.address_space<workgroup>>
            %37 = arith.addf %35, %36 : f32
            memref.store %37, %reinterpret_cast_10[%26] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %30 = arith.cmpi slt, %23, %c2 : index
          scf.if %30 {
            %34 = arith.addi %26, %c2 : index
            %35 = memref.load %reinterpret_cast_10[%26] : memref<256xf32, #gpu.address_space<workgroup>>
            %36 = memref.load %reinterpret_cast_10[%34] : memref<256xf32, #gpu.address_space<workgroup>>
            %37 = arith.addf %35, %36 : f32
            memref.store %37, %reinterpret_cast_10[%26] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %31 = arith.cmpi slt, %23, %c1 : index
          scf.if %31 {
            %34 = arith.addi %26, %c1 : index
            %35 = memref.load %reinterpret_cast_10[%26] : memref<256xf32, #gpu.address_space<workgroup>>
            %36 = memref.load %reinterpret_cast_10[%34] : memref<256xf32, #gpu.address_space<workgroup>>
            %37 = arith.addf %35, %36 : f32
            memref.store %37, %reinterpret_cast_10[%26] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %32 = arith.cmpi eq, %23, %c0 : index
          %33 = arith.andi %32, %27 : i1
          scf.if %33 {
            %34 = memref.load %reinterpret_cast_10[%26] : memref<256xf32, #gpu.address_space<workgroup>>
            %35 = memref.atomic_rmw addf %34, %alloc_3[%24] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "block_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
    }
  }
  memref.dealloc %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc : memref<?x4xf32, #gpu.address_space<global>>
  %alloca = memref.alloca() : memref<0xindex>
  %12 = "disc_ral.dispatch"(%arg0, %5, %alloc_3, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1xf32, #gpu.address_space<global>>, memref<0xindex>) -> memref<f32, #gpu.address_space<global>>
  %reinterpret_cast_4 = memref.reinterpret_cast %12 to offset: [0], sizes: [], strides: [] : memref<f32, #gpu.address_space<global>> to memref<f32, #gpu.address_space<global>>
  memref.dealloc %alloc_3 : memref<1xf32, #gpu.address_space<global>>
  %alloc_5 = memref.alloc() : memref<f32>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_4, %alloc_5) {backend_config = "", call_target_name = "d2h", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<f32, #gpu.address_space<global>>, memref<f32>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  memref.dealloc %reinterpret_cast_4 : memref<f32, #gpu.address_space<global>>
  "disc_ral.dispatch"(%arg0, %c0, %alloc_5) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<f32>) -> ()
  return
}

// -----// IR Dump After SCFParallelLoopTiling (disc-parallel-loop-tiling) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c512 = arith.constant 512 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c4_i32 = arith.constant 4 : i32
  %c4 = arith.constant 4 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
  %2 = "disc_ral.dispatch"(%arg0, %c1) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
  %dim = memref.dim %2, %c0 : memref<?x4xf32>
  %dim_0 = memref.dim %1, %c0 : memref<?x4xf32>
  %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %reinterpret_cast_1 = memref.reinterpret_cast %2 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %3 = arith.cmpi eq, %dim, %c1 : index
  %4 = arith.select %3, %dim_0, %dim : index
  %alloc = memref.alloc(%dim_0) {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %5 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast, %alloc) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  %alloc_2 = memref.alloc(%dim) {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_1, %alloc_2) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  %6 = arith.index_cast %4 : index to i32
  %7 = arith.muli %6, %c4_i32 : i32
  %8 = arith.index_cast %7 : i32 to index
  %9 = arith.cmpi eq, %dim_0, %4 : index
  %10 = arith.cmpi eq, %dim, %4 : index
  %11 = arith.andi %10, %9 : i1
  %alloc_3 = memref.alloc() : memref<1xf32, #gpu.address_space<global>>
  scf.if %11 {
    %13 = arith.cmpi slt, %8, %c1 : index
    scf.if %13 {
      "lmhlo.fusion"() ({
        %c0_6 = arith.constant 0 : index
        %c512_7 = arith.constant 512 : index
        %14 = arith.muli %c1, %c512_7 : index
        scf.parallel (%arg1) = (%c0) to (%c1) step (%14) {
          scf.parallel (%arg2) = (%c0_6) to (%14) step (%c1) {
            %23 = arith.addi %arg2, %arg1 : index
            %true = arith.constant true
            %24 = arith.muli %arg2, %c1 : index
            %25 = arith.addi %24, %arg1 : index
            %26 = arith.cmpi ult, %25, %c1 : index
            %27 = arith.andi %true, %26 : i1
            scf.if %27 {
              %reinterpret_cast_13 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
              memref.store %cst, %reinterpret_cast_13[%23] : memref<1xf32, #gpu.address_space<global>>
            }
            scf.yield
          }
          scf.yield
        }
        %15 = arith.cmpi eq, %8, %c0 : index
        %16 = arith.subi %8, %c1 : index
        %17 = arith.divui %16, %c32 : index
        %18 = arith.addi %17, %c1 : index
        %19 = arith.select %15, %c0, %18 : index
        %c0_8 = arith.constant 0 : index
        %c1_9 = arith.constant 1 : index
        %c1_10 = arith.constant 1 : index
        %20 = arith.muli %c1_10, %19 : index
        %21 = arith.muli %20, %c512 : index
        %c0_11 = arith.constant 0 : index
        %c512_12 = arith.constant 512 : index
        %22 = arith.muli %c1_9, %c512_12 : index
        scf.parallel (%arg1) = (%c0_8) to (%21) step (%22) {
          scf.parallel (%arg2) = (%c0_11) to (%22) step (%c1_9) {
            %23 = arith.addi %arg2, %arg1 : index
            %true = arith.constant true
            %24 = arith.muli %arg2, %c1_9 : index
            %25 = arith.addi %24, %arg1 : index
            %26 = arith.cmpi ult, %25, %21 : index
            %27 = arith.andi %true, %26 : i1
            scf.if %27 {
              %28 = arith.remsi %23, %c512 : index
              %29 = arith.divsi %23, %c512 : index
              %30 = arith.cmpi ult, %28, %c1 : index
              scf.if %30 {
                %31 = scf.for %arg3 = %c0 to %c32 step %c1 iter_args(%arg4 = %cst) -> (f32) {
                  %33 = arith.muli %29, %c32 : index
                  %34 = arith.addi %33, %arg3 : index
                  %35 = arith.cmpi slt, %34, %8 : index
                  %36 = scf.if %35 -> (f32) {
                    %37 = arith.muli %4, %c4 : index
                    %reinterpret_cast_13 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%37], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                    %38 = memref.load %reinterpret_cast_13[%34] : memref<?xf32, #gpu.address_space<global>>
                    %reinterpret_cast_14 = memref.reinterpret_cast %alloc_2 to offset: [%c0], sizes: [%37], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                    %39 = memref.load %reinterpret_cast_14[%34] : memref<?xf32, #gpu.address_space<global>>
                    %40 = arith.mulf %38, %39 : f32
                    %41 = arith.addf %arg4, %40 : f32
                    scf.yield %41 : f32
                  } else {
                    scf.yield %arg4 : f32
                  }
                  scf.yield %36 : f32
                }
                %32 = memref.atomic_rmw addf %31, %alloc_3[%28] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
              }
            }
            scf.yield
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXthread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        %c0_6 = arith.constant 0 : index
        %c256_7 = arith.constant 256 : index
        %14 = arith.muli %c1, %c256_7 : index
        scf.parallel (%arg1) = (%c0) to (%c1) step (%14) {
          scf.parallel (%arg2) = (%c0_6) to (%14) step (%c1) {
            %23 = arith.addi %arg2, %arg1 : index
            %true = arith.constant true
            %24 = arith.muli %arg2, %c1 : index
            %25 = arith.addi %24, %arg1 : index
            %26 = arith.cmpi ult, %25, %c1 : index
            %27 = arith.andi %true, %26 : i1
            scf.if %27 {
              %reinterpret_cast_13 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
              memref.store %cst, %reinterpret_cast_13[%23] : memref<1xf32, #gpu.address_space<global>>
            }
            scf.yield
          }
          scf.yield
        }
        %15 = arith.cmpi eq, %8, %c0 : index
        %16 = arith.subi %8, %c1 : index
        %17 = arith.divui %16, %c512 : index
        %18 = arith.addi %17, %c1 : index
        %19 = arith.select %15, %c0, %18 : index
        %c0_8 = arith.constant 0 : index
        %c1_9 = arith.constant 1 : index
        %c1_10 = arith.constant 1 : index
        %20 = arith.muli %c1_10, %19 : index
        %21 = arith.muli %20, %c256 : index
        %c0_11 = arith.constant 0 : index
        %c256_12 = arith.constant 256 : index
        %22 = arith.muli %c1_9, %c256_12 : index
        scf.parallel (%arg1) = (%c0_8) to (%21) step (%22) {
          scf.parallel (%arg2) = (%c0_11) to (%22) step (%c1_9) {
            %23 = arith.addi %arg2, %arg1 : index
            %true = arith.constant true
            %24 = arith.muli %arg2, %c1_9 : index
            %25 = arith.addi %24, %arg1 : index
            %26 = arith.cmpi ult, %25, %21 : index
            %27 = arith.andi %true, %26 : i1
            scf.if %27 {
              %28 = arith.remsi %23, %c256 : index
              %29 = arith.divsi %23, %c256 : index
              %30 = arith.divui %28, %c32 : index
              %31 = arith.remui %28, %c32 : index
              %32 = arith.muli %31, %c8 : index
              %33 = arith.addi %30, %32 : index
              %alloc_13 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
              %34 = arith.cmpi ult, %31, %c1 : index
              %35 = scf.if %34 -> (f32) {
                %41 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %cst) -> (f32) {
                  %42 = arith.muli %29, %c8 : index
                  %43 = arith.addi %30, %42 : index
                  %44 = arith.muli %43, %c64 : index
                  %45 = arith.addi %arg3, %44 : index
                  %46 = arith.cmpi slt, %45, %8 : index
                  %47 = scf.if %46 -> (f32) {
                    %48 = arith.muli %4, %c4 : index
                    %reinterpret_cast_15 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%48], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                    %49 = memref.load %reinterpret_cast_15[%45] : memref<?xf32, #gpu.address_space<global>>
                    %reinterpret_cast_16 = memref.reinterpret_cast %alloc_2 to offset: [%c0], sizes: [%48], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                    %50 = memref.load %reinterpret_cast_16[%45] : memref<?xf32, #gpu.address_space<global>>
                    %51 = arith.mulf %49, %50 : f32
                    %52 = arith.addf %arg4, %51 : f32
                    scf.yield %52 : f32
                  } else {
                    scf.yield %arg4 : f32
                  }
                  scf.yield %47 : f32
                }
                scf.yield %41 : f32
              } else {
                scf.yield %cst : f32
              }
              %reinterpret_cast_14 = memref.reinterpret_cast %alloc_13 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
              memref.store %35, %reinterpret_cast_14[%33] : memref<256xf32, #gpu.address_space<workgroup>>
              gpu.barrier
              %36 = arith.cmpi slt, %30, %c4 : index
              scf.if %36 {
                %41 = arith.addi %33, %c4 : index
                %42 = memref.load %reinterpret_cast_14[%33] : memref<256xf32, #gpu.address_space<workgroup>>
                %43 = memref.load %reinterpret_cast_14[%41] : memref<256xf32, #gpu.address_space<workgroup>>
                %44 = arith.addf %42, %43 : f32
                memref.store %44, %reinterpret_cast_14[%33] : memref<256xf32, #gpu.address_space<workgroup>>
              }
              gpu.barrier
              %37 = arith.cmpi slt, %30, %c2 : index
              scf.if %37 {
                %41 = arith.addi %33, %c2 : index
                %42 = memref.load %reinterpret_cast_14[%33] : memref<256xf32, #gpu.address_space<workgroup>>
                %43 = memref.load %reinterpret_cast_14[%41] : memref<256xf32, #gpu.address_space<workgroup>>
                %44 = arith.addf %42, %43 : f32
                memref.store %44, %reinterpret_cast_14[%33] : memref<256xf32, #gpu.address_space<workgroup>>
              }
              gpu.barrier
              %38 = arith.cmpi slt, %30, %c1 : index
              scf.if %38 {
                %41 = arith.addi %33, %c1 : index
                %42 = memref.load %reinterpret_cast_14[%33] : memref<256xf32, #gpu.address_space<workgroup>>
                %43 = memref.load %reinterpret_cast_14[%41] : memref<256xf32, #gpu.address_space<workgroup>>
                %44 = arith.addf %42, %43 : f32
                memref.store %44, %reinterpret_cast_14[%33] : memref<256xf32, #gpu.address_space<workgroup>>
              }
              gpu.barrier
              %39 = arith.cmpi eq, %30, %c0 : index
              %40 = arith.andi %39, %34 : i1
              scf.if %40 {
                %41 = memref.load %reinterpret_cast_14[%33] : memref<256xf32, #gpu.address_space<workgroup>>
                %42 = memref.atomic_rmw addf %41, %alloc_3[%31] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
              }
            }
            scf.yield
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXblock_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
    }
  } else {
    %13 = arith.cmpi slt, %8, %c1 : index
    scf.if %13 {
      "lmhlo.fusion"() ({
        %c0_6 = arith.constant 0 : index
        %c512_7 = arith.constant 512 : index
        %14 = arith.muli %c1, %c512_7 : index
        scf.parallel (%arg1) = (%c0) to (%c1) step (%14) {
          scf.parallel (%arg2) = (%c0_6) to (%14) step (%c1) {
            %23 = arith.addi %arg2, %arg1 : index
            %true = arith.constant true
            %24 = arith.muli %arg2, %c1 : index
            %25 = arith.addi %24, %arg1 : index
            %26 = arith.cmpi ult, %25, %c1 : index
            %27 = arith.andi %true, %26 : i1
            scf.if %27 {
              %reinterpret_cast_13 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
              memref.store %cst, %reinterpret_cast_13[%23] : memref<1xf32, #gpu.address_space<global>>
            }
            scf.yield
          }
          scf.yield
        }
        %15 = arith.cmpi eq, %8, %c0 : index
        %16 = arith.subi %8, %c1 : index
        %17 = arith.divui %16, %c32 : index
        %18 = arith.addi %17, %c1 : index
        %19 = arith.select %15, %c0, %18 : index
        %c0_8 = arith.constant 0 : index
        %c1_9 = arith.constant 1 : index
        %c1_10 = arith.constant 1 : index
        %20 = arith.muli %c1_10, %19 : index
        %21 = arith.muli %20, %c512 : index
        %c0_11 = arith.constant 0 : index
        %c512_12 = arith.constant 512 : index
        %22 = arith.muli %c1_9, %c512_12 : index
        scf.parallel (%arg1) = (%c0_8) to (%21) step (%22) {
          scf.parallel (%arg2) = (%c0_11) to (%22) step (%c1_9) {
            %23 = arith.addi %arg2, %arg1 : index
            %true = arith.constant true
            %24 = arith.muli %arg2, %c1_9 : index
            %25 = arith.addi %24, %arg1 : index
            %26 = arith.cmpi ult, %25, %21 : index
            %27 = arith.andi %true, %26 : i1
            scf.if %27 {
              %28 = arith.remsi %23, %c512 : index
              %29 = arith.divsi %23, %c512 : index
              %30 = arith.cmpi ult, %28, %c1 : index
              scf.if %30 {
                %31 = scf.for %arg3 = %c0 to %c32 step %c1 iter_args(%arg4 = %cst) -> (f32) {
                  %33 = arith.muli %29, %c32 : index
                  %34 = arith.addi %33, %arg3 : index
                  %35 = arith.cmpi slt, %34, %8 : index
                  %36 = scf.if %35 -> (f32) {
                    %37 = arith.remui %34, %c4 : index
                    %38 = arith.divui %34, %c4 : index
                    %39 = arith.select %9, %38, %c0 : index
                    %40 = arith.muli %39, %c4 : index
                    %41 = arith.addi %40, %37 : index
                    %42 = arith.muli %dim_0, %c4 : index
                    %reinterpret_cast_13 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%42], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                    %43 = memref.load %reinterpret_cast_13[%41] : memref<?xf32, #gpu.address_space<global>>
                    %44 = arith.select %10, %38, %c0 : index
                    %45 = arith.muli %44, %c4 : index
                    %46 = arith.addi %45, %37 : index
                    %47 = arith.muli %dim, %c4 : index
                    %reinterpret_cast_14 = memref.reinterpret_cast %alloc_2 to offset: [%c0], sizes: [%47], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                    %48 = memref.load %reinterpret_cast_14[%46] : memref<?xf32, #gpu.address_space<global>>
                    %49 = arith.mulf %43, %48 : f32
                    %50 = arith.addf %arg4, %49 : f32
                    scf.yield %50 : f32
                  } else {
                    scf.yield %arg4 : f32
                  }
                  scf.yield %36 : f32
                }
                %32 = memref.atomic_rmw addf %31, %alloc_3[%28] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
              }
            }
            scf.yield
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "thread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        %c0_6 = arith.constant 0 : index
        %c256_7 = arith.constant 256 : index
        %14 = arith.muli %c1, %c256_7 : index
        scf.parallel (%arg1) = (%c0) to (%c1) step (%14) {
          scf.parallel (%arg2) = (%c0_6) to (%14) step (%c1) {
            %23 = arith.addi %arg2, %arg1 : index
            %true = arith.constant true
            %24 = arith.muli %arg2, %c1 : index
            %25 = arith.addi %24, %arg1 : index
            %26 = arith.cmpi ult, %25, %c1 : index
            %27 = arith.andi %true, %26 : i1
            scf.if %27 {
              %reinterpret_cast_13 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
              memref.store %cst, %reinterpret_cast_13[%23] : memref<1xf32, #gpu.address_space<global>>
            }
            scf.yield
          }
          scf.yield
        }
        %15 = arith.cmpi eq, %8, %c0 : index
        %16 = arith.subi %8, %c1 : index
        %17 = arith.divui %16, %c512 : index
        %18 = arith.addi %17, %c1 : index
        %19 = arith.select %15, %c0, %18 : index
        %c0_8 = arith.constant 0 : index
        %c1_9 = arith.constant 1 : index
        %c1_10 = arith.constant 1 : index
        %20 = arith.muli %c1_10, %19 : index
        %21 = arith.muli %20, %c256 : index
        %c0_11 = arith.constant 0 : index
        %c256_12 = arith.constant 256 : index
        %22 = arith.muli %c1_9, %c256_12 : index
        scf.parallel (%arg1) = (%c0_8) to (%21) step (%22) {
          scf.parallel (%arg2) = (%c0_11) to (%22) step (%c1_9) {
            %23 = arith.addi %arg2, %arg1 : index
            %true = arith.constant true
            %24 = arith.muli %arg2, %c1_9 : index
            %25 = arith.addi %24, %arg1 : index
            %26 = arith.cmpi ult, %25, %21 : index
            %27 = arith.andi %true, %26 : i1
            scf.if %27 {
              %28 = arith.remsi %23, %c256 : index
              %29 = arith.divsi %23, %c256 : index
              %30 = arith.divui %28, %c32 : index
              %31 = arith.remui %28, %c32 : index
              %32 = arith.muli %31, %c8 : index
              %33 = arith.addi %30, %32 : index
              %alloc_13 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
              %34 = arith.cmpi ult, %31, %c1 : index
              %35 = scf.if %34 -> (f32) {
                %41 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %cst) -> (f32) {
                  %42 = arith.muli %29, %c8 : index
                  %43 = arith.addi %30, %42 : index
                  %44 = arith.muli %43, %c64 : index
                  %45 = arith.addi %arg3, %44 : index
                  %46 = arith.cmpi slt, %45, %8 : index
                  %47 = scf.if %46 -> (f32) {
                    %48 = arith.remui %45, %c4 : index
                    %49 = arith.divui %45, %c4 : index
                    %50 = arith.select %9, %49, %c0 : index
                    %51 = arith.muli %50, %c4 : index
                    %52 = arith.addi %51, %48 : index
                    %53 = arith.muli %dim_0, %c4 : index
                    %reinterpret_cast_15 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%53], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                    %54 = memref.load %reinterpret_cast_15[%52] : memref<?xf32, #gpu.address_space<global>>
                    %55 = arith.select %10, %49, %c0 : index
                    %56 = arith.muli %55, %c4 : index
                    %57 = arith.addi %56, %48 : index
                    %58 = arith.muli %dim, %c4 : index
                    %reinterpret_cast_16 = memref.reinterpret_cast %alloc_2 to offset: [%c0], sizes: [%58], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                    %59 = memref.load %reinterpret_cast_16[%57] : memref<?xf32, #gpu.address_space<global>>
                    %60 = arith.mulf %54, %59 : f32
                    %61 = arith.addf %arg4, %60 : f32
                    scf.yield %61 : f32
                  } else {
                    scf.yield %arg4 : f32
                  }
                  scf.yield %47 : f32
                }
                scf.yield %41 : f32
              } else {
                scf.yield %cst : f32
              }
              %reinterpret_cast_14 = memref.reinterpret_cast %alloc_13 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
              memref.store %35, %reinterpret_cast_14[%33] : memref<256xf32, #gpu.address_space<workgroup>>
              gpu.barrier
              %36 = arith.cmpi slt, %30, %c4 : index
              scf.if %36 {
                %41 = arith.addi %33, %c4 : index
                %42 = memref.load %reinterpret_cast_14[%33] : memref<256xf32, #gpu.address_space<workgroup>>
                %43 = memref.load %reinterpret_cast_14[%41] : memref<256xf32, #gpu.address_space<workgroup>>
                %44 = arith.addf %42, %43 : f32
                memref.store %44, %reinterpret_cast_14[%33] : memref<256xf32, #gpu.address_space<workgroup>>
              }
              gpu.barrier
              %37 = arith.cmpi slt, %30, %c2 : index
              scf.if %37 {
                %41 = arith.addi %33, %c2 : index
                %42 = memref.load %reinterpret_cast_14[%33] : memref<256xf32, #gpu.address_space<workgroup>>
                %43 = memref.load %reinterpret_cast_14[%41] : memref<256xf32, #gpu.address_space<workgroup>>
                %44 = arith.addf %42, %43 : f32
                memref.store %44, %reinterpret_cast_14[%33] : memref<256xf32, #gpu.address_space<workgroup>>
              }
              gpu.barrier
              %38 = arith.cmpi slt, %30, %c1 : index
              scf.if %38 {
                %41 = arith.addi %33, %c1 : index
                %42 = memref.load %reinterpret_cast_14[%33] : memref<256xf32, #gpu.address_space<workgroup>>
                %43 = memref.load %reinterpret_cast_14[%41] : memref<256xf32, #gpu.address_space<workgroup>>
                %44 = arith.addf %42, %43 : f32
                memref.store %44, %reinterpret_cast_14[%33] : memref<256xf32, #gpu.address_space<workgroup>>
              }
              gpu.barrier
              %39 = arith.cmpi eq, %30, %c0 : index
              %40 = arith.andi %39, %34 : i1
              scf.if %40 {
                %41 = memref.load %reinterpret_cast_14[%33] : memref<256xf32, #gpu.address_space<workgroup>>
                %42 = memref.atomic_rmw addf %41, %alloc_3[%31] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
              }
            }
            scf.yield
          }
          scf.yield
        }
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "block_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
    }
  }
  memref.dealloc %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc : memref<?x4xf32, #gpu.address_space<global>>
  %alloca = memref.alloca() : memref<0xindex>
  %12 = "disc_ral.dispatch"(%arg0, %5, %alloc_3, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1xf32, #gpu.address_space<global>>, memref<0xindex>) -> memref<f32, #gpu.address_space<global>>
  %reinterpret_cast_4 = memref.reinterpret_cast %12 to offset: [0], sizes: [], strides: [] : memref<f32, #gpu.address_space<global>> to memref<f32, #gpu.address_space<global>>
  memref.dealloc %alloc_3 : memref<1xf32, #gpu.address_space<global>>
  %alloc_5 = memref.alloc() : memref<f32>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_4, %alloc_5) {backend_config = "", call_target_name = "d2h", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<f32, #gpu.address_space<global>>, memref<f32>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  memref.dealloc %reinterpret_cast_4 : memref<f32, #gpu.address_space<global>>
  "disc_ral.dispatch"(%arg0, %c0, %alloc_5) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<f32>) -> ()
  return
}

// -----// IR Dump After GpuMapParallelLoopsPass (gpu-map-parallel-loops) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c512 = arith.constant 512 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c4_i32 = arith.constant 4 : i32
  %c4 = arith.constant 4 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
  %2 = "disc_ral.dispatch"(%arg0, %c1) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
  %dim = memref.dim %2, %c0 : memref<?x4xf32>
  %dim_0 = memref.dim %1, %c0 : memref<?x4xf32>
  %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %reinterpret_cast_1 = memref.reinterpret_cast %2 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %3 = arith.cmpi eq, %dim, %c1 : index
  %4 = arith.select %3, %dim_0, %dim : index
  %alloc = memref.alloc(%dim_0) {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %5 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast, %alloc) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  %alloc_2 = memref.alloc(%dim) {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_1, %alloc_2) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  %6 = arith.index_cast %4 : index to i32
  %7 = arith.muli %6, %c4_i32 : i32
  %8 = arith.index_cast %7 : i32 to index
  %9 = arith.cmpi eq, %dim_0, %4 : index
  %10 = arith.cmpi eq, %dim, %4 : index
  %11 = arith.andi %10, %9 : i1
  %alloc_3 = memref.alloc() : memref<1xf32, #gpu.address_space<global>>
  scf.if %11 {
    %13 = arith.cmpi slt, %8, %c1 : index
    scf.if %13 {
      "lmhlo.fusion"() ({
        %c0_6 = arith.constant 0 : index
        %c512_7 = arith.constant 512 : index
        %14 = arith.muli %c1, %c512_7 : index
        scf.parallel (%arg1) = (%c0) to (%c1) step (%14) {
          scf.parallel (%arg2) = (%c0_6) to (%14) step (%c1) {
            %23 = arith.addi %arg2, %arg1 : index
            %true = arith.constant true
            %24 = arith.muli %arg2, %c1 : index
            %25 = arith.addi %24, %arg1 : index
            %26 = arith.cmpi ult, %25, %c1 : index
            %27 = arith.andi %true, %26 : i1
            scf.if %27 {
              %reinterpret_cast_13 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
              memref.store %cst, %reinterpret_cast_13[%23] : memref<1xf32, #gpu.address_space<global>>
            }
            scf.yield
          } {mapping = [#gpu.loop_dim_map<processor = thread_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
          scf.yield
        } {mapping = [#gpu.loop_dim_map<processor = block_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
        %15 = arith.cmpi eq, %8, %c0 : index
        %16 = arith.subi %8, %c1 : index
        %17 = arith.divui %16, %c32 : index
        %18 = arith.addi %17, %c1 : index
        %19 = arith.select %15, %c0, %18 : index
        %c0_8 = arith.constant 0 : index
        %c1_9 = arith.constant 1 : index
        %c1_10 = arith.constant 1 : index
        %20 = arith.muli %c1_10, %19 : index
        %21 = arith.muli %20, %c512 : index
        %c0_11 = arith.constant 0 : index
        %c512_12 = arith.constant 512 : index
        %22 = arith.muli %c1_9, %c512_12 : index
        scf.parallel (%arg1) = (%c0_8) to (%21) step (%22) {
          scf.parallel (%arg2) = (%c0_11) to (%22) step (%c1_9) {
            %23 = arith.addi %arg2, %arg1 : index
            %true = arith.constant true
            %24 = arith.muli %arg2, %c1_9 : index
            %25 = arith.addi %24, %arg1 : index
            %26 = arith.cmpi ult, %25, %21 : index
            %27 = arith.andi %true, %26 : i1
            scf.if %27 {
              %28 = arith.remsi %23, %c512 : index
              %29 = arith.divsi %23, %c512 : index
              %30 = arith.cmpi ult, %28, %c1 : index
              scf.if %30 {
                %31 = scf.for %arg3 = %c0 to %c32 step %c1 iter_args(%arg4 = %cst) -> (f32) {
                  %33 = arith.muli %29, %c32 : index
                  %34 = arith.addi %33, %arg3 : index
                  %35 = arith.cmpi slt, %34, %8 : index
                  %36 = scf.if %35 -> (f32) {
                    %37 = arith.muli %4, %c4 : index
                    %reinterpret_cast_13 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%37], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                    %38 = memref.load %reinterpret_cast_13[%34] : memref<?xf32, #gpu.address_space<global>>
                    %reinterpret_cast_14 = memref.reinterpret_cast %alloc_2 to offset: [%c0], sizes: [%37], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                    %39 = memref.load %reinterpret_cast_14[%34] : memref<?xf32, #gpu.address_space<global>>
                    %40 = arith.mulf %38, %39 : f32
                    %41 = arith.addf %arg4, %40 : f32
                    scf.yield %41 : f32
                  } else {
                    scf.yield %arg4 : f32
                  }
                  scf.yield %36 : f32
                }
                %32 = memref.atomic_rmw addf %31, %alloc_3[%28] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
              }
            }
            scf.yield
          } {mapping = [#gpu.loop_dim_map<processor = thread_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
          scf.yield
        } {mapping = [#gpu.loop_dim_map<processor = block_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXthread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        %c0_6 = arith.constant 0 : index
        %c256_7 = arith.constant 256 : index
        %14 = arith.muli %c1, %c256_7 : index
        scf.parallel (%arg1) = (%c0) to (%c1) step (%14) {
          scf.parallel (%arg2) = (%c0_6) to (%14) step (%c1) {
            %23 = arith.addi %arg2, %arg1 : index
            %true = arith.constant true
            %24 = arith.muli %arg2, %c1 : index
            %25 = arith.addi %24, %arg1 : index
            %26 = arith.cmpi ult, %25, %c1 : index
            %27 = arith.andi %true, %26 : i1
            scf.if %27 {
              %reinterpret_cast_13 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
              memref.store %cst, %reinterpret_cast_13[%23] : memref<1xf32, #gpu.address_space<global>>
            }
            scf.yield
          } {mapping = [#gpu.loop_dim_map<processor = thread_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
          scf.yield
        } {mapping = [#gpu.loop_dim_map<processor = block_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
        %15 = arith.cmpi eq, %8, %c0 : index
        %16 = arith.subi %8, %c1 : index
        %17 = arith.divui %16, %c512 : index
        %18 = arith.addi %17, %c1 : index
        %19 = arith.select %15, %c0, %18 : index
        %c0_8 = arith.constant 0 : index
        %c1_9 = arith.constant 1 : index
        %c1_10 = arith.constant 1 : index
        %20 = arith.muli %c1_10, %19 : index
        %21 = arith.muli %20, %c256 : index
        %c0_11 = arith.constant 0 : index
        %c256_12 = arith.constant 256 : index
        %22 = arith.muli %c1_9, %c256_12 : index
        scf.parallel (%arg1) = (%c0_8) to (%21) step (%22) {
          scf.parallel (%arg2) = (%c0_11) to (%22) step (%c1_9) {
            %23 = arith.addi %arg2, %arg1 : index
            %true = arith.constant true
            %24 = arith.muli %arg2, %c1_9 : index
            %25 = arith.addi %24, %arg1 : index
            %26 = arith.cmpi ult, %25, %21 : index
            %27 = arith.andi %true, %26 : i1
            scf.if %27 {
              %28 = arith.remsi %23, %c256 : index
              %29 = arith.divsi %23, %c256 : index
              %30 = arith.divui %28, %c32 : index
              %31 = arith.remui %28, %c32 : index
              %32 = arith.muli %31, %c8 : index
              %33 = arith.addi %30, %32 : index
              %alloc_13 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
              %34 = arith.cmpi ult, %31, %c1 : index
              %35 = scf.if %34 -> (f32) {
                %41 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %cst) -> (f32) {
                  %42 = arith.muli %29, %c8 : index
                  %43 = arith.addi %30, %42 : index
                  %44 = arith.muli %43, %c64 : index
                  %45 = arith.addi %arg3, %44 : index
                  %46 = arith.cmpi slt, %45, %8 : index
                  %47 = scf.if %46 -> (f32) {
                    %48 = arith.muli %4, %c4 : index
                    %reinterpret_cast_15 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%48], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                    %49 = memref.load %reinterpret_cast_15[%45] : memref<?xf32, #gpu.address_space<global>>
                    %reinterpret_cast_16 = memref.reinterpret_cast %alloc_2 to offset: [%c0], sizes: [%48], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                    %50 = memref.load %reinterpret_cast_16[%45] : memref<?xf32, #gpu.address_space<global>>
                    %51 = arith.mulf %49, %50 : f32
                    %52 = arith.addf %arg4, %51 : f32
                    scf.yield %52 : f32
                  } else {
                    scf.yield %arg4 : f32
                  }
                  scf.yield %47 : f32
                }
                scf.yield %41 : f32
              } else {
                scf.yield %cst : f32
              }
              %reinterpret_cast_14 = memref.reinterpret_cast %alloc_13 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
              memref.store %35, %reinterpret_cast_14[%33] : memref<256xf32, #gpu.address_space<workgroup>>
              gpu.barrier
              %36 = arith.cmpi slt, %30, %c4 : index
              scf.if %36 {
                %41 = arith.addi %33, %c4 : index
                %42 = memref.load %reinterpret_cast_14[%33] : memref<256xf32, #gpu.address_space<workgroup>>
                %43 = memref.load %reinterpret_cast_14[%41] : memref<256xf32, #gpu.address_space<workgroup>>
                %44 = arith.addf %42, %43 : f32
                memref.store %44, %reinterpret_cast_14[%33] : memref<256xf32, #gpu.address_space<workgroup>>
              }
              gpu.barrier
              %37 = arith.cmpi slt, %30, %c2 : index
              scf.if %37 {
                %41 = arith.addi %33, %c2 : index
                %42 = memref.load %reinterpret_cast_14[%33] : memref<256xf32, #gpu.address_space<workgroup>>
                %43 = memref.load %reinterpret_cast_14[%41] : memref<256xf32, #gpu.address_space<workgroup>>
                %44 = arith.addf %42, %43 : f32
                memref.store %44, %reinterpret_cast_14[%33] : memref<256xf32, #gpu.address_space<workgroup>>
              }
              gpu.barrier
              %38 = arith.cmpi slt, %30, %c1 : index
              scf.if %38 {
                %41 = arith.addi %33, %c1 : index
                %42 = memref.load %reinterpret_cast_14[%33] : memref<256xf32, #gpu.address_space<workgroup>>
                %43 = memref.load %reinterpret_cast_14[%41] : memref<256xf32, #gpu.address_space<workgroup>>
                %44 = arith.addf %42, %43 : f32
                memref.store %44, %reinterpret_cast_14[%33] : memref<256xf32, #gpu.address_space<workgroup>>
              }
              gpu.barrier
              %39 = arith.cmpi eq, %30, %c0 : index
              %40 = arith.andi %39, %34 : i1
              scf.if %40 {
                %41 = memref.load %reinterpret_cast_14[%33] : memref<256xf32, #gpu.address_space<workgroup>>
                %42 = memref.atomic_rmw addf %41, %alloc_3[%31] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
              }
            }
            scf.yield
          } {mapping = [#gpu.loop_dim_map<processor = thread_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
          scf.yield
        } {mapping = [#gpu.loop_dim_map<processor = block_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXblock_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
    }
  } else {
    %13 = arith.cmpi slt, %8, %c1 : index
    scf.if %13 {
      "lmhlo.fusion"() ({
        %c0_6 = arith.constant 0 : index
        %c512_7 = arith.constant 512 : index
        %14 = arith.muli %c1, %c512_7 : index
        scf.parallel (%arg1) = (%c0) to (%c1) step (%14) {
          scf.parallel (%arg2) = (%c0_6) to (%14) step (%c1) {
            %23 = arith.addi %arg2, %arg1 : index
            %true = arith.constant true
            %24 = arith.muli %arg2, %c1 : index
            %25 = arith.addi %24, %arg1 : index
            %26 = arith.cmpi ult, %25, %c1 : index
            %27 = arith.andi %true, %26 : i1
            scf.if %27 {
              %reinterpret_cast_13 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
              memref.store %cst, %reinterpret_cast_13[%23] : memref<1xf32, #gpu.address_space<global>>
            }
            scf.yield
          } {mapping = [#gpu.loop_dim_map<processor = thread_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
          scf.yield
        } {mapping = [#gpu.loop_dim_map<processor = block_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
        %15 = arith.cmpi eq, %8, %c0 : index
        %16 = arith.subi %8, %c1 : index
        %17 = arith.divui %16, %c32 : index
        %18 = arith.addi %17, %c1 : index
        %19 = arith.select %15, %c0, %18 : index
        %c0_8 = arith.constant 0 : index
        %c1_9 = arith.constant 1 : index
        %c1_10 = arith.constant 1 : index
        %20 = arith.muli %c1_10, %19 : index
        %21 = arith.muli %20, %c512 : index
        %c0_11 = arith.constant 0 : index
        %c512_12 = arith.constant 512 : index
        %22 = arith.muli %c1_9, %c512_12 : index
        scf.parallel (%arg1) = (%c0_8) to (%21) step (%22) {
          scf.parallel (%arg2) = (%c0_11) to (%22) step (%c1_9) {
            %23 = arith.addi %arg2, %arg1 : index
            %true = arith.constant true
            %24 = arith.muli %arg2, %c1_9 : index
            %25 = arith.addi %24, %arg1 : index
            %26 = arith.cmpi ult, %25, %21 : index
            %27 = arith.andi %true, %26 : i1
            scf.if %27 {
              %28 = arith.remsi %23, %c512 : index
              %29 = arith.divsi %23, %c512 : index
              %30 = arith.cmpi ult, %28, %c1 : index
              scf.if %30 {
                %31 = scf.for %arg3 = %c0 to %c32 step %c1 iter_args(%arg4 = %cst) -> (f32) {
                  %33 = arith.muli %29, %c32 : index
                  %34 = arith.addi %33, %arg3 : index
                  %35 = arith.cmpi slt, %34, %8 : index
                  %36 = scf.if %35 -> (f32) {
                    %37 = arith.remui %34, %c4 : index
                    %38 = arith.divui %34, %c4 : index
                    %39 = arith.select %9, %38, %c0 : index
                    %40 = arith.muli %39, %c4 : index
                    %41 = arith.addi %40, %37 : index
                    %42 = arith.muli %dim_0, %c4 : index
                    %reinterpret_cast_13 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%42], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                    %43 = memref.load %reinterpret_cast_13[%41] : memref<?xf32, #gpu.address_space<global>>
                    %44 = arith.select %10, %38, %c0 : index
                    %45 = arith.muli %44, %c4 : index
                    %46 = arith.addi %45, %37 : index
                    %47 = arith.muli %dim, %c4 : index
                    %reinterpret_cast_14 = memref.reinterpret_cast %alloc_2 to offset: [%c0], sizes: [%47], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                    %48 = memref.load %reinterpret_cast_14[%46] : memref<?xf32, #gpu.address_space<global>>
                    %49 = arith.mulf %43, %48 : f32
                    %50 = arith.addf %arg4, %49 : f32
                    scf.yield %50 : f32
                  } else {
                    scf.yield %arg4 : f32
                  }
                  scf.yield %36 : f32
                }
                %32 = memref.atomic_rmw addf %31, %alloc_3[%28] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
              }
            }
            scf.yield
          } {mapping = [#gpu.loop_dim_map<processor = thread_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
          scf.yield
        } {mapping = [#gpu.loop_dim_map<processor = block_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "thread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        %c0_6 = arith.constant 0 : index
        %c256_7 = arith.constant 256 : index
        %14 = arith.muli %c1, %c256_7 : index
        scf.parallel (%arg1) = (%c0) to (%c1) step (%14) {
          scf.parallel (%arg2) = (%c0_6) to (%14) step (%c1) {
            %23 = arith.addi %arg2, %arg1 : index
            %true = arith.constant true
            %24 = arith.muli %arg2, %c1 : index
            %25 = arith.addi %24, %arg1 : index
            %26 = arith.cmpi ult, %25, %c1 : index
            %27 = arith.andi %true, %26 : i1
            scf.if %27 {
              %reinterpret_cast_13 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
              memref.store %cst, %reinterpret_cast_13[%23] : memref<1xf32, #gpu.address_space<global>>
            }
            scf.yield
          } {mapping = [#gpu.loop_dim_map<processor = thread_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
          scf.yield
        } {mapping = [#gpu.loop_dim_map<processor = block_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
        %15 = arith.cmpi eq, %8, %c0 : index
        %16 = arith.subi %8, %c1 : index
        %17 = arith.divui %16, %c512 : index
        %18 = arith.addi %17, %c1 : index
        %19 = arith.select %15, %c0, %18 : index
        %c0_8 = arith.constant 0 : index
        %c1_9 = arith.constant 1 : index
        %c1_10 = arith.constant 1 : index
        %20 = arith.muli %c1_10, %19 : index
        %21 = arith.muli %20, %c256 : index
        %c0_11 = arith.constant 0 : index
        %c256_12 = arith.constant 256 : index
        %22 = arith.muli %c1_9, %c256_12 : index
        scf.parallel (%arg1) = (%c0_8) to (%21) step (%22) {
          scf.parallel (%arg2) = (%c0_11) to (%22) step (%c1_9) {
            %23 = arith.addi %arg2, %arg1 : index
            %true = arith.constant true
            %24 = arith.muli %arg2, %c1_9 : index
            %25 = arith.addi %24, %arg1 : index
            %26 = arith.cmpi ult, %25, %21 : index
            %27 = arith.andi %true, %26 : i1
            scf.if %27 {
              %28 = arith.remsi %23, %c256 : index
              %29 = arith.divsi %23, %c256 : index
              %30 = arith.divui %28, %c32 : index
              %31 = arith.remui %28, %c32 : index
              %32 = arith.muli %31, %c8 : index
              %33 = arith.addi %30, %32 : index
              %alloc_13 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
              %34 = arith.cmpi ult, %31, %c1 : index
              %35 = scf.if %34 -> (f32) {
                %41 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %cst) -> (f32) {
                  %42 = arith.muli %29, %c8 : index
                  %43 = arith.addi %30, %42 : index
                  %44 = arith.muli %43, %c64 : index
                  %45 = arith.addi %arg3, %44 : index
                  %46 = arith.cmpi slt, %45, %8 : index
                  %47 = scf.if %46 -> (f32) {
                    %48 = arith.remui %45, %c4 : index
                    %49 = arith.divui %45, %c4 : index
                    %50 = arith.select %9, %49, %c0 : index
                    %51 = arith.muli %50, %c4 : index
                    %52 = arith.addi %51, %48 : index
                    %53 = arith.muli %dim_0, %c4 : index
                    %reinterpret_cast_15 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%53], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                    %54 = memref.load %reinterpret_cast_15[%52] : memref<?xf32, #gpu.address_space<global>>
                    %55 = arith.select %10, %49, %c0 : index
                    %56 = arith.muli %55, %c4 : index
                    %57 = arith.addi %56, %48 : index
                    %58 = arith.muli %dim, %c4 : index
                    %reinterpret_cast_16 = memref.reinterpret_cast %alloc_2 to offset: [%c0], sizes: [%58], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                    %59 = memref.load %reinterpret_cast_16[%57] : memref<?xf32, #gpu.address_space<global>>
                    %60 = arith.mulf %54, %59 : f32
                    %61 = arith.addf %arg4, %60 : f32
                    scf.yield %61 : f32
                  } else {
                    scf.yield %arg4 : f32
                  }
                  scf.yield %47 : f32
                }
                scf.yield %41 : f32
              } else {
                scf.yield %cst : f32
              }
              %reinterpret_cast_14 = memref.reinterpret_cast %alloc_13 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
              memref.store %35, %reinterpret_cast_14[%33] : memref<256xf32, #gpu.address_space<workgroup>>
              gpu.barrier
              %36 = arith.cmpi slt, %30, %c4 : index
              scf.if %36 {
                %41 = arith.addi %33, %c4 : index
                %42 = memref.load %reinterpret_cast_14[%33] : memref<256xf32, #gpu.address_space<workgroup>>
                %43 = memref.load %reinterpret_cast_14[%41] : memref<256xf32, #gpu.address_space<workgroup>>
                %44 = arith.addf %42, %43 : f32
                memref.store %44, %reinterpret_cast_14[%33] : memref<256xf32, #gpu.address_space<workgroup>>
              }
              gpu.barrier
              %37 = arith.cmpi slt, %30, %c2 : index
              scf.if %37 {
                %41 = arith.addi %33, %c2 : index
                %42 = memref.load %reinterpret_cast_14[%33] : memref<256xf32, #gpu.address_space<workgroup>>
                %43 = memref.load %reinterpret_cast_14[%41] : memref<256xf32, #gpu.address_space<workgroup>>
                %44 = arith.addf %42, %43 : f32
                memref.store %44, %reinterpret_cast_14[%33] : memref<256xf32, #gpu.address_space<workgroup>>
              }
              gpu.barrier
              %38 = arith.cmpi slt, %30, %c1 : index
              scf.if %38 {
                %41 = arith.addi %33, %c1 : index
                %42 = memref.load %reinterpret_cast_14[%33] : memref<256xf32, #gpu.address_space<workgroup>>
                %43 = memref.load %reinterpret_cast_14[%41] : memref<256xf32, #gpu.address_space<workgroup>>
                %44 = arith.addf %42, %43 : f32
                memref.store %44, %reinterpret_cast_14[%33] : memref<256xf32, #gpu.address_space<workgroup>>
              }
              gpu.barrier
              %39 = arith.cmpi eq, %30, %c0 : index
              %40 = arith.andi %39, %34 : i1
              scf.if %40 {
                %41 = memref.load %reinterpret_cast_14[%33] : memref<256xf32, #gpu.address_space<workgroup>>
                %42 = memref.atomic_rmw addf %41, %alloc_3[%31] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
              }
            }
            scf.yield
          } {mapping = [#gpu.loop_dim_map<processor = thread_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
          scf.yield
        } {mapping = [#gpu.loop_dim_map<processor = block_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "block_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
    }
  }
  memref.dealloc %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc : memref<?x4xf32, #gpu.address_space<global>>
  %alloca = memref.alloca() : memref<0xindex>
  %12 = "disc_ral.dispatch"(%arg0, %5, %alloc_3, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1xf32, #gpu.address_space<global>>, memref<0xindex>) -> memref<f32, #gpu.address_space<global>>
  %reinterpret_cast_4 = memref.reinterpret_cast %12 to offset: [0], sizes: [], strides: [] : memref<f32, #gpu.address_space<global>> to memref<f32, #gpu.address_space<global>>
  memref.dealloc %alloc_3 : memref<1xf32, #gpu.address_space<global>>
  %alloc_5 = memref.alloc() : memref<f32>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_4, %alloc_5) {backend_config = "", call_target_name = "d2h", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<f32, #gpu.address_space<global>>, memref<f32>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  memref.dealloc %reinterpret_cast_4 : memref<f32, #gpu.address_space<global>>
  "disc_ral.dispatch"(%arg0, %c0, %alloc_5) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<f32>) -> ()
  return
}

// -----// IR Dump After ConvertParallelLoopToGpu (convert-parallel-loops-to-gpu) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c512 = arith.constant 512 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c4_i32 = arith.constant 4 : i32
  %c4 = arith.constant 4 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
  %2 = "disc_ral.dispatch"(%arg0, %c1) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
  %dim = memref.dim %2, %c0 : memref<?x4xf32>
  %dim_0 = memref.dim %1, %c0 : memref<?x4xf32>
  %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %reinterpret_cast_1 = memref.reinterpret_cast %2 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %3 = arith.cmpi eq, %dim, %c1 : index
  %4 = arith.select %3, %dim_0, %dim : index
  %alloc = memref.alloc(%dim_0) {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %5 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast, %alloc) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  %alloc_2 = memref.alloc(%dim) {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_1, %alloc_2) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  %6 = arith.index_cast %4 : index to i32
  %7 = arith.muli %6, %c4_i32 : i32
  %8 = arith.index_cast %7 : i32 to index
  %9 = arith.cmpi eq, %dim_0, %4 : index
  %10 = arith.cmpi eq, %dim, %4 : index
  %11 = arith.andi %10, %9 : i1
  %alloc_3 = memref.alloc() : memref<1xf32, #gpu.address_space<global>>
  scf.if %11 {
    %13 = arith.cmpi slt, %8, %c1 : index
    scf.if %13 {
      "lmhlo.fusion"() ({
        %c0_6 = arith.constant 0 : index
        %c512_7 = arith.constant 512 : index
        %14 = arith.muli %c1, %c512_7 : index
        %c1_8 = arith.constant 1 : index
        %15 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%c1)[%c0, %14]
        %16 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%14)[%c0_6, %c1]
        gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %15, %arg8 = %c1_8, %arg9 = %c1_8) threads(%arg4, %arg5, %arg6) in (%arg10 = %16, %arg11 = %c1_8, %arg12 = %c1_8) {
          %27 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg1)[%14, %c0]
          %28 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg4)[%c1, %c0_6]
          %29 = arith.addi %28, %27 : index
          %true = arith.constant true
          %30 = arith.muli %28, %c1 : index
          %31 = arith.addi %30, %27 : index
          %32 = arith.cmpi ult, %31, %c1 : index
          %33 = arith.andi %true, %32 : i1
          scf.if %33 {
            %reinterpret_cast_15 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
            memref.store %cst, %reinterpret_cast_15[%29] : memref<1xf32, #gpu.address_space<global>>
          }
          gpu.terminator
        } {SCFToGPU_visited}
        %17 = arith.cmpi eq, %8, %c0 : index
        %18 = arith.subi %8, %c1 : index
        %19 = arith.divui %18, %c32 : index
        %20 = arith.addi %19, %c1 : index
        %21 = arith.select %17, %c0, %20 : index
        %c0_9 = arith.constant 0 : index
        %c1_10 = arith.constant 1 : index
        %c1_11 = arith.constant 1 : index
        %22 = arith.muli %c1_11, %21 : index
        %23 = arith.muli %22, %c512 : index
        %c0_12 = arith.constant 0 : index
        %c512_13 = arith.constant 512 : index
        %24 = arith.muli %c1_10, %c512_13 : index
        %c1_14 = arith.constant 1 : index
        %25 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%23)[%c0_9, %24]
        %26 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%24)[%c0_12, %c1_10]
        gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %25, %arg8 = %c1_14, %arg9 = %c1_14) threads(%arg4, %arg5, %arg6) in (%arg10 = %26, %arg11 = %c1_14, %arg12 = %c1_14) {
          %27 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg1)[%24, %c0_9]
          %28 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg4)[%c1_10, %c0_12]
          %29 = arith.addi %28, %27 : index
          %true = arith.constant true
          %30 = arith.muli %28, %c1_10 : index
          %31 = arith.addi %30, %27 : index
          %32 = arith.cmpi ult, %31, %23 : index
          %33 = arith.andi %true, %32 : i1
          scf.if %33 {
            %34 = arith.remsi %29, %c512 : index
            %35 = arith.divsi %29, %c512 : index
            %36 = arith.cmpi ult, %34, %c1 : index
            scf.if %36 {
              %37 = scf.for %arg13 = %c0 to %c32 step %c1 iter_args(%arg14 = %cst) -> (f32) {
                %39 = arith.muli %35, %c32 : index
                %40 = arith.addi %39, %arg13 : index
                %41 = arith.cmpi slt, %40, %8 : index
                %42 = scf.if %41 -> (f32) {
                  %43 = arith.muli %4, %c4 : index
                  %reinterpret_cast_15 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%43], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                  %44 = memref.load %reinterpret_cast_15[%40] : memref<?xf32, #gpu.address_space<global>>
                  %reinterpret_cast_16 = memref.reinterpret_cast %alloc_2 to offset: [%c0], sizes: [%43], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                  %45 = memref.load %reinterpret_cast_16[%40] : memref<?xf32, #gpu.address_space<global>>
                  %46 = arith.mulf %44, %45 : f32
                  %47 = arith.addf %arg14, %46 : f32
                  scf.yield %47 : f32
                } else {
                  scf.yield %arg14 : f32
                }
                scf.yield %42 : f32
              }
              %38 = memref.atomic_rmw addf %37, %alloc_3[%34] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
            }
          }
          gpu.terminator
        } {SCFToGPU_visited}
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXthread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        %c0_6 = arith.constant 0 : index
        %c256_7 = arith.constant 256 : index
        %14 = arith.muli %c1, %c256_7 : index
        %c1_8 = arith.constant 1 : index
        %15 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%c1)[%c0, %14]
        %16 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%14)[%c0_6, %c1]
        gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %15, %arg8 = %c1_8, %arg9 = %c1_8) threads(%arg4, %arg5, %arg6) in (%arg10 = %16, %arg11 = %c1_8, %arg12 = %c1_8) {
          %27 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg1)[%14, %c0]
          %28 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg4)[%c1, %c0_6]
          %29 = arith.addi %28, %27 : index
          %true = arith.constant true
          %30 = arith.muli %28, %c1 : index
          %31 = arith.addi %30, %27 : index
          %32 = arith.cmpi ult, %31, %c1 : index
          %33 = arith.andi %true, %32 : i1
          scf.if %33 {
            %reinterpret_cast_15 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
            memref.store %cst, %reinterpret_cast_15[%29] : memref<1xf32, #gpu.address_space<global>>
          }
          gpu.terminator
        } {SCFToGPU_visited}
        %17 = arith.cmpi eq, %8, %c0 : index
        %18 = arith.subi %8, %c1 : index
        %19 = arith.divui %18, %c512 : index
        %20 = arith.addi %19, %c1 : index
        %21 = arith.select %17, %c0, %20 : index
        %c0_9 = arith.constant 0 : index
        %c1_10 = arith.constant 1 : index
        %c1_11 = arith.constant 1 : index
        %22 = arith.muli %c1_11, %21 : index
        %23 = arith.muli %22, %c256 : index
        %c0_12 = arith.constant 0 : index
        %c256_13 = arith.constant 256 : index
        %24 = arith.muli %c1_10, %c256_13 : index
        %c1_14 = arith.constant 1 : index
        %25 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%23)[%c0_9, %24]
        %26 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%24)[%c0_12, %c1_10]
        gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %25, %arg8 = %c1_14, %arg9 = %c1_14) threads(%arg4, %arg5, %arg6) in (%arg10 = %26, %arg11 = %c1_14, %arg12 = %c1_14) {
          %27 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg1)[%24, %c0_9]
          %28 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg4)[%c1_10, %c0_12]
          %29 = arith.addi %28, %27 : index
          %true = arith.constant true
          %30 = arith.muli %28, %c1_10 : index
          %31 = arith.addi %30, %27 : index
          %32 = arith.cmpi ult, %31, %23 : index
          %33 = arith.andi %true, %32 : i1
          scf.if %33 {
            %34 = arith.remsi %29, %c256 : index
            %35 = arith.divsi %29, %c256 : index
            %36 = arith.divui %34, %c32 : index
            %37 = arith.remui %34, %c32 : index
            %38 = arith.muli %37, %c8 : index
            %39 = arith.addi %36, %38 : index
            %alloc_15 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
            %40 = arith.cmpi ult, %37, %c1 : index
            %41 = scf.if %40 -> (f32) {
              %47 = scf.for %arg13 = %c0 to %c64 step %c1 iter_args(%arg14 = %cst) -> (f32) {
                %48 = arith.muli %35, %c8 : index
                %49 = arith.addi %36, %48 : index
                %50 = arith.muli %49, %c64 : index
                %51 = arith.addi %arg13, %50 : index
                %52 = arith.cmpi slt, %51, %8 : index
                %53 = scf.if %52 -> (f32) {
                  %54 = arith.muli %4, %c4 : index
                  %reinterpret_cast_17 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%54], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                  %55 = memref.load %reinterpret_cast_17[%51] : memref<?xf32, #gpu.address_space<global>>
                  %reinterpret_cast_18 = memref.reinterpret_cast %alloc_2 to offset: [%c0], sizes: [%54], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                  %56 = memref.load %reinterpret_cast_18[%51] : memref<?xf32, #gpu.address_space<global>>
                  %57 = arith.mulf %55, %56 : f32
                  %58 = arith.addf %arg14, %57 : f32
                  scf.yield %58 : f32
                } else {
                  scf.yield %arg14 : f32
                }
                scf.yield %53 : f32
              }
              scf.yield %47 : f32
            } else {
              scf.yield %cst : f32
            }
            %reinterpret_cast_16 = memref.reinterpret_cast %alloc_15 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            memref.store %41, %reinterpret_cast_16[%39] : memref<256xf32, #gpu.address_space<workgroup>>
            gpu.barrier
            %42 = arith.cmpi slt, %36, %c4 : index
            scf.if %42 {
              %47 = arith.addi %39, %c4 : index
              %48 = memref.load %reinterpret_cast_16[%39] : memref<256xf32, #gpu.address_space<workgroup>>
              %49 = memref.load %reinterpret_cast_16[%47] : memref<256xf32, #gpu.address_space<workgroup>>
              %50 = arith.addf %48, %49 : f32
              memref.store %50, %reinterpret_cast_16[%39] : memref<256xf32, #gpu.address_space<workgroup>>
            }
            gpu.barrier
            %43 = arith.cmpi slt, %36, %c2 : index
            scf.if %43 {
              %47 = arith.addi %39, %c2 : index
              %48 = memref.load %reinterpret_cast_16[%39] : memref<256xf32, #gpu.address_space<workgroup>>
              %49 = memref.load %reinterpret_cast_16[%47] : memref<256xf32, #gpu.address_space<workgroup>>
              %50 = arith.addf %48, %49 : f32
              memref.store %50, %reinterpret_cast_16[%39] : memref<256xf32, #gpu.address_space<workgroup>>
            }
            gpu.barrier
            %44 = arith.cmpi slt, %36, %c1 : index
            scf.if %44 {
              %47 = arith.addi %39, %c1 : index
              %48 = memref.load %reinterpret_cast_16[%39] : memref<256xf32, #gpu.address_space<workgroup>>
              %49 = memref.load %reinterpret_cast_16[%47] : memref<256xf32, #gpu.address_space<workgroup>>
              %50 = arith.addf %48, %49 : f32
              memref.store %50, %reinterpret_cast_16[%39] : memref<256xf32, #gpu.address_space<workgroup>>
            }
            gpu.barrier
            %45 = arith.cmpi eq, %36, %c0 : index
            %46 = arith.andi %45, %40 : i1
            scf.if %46 {
              %47 = memref.load %reinterpret_cast_16[%39] : memref<256xf32, #gpu.address_space<workgroup>>
              %48 = memref.atomic_rmw addf %47, %alloc_3[%37] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
            }
          }
          gpu.terminator
        } {SCFToGPU_visited}
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXblock_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
    }
  } else {
    %13 = arith.cmpi slt, %8, %c1 : index
    scf.if %13 {
      "lmhlo.fusion"() ({
        %c0_6 = arith.constant 0 : index
        %c512_7 = arith.constant 512 : index
        %14 = arith.muli %c1, %c512_7 : index
        %c1_8 = arith.constant 1 : index
        %15 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%c1)[%c0, %14]
        %16 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%14)[%c0_6, %c1]
        gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %15, %arg8 = %c1_8, %arg9 = %c1_8) threads(%arg4, %arg5, %arg6) in (%arg10 = %16, %arg11 = %c1_8, %arg12 = %c1_8) {
          %27 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg1)[%14, %c0]
          %28 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg4)[%c1, %c0_6]
          %29 = arith.addi %28, %27 : index
          %true = arith.constant true
          %30 = arith.muli %28, %c1 : index
          %31 = arith.addi %30, %27 : index
          %32 = arith.cmpi ult, %31, %c1 : index
          %33 = arith.andi %true, %32 : i1
          scf.if %33 {
            %reinterpret_cast_15 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
            memref.store %cst, %reinterpret_cast_15[%29] : memref<1xf32, #gpu.address_space<global>>
          }
          gpu.terminator
        } {SCFToGPU_visited}
        %17 = arith.cmpi eq, %8, %c0 : index
        %18 = arith.subi %8, %c1 : index
        %19 = arith.divui %18, %c32 : index
        %20 = arith.addi %19, %c1 : index
        %21 = arith.select %17, %c0, %20 : index
        %c0_9 = arith.constant 0 : index
        %c1_10 = arith.constant 1 : index
        %c1_11 = arith.constant 1 : index
        %22 = arith.muli %c1_11, %21 : index
        %23 = arith.muli %22, %c512 : index
        %c0_12 = arith.constant 0 : index
        %c512_13 = arith.constant 512 : index
        %24 = arith.muli %c1_10, %c512_13 : index
        %c1_14 = arith.constant 1 : index
        %25 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%23)[%c0_9, %24]
        %26 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%24)[%c0_12, %c1_10]
        gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %25, %arg8 = %c1_14, %arg9 = %c1_14) threads(%arg4, %arg5, %arg6) in (%arg10 = %26, %arg11 = %c1_14, %arg12 = %c1_14) {
          %27 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg1)[%24, %c0_9]
          %28 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg4)[%c1_10, %c0_12]
          %29 = arith.addi %28, %27 : index
          %true = arith.constant true
          %30 = arith.muli %28, %c1_10 : index
          %31 = arith.addi %30, %27 : index
          %32 = arith.cmpi ult, %31, %23 : index
          %33 = arith.andi %true, %32 : i1
          scf.if %33 {
            %34 = arith.remsi %29, %c512 : index
            %35 = arith.divsi %29, %c512 : index
            %36 = arith.cmpi ult, %34, %c1 : index
            scf.if %36 {
              %37 = scf.for %arg13 = %c0 to %c32 step %c1 iter_args(%arg14 = %cst) -> (f32) {
                %39 = arith.muli %35, %c32 : index
                %40 = arith.addi %39, %arg13 : index
                %41 = arith.cmpi slt, %40, %8 : index
                %42 = scf.if %41 -> (f32) {
                  %43 = arith.remui %40, %c4 : index
                  %44 = arith.divui %40, %c4 : index
                  %45 = arith.select %9, %44, %c0 : index
                  %46 = arith.muli %45, %c4 : index
                  %47 = arith.addi %46, %43 : index
                  %48 = arith.muli %dim_0, %c4 : index
                  %reinterpret_cast_15 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%48], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                  %49 = memref.load %reinterpret_cast_15[%47] : memref<?xf32, #gpu.address_space<global>>
                  %50 = arith.select %10, %44, %c0 : index
                  %51 = arith.muli %50, %c4 : index
                  %52 = arith.addi %51, %43 : index
                  %53 = arith.muli %dim, %c4 : index
                  %reinterpret_cast_16 = memref.reinterpret_cast %alloc_2 to offset: [%c0], sizes: [%53], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                  %54 = memref.load %reinterpret_cast_16[%52] : memref<?xf32, #gpu.address_space<global>>
                  %55 = arith.mulf %49, %54 : f32
                  %56 = arith.addf %arg14, %55 : f32
                  scf.yield %56 : f32
                } else {
                  scf.yield %arg14 : f32
                }
                scf.yield %42 : f32
              }
              %38 = memref.atomic_rmw addf %37, %alloc_3[%34] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
            }
          }
          gpu.terminator
        } {SCFToGPU_visited}
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "thread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        %c0_6 = arith.constant 0 : index
        %c256_7 = arith.constant 256 : index
        %14 = arith.muli %c1, %c256_7 : index
        %c1_8 = arith.constant 1 : index
        %15 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%c1)[%c0, %14]
        %16 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%14)[%c0_6, %c1]
        gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %15, %arg8 = %c1_8, %arg9 = %c1_8) threads(%arg4, %arg5, %arg6) in (%arg10 = %16, %arg11 = %c1_8, %arg12 = %c1_8) {
          %27 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg1)[%14, %c0]
          %28 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg4)[%c1, %c0_6]
          %29 = arith.addi %28, %27 : index
          %true = arith.constant true
          %30 = arith.muli %28, %c1 : index
          %31 = arith.addi %30, %27 : index
          %32 = arith.cmpi ult, %31, %c1 : index
          %33 = arith.andi %true, %32 : i1
          scf.if %33 {
            %reinterpret_cast_15 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
            memref.store %cst, %reinterpret_cast_15[%29] : memref<1xf32, #gpu.address_space<global>>
          }
          gpu.terminator
        } {SCFToGPU_visited}
        %17 = arith.cmpi eq, %8, %c0 : index
        %18 = arith.subi %8, %c1 : index
        %19 = arith.divui %18, %c512 : index
        %20 = arith.addi %19, %c1 : index
        %21 = arith.select %17, %c0, %20 : index
        %c0_9 = arith.constant 0 : index
        %c1_10 = arith.constant 1 : index
        %c1_11 = arith.constant 1 : index
        %22 = arith.muli %c1_11, %21 : index
        %23 = arith.muli %22, %c256 : index
        %c0_12 = arith.constant 0 : index
        %c256_13 = arith.constant 256 : index
        %24 = arith.muli %c1_10, %c256_13 : index
        %c1_14 = arith.constant 1 : index
        %25 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%23)[%c0_9, %24]
        %26 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%24)[%c0_12, %c1_10]
        gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %25, %arg8 = %c1_14, %arg9 = %c1_14) threads(%arg4, %arg5, %arg6) in (%arg10 = %26, %arg11 = %c1_14, %arg12 = %c1_14) {
          %27 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg1)[%24, %c0_9]
          %28 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg4)[%c1_10, %c0_12]
          %29 = arith.addi %28, %27 : index
          %true = arith.constant true
          %30 = arith.muli %28, %c1_10 : index
          %31 = arith.addi %30, %27 : index
          %32 = arith.cmpi ult, %31, %23 : index
          %33 = arith.andi %true, %32 : i1
          scf.if %33 {
            %34 = arith.remsi %29, %c256 : index
            %35 = arith.divsi %29, %c256 : index
            %36 = arith.divui %34, %c32 : index
            %37 = arith.remui %34, %c32 : index
            %38 = arith.muli %37, %c8 : index
            %39 = arith.addi %36, %38 : index
            %alloc_15 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
            %40 = arith.cmpi ult, %37, %c1 : index
            %41 = scf.if %40 -> (f32) {
              %47 = scf.for %arg13 = %c0 to %c64 step %c1 iter_args(%arg14 = %cst) -> (f32) {
                %48 = arith.muli %35, %c8 : index
                %49 = arith.addi %36, %48 : index
                %50 = arith.muli %49, %c64 : index
                %51 = arith.addi %arg13, %50 : index
                %52 = arith.cmpi slt, %51, %8 : index
                %53 = scf.if %52 -> (f32) {
                  %54 = arith.remui %51, %c4 : index
                  %55 = arith.divui %51, %c4 : index
                  %56 = arith.select %9, %55, %c0 : index
                  %57 = arith.muli %56, %c4 : index
                  %58 = arith.addi %57, %54 : index
                  %59 = arith.muli %dim_0, %c4 : index
                  %reinterpret_cast_17 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%59], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                  %60 = memref.load %reinterpret_cast_17[%58] : memref<?xf32, #gpu.address_space<global>>
                  %61 = arith.select %10, %55, %c0 : index
                  %62 = arith.muli %61, %c4 : index
                  %63 = arith.addi %62, %54 : index
                  %64 = arith.muli %dim, %c4 : index
                  %reinterpret_cast_18 = memref.reinterpret_cast %alloc_2 to offset: [%c0], sizes: [%64], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                  %65 = memref.load %reinterpret_cast_18[%63] : memref<?xf32, #gpu.address_space<global>>
                  %66 = arith.mulf %60, %65 : f32
                  %67 = arith.addf %arg14, %66 : f32
                  scf.yield %67 : f32
                } else {
                  scf.yield %arg14 : f32
                }
                scf.yield %53 : f32
              }
              scf.yield %47 : f32
            } else {
              scf.yield %cst : f32
            }
            %reinterpret_cast_16 = memref.reinterpret_cast %alloc_15 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            memref.store %41, %reinterpret_cast_16[%39] : memref<256xf32, #gpu.address_space<workgroup>>
            gpu.barrier
            %42 = arith.cmpi slt, %36, %c4 : index
            scf.if %42 {
              %47 = arith.addi %39, %c4 : index
              %48 = memref.load %reinterpret_cast_16[%39] : memref<256xf32, #gpu.address_space<workgroup>>
              %49 = memref.load %reinterpret_cast_16[%47] : memref<256xf32, #gpu.address_space<workgroup>>
              %50 = arith.addf %48, %49 : f32
              memref.store %50, %reinterpret_cast_16[%39] : memref<256xf32, #gpu.address_space<workgroup>>
            }
            gpu.barrier
            %43 = arith.cmpi slt, %36, %c2 : index
            scf.if %43 {
              %47 = arith.addi %39, %c2 : index
              %48 = memref.load %reinterpret_cast_16[%39] : memref<256xf32, #gpu.address_space<workgroup>>
              %49 = memref.load %reinterpret_cast_16[%47] : memref<256xf32, #gpu.address_space<workgroup>>
              %50 = arith.addf %48, %49 : f32
              memref.store %50, %reinterpret_cast_16[%39] : memref<256xf32, #gpu.address_space<workgroup>>
            }
            gpu.barrier
            %44 = arith.cmpi slt, %36, %c1 : index
            scf.if %44 {
              %47 = arith.addi %39, %c1 : index
              %48 = memref.load %reinterpret_cast_16[%39] : memref<256xf32, #gpu.address_space<workgroup>>
              %49 = memref.load %reinterpret_cast_16[%47] : memref<256xf32, #gpu.address_space<workgroup>>
              %50 = arith.addf %48, %49 : f32
              memref.store %50, %reinterpret_cast_16[%39] : memref<256xf32, #gpu.address_space<workgroup>>
            }
            gpu.barrier
            %45 = arith.cmpi eq, %36, %c0 : index
            %46 = arith.andi %45, %40 : i1
            scf.if %46 {
              %47 = memref.load %reinterpret_cast_16[%39] : memref<256xf32, #gpu.address_space<workgroup>>
              %48 = memref.atomic_rmw addf %47, %alloc_3[%37] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
            }
          }
          gpu.terminator
        } {SCFToGPU_visited}
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "block_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
    }
  }
  memref.dealloc %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc : memref<?x4xf32, #gpu.address_space<global>>
  %alloca = memref.alloca() : memref<0xindex>
  %12 = "disc_ral.dispatch"(%arg0, %5, %alloc_3, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1xf32, #gpu.address_space<global>>, memref<0xindex>) -> memref<f32, #gpu.address_space<global>>
  %reinterpret_cast_4 = memref.reinterpret_cast %12 to offset: [0], sizes: [], strides: [] : memref<f32, #gpu.address_space<global>> to memref<f32, #gpu.address_space<global>>
  memref.dealloc %alloc_3 : memref<1xf32, #gpu.address_space<global>>
  %alloc_5 = memref.alloc() : memref<f32>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_4, %alloc_5) {backend_config = "", call_target_name = "d2h", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<f32, #gpu.address_space<global>>, memref<f32>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  memref.dealloc %reinterpret_cast_4 : memref<f32, #gpu.address_space<global>>
  "disc_ral.dispatch"(%arg0, %c0, %alloc_5) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<f32>) -> ()
  return
}

// -----// IR Dump After DiscMemRefLoadStoreSimplifierPass (disc-memref-load-store-simplifier) //----- //
func.func @shape_constraint_graph() {
  %c4 = arith.constant 4 : index
  %0 = "disc_shape.dim"() {name = @S2} : () -> index
  %1 = "disc_shape.dim"() {name = @S3} : () -> index
  "disc_shape.tie_product_equal"(%c4, %0, %1) {operand_segment_sizes = array<i32: 2, 1>} : (index, index, index) -> ()
  %2 = "disc_shape.dim"() {name = @S4} : () -> index
  %3 = "disc_shape.dim"() {name = @S5} : () -> index
  "disc_shape.tie_product_equal"(%c4, %2, %3) {operand_segment_sizes = array<i32: 2, 1>} : (index, index, index) -> ()
  return
}

// -----// IR Dump After GpuLaunchSinkIndexComputations (gpu-launch-sink-index-computations) //----- //
#map = affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>
#map1 = affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>
module {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
    %cst = arith.constant 0.000000e+00 : f32
    %c2 = arith.constant 2 : index
    %c256 = arith.constant 256 : index
    %c64 = arith.constant 64 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c512 = arith.constant 512 : index
    %0 = llvm.mlir.constant(0 : i32) : i32
    %c4_i32 = arith.constant 4 : i32
    %c4 = arith.constant 4 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
    %2 = "disc_ral.dispatch"(%arg0, %c1) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
    %dim = memref.dim %2, %c0 : memref<?x4xf32>
    %dim_0 = memref.dim %1, %c0 : memref<?x4xf32>
    %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
    %reinterpret_cast_1 = memref.reinterpret_cast %2 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
    %3 = arith.cmpi eq, %dim, %c1 : index
    %4 = arith.select %3, %dim_0, %dim : index
    %alloc = memref.alloc(%dim_0) {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
    %5 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
    "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast, %alloc) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
    "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
    %alloc_2 = memref.alloc(%dim) {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
    "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_1, %alloc_2) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
    "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
    %6 = arith.index_cast %4 : index to i32
    %7 = arith.muli %6, %c4_i32 : i32
    %8 = arith.index_cast %7 : i32 to index
    %9 = arith.cmpi eq, %dim_0, %4 : index
    %10 = arith.cmpi eq, %dim, %4 : index
    %11 = arith.andi %10, %9 : i1
    %alloc_3 = memref.alloc() : memref<1xf32, #gpu.address_space<global>>
    scf.if %11 {
      %13 = arith.cmpi slt, %8, %c1 : index
      scf.if %13 {
        "lmhlo.fusion"() ({
          %c0_6 = arith.constant 0 : index
          %c512_7 = arith.constant 512 : index
          %14 = arith.muli %c1, %c512_7 : index
          %c1_8 = arith.constant 1 : index
          %15 = affine.apply #map(%c1)[%c0, %14]
          %16 = affine.apply #map(%14)[%c0_6, %c1]
          gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %15, %arg8 = %c1_8, %arg9 = %c1_8) threads(%arg4, %arg5, %arg6) in (%arg10 = %16, %arg11 = %c1_8, %arg12 = %c1_8) {
            %c0_15 = arith.constant 0 : index
            %c1_16 = arith.constant 1 : index
            %c0_17 = arith.constant 0 : index
            %cst_18 = arith.constant 0.000000e+00 : f32
            %27 = affine.apply #map1(%arg1)[%14, %c0_15]
            %28 = affine.apply #map1(%arg4)[%c1_16, %c0_17]
            %29 = arith.addi %28, %27 : index
            %true = arith.constant true
            %30 = arith.muli %28, %c1_16 : index
            %31 = arith.addi %30, %27 : index
            %32 = arith.cmpi ult, %31, %c1_16 : index
            %33 = arith.andi %true, %32 : i1
            scf.if %33 {
              %reinterpret_cast_19 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
              memref.store %cst_18, %reinterpret_cast_19[%29] : memref<1xf32, #gpu.address_space<global>>
            }
            gpu.terminator
          } {SCFToGPU_visited}
          %17 = arith.cmpi eq, %8, %c0 : index
          %18 = arith.subi %8, %c1 : index
          %19 = arith.divui %18, %c32 : index
          %20 = arith.addi %19, %c1 : index
          %21 = arith.select %17, %c0, %20 : index
          %c0_9 = arith.constant 0 : index
          %c1_10 = arith.constant 1 : index
          %c1_11 = arith.constant 1 : index
          %22 = arith.muli %c1_11, %21 : index
          %23 = arith.muli %22, %c512 : index
          %c0_12 = arith.constant 0 : index
          %c512_13 = arith.constant 512 : index
          %24 = arith.muli %c1_10, %c512_13 : index
          %c1_14 = arith.constant 1 : index
          %25 = affine.apply #map(%23)[%c0_9, %24]
          %26 = affine.apply #map(%24)[%c0_12, %c1_10]
          gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %25, %arg8 = %c1_14, %arg9 = %c1_14) threads(%arg4, %arg5, %arg6) in (%arg10 = %26, %arg11 = %c1_14, %arg12 = %c1_14) {
            %c0_15 = arith.constant 0 : index
            %c1_16 = arith.constant 1 : index
            %c0_17 = arith.constant 0 : index
            %c512_18 = arith.constant 512 : index
            %c1_19 = arith.constant 1 : index
            %c32_20 = arith.constant 32 : index
            %c4_21 = arith.constant 4 : index
            %c0_22 = arith.constant 0 : index
            %cst_23 = arith.constant 0.000000e+00 : f32
            %27 = affine.apply #map1(%arg1)[%24, %c0_15]
            %28 = affine.apply #map1(%arg4)[%c1_16, %c0_17]
            %29 = arith.addi %28, %27 : index
            %true = arith.constant true
            %30 = arith.muli %28, %c1_16 : index
            %31 = arith.addi %30, %27 : index
            %32 = arith.cmpi ult, %31, %23 : index
            %33 = arith.andi %true, %32 : i1
            scf.if %33 {
              %34 = arith.remsi %29, %c512_18 : index
              %35 = arith.divsi %29, %c512_18 : index
              %36 = arith.cmpi ult, %34, %c1_19 : index
              scf.if %36 {
                %37 = scf.for %arg13 = %c0_22 to %c32_20 step %c1_19 iter_args(%arg14 = %cst_23) -> (f32) {
                  %39 = arith.muli %35, %c32_20 : index
                  %40 = arith.addi %39, %arg13 : index
                  %41 = arith.cmpi slt, %40, %8 : index
                  %42 = scf.if %41 -> (f32) {
                    %43 = arith.muli %4, %c4_21 : index
                    %reinterpret_cast_24 = memref.reinterpret_cast %alloc to offset: [%c0_22], sizes: [%43], strides: [%c1_19] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                    %44 = memref.load %reinterpret_cast_24[%40] : memref<?xf32, #gpu.address_space<global>>
                    %reinterpret_cast_25 = memref.reinterpret_cast %alloc_2 to offset: [%c0_22], sizes: [%43], strides: [%c1_19] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                    %45 = memref.load %reinterpret_cast_25[%40] : memref<?xf32, #gpu.address_space<global>>
                    %46 = arith.mulf %44, %45 : f32
                    %47 = arith.addf %arg14, %46 : f32
                    scf.yield %47 : f32
                  } else {
                    scf.yield %arg14 : f32
                  }
                  scf.yield %42 : f32
                }
                %38 = memref.atomic_rmw addf %37, %alloc_3[%34] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
              }
            }
            gpu.terminator
          } {SCFToGPU_visited}
          "lmhlo.terminator"() : () -> ()
        }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXthread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
      } else {
        "lmhlo.fusion"() ({
          %c0_6 = arith.constant 0 : index
          %c256_7 = arith.constant 256 : index
          %14 = arith.muli %c1, %c256_7 : index
          %c1_8 = arith.constant 1 : index
          %15 = affine.apply #map(%c1)[%c0, %14]
          %16 = affine.apply #map(%14)[%c0_6, %c1]
          gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %15, %arg8 = %c1_8, %arg9 = %c1_8) threads(%arg4, %arg5, %arg6) in (%arg10 = %16, %arg11 = %c1_8, %arg12 = %c1_8) {
            %c0_15 = arith.constant 0 : index
            %c1_16 = arith.constant 1 : index
            %c0_17 = arith.constant 0 : index
            %cst_18 = arith.constant 0.000000e+00 : f32
            %27 = affine.apply #map1(%arg1)[%14, %c0_15]
            %28 = affine.apply #map1(%arg4)[%c1_16, %c0_17]
            %29 = arith.addi %28, %27 : index
            %true = arith.constant true
            %30 = arith.muli %28, %c1_16 : index
            %31 = arith.addi %30, %27 : index
            %32 = arith.cmpi ult, %31, %c1_16 : index
            %33 = arith.andi %true, %32 : i1
            scf.if %33 {
              %reinterpret_cast_19 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
              memref.store %cst_18, %reinterpret_cast_19[%29] : memref<1xf32, #gpu.address_space<global>>
            }
            gpu.terminator
          } {SCFToGPU_visited}
          %17 = arith.cmpi eq, %8, %c0 : index
          %18 = arith.subi %8, %c1 : index
          %19 = arith.divui %18, %c512 : index
          %20 = arith.addi %19, %c1 : index
          %21 = arith.select %17, %c0, %20 : index
          %c0_9 = arith.constant 0 : index
          %c1_10 = arith.constant 1 : index
          %c1_11 = arith.constant 1 : index
          %22 = arith.muli %c1_11, %21 : index
          %23 = arith.muli %22, %c256 : index
          %c0_12 = arith.constant 0 : index
          %c256_13 = arith.constant 256 : index
          %24 = arith.muli %c1_10, %c256_13 : index
          %c1_14 = arith.constant 1 : index
          %25 = affine.apply #map(%23)[%c0_9, %24]
          %26 = affine.apply #map(%24)[%c0_12, %c1_10]
          gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %25, %arg8 = %c1_14, %arg9 = %c1_14) threads(%arg4, %arg5, %arg6) in (%arg10 = %26, %arg11 = %c1_14, %arg12 = %c1_14) {
            %c0_15 = arith.constant 0 : index
            %c1_16 = arith.constant 1 : index
            %c0_17 = arith.constant 0 : index
            %c256_18 = arith.constant 256 : index
            %c32_19 = arith.constant 32 : index
            %c8_20 = arith.constant 8 : index
            %c1_21 = arith.constant 1 : index
            %c64_22 = arith.constant 64 : index
            %c4_23 = arith.constant 4 : index
            %c0_24 = arith.constant 0 : index
            %cst_25 = arith.constant 0.000000e+00 : f32
            %c2_26 = arith.constant 2 : index
            %27 = affine.apply #map1(%arg1)[%24, %c0_15]
            %28 = affine.apply #map1(%arg4)[%c1_16, %c0_17]
            %29 = arith.addi %28, %27 : index
            %true = arith.constant true
            %30 = arith.muli %28, %c1_16 : index
            %31 = arith.addi %30, %27 : index
            %32 = arith.cmpi ult, %31, %23 : index
            %33 = arith.andi %true, %32 : i1
            scf.if %33 {
              %34 = arith.remsi %29, %c256_18 : index
              %35 = arith.divsi %29, %c256_18 : index
              %36 = arith.divui %34, %c32_19 : index
              %37 = arith.remui %34, %c32_19 : index
              %38 = arith.muli %37, %c8_20 : index
              %39 = arith.addi %36, %38 : index
              %alloc_27 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
              %40 = arith.cmpi ult, %37, %c1_21 : index
              %41 = scf.if %40 -> (f32) {
                %47 = scf.for %arg13 = %c0_24 to %c64_22 step %c1_21 iter_args(%arg14 = %cst_25) -> (f32) {
                  %48 = arith.muli %35, %c8_20 : index
                  %49 = arith.addi %36, %48 : index
                  %50 = arith.muli %49, %c64_22 : index
                  %51 = arith.addi %arg13, %50 : index
                  %52 = arith.cmpi slt, %51, %8 : index
                  %53 = scf.if %52 -> (f32) {
                    %54 = arith.muli %4, %c4_23 : index
                    %reinterpret_cast_29 = memref.reinterpret_cast %alloc to offset: [%c0_24], sizes: [%54], strides: [%c1_21] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                    %55 = memref.load %reinterpret_cast_29[%51] : memref<?xf32, #gpu.address_space<global>>
                    %reinterpret_cast_30 = memref.reinterpret_cast %alloc_2 to offset: [%c0_24], sizes: [%54], strides: [%c1_21] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                    %56 = memref.load %reinterpret_cast_30[%51] : memref<?xf32, #gpu.address_space<global>>
                    %57 = arith.mulf %55, %56 : f32
                    %58 = arith.addf %arg14, %57 : f32
                    scf.yield %58 : f32
                  } else {
                    scf.yield %arg14 : f32
                  }
                  scf.yield %53 : f32
                }
                scf.yield %47 : f32
              } else {
                scf.yield %cst_25 : f32
              }
              %reinterpret_cast_28 = memref.reinterpret_cast %alloc_27 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
              memref.store %41, %reinterpret_cast_28[%39] : memref<256xf32, #gpu.address_space<workgroup>>
              gpu.barrier
              %42 = arith.cmpi slt, %36, %c4_23 : index
              scf.if %42 {
                %47 = arith.addi %39, %c4_23 : index
                %48 = memref.load %reinterpret_cast_28[%39] : memref<256xf32, #gpu.address_space<workgroup>>
                %49 = memref.load %reinterpret_cast_28[%47] : memref<256xf32, #gpu.address_space<workgroup>>
                %50 = arith.addf %48, %49 : f32
                memref.store %50, %reinterpret_cast_28[%39] : memref<256xf32, #gpu.address_space<workgroup>>
              }
              gpu.barrier
              %43 = arith.cmpi slt, %36, %c2_26 : index
              scf.if %43 {
                %47 = arith.addi %39, %c2_26 : index
                %48 = memref.load %reinterpret_cast_28[%39] : memref<256xf32, #gpu.address_space<workgroup>>
                %49 = memref.load %reinterpret_cast_28[%47] : memref<256xf32, #gpu.address_space<workgroup>>
                %50 = arith.addf %48, %49 : f32
                memref.store %50, %reinterpret_cast_28[%39] : memref<256xf32, #gpu.address_space<workgroup>>
              }
              gpu.barrier
              %44 = arith.cmpi slt, %36, %c1_21 : index
              scf.if %44 {
                %47 = arith.addi %39, %c1_21 : index
                %48 = memref.load %reinterpret_cast_28[%39] : memref<256xf32, #gpu.address_space<workgroup>>
                %49 = memref.load %reinterpret_cast_28[%47] : memref<256xf32, #gpu.address_space<workgroup>>
                %50 = arith.addf %48, %49 : f32
                memref.store %50, %reinterpret_cast_28[%39] : memref<256xf32, #gpu.address_space<workgroup>>
              }
              gpu.barrier
              %45 = arith.cmpi eq, %36, %c0_24 : index
              %46 = arith.andi %45, %40 : i1
              scf.if %46 {
                %47 = memref.load %reinterpret_cast_28[%39] : memref<256xf32, #gpu.address_space<workgroup>>
                %48 = memref.atomic_rmw addf %47, %alloc_3[%37] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
              }
            }
            gpu.terminator
          } {SCFToGPU_visited}
          "lmhlo.terminator"() : () -> ()
        }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXblock_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
      }
    } else {
      %13 = arith.cmpi slt, %8, %c1 : index
      scf.if %13 {
        "lmhlo.fusion"() ({
          %c0_6 = arith.constant 0 : index
          %c512_7 = arith.constant 512 : index
          %14 = arith.muli %c1, %c512_7 : index
          %c1_8 = arith.constant 1 : index
          %15 = affine.apply #map(%c1)[%c0, %14]
          %16 = affine.apply #map(%14)[%c0_6, %c1]
          gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %15, %arg8 = %c1_8, %arg9 = %c1_8) threads(%arg4, %arg5, %arg6) in (%arg10 = %16, %arg11 = %c1_8, %arg12 = %c1_8) {
            %c0_15 = arith.constant 0 : index
            %c1_16 = arith.constant 1 : index
            %c0_17 = arith.constant 0 : index
            %cst_18 = arith.constant 0.000000e+00 : f32
            %27 = affine.apply #map1(%arg1)[%14, %c0_15]
            %28 = affine.apply #map1(%arg4)[%c1_16, %c0_17]
            %29 = arith.addi %28, %27 : index
            %true = arith.constant true
            %30 = arith.muli %28, %c1_16 : index
            %31 = arith.addi %30, %27 : index
            %32 = arith.cmpi ult, %31, %c1_16 : index
            %33 = arith.andi %true, %32 : i1
            scf.if %33 {
              %reinterpret_cast_19 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
              memref.store %cst_18, %reinterpret_cast_19[%29] : memref<1xf32, #gpu.address_space<global>>
            }
            gpu.terminator
          } {SCFToGPU_visited}
          %17 = arith.cmpi eq, %8, %c0 : index
          %18 = arith.subi %8, %c1 : index
          %19 = arith.divui %18, %c32 : index
          %20 = arith.addi %19, %c1 : index
          %21 = arith.select %17, %c0, %20 : index
          %c0_9 = arith.constant 0 : index
          %c1_10 = arith.constant 1 : index
          %c1_11 = arith.constant 1 : index
          %22 = arith.muli %c1_11, %21 : index
          %23 = arith.muli %22, %c512 : index
          %c0_12 = arith.constant 0 : index
          %c512_13 = arith.constant 512 : index
          %24 = arith.muli %c1_10, %c512_13 : index
          %c1_14 = arith.constant 1 : index
          %25 = affine.apply #map(%23)[%c0_9, %24]
          %26 = affine.apply #map(%24)[%c0_12, %c1_10]
          gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %25, %arg8 = %c1_14, %arg9 = %c1_14) threads(%arg4, %arg5, %arg6) in (%arg10 = %26, %arg11 = %c1_14, %arg12 = %c1_14) {
            %c0_15 = arith.constant 0 : index
            %c1_16 = arith.constant 1 : index
            %c0_17 = arith.constant 0 : index
            %c512_18 = arith.constant 512 : index
            %c1_19 = arith.constant 1 : index
            %c32_20 = arith.constant 32 : index
            %c4_21 = arith.constant 4 : index
            %27 = arith.cmpi eq, %dim, %c1_19 : index
            %28 = arith.select %27, %dim_0, %dim : index
            %29 = arith.cmpi eq, %dim_0, %28 : index
            %c0_22 = arith.constant 0 : index
            %30 = arith.cmpi eq, %dim, %28 : index
            %cst_23 = arith.constant 0.000000e+00 : f32
            %31 = affine.apply #map1(%arg1)[%24, %c0_15]
            %32 = affine.apply #map1(%arg4)[%c1_16, %c0_17]
            %33 = arith.addi %32, %31 : index
            %true = arith.constant true
            %34 = arith.muli %32, %c1_16 : index
            %35 = arith.addi %34, %31 : index
            %36 = arith.cmpi ult, %35, %23 : index
            %37 = arith.andi %true, %36 : i1
            scf.if %37 {
              %38 = arith.remsi %33, %c512_18 : index
              %39 = arith.divsi %33, %c512_18 : index
              %40 = arith.cmpi ult, %38, %c1_19 : index
              scf.if %40 {
                %41 = scf.for %arg13 = %c0_22 to %c32_20 step %c1_19 iter_args(%arg14 = %cst_23) -> (f32) {
                  %43 = arith.muli %39, %c32_20 : index
                  %44 = arith.addi %43, %arg13 : index
                  %45 = arith.cmpi slt, %44, %8 : index
                  %46 = scf.if %45 -> (f32) {
                    %47 = arith.remui %44, %c4_21 : index
                    %48 = arith.divui %44, %c4_21 : index
                    %49 = arith.select %29, %48, %c0_22 : index
                    %50 = arith.muli %49, %c4_21 : index
                    %51 = arith.addi %50, %47 : index
                    %52 = arith.muli %dim_0, %c4_21 : index
                    %reinterpret_cast_24 = memref.reinterpret_cast %alloc to offset: [%c0_22], sizes: [%52], strides: [%c1_19] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                    %53 = memref.load %reinterpret_cast_24[%51] : memref<?xf32, #gpu.address_space<global>>
                    %54 = arith.select %30, %48, %c0_22 : index
                    %55 = arith.muli %54, %c4_21 : index
                    %56 = arith.addi %55, %47 : index
                    %57 = arith.muli %dim, %c4_21 : index
                    %reinterpret_cast_25 = memref.reinterpret_cast %alloc_2 to offset: [%c0_22], sizes: [%57], strides: [%c1_19] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                    %58 = memref.load %reinterpret_cast_25[%56] : memref<?xf32, #gpu.address_space<global>>
                    %59 = arith.mulf %53, %58 : f32
                    %60 = arith.addf %arg14, %59 : f32
                    scf.yield %60 : f32
                  } else {
                    scf.yield %arg14 : f32
                  }
                  scf.yield %46 : f32
                }
                %42 = memref.atomic_rmw addf %41, %alloc_3[%38] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
              }
            }
            gpu.terminator
          } {SCFToGPU_visited}
          "lmhlo.terminator"() : () -> ()
        }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "thread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
      } else {
        "lmhlo.fusion"() ({
          %c0_6 = arith.constant 0 : index
          %c256_7 = arith.constant 256 : index
          %14 = arith.muli %c1, %c256_7 : index
          %c1_8 = arith.constant 1 : index
          %15 = affine.apply #map(%c1)[%c0, %14]
          %16 = affine.apply #map(%14)[%c0_6, %c1]
          gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %15, %arg8 = %c1_8, %arg9 = %c1_8) threads(%arg4, %arg5, %arg6) in (%arg10 = %16, %arg11 = %c1_8, %arg12 = %c1_8) {
            %c0_15 = arith.constant 0 : index
            %c1_16 = arith.constant 1 : index
            %c0_17 = arith.constant 0 : index
            %cst_18 = arith.constant 0.000000e+00 : f32
            %27 = affine.apply #map1(%arg1)[%14, %c0_15]
            %28 = affine.apply #map1(%arg4)[%c1_16, %c0_17]
            %29 = arith.addi %28, %27 : index
            %true = arith.constant true
            %30 = arith.muli %28, %c1_16 : index
            %31 = arith.addi %30, %27 : index
            %32 = arith.cmpi ult, %31, %c1_16 : index
            %33 = arith.andi %true, %32 : i1
            scf.if %33 {
              %reinterpret_cast_19 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
              memref.store %cst_18, %reinterpret_cast_19[%29] : memref<1xf32, #gpu.address_space<global>>
            }
            gpu.terminator
          } {SCFToGPU_visited}
          %17 = arith.cmpi eq, %8, %c0 : index
          %18 = arith.subi %8, %c1 : index
          %19 = arith.divui %18, %c512 : index
          %20 = arith.addi %19, %c1 : index
          %21 = arith.select %17, %c0, %20 : index
          %c0_9 = arith.constant 0 : index
          %c1_10 = arith.constant 1 : index
          %c1_11 = arith.constant 1 : index
          %22 = arith.muli %c1_11, %21 : index
          %23 = arith.muli %22, %c256 : index
          %c0_12 = arith.constant 0 : index
          %c256_13 = arith.constant 256 : index
          %24 = arith.muli %c1_10, %c256_13 : index
          %c1_14 = arith.constant 1 : index
          %25 = affine.apply #map(%23)[%c0_9, %24]
          %26 = affine.apply #map(%24)[%c0_12, %c1_10]
          gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %25, %arg8 = %c1_14, %arg9 = %c1_14) threads(%arg4, %arg5, %arg6) in (%arg10 = %26, %arg11 = %c1_14, %arg12 = %c1_14) {
            %c0_15 = arith.constant 0 : index
            %c1_16 = arith.constant 1 : index
            %c0_17 = arith.constant 0 : index
            %c256_18 = arith.constant 256 : index
            %c32_19 = arith.constant 32 : index
            %c8_20 = arith.constant 8 : index
            %c1_21 = arith.constant 1 : index
            %c64_22 = arith.constant 64 : index
            %c4_23 = arith.constant 4 : index
            %27 = arith.cmpi eq, %dim, %c1_21 : index
            %28 = arith.select %27, %dim_0, %dim : index
            %29 = arith.cmpi eq, %dim_0, %28 : index
            %c0_24 = arith.constant 0 : index
            %30 = arith.cmpi eq, %dim, %28 : index
            %cst_25 = arith.constant 0.000000e+00 : f32
            %c2_26 = arith.constant 2 : index
            %31 = affine.apply #map1(%arg1)[%24, %c0_15]
            %32 = affine.apply #map1(%arg4)[%c1_16, %c0_17]
            %33 = arith.addi %32, %31 : index
            %true = arith.constant true
            %34 = arith.muli %32, %c1_16 : index
            %35 = arith.addi %34, %31 : index
            %36 = arith.cmpi ult, %35, %23 : index
            %37 = arith.andi %true, %36 : i1
            scf.if %37 {
              %38 = arith.remsi %33, %c256_18 : index
              %39 = arith.divsi %33, %c256_18 : index
              %40 = arith.divui %38, %c32_19 : index
              %41 = arith.remui %38, %c32_19 : index
              %42 = arith.muli %41, %c8_20 : index
              %43 = arith.addi %40, %42 : index
              %alloc_27 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
              %44 = arith.cmpi ult, %41, %c1_21 : index
              %45 = scf.if %44 -> (f32) {
                %51 = scf.for %arg13 = %c0_24 to %c64_22 step %c1_21 iter_args(%arg14 = %cst_25) -> (f32) {
                  %52 = arith.muli %39, %c8_20 : index
                  %53 = arith.addi %40, %52 : index
                  %54 = arith.muli %53, %c64_22 : index
                  %55 = arith.addi %arg13, %54 : index
                  %56 = arith.cmpi slt, %55, %8 : index
                  %57 = scf.if %56 -> (f32) {
                    %58 = arith.remui %55, %c4_23 : index
                    %59 = arith.divui %55, %c4_23 : index
                    %60 = arith.select %29, %59, %c0_24 : index
                    %61 = arith.muli %60, %c4_23 : index
                    %62 = arith.addi %61, %58 : index
                    %63 = arith.muli %dim_0, %c4_23 : index
                    %reinterpret_cast_29 = memref.reinterpret_cast %alloc to offset: [%c0_24], sizes: [%63], strides: [%c1_21] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                    %64 = memref.load %reinterpret_cast_29[%62] : memref<?xf32, #gpu.address_space<global>>
                    %65 = arith.select %30, %59, %c0_24 : index
                    %66 = arith.muli %65, %c4_23 : index
                    %67 = arith.addi %66, %58 : index
                    %68 = arith.muli %dim, %c4_23 : index
                    %reinterpret_cast_30 = memref.reinterpret_cast %alloc_2 to offset: [%c0_24], sizes: [%68], strides: [%c1_21] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
                    %69 = memref.load %reinterpret_cast_30[%67] : memref<?xf32, #gpu.address_space<global>>
                    %70 = arith.mulf %64, %69 : f32
                    %71 = arith.addf %arg14, %70 : f32
                    scf.yield %71 : f32
                  } else {
                    scf.yield %arg14 : f32
                  }
                  scf.yield %57 : f32
                }
                scf.yield %51 : f32
              } else {
                scf.yield %cst_25 : f32
              }
              %reinterpret_cast_28 = memref.reinterpret_cast %alloc_27 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
              memref.store %45, %reinterpret_cast_28[%43] : memref<256xf32, #gpu.address_space<workgroup>>
              gpu.barrier
              %46 = arith.cmpi slt, %40, %c4_23 : index
              scf.if %46 {
                %51 = arith.addi %43, %c4_23 : index
                %52 = memref.load %reinterpret_cast_28[%43] : memref<256xf32, #gpu.address_space<workgroup>>
                %53 = memref.load %reinterpret_cast_28[%51] : memref<256xf32, #gpu.address_space<workgroup>>
                %54 = arith.addf %52, %53 : f32
                memref.store %54, %reinterpret_cast_28[%43] : memref<256xf32, #gpu.address_space<workgroup>>
              }
              gpu.barrier
              %47 = arith.cmpi slt, %40, %c2_26 : index
              scf.if %47 {
                %51 = arith.addi %43, %c2_26 : index
                %52 = memref.load %reinterpret_cast_28[%43] : memref<256xf32, #gpu.address_space<workgroup>>
                %53 = memref.load %reinterpret_cast_28[%51] : memref<256xf32, #gpu.address_space<workgroup>>
                %54 = arith.addf %52, %53 : f32
                memref.store %54, %reinterpret_cast_28[%43] : memref<256xf32, #gpu.address_space<workgroup>>
              }
              gpu.barrier
              %48 = arith.cmpi slt, %40, %c1_21 : index
              scf.if %48 {
                %51 = arith.addi %43, %c1_21 : index
                %52 = memref.load %reinterpret_cast_28[%43] : memref<256xf32, #gpu.address_space<workgroup>>
                %53 = memref.load %reinterpret_cast_28[%51] : memref<256xf32, #gpu.address_space<workgroup>>
                %54 = arith.addf %52, %53 : f32
                memref.store %54, %reinterpret_cast_28[%43] : memref<256xf32, #gpu.address_space<workgroup>>
              }
              gpu.barrier
              %49 = arith.cmpi eq, %40, %c0_24 : index
              %50 = arith.andi %49, %44 : i1
              scf.if %50 {
                %51 = memref.load %reinterpret_cast_28[%43] : memref<256xf32, #gpu.address_space<workgroup>>
                %52 = memref.atomic_rmw addf %51, %alloc_3[%41] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
              }
            }
            gpu.terminator
          } {SCFToGPU_visited}
          "lmhlo.terminator"() : () -> ()
        }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "block_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
      }
    }
    memref.dealloc %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>
    memref.dealloc %alloc : memref<?x4xf32, #gpu.address_space<global>>
    %alloca = memref.alloca() : memref<0xindex>
    %12 = "disc_ral.dispatch"(%arg0, %5, %alloc_3, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1xf32, #gpu.address_space<global>>, memref<0xindex>) -> memref<f32, #gpu.address_space<global>>
    %reinterpret_cast_4 = memref.reinterpret_cast %12 to offset: [0], sizes: [], strides: [] : memref<f32, #gpu.address_space<global>> to memref<f32, #gpu.address_space<global>>
    memref.dealloc %alloc_3 : memref<1xf32, #gpu.address_space<global>>
    %alloc_5 = memref.alloc() : memref<f32>
    "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_4, %alloc_5) {backend_config = "", call_target_name = "d2h", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<f32, #gpu.address_space<global>>, memref<f32>) -> ()
    "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
    memref.dealloc %reinterpret_cast_4 : memref<f32, #gpu.address_space<global>>
    "disc_ral.dispatch"(%arg0, %c0, %alloc_5) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<f32>) -> ()
    return
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = true, sym_name = "C4", value = 4 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = true, sym_name = "C1", value = 1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S4", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = false, sym_name = "S5", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %c4 = arith.constant 4 : index
    %0 = "disc_shape.dim"() {name = @S2} : () -> index
    %1 = "disc_shape.dim"() {name = @S3} : () -> index
    "disc_shape.tie_product_equal"(%c4, %0, %1) {operand_segment_sizes = array<i32: 2, 1>} : (index, index, index) -> ()
    %2 = "disc_shape.dim"() {name = @S4} : () -> index
    %3 = "disc_shape.dim"() {name = @S5} : () -> index
    "disc_shape.tie_product_equal"(%c4, %2, %3) {operand_segment_sizes = array<i32: 2, 1>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After GpuKernelOutlining (gpu-kernel-outlining) //----- //
#map = affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>
#map1 = affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>
module attributes {gpu.container_module} {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
    %cst = arith.constant 0.000000e+00 : f32
    %c2 = arith.constant 2 : index
    %c256 = arith.constant 256 : index
    %c64 = arith.constant 64 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c512 = arith.constant 512 : index
    %0 = llvm.mlir.constant(0 : i32) : i32
    %c4_i32 = arith.constant 4 : i32
    %c4 = arith.constant 4 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
    %2 = "disc_ral.dispatch"(%arg0, %c1) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
    %dim = memref.dim %2, %c0 : memref<?x4xf32>
    %dim_0 = memref.dim %1, %c0 : memref<?x4xf32>
    %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
    %reinterpret_cast_1 = memref.reinterpret_cast %2 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
    %3 = arith.cmpi eq, %dim, %c1 : index
    %4 = arith.select %3, %dim_0, %dim : index
    %alloc = memref.alloc(%dim_0) {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
    %5 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
    "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast, %alloc) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
    "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
    %alloc_2 = memref.alloc(%dim) {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
    "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_1, %alloc_2) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
    "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
    %6 = arith.index_cast %4 : index to i32
    %7 = arith.muli %6, %c4_i32 : i32
    %8 = arith.index_cast %7 : i32 to index
    %9 = arith.cmpi eq, %dim_0, %4 : index
    %10 = arith.cmpi eq, %dim, %4 : index
    %11 = arith.andi %10, %9 : i1
    %alloc_3 = memref.alloc() : memref<1xf32, #gpu.address_space<global>>
    scf.if %11 {
      %13 = arith.cmpi slt, %8, %c1 : index
      scf.if %13 {
        "lmhlo.fusion"() ({
          %c0_6 = arith.constant 0 : index
          %c512_7 = arith.constant 512 : index
          %14 = arith.muli %c1, %c512_7 : index
          %c1_8 = arith.constant 1 : index
          %15 = affine.apply #map(%c1)[%c0, %14]
          %16 = affine.apply #map(%14)[%c0_6, %c1]
          gpu.launch_func  @main_kernel::@main_kernel blocks in (%15, %c1_8, %c1_8) threads in (%16, %c1_8, %c1_8) args(%14 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
          %17 = arith.cmpi eq, %8, %c0 : index
          %18 = arith.subi %8, %c1 : index
          %19 = arith.divui %18, %c32 : index
          %20 = arith.addi %19, %c1 : index
          %21 = arith.select %17, %c0, %20 : index
          %c0_9 = arith.constant 0 : index
          %c1_10 = arith.constant 1 : index
          %c1_11 = arith.constant 1 : index
          %22 = arith.muli %c1_11, %21 : index
          %23 = arith.muli %22, %c512 : index
          %c0_12 = arith.constant 0 : index
          %c512_13 = arith.constant 512 : index
          %24 = arith.muli %c1_10, %c512_13 : index
          %c1_14 = arith.constant 1 : index
          %25 = affine.apply #map(%23)[%c0_9, %24]
          %26 = affine.apply #map(%24)[%c0_12, %c1_10]
          gpu.launch_func  @main_kernel_0::@main_kernel blocks in (%25, %c1_14, %c1_14) threads in (%26, %c1_14, %c1_14) args(%24 : index, %23 : index, %8 : index, %4 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
          "lmhlo.terminator"() : () -> ()
        }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXthread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
      } else {
        "lmhlo.fusion"() ({
          %c0_6 = arith.constant 0 : index
          %c256_7 = arith.constant 256 : index
          %14 = arith.muli %c1, %c256_7 : index
          %c1_8 = arith.constant 1 : index
          %15 = affine.apply #map(%c1)[%c0, %14]
          %16 = affine.apply #map(%14)[%c0_6, %c1]
          gpu.launch_func  @main_kernel_1::@main_kernel blocks in (%15, %c1_8, %c1_8) threads in (%16, %c1_8, %c1_8) args(%14 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
          %17 = arith.cmpi eq, %8, %c0 : index
          %18 = arith.subi %8, %c1 : index
          %19 = arith.divui %18, %c512 : index
          %20 = arith.addi %19, %c1 : index
          %21 = arith.select %17, %c0, %20 : index
          %c0_9 = arith.constant 0 : index
          %c1_10 = arith.constant 1 : index
          %c1_11 = arith.constant 1 : index
          %22 = arith.muli %c1_11, %21 : index
          %23 = arith.muli %22, %c256 : index
          %c0_12 = arith.constant 0 : index
          %c256_13 = arith.constant 256 : index
          %24 = arith.muli %c1_10, %c256_13 : index
          %c1_14 = arith.constant 1 : index
          %25 = affine.apply #map(%23)[%c0_9, %24]
          %26 = affine.apply #map(%24)[%c0_12, %c1_10]
          gpu.launch_func  @main_kernel_2::@main_kernel blocks in (%25, %c1_14, %c1_14) threads in (%26, %c1_14, %c1_14) args(%24 : index, %23 : index, %8 : index, %4 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
          "lmhlo.terminator"() : () -> ()
        }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXblock_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
      }
    } else {
      %13 = arith.cmpi slt, %8, %c1 : index
      scf.if %13 {
        "lmhlo.fusion"() ({
          %c0_6 = arith.constant 0 : index
          %c512_7 = arith.constant 512 : index
          %14 = arith.muli %c1, %c512_7 : index
          %c1_8 = arith.constant 1 : index
          %15 = affine.apply #map(%c1)[%c0, %14]
          %16 = affine.apply #map(%14)[%c0_6, %c1]
          gpu.launch_func  @main_kernel_3::@main_kernel blocks in (%15, %c1_8, %c1_8) threads in (%16, %c1_8, %c1_8) args(%14 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
          %17 = arith.cmpi eq, %8, %c0 : index
          %18 = arith.subi %8, %c1 : index
          %19 = arith.divui %18, %c32 : index
          %20 = arith.addi %19, %c1 : index
          %21 = arith.select %17, %c0, %20 : index
          %c0_9 = arith.constant 0 : index
          %c1_10 = arith.constant 1 : index
          %c1_11 = arith.constant 1 : index
          %22 = arith.muli %c1_11, %21 : index
          %23 = arith.muli %22, %c512 : index
          %c0_12 = arith.constant 0 : index
          %c512_13 = arith.constant 512 : index
          %24 = arith.muli %c1_10, %c512_13 : index
          %c1_14 = arith.constant 1 : index
          %25 = affine.apply #map(%23)[%c0_9, %24]
          %26 = affine.apply #map(%24)[%c0_12, %c1_10]
          gpu.launch_func  @main_kernel_4::@main_kernel blocks in (%25, %c1_14, %c1_14) threads in (%26, %c1_14, %c1_14) args(%dim : index, %dim_0 : index, %24 : index, %23 : index, %8 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
          "lmhlo.terminator"() : () -> ()
        }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "thread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
      } else {
        "lmhlo.fusion"() ({
          %c0_6 = arith.constant 0 : index
          %c256_7 = arith.constant 256 : index
          %14 = arith.muli %c1, %c256_7 : index
          %c1_8 = arith.constant 1 : index
          %15 = affine.apply #map(%c1)[%c0, %14]
          %16 = affine.apply #map(%14)[%c0_6, %c1]
          gpu.launch_func  @main_kernel_5::@main_kernel blocks in (%15, %c1_8, %c1_8) threads in (%16, %c1_8, %c1_8) args(%14 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
          %17 = arith.cmpi eq, %8, %c0 : index
          %18 = arith.subi %8, %c1 : index
          %19 = arith.divui %18, %c512 : index
          %20 = arith.addi %19, %c1 : index
          %21 = arith.select %17, %c0, %20 : index
          %c0_9 = arith.constant 0 : index
          %c1_10 = arith.constant 1 : index
          %c1_11 = arith.constant 1 : index
          %22 = arith.muli %c1_11, %21 : index
          %23 = arith.muli %22, %c256 : index
          %c0_12 = arith.constant 0 : index
          %c256_13 = arith.constant 256 : index
          %24 = arith.muli %c1_10, %c256_13 : index
          %c1_14 = arith.constant 1 : index
          %25 = affine.apply #map(%23)[%c0_9, %24]
          %26 = affine.apply #map(%24)[%c0_12, %c1_10]
          gpu.launch_func  @main_kernel_6::@main_kernel blocks in (%25, %c1_14, %c1_14) threads in (%26, %c1_14, %c1_14) args(%dim : index, %dim_0 : index, %24 : index, %23 : index, %8 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
          "lmhlo.terminator"() : () -> ()
        }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "block_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
      }
    }
    memref.dealloc %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>
    memref.dealloc %alloc : memref<?x4xf32, #gpu.address_space<global>>
    %alloca = memref.alloca() : memref<0xindex>
    %12 = "disc_ral.dispatch"(%arg0, %5, %alloc_3, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1xf32, #gpu.address_space<global>>, memref<0xindex>) -> memref<f32, #gpu.address_space<global>>
    %reinterpret_cast_4 = memref.reinterpret_cast %12 to offset: [0], sizes: [], strides: [] : memref<f32, #gpu.address_space<global>> to memref<f32, #gpu.address_space<global>>
    memref.dealloc %alloc_3 : memref<1xf32, #gpu.address_space<global>>
    %alloc_5 = memref.alloc() : memref<f32>
    "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_4, %alloc_5) {backend_config = "", call_target_name = "d2h", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<f32, #gpu.address_space<global>>, memref<f32>) -> ()
    "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
    memref.dealloc %reinterpret_cast_4 : memref<f32, #gpu.address_space<global>>
    "disc_ral.dispatch"(%arg0, %c0, %alloc_5) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<f32>) -> ()
    return
  }
  gpu.module @main_kernel {
    gpu.func @main_kernel(%arg0: index, %arg1: memref<1xf32, #gpu.address_space<global>>) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %cst = arith.constant 0.000000e+00 : f32
      %12 = affine.apply #map1(%0)[%arg0, %c0]
      %13 = affine.apply #map1(%3)[%c1, %c0_0]
      %14 = arith.addi %13, %12 : index
      %true = arith.constant true
      %15 = arith.muli %13, %c1 : index
      %16 = arith.addi %15, %12 : index
      %17 = arith.cmpi ult, %16, %c1 : index
      %18 = arith.andi %true, %17 : i1
      scf.if %18 {
        %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
        memref.store %cst, %reinterpret_cast[%14] : memref<1xf32, #gpu.address_space<global>>
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_0 {
    gpu.func @main_kernel(%arg0: index, %arg1: index, %arg2: index, %arg3: index, %arg4: memref<?x4xf32, #gpu.address_space<global>>, %arg5: memref<?x4xf32, #gpu.address_space<global>>, %arg6: memref<1xf32, #gpu.address_space<global>>) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %c512 = arith.constant 512 : index
      %c1_1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %c4 = arith.constant 4 : index
      %c0_2 = arith.constant 0 : index
      %cst = arith.constant 0.000000e+00 : f32
      %12 = affine.apply #map1(%0)[%arg0, %c0]
      %13 = affine.apply #map1(%3)[%c1, %c0_0]
      %14 = arith.addi %13, %12 : index
      %true = arith.constant true
      %15 = arith.muli %13, %c1 : index
      %16 = arith.addi %15, %12 : index
      %17 = arith.cmpi ult, %16, %arg1 : index
      %18 = arith.andi %true, %17 : i1
      scf.if %18 {
        %19 = arith.remsi %14, %c512 : index
        %20 = arith.divsi %14, %c512 : index
        %21 = arith.cmpi ult, %19, %c1_1 : index
        scf.if %21 {
          %22 = scf.for %arg7 = %c0_2 to %c32 step %c1_1 iter_args(%arg8 = %cst) -> (f32) {
            %24 = arith.muli %20, %c32 : index
            %25 = arith.addi %24, %arg7 : index
            %26 = arith.cmpi slt, %25, %arg2 : index
            %27 = scf.if %26 -> (f32) {
              %28 = arith.muli %arg3, %c4 : index
              %reinterpret_cast = memref.reinterpret_cast %arg4 to offset: [%c0_2], sizes: [%28], strides: [%c1_1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
              %29 = memref.load %reinterpret_cast[%25] : memref<?xf32, #gpu.address_space<global>>
              %reinterpret_cast_3 = memref.reinterpret_cast %arg5 to offset: [%c0_2], sizes: [%28], strides: [%c1_1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
              %30 = memref.load %reinterpret_cast_3[%25] : memref<?xf32, #gpu.address_space<global>>
              %31 = arith.mulf %29, %30 : f32
              %32 = arith.addf %arg8, %31 : f32
              scf.yield %32 : f32
            } else {
              scf.yield %arg8 : f32
            }
            scf.yield %27 : f32
          }
          %23 = memref.atomic_rmw addf %22, %arg6[%19] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
        }
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_1 {
    gpu.func @main_kernel(%arg0: index, %arg1: memref<1xf32, #gpu.address_space<global>>) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %cst = arith.constant 0.000000e+00 : f32
      %12 = affine.apply #map1(%0)[%arg0, %c0]
      %13 = affine.apply #map1(%3)[%c1, %c0_0]
      %14 = arith.addi %13, %12 : index
      %true = arith.constant true
      %15 = arith.muli %13, %c1 : index
      %16 = arith.addi %15, %12 : index
      %17 = arith.cmpi ult, %16, %c1 : index
      %18 = arith.andi %true, %17 : i1
      scf.if %18 {
        %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
        memref.store %cst, %reinterpret_cast[%14] : memref<1xf32, #gpu.address_space<global>>
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_2 {
    gpu.func @main_kernel(%arg0: index, %arg1: index, %arg2: index, %arg3: index, %arg4: memref<?x4xf32, #gpu.address_space<global>>, %arg5: memref<?x4xf32, #gpu.address_space<global>>, %arg6: memref<1xf32, #gpu.address_space<global>>) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %c256 = arith.constant 256 : index
      %c32 = arith.constant 32 : index
      %c8 = arith.constant 8 : index
      %c1_1 = arith.constant 1 : index
      %c64 = arith.constant 64 : index
      %c4 = arith.constant 4 : index
      %c0_2 = arith.constant 0 : index
      %cst = arith.constant 0.000000e+00 : f32
      %c2 = arith.constant 2 : index
      %12 = affine.apply #map1(%0)[%arg0, %c0]
      %13 = affine.apply #map1(%3)[%c1, %c0_0]
      %14 = arith.addi %13, %12 : index
      %true = arith.constant true
      %15 = arith.muli %13, %c1 : index
      %16 = arith.addi %15, %12 : index
      %17 = arith.cmpi ult, %16, %arg1 : index
      %18 = arith.andi %true, %17 : i1
      scf.if %18 {
        %19 = arith.remsi %14, %c256 : index
        %20 = arith.divsi %14, %c256 : index
        %21 = arith.divui %19, %c32 : index
        %22 = arith.remui %19, %c32 : index
        %23 = arith.muli %22, %c8 : index
        %24 = arith.addi %21, %23 : index
        %alloc = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
        %25 = arith.cmpi ult, %22, %c1_1 : index
        %26 = scf.if %25 -> (f32) {
          %32 = scf.for %arg7 = %c0_2 to %c64 step %c1_1 iter_args(%arg8 = %cst) -> (f32) {
            %33 = arith.muli %20, %c8 : index
            %34 = arith.addi %21, %33 : index
            %35 = arith.muli %34, %c64 : index
            %36 = arith.addi %arg7, %35 : index
            %37 = arith.cmpi slt, %36, %arg2 : index
            %38 = scf.if %37 -> (f32) {
              %39 = arith.muli %arg3, %c4 : index
              %reinterpret_cast_3 = memref.reinterpret_cast %arg4 to offset: [%c0_2], sizes: [%39], strides: [%c1_1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
              %40 = memref.load %reinterpret_cast_3[%36] : memref<?xf32, #gpu.address_space<global>>
              %reinterpret_cast_4 = memref.reinterpret_cast %arg5 to offset: [%c0_2], sizes: [%39], strides: [%c1_1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
              %41 = memref.load %reinterpret_cast_4[%36] : memref<?xf32, #gpu.address_space<global>>
              %42 = arith.mulf %40, %41 : f32
              %43 = arith.addf %arg8, %42 : f32
              scf.yield %43 : f32
            } else {
              scf.yield %arg8 : f32
            }
            scf.yield %38 : f32
          }
          scf.yield %32 : f32
        } else {
          scf.yield %cst : f32
        }
        %reinterpret_cast = memref.reinterpret_cast %alloc to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        memref.store %26, %reinterpret_cast[%24] : memref<256xf32, #gpu.address_space<workgroup>>
        gpu.barrier
        %27 = arith.cmpi slt, %21, %c4 : index
        scf.if %27 {
          %32 = arith.addi %24, %c4 : index
          %33 = memref.load %reinterpret_cast[%24] : memref<256xf32, #gpu.address_space<workgroup>>
          %34 = memref.load %reinterpret_cast[%32] : memref<256xf32, #gpu.address_space<workgroup>>
          %35 = arith.addf %33, %34 : f32
          memref.store %35, %reinterpret_cast[%24] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %28 = arith.cmpi slt, %21, %c2 : index
        scf.if %28 {
          %32 = arith.addi %24, %c2 : index
          %33 = memref.load %reinterpret_cast[%24] : memref<256xf32, #gpu.address_space<workgroup>>
          %34 = memref.load %reinterpret_cast[%32] : memref<256xf32, #gpu.address_space<workgroup>>
          %35 = arith.addf %33, %34 : f32
          memref.store %35, %reinterpret_cast[%24] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %29 = arith.cmpi slt, %21, %c1_1 : index
        scf.if %29 {
          %32 = arith.addi %24, %c1_1 : index
          %33 = memref.load %reinterpret_cast[%24] : memref<256xf32, #gpu.address_space<workgroup>>
          %34 = memref.load %reinterpret_cast[%32] : memref<256xf32, #gpu.address_space<workgroup>>
          %35 = arith.addf %33, %34 : f32
          memref.store %35, %reinterpret_cast[%24] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %30 = arith.cmpi eq, %21, %c0_2 : index
        %31 = arith.andi %30, %25 : i1
        scf.if %31 {
          %32 = memref.load %reinterpret_cast[%24] : memref<256xf32, #gpu.address_space<workgroup>>
          %33 = memref.atomic_rmw addf %32, %arg6[%22] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
        }
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_3 {
    gpu.func @main_kernel(%arg0: index, %arg1: memref<1xf32, #gpu.address_space<global>>) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %cst = arith.constant 0.000000e+00 : f32
      %12 = affine.apply #map1(%0)[%arg0, %c0]
      %13 = affine.apply #map1(%3)[%c1, %c0_0]
      %14 = arith.addi %13, %12 : index
      %true = arith.constant true
      %15 = arith.muli %13, %c1 : index
      %16 = arith.addi %15, %12 : index
      %17 = arith.cmpi ult, %16, %c1 : index
      %18 = arith.andi %true, %17 : i1
      scf.if %18 {
        %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
        memref.store %cst, %reinterpret_cast[%14] : memref<1xf32, #gpu.address_space<global>>
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_4 {
    gpu.func @main_kernel(%arg0: index, %arg1: index, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?x4xf32, #gpu.address_space<global>>, %arg6: memref<?x4xf32, #gpu.address_space<global>>, %arg7: memref<1xf32, #gpu.address_space<global>>) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %c512 = arith.constant 512 : index
      %c1_1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %c4 = arith.constant 4 : index
      %12 = arith.cmpi eq, %arg0, %c1_1 : index
      %13 = arith.select %12, %arg1, %arg0 : index
      %14 = arith.cmpi eq, %arg1, %13 : index
      %c0_2 = arith.constant 0 : index
      %15 = arith.cmpi eq, %arg0, %13 : index
      %cst = arith.constant 0.000000e+00 : f32
      %16 = affine.apply #map1(%0)[%arg2, %c0]
      %17 = affine.apply #map1(%3)[%c1, %c0_0]
      %18 = arith.addi %17, %16 : index
      %true = arith.constant true
      %19 = arith.muli %17, %c1 : index
      %20 = arith.addi %19, %16 : index
      %21 = arith.cmpi ult, %20, %arg3 : index
      %22 = arith.andi %true, %21 : i1
      scf.if %22 {
        %23 = arith.remsi %18, %c512 : index
        %24 = arith.divsi %18, %c512 : index
        %25 = arith.cmpi ult, %23, %c1_1 : index
        scf.if %25 {
          %26 = scf.for %arg8 = %c0_2 to %c32 step %c1_1 iter_args(%arg9 = %cst) -> (f32) {
            %28 = arith.muli %24, %c32 : index
            %29 = arith.addi %28, %arg8 : index
            %30 = arith.cmpi slt, %29, %arg4 : index
            %31 = scf.if %30 -> (f32) {
              %32 = arith.remui %29, %c4 : index
              %33 = arith.divui %29, %c4 : index
              %34 = arith.select %14, %33, %c0_2 : index
              %35 = arith.muli %34, %c4 : index
              %36 = arith.addi %35, %32 : index
              %37 = arith.muli %arg1, %c4 : index
              %reinterpret_cast = memref.reinterpret_cast %arg5 to offset: [%c0_2], sizes: [%37], strides: [%c1_1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
              %38 = memref.load %reinterpret_cast[%36] : memref<?xf32, #gpu.address_space<global>>
              %39 = arith.select %15, %33, %c0_2 : index
              %40 = arith.muli %39, %c4 : index
              %41 = arith.addi %40, %32 : index
              %42 = arith.muli %arg0, %c4 : index
              %reinterpret_cast_3 = memref.reinterpret_cast %arg6 to offset: [%c0_2], sizes: [%42], strides: [%c1_1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
              %43 = memref.load %reinterpret_cast_3[%41] : memref<?xf32, #gpu.address_space<global>>
              %44 = arith.mulf %38, %43 : f32
              %45 = arith.addf %arg9, %44 : f32
              scf.yield %45 : f32
            } else {
              scf.yield %arg9 : f32
            }
            scf.yield %31 : f32
          }
          %27 = memref.atomic_rmw addf %26, %arg7[%23] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
        }
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_5 {
    gpu.func @main_kernel(%arg0: index, %arg1: memref<1xf32, #gpu.address_space<global>>) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %cst = arith.constant 0.000000e+00 : f32
      %12 = affine.apply #map1(%0)[%arg0, %c0]
      %13 = affine.apply #map1(%3)[%c1, %c0_0]
      %14 = arith.addi %13, %12 : index
      %true = arith.constant true
      %15 = arith.muli %13, %c1 : index
      %16 = arith.addi %15, %12 : index
      %17 = arith.cmpi ult, %16, %c1 : index
      %18 = arith.andi %true, %17 : i1
      scf.if %18 {
        %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
        memref.store %cst, %reinterpret_cast[%14] : memref<1xf32, #gpu.address_space<global>>
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_6 {
    gpu.func @main_kernel(%arg0: index, %arg1: index, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?x4xf32, #gpu.address_space<global>>, %arg6: memref<?x4xf32, #gpu.address_space<global>>, %arg7: memref<1xf32, #gpu.address_space<global>>) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %c256 = arith.constant 256 : index
      %c32 = arith.constant 32 : index
      %c8 = arith.constant 8 : index
      %c1_1 = arith.constant 1 : index
      %c64 = arith.constant 64 : index
      %c4 = arith.constant 4 : index
      %12 = arith.cmpi eq, %arg0, %c1_1 : index
      %13 = arith.select %12, %arg1, %arg0 : index
      %14 = arith.cmpi eq, %arg1, %13 : index
      %c0_2 = arith.constant 0 : index
      %15 = arith.cmpi eq, %arg0, %13 : index
      %cst = arith.constant 0.000000e+00 : f32
      %c2 = arith.constant 2 : index
      %16 = affine.apply #map1(%0)[%arg2, %c0]
      %17 = affine.apply #map1(%3)[%c1, %c0_0]
      %18 = arith.addi %17, %16 : index
      %true = arith.constant true
      %19 = arith.muli %17, %c1 : index
      %20 = arith.addi %19, %16 : index
      %21 = arith.cmpi ult, %20, %arg3 : index
      %22 = arith.andi %true, %21 : i1
      scf.if %22 {
        %23 = arith.remsi %18, %c256 : index
        %24 = arith.divsi %18, %c256 : index
        %25 = arith.divui %23, %c32 : index
        %26 = arith.remui %23, %c32 : index
        %27 = arith.muli %26, %c8 : index
        %28 = arith.addi %25, %27 : index
        %alloc = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
        %29 = arith.cmpi ult, %26, %c1_1 : index
        %30 = scf.if %29 -> (f32) {
          %36 = scf.for %arg8 = %c0_2 to %c64 step %c1_1 iter_args(%arg9 = %cst) -> (f32) {
            %37 = arith.muli %24, %c8 : index
            %38 = arith.addi %25, %37 : index
            %39 = arith.muli %38, %c64 : index
            %40 = arith.addi %arg8, %39 : index
            %41 = arith.cmpi slt, %40, %arg4 : index
            %42 = scf.if %41 -> (f32) {
              %43 = arith.remui %40, %c4 : index
              %44 = arith.divui %40, %c4 : index
              %45 = arith.select %14, %44, %c0_2 : index
              %46 = arith.muli %45, %c4 : index
              %47 = arith.addi %46, %43 : index
              %48 = arith.muli %arg1, %c4 : index
              %reinterpret_cast_3 = memref.reinterpret_cast %arg5 to offset: [%c0_2], sizes: [%48], strides: [%c1_1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
              %49 = memref.load %reinterpret_cast_3[%47] : memref<?xf32, #gpu.address_space<global>>
              %50 = arith.select %15, %44, %c0_2 : index
              %51 = arith.muli %50, %c4 : index
              %52 = arith.addi %51, %43 : index
              %53 = arith.muli %arg0, %c4 : index
              %reinterpret_cast_4 = memref.reinterpret_cast %arg6 to offset: [%c0_2], sizes: [%53], strides: [%c1_1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
              %54 = memref.load %reinterpret_cast_4[%52] : memref<?xf32, #gpu.address_space<global>>
              %55 = arith.mulf %49, %54 : f32
              %56 = arith.addf %arg9, %55 : f32
              scf.yield %56 : f32
            } else {
              scf.yield %arg9 : f32
            }
            scf.yield %42 : f32
          }
          scf.yield %36 : f32
        } else {
          scf.yield %cst : f32
        }
        %reinterpret_cast = memref.reinterpret_cast %alloc to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        memref.store %30, %reinterpret_cast[%28] : memref<256xf32, #gpu.address_space<workgroup>>
        gpu.barrier
        %31 = arith.cmpi slt, %25, %c4 : index
        scf.if %31 {
          %36 = arith.addi %28, %c4 : index
          %37 = memref.load %reinterpret_cast[%28] : memref<256xf32, #gpu.address_space<workgroup>>
          %38 = memref.load %reinterpret_cast[%36] : memref<256xf32, #gpu.address_space<workgroup>>
          %39 = arith.addf %37, %38 : f32
          memref.store %39, %reinterpret_cast[%28] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %32 = arith.cmpi slt, %25, %c2 : index
        scf.if %32 {
          %36 = arith.addi %28, %c2 : index
          %37 = memref.load %reinterpret_cast[%28] : memref<256xf32, #gpu.address_space<workgroup>>
          %38 = memref.load %reinterpret_cast[%36] : memref<256xf32, #gpu.address_space<workgroup>>
          %39 = arith.addf %37, %38 : f32
          memref.store %39, %reinterpret_cast[%28] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %33 = arith.cmpi slt, %25, %c1_1 : index
        scf.if %33 {
          %36 = arith.addi %28, %c1_1 : index
          %37 = memref.load %reinterpret_cast[%28] : memref<256xf32, #gpu.address_space<workgroup>>
          %38 = memref.load %reinterpret_cast[%36] : memref<256xf32, #gpu.address_space<workgroup>>
          %39 = arith.addf %37, %38 : f32
          memref.store %39, %reinterpret_cast[%28] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %34 = arith.cmpi eq, %25, %c0_2 : index
        %35 = arith.andi %34, %29 : i1
        scf.if %35 {
          %36 = memref.load %reinterpret_cast[%28] : memref<256xf32, #gpu.address_space<workgroup>>
          %37 = memref.atomic_rmw addf %36, %arg7[%26] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
        }
      }
      gpu.return
    }
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = true, sym_name = "C4", value = 4 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = true, sym_name = "C1", value = 1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S4", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = false, sym_name = "S5", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %c4 = arith.constant 4 : index
    %0 = "disc_shape.dim"() {name = @S2} : () -> index
    %1 = "disc_shape.dim"() {name = @S3} : () -> index
    "disc_shape.tie_product_equal"(%c4, %0, %1) {operand_segment_sizes = array<i32: 2, 1>} : (index, index, index) -> ()
    %2 = "disc_shape.dim"() {name = @S4} : () -> index
    %3 = "disc_shape.dim"() {name = @S5} : () -> index
    "disc_shape.tie_product_equal"(%c4, %2, %3) {operand_segment_sizes = array<i32: 2, 1>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After AssignKernelNamePass (disc-assign-kernel-name) //----- //
#map = affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>
#map1 = affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>
module attributes {gpu.container_module} {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
    %cst = arith.constant 0.000000e+00 : f32
    %c2 = arith.constant 2 : index
    %c256 = arith.constant 256 : index
    %c64 = arith.constant 64 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c512 = arith.constant 512 : index
    %0 = llvm.mlir.constant(0 : i32) : i32
    %c4_i32 = arith.constant 4 : i32
    %c4 = arith.constant 4 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
    %2 = "disc_ral.dispatch"(%arg0, %c1) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
    %dim = memref.dim %2, %c0 : memref<?x4xf32>
    %dim_0 = memref.dim %1, %c0 : memref<?x4xf32>
    %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
    %reinterpret_cast_1 = memref.reinterpret_cast %2 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
    %3 = arith.cmpi eq, %dim, %c1 : index
    %4 = arith.select %3, %dim_0, %dim : index
    %alloc = memref.alloc(%dim_0) {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
    %5 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
    "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast, %alloc) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
    "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
    %alloc_2 = memref.alloc(%dim) {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
    "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_1, %alloc_2) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
    "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
    %6 = arith.index_cast %4 : index to i32
    %7 = arith.muli %6, %c4_i32 : i32
    %8 = arith.index_cast %7 : i32 to index
    %9 = arith.cmpi eq, %dim_0, %4 : index
    %10 = arith.cmpi eq, %dim, %4 : index
    %11 = arith.andi %10, %9 : i1
    %alloc_3 = memref.alloc() : memref<1xf32, #gpu.address_space<global>>
    scf.if %11 {
      %13 = arith.cmpi slt, %8, %c1 : index
      scf.if %13 {
        "lmhlo.fusion"() ({
          %c0_6 = arith.constant 0 : index
          %c512_7 = arith.constant 512 : index
          %14 = arith.muli %c1, %c512_7 : index
          %c1_8 = arith.constant 1 : index
          %15 = affine.apply #map(%c1)[%c0, %14]
          %16 = affine.apply #map(%14)[%c0_6, %c1]
          gpu.launch_func  @main_kernel::@main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32 blocks in (%15, %c1_8, %c1_8) threads in (%16, %c1_8, %c1_8) args(%14 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
          %17 = arith.cmpi eq, %8, %c0 : index
          %18 = arith.subi %8, %c1 : index
          %19 = arith.divui %18, %c32 : index
          %20 = arith.addi %19, %c1 : index
          %21 = arith.select %17, %c0, %20 : index
          %c0_9 = arith.constant 0 : index
          %c1_10 = arith.constant 1 : index
          %c1_11 = arith.constant 1 : index
          %22 = arith.muli %c1_11, %21 : index
          %23 = arith.muli %22, %c512 : index
          %c0_12 = arith.constant 0 : index
          %c512_13 = arith.constant 512 : index
          %24 = arith.muli %c1_10, %c512_13 : index
          %c1_14 = arith.constant 1 : index
          %25 = affine.apply #map(%23)[%c0_9, %24]
          %26 = affine.apply #map(%24)[%c0_12, %c1_10]
          gpu.launch_func  @main_kernel_0::@main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_1 blocks in (%25, %c1_14, %c1_14) threads in (%26, %c1_14, %c1_14) args(%24 : index, %23 : index, %8 : index, %4 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
          "lmhlo.terminator"() : () -> ()
        }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXthread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
      } else {
        "lmhlo.fusion"() ({
          %c0_6 = arith.constant 0 : index
          %c256_7 = arith.constant 256 : index
          %14 = arith.muli %c1, %c256_7 : index
          %c1_8 = arith.constant 1 : index
          %15 = affine.apply #map(%c1)[%c0, %14]
          %16 = affine.apply #map(%14)[%c0_6, %c1]
          gpu.launch_func  @main_kernel_1::@main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64 blocks in (%15, %c1_8, %c1_8) threads in (%16, %c1_8, %c1_8) args(%14 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
          %17 = arith.cmpi eq, %8, %c0 : index
          %18 = arith.subi %8, %c1 : index
          %19 = arith.divui %18, %c512 : index
          %20 = arith.addi %19, %c1 : index
          %21 = arith.select %17, %c0, %20 : index
          %c0_9 = arith.constant 0 : index
          %c1_10 = arith.constant 1 : index
          %c1_11 = arith.constant 1 : index
          %22 = arith.muli %c1_11, %21 : index
          %23 = arith.muli %22, %c256 : index
          %c0_12 = arith.constant 0 : index
          %c256_13 = arith.constant 256 : index
          %24 = arith.muli %c1_10, %c256_13 : index
          %c1_14 = arith.constant 1 : index
          %25 = affine.apply #map(%23)[%c0_9, %24]
          %26 = affine.apply #map(%24)[%c0_12, %c1_10]
          gpu.launch_func  @main_kernel_2::@main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1 blocks in (%25, %c1_14, %c1_14) threads in (%26, %c1_14, %c1_14) args(%24 : index, %23 : index, %8 : index, %4 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
          "lmhlo.terminator"() : () -> ()
        }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "no_ibXblock_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
      }
    } else {
      %13 = arith.cmpi slt, %8, %c1 : index
      scf.if %13 {
        "lmhlo.fusion"() ({
          %c0_6 = arith.constant 0 : index
          %c512_7 = arith.constant 512 : index
          %14 = arith.muli %c1, %c512_7 : index
          %c1_8 = arith.constant 1 : index
          %15 = affine.apply #map(%c1)[%c0, %14]
          %16 = affine.apply #map(%14)[%c0_6, %c1]
          gpu.launch_func  @main_kernel_3::@main_kColReduction_reduce__6_1_0___thread_tile_h32 blocks in (%15, %c1_8, %c1_8) threads in (%16, %c1_8, %c1_8) args(%14 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
          %17 = arith.cmpi eq, %8, %c0 : index
          %18 = arith.subi %8, %c1 : index
          %19 = arith.divui %18, %c32 : index
          %20 = arith.addi %19, %c1 : index
          %21 = arith.select %17, %c0, %20 : index
          %c0_9 = arith.constant 0 : index
          %c1_10 = arith.constant 1 : index
          %c1_11 = arith.constant 1 : index
          %22 = arith.muli %c1_11, %21 : index
          %23 = arith.muli %22, %c512 : index
          %c0_12 = arith.constant 0 : index
          %c512_13 = arith.constant 512 : index
          %24 = arith.muli %c1_10, %c512_13 : index
          %c1_14 = arith.constant 1 : index
          %25 = affine.apply #map(%23)[%c0_9, %24]
          %26 = affine.apply #map(%24)[%c0_12, %c1_10]
          gpu.launch_func  @main_kernel_4::@main_kColReduction_reduce__6_1_0___thread_tile_h32_1 blocks in (%25, %c1_14, %c1_14) threads in (%26, %c1_14, %c1_14) args(%dim : index, %dim_0 : index, %24 : index, %23 : index, %8 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
          "lmhlo.terminator"() : () -> ()
        }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "thread_tile_h32", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_cta_size_hint = 512 : i32} : () -> ()
      } else {
        "lmhlo.fusion"() ({
          %c0_6 = arith.constant 0 : index
          %c256_7 = arith.constant 256 : index
          %14 = arith.muli %c1, %c256_7 : index
          %c1_8 = arith.constant 1 : index
          %15 = affine.apply #map(%c1)[%c0, %14]
          %16 = affine.apply #map(%14)[%c0_6, %c1]
          gpu.launch_func  @main_kernel_5::@main_kColReduction_reduce__6_1_0___block_tile_h64 blocks in (%15, %c1_8, %c1_8) threads in (%16, %c1_8, %c1_8) args(%14 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
          %17 = arith.cmpi eq, %8, %c0 : index
          %18 = arith.subi %8, %c1 : index
          %19 = arith.divui %18, %c512 : index
          %20 = arith.addi %19, %c1 : index
          %21 = arith.select %17, %c0, %20 : index
          %c0_9 = arith.constant 0 : index
          %c1_10 = arith.constant 1 : index
          %c1_11 = arith.constant 1 : index
          %22 = arith.muli %c1_11, %21 : index
          %23 = arith.muli %22, %c256 : index
          %c0_12 = arith.constant 0 : index
          %c256_13 = arith.constant 256 : index
          %24 = arith.muli %c1_10, %c256_13 : index
          %c1_14 = arith.constant 1 : index
          %25 = affine.apply #map(%23)[%c0_9, %24]
          %26 = affine.apply #map(%24)[%c0_12, %c1_10]
          gpu.launch_func  @main_kernel_6::@main_kColReduction_reduce__6_1_0___block_tile_h64_1 blocks in (%25, %c1_14, %c1_14) threads in (%26, %c1_14, %c1_14) args(%dim : index, %dim_0 : index, %24 : index, %23 : index, %8 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
          "lmhlo.terminator"() : () -> ()
        }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__6_1_0", disc.fusion.tag = "block_tile_h64", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_cta_size_hint = 256 : i32} : () -> ()
      }
    }
    memref.dealloc %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>
    memref.dealloc %alloc : memref<?x4xf32, #gpu.address_space<global>>
    %alloca = memref.alloca() : memref<0xindex>
    %12 = "disc_ral.dispatch"(%arg0, %5, %alloc_3, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1xf32, #gpu.address_space<global>>, memref<0xindex>) -> memref<f32, #gpu.address_space<global>>
    %reinterpret_cast_4 = memref.reinterpret_cast %12 to offset: [0], sizes: [], strides: [] : memref<f32, #gpu.address_space<global>> to memref<f32, #gpu.address_space<global>>
    memref.dealloc %alloc_3 : memref<1xf32, #gpu.address_space<global>>
    %alloc_5 = memref.alloc() : memref<f32>
    "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_4, %alloc_5) {backend_config = "", call_target_name = "d2h", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<f32, #gpu.address_space<global>>, memref<f32>) -> ()
    "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
    memref.dealloc %reinterpret_cast_4 : memref<f32, #gpu.address_space<global>>
    "disc_ral.dispatch"(%arg0, %c0, %alloc_5) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<f32>) -> ()
    return
  }
  gpu.module @main_kernel {
    gpu.func @main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32(%arg0: index, %arg1: memref<1xf32, #gpu.address_space<global>>) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %cst = arith.constant 0.000000e+00 : f32
      %12 = affine.apply #map1(%0)[%arg0, %c0]
      %13 = affine.apply #map1(%3)[%c1, %c0_0]
      %14 = arith.addi %13, %12 : index
      %true = arith.constant true
      %15 = arith.muli %13, %c1 : index
      %16 = arith.addi %15, %12 : index
      %17 = arith.cmpi ult, %16, %c1 : index
      %18 = arith.andi %true, %17 : i1
      scf.if %18 {
        %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
        memref.store %cst, %reinterpret_cast[%14] : memref<1xf32, #gpu.address_space<global>>
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_0 {
    gpu.func @main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_1(%arg0: index, %arg1: index, %arg2: index, %arg3: index, %arg4: memref<?x4xf32, #gpu.address_space<global>>, %arg5: memref<?x4xf32, #gpu.address_space<global>>, %arg6: memref<1xf32, #gpu.address_space<global>>) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %c512 = arith.constant 512 : index
      %c1_1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %c4 = arith.constant 4 : index
      %c0_2 = arith.constant 0 : index
      %cst = arith.constant 0.000000e+00 : f32
      %12 = affine.apply #map1(%0)[%arg0, %c0]
      %13 = affine.apply #map1(%3)[%c1, %c0_0]
      %14 = arith.addi %13, %12 : index
      %true = arith.constant true
      %15 = arith.muli %13, %c1 : index
      %16 = arith.addi %15, %12 : index
      %17 = arith.cmpi ult, %16, %arg1 : index
      %18 = arith.andi %true, %17 : i1
      scf.if %18 {
        %19 = arith.remsi %14, %c512 : index
        %20 = arith.divsi %14, %c512 : index
        %21 = arith.cmpi ult, %19, %c1_1 : index
        scf.if %21 {
          %22 = scf.for %arg7 = %c0_2 to %c32 step %c1_1 iter_args(%arg8 = %cst) -> (f32) {
            %24 = arith.muli %20, %c32 : index
            %25 = arith.addi %24, %arg7 : index
            %26 = arith.cmpi slt, %25, %arg2 : index
            %27 = scf.if %26 -> (f32) {
              %28 = arith.muli %arg3, %c4 : index
              %reinterpret_cast = memref.reinterpret_cast %arg4 to offset: [%c0_2], sizes: [%28], strides: [%c1_1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
              %29 = memref.load %reinterpret_cast[%25] : memref<?xf32, #gpu.address_space<global>>
              %reinterpret_cast_3 = memref.reinterpret_cast %arg5 to offset: [%c0_2], sizes: [%28], strides: [%c1_1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
              %30 = memref.load %reinterpret_cast_3[%25] : memref<?xf32, #gpu.address_space<global>>
              %31 = arith.mulf %29, %30 : f32
              %32 = arith.addf %arg8, %31 : f32
              scf.yield %32 : f32
            } else {
              scf.yield %arg8 : f32
            }
            scf.yield %27 : f32
          }
          %23 = memref.atomic_rmw addf %22, %arg6[%19] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
        }
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_1 {
    gpu.func @main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64(%arg0: index, %arg1: memref<1xf32, #gpu.address_space<global>>) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %cst = arith.constant 0.000000e+00 : f32
      %12 = affine.apply #map1(%0)[%arg0, %c0]
      %13 = affine.apply #map1(%3)[%c1, %c0_0]
      %14 = arith.addi %13, %12 : index
      %true = arith.constant true
      %15 = arith.muli %13, %c1 : index
      %16 = arith.addi %15, %12 : index
      %17 = arith.cmpi ult, %16, %c1 : index
      %18 = arith.andi %true, %17 : i1
      scf.if %18 {
        %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
        memref.store %cst, %reinterpret_cast[%14] : memref<1xf32, #gpu.address_space<global>>
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_2 {
    gpu.func @main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1(%arg0: index, %arg1: index, %arg2: index, %arg3: index, %arg4: memref<?x4xf32, #gpu.address_space<global>>, %arg5: memref<?x4xf32, #gpu.address_space<global>>, %arg6: memref<1xf32, #gpu.address_space<global>>) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %c256 = arith.constant 256 : index
      %c32 = arith.constant 32 : index
      %c8 = arith.constant 8 : index
      %c1_1 = arith.constant 1 : index
      %c64 = arith.constant 64 : index
      %c4 = arith.constant 4 : index
      %c0_2 = arith.constant 0 : index
      %cst = arith.constant 0.000000e+00 : f32
      %c2 = arith.constant 2 : index
      %12 = affine.apply #map1(%0)[%arg0, %c0]
      %13 = affine.apply #map1(%3)[%c1, %c0_0]
      %14 = arith.addi %13, %12 : index
      %true = arith.constant true
      %15 = arith.muli %13, %c1 : index
      %16 = arith.addi %15, %12 : index
      %17 = arith.cmpi ult, %16, %arg1 : index
      %18 = arith.andi %true, %17 : i1
      scf.if %18 {
        %19 = arith.remsi %14, %c256 : index
        %20 = arith.divsi %14, %c256 : index
        %21 = arith.divui %19, %c32 : index
        %22 = arith.remui %19, %c32 : index
        %23 = arith.muli %22, %c8 : index
        %24 = arith.addi %21, %23 : index
        %alloc = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
        %25 = arith.cmpi ult, %22, %c1_1 : index
        %26 = scf.if %25 -> (f32) {
          %32 = scf.for %arg7 = %c0_2 to %c64 step %c1_1 iter_args(%arg8 = %cst) -> (f32) {
            %33 = arith.muli %20, %c8 : index
            %34 = arith.addi %21, %33 : index
            %35 = arith.muli %34, %c64 : index
            %36 = arith.addi %arg7, %35 : index
            %37 = arith.cmpi slt, %36, %arg2 : index
            %38 = scf.if %37 -> (f32) {
              %39 = arith.muli %arg3, %c4 : index
              %reinterpret_cast_3 = memref.reinterpret_cast %arg4 to offset: [%c0_2], sizes: [%39], strides: [%c1_1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
              %40 = memref.load %reinterpret_cast_3[%36] : memref<?xf32, #gpu.address_space<global>>
              %reinterpret_cast_4 = memref.reinterpret_cast %arg5 to offset: [%c0_2], sizes: [%39], strides: [%c1_1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
              %41 = memref.load %reinterpret_cast_4[%36] : memref<?xf32, #gpu.address_space<global>>
              %42 = arith.mulf %40, %41 : f32
              %43 = arith.addf %arg8, %42 : f32
              scf.yield %43 : f32
            } else {
              scf.yield %arg8 : f32
            }
            scf.yield %38 : f32
          }
          scf.yield %32 : f32
        } else {
          scf.yield %cst : f32
        }
        %reinterpret_cast = memref.reinterpret_cast %alloc to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        memref.store %26, %reinterpret_cast[%24] : memref<256xf32, #gpu.address_space<workgroup>>
        gpu.barrier
        %27 = arith.cmpi slt, %21, %c4 : index
        scf.if %27 {
          %32 = arith.addi %24, %c4 : index
          %33 = memref.load %reinterpret_cast[%24] : memref<256xf32, #gpu.address_space<workgroup>>
          %34 = memref.load %reinterpret_cast[%32] : memref<256xf32, #gpu.address_space<workgroup>>
          %35 = arith.addf %33, %34 : f32
          memref.store %35, %reinterpret_cast[%24] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %28 = arith.cmpi slt, %21, %c2 : index
        scf.if %28 {
          %32 = arith.addi %24, %c2 : index
          %33 = memref.load %reinterpret_cast[%24] : memref<256xf32, #gpu.address_space<workgroup>>
          %34 = memref.load %reinterpret_cast[%32] : memref<256xf32, #gpu.address_space<workgroup>>
          %35 = arith.addf %33, %34 : f32
          memref.store %35, %reinterpret_cast[%24] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %29 = arith.cmpi slt, %21, %c1_1 : index
        scf.if %29 {
          %32 = arith.addi %24, %c1_1 : index
          %33 = memref.load %reinterpret_cast[%24] : memref<256xf32, #gpu.address_space<workgroup>>
          %34 = memref.load %reinterpret_cast[%32] : memref<256xf32, #gpu.address_space<workgroup>>
          %35 = arith.addf %33, %34 : f32
          memref.store %35, %reinterpret_cast[%24] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %30 = arith.cmpi eq, %21, %c0_2 : index
        %31 = arith.andi %30, %25 : i1
        scf.if %31 {
          %32 = memref.load %reinterpret_cast[%24] : memref<256xf32, #gpu.address_space<workgroup>>
          %33 = memref.atomic_rmw addf %32, %arg6[%22] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
        }
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_3 {
    gpu.func @main_kColReduction_reduce__6_1_0___thread_tile_h32(%arg0: index, %arg1: memref<1xf32, #gpu.address_space<global>>) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %cst = arith.constant 0.000000e+00 : f32
      %12 = affine.apply #map1(%0)[%arg0, %c0]
      %13 = affine.apply #map1(%3)[%c1, %c0_0]
      %14 = arith.addi %13, %12 : index
      %true = arith.constant true
      %15 = arith.muli %13, %c1 : index
      %16 = arith.addi %15, %12 : index
      %17 = arith.cmpi ult, %16, %c1 : index
      %18 = arith.andi %true, %17 : i1
      scf.if %18 {
        %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
        memref.store %cst, %reinterpret_cast[%14] : memref<1xf32, #gpu.address_space<global>>
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_4 {
    gpu.func @main_kColReduction_reduce__6_1_0___thread_tile_h32_1(%arg0: index, %arg1: index, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?x4xf32, #gpu.address_space<global>>, %arg6: memref<?x4xf32, #gpu.address_space<global>>, %arg7: memref<1xf32, #gpu.address_space<global>>) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %c512 = arith.constant 512 : index
      %c1_1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %c4 = arith.constant 4 : index
      %12 = arith.cmpi eq, %arg0, %c1_1 : index
      %13 = arith.select %12, %arg1, %arg0 : index
      %14 = arith.cmpi eq, %arg1, %13 : index
      %c0_2 = arith.constant 0 : index
      %15 = arith.cmpi eq, %arg0, %13 : index
      %cst = arith.constant 0.000000e+00 : f32
      %16 = affine.apply #map1(%0)[%arg2, %c0]
      %17 = affine.apply #map1(%3)[%c1, %c0_0]
      %18 = arith.addi %17, %16 : index
      %true = arith.constant true
      %19 = arith.muli %17, %c1 : index
      %20 = arith.addi %19, %16 : index
      %21 = arith.cmpi ult, %20, %arg3 : index
      %22 = arith.andi %true, %21 : i1
      scf.if %22 {
        %23 = arith.remsi %18, %c512 : index
        %24 = arith.divsi %18, %c512 : index
        %25 = arith.cmpi ult, %23, %c1_1 : index
        scf.if %25 {
          %26 = scf.for %arg8 = %c0_2 to %c32 step %c1_1 iter_args(%arg9 = %cst) -> (f32) {
            %28 = arith.muli %24, %c32 : index
            %29 = arith.addi %28, %arg8 : index
            %30 = arith.cmpi slt, %29, %arg4 : index
            %31 = scf.if %30 -> (f32) {
              %32 = arith.remui %29, %c4 : index
              %33 = arith.divui %29, %c4 : index
              %34 = arith.select %14, %33, %c0_2 : index
              %35 = arith.muli %34, %c4 : index
              %36 = arith.addi %35, %32 : index
              %37 = arith.muli %arg1, %c4 : index
              %reinterpret_cast = memref.reinterpret_cast %arg5 to offset: [%c0_2], sizes: [%37], strides: [%c1_1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
              %38 = memref.load %reinterpret_cast[%36] : memref<?xf32, #gpu.address_space<global>>
              %39 = arith.select %15, %33, %c0_2 : index
              %40 = arith.muli %39, %c4 : index
              %41 = arith.addi %40, %32 : index
              %42 = arith.muli %arg0, %c4 : index
              %reinterpret_cast_3 = memref.reinterpret_cast %arg6 to offset: [%c0_2], sizes: [%42], strides: [%c1_1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
              %43 = memref.load %reinterpret_cast_3[%41] : memref<?xf32, #gpu.address_space<global>>
              %44 = arith.mulf %38, %43 : f32
              %45 = arith.addf %arg9, %44 : f32
              scf.yield %45 : f32
            } else {
              scf.yield %arg9 : f32
            }
            scf.yield %31 : f32
          }
          %27 = memref.atomic_rmw addf %26, %arg7[%23] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
        }
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_5 {
    gpu.func @main_kColReduction_reduce__6_1_0___block_tile_h64(%arg0: index, %arg1: memref<1xf32, #gpu.address_space<global>>) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %cst = arith.constant 0.000000e+00 : f32
      %12 = affine.apply #map1(%0)[%arg0, %c0]
      %13 = affine.apply #map1(%3)[%c1, %c0_0]
      %14 = arith.addi %13, %12 : index
      %true = arith.constant true
      %15 = arith.muli %13, %c1 : index
      %16 = arith.addi %15, %12 : index
      %17 = arith.cmpi ult, %16, %c1 : index
      %18 = arith.andi %true, %17 : i1
      scf.if %18 {
        %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
        memref.store %cst, %reinterpret_cast[%14] : memref<1xf32, #gpu.address_space<global>>
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_6 {
    gpu.func @main_kColReduction_reduce__6_1_0___block_tile_h64_1(%arg0: index, %arg1: index, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?x4xf32, #gpu.address_space<global>>, %arg6: memref<?x4xf32, #gpu.address_space<global>>, %arg7: memref<1xf32, #gpu.address_space<global>>) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %c256 = arith.constant 256 : index
      %c32 = arith.constant 32 : index
      %c8 = arith.constant 8 : index
      %c1_1 = arith.constant 1 : index
      %c64 = arith.constant 64 : index
      %c4 = arith.constant 4 : index
      %12 = arith.cmpi eq, %arg0, %c1_1 : index
      %13 = arith.select %12, %arg1, %arg0 : index
      %14 = arith.cmpi eq, %arg1, %13 : index
      %c0_2 = arith.constant 0 : index
      %15 = arith.cmpi eq, %arg0, %13 : index
      %cst = arith.constant 0.000000e+00 : f32
      %c2 = arith.constant 2 : index
      %16 = affine.apply #map1(%0)[%arg2, %c0]
      %17 = affine.apply #map1(%3)[%c1, %c0_0]
      %18 = arith.addi %17, %16 : index
      %true = arith.constant true
      %19 = arith.muli %17, %c1 : index
      %20 = arith.addi %19, %16 : index
      %21 = arith.cmpi ult, %20, %arg3 : index
      %22 = arith.andi %true, %21 : i1
      scf.if %22 {
        %23 = arith.remsi %18, %c256 : index
        %24 = arith.divsi %18, %c256 : index
        %25 = arith.divui %23, %c32 : index
        %26 = arith.remui %23, %c32 : index
        %27 = arith.muli %26, %c8 : index
        %28 = arith.addi %25, %27 : index
        %alloc = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
        %29 = arith.cmpi ult, %26, %c1_1 : index
        %30 = scf.if %29 -> (f32) {
          %36 = scf.for %arg8 = %c0_2 to %c64 step %c1_1 iter_args(%arg9 = %cst) -> (f32) {
            %37 = arith.muli %24, %c8 : index
            %38 = arith.addi %25, %37 : index
            %39 = arith.muli %38, %c64 : index
            %40 = arith.addi %arg8, %39 : index
            %41 = arith.cmpi slt, %40, %arg4 : index
            %42 = scf.if %41 -> (f32) {
              %43 = arith.remui %40, %c4 : index
              %44 = arith.divui %40, %c4 : index
              %45 = arith.select %14, %44, %c0_2 : index
              %46 = arith.muli %45, %c4 : index
              %47 = arith.addi %46, %43 : index
              %48 = arith.muli %arg1, %c4 : index
              %reinterpret_cast_3 = memref.reinterpret_cast %arg5 to offset: [%c0_2], sizes: [%48], strides: [%c1_1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
              %49 = memref.load %reinterpret_cast_3[%47] : memref<?xf32, #gpu.address_space<global>>
              %50 = arith.select %15, %44, %c0_2 : index
              %51 = arith.muli %50, %c4 : index
              %52 = arith.addi %51, %43 : index
              %53 = arith.muli %arg0, %c4 : index
              %reinterpret_cast_4 = memref.reinterpret_cast %arg6 to offset: [%c0_2], sizes: [%53], strides: [%c1_1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
              %54 = memref.load %reinterpret_cast_4[%52] : memref<?xf32, #gpu.address_space<global>>
              %55 = arith.mulf %49, %54 : f32
              %56 = arith.addf %arg9, %55 : f32
              scf.yield %56 : f32
            } else {
              scf.yield %arg9 : f32
            }
            scf.yield %42 : f32
          }
          scf.yield %36 : f32
        } else {
          scf.yield %cst : f32
        }
        %reinterpret_cast = memref.reinterpret_cast %alloc to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        memref.store %30, %reinterpret_cast[%28] : memref<256xf32, #gpu.address_space<workgroup>>
        gpu.barrier
        %31 = arith.cmpi slt, %25, %c4 : index
        scf.if %31 {
          %36 = arith.addi %28, %c4 : index
          %37 = memref.load %reinterpret_cast[%28] : memref<256xf32, #gpu.address_space<workgroup>>
          %38 = memref.load %reinterpret_cast[%36] : memref<256xf32, #gpu.address_space<workgroup>>
          %39 = arith.addf %37, %38 : f32
          memref.store %39, %reinterpret_cast[%28] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %32 = arith.cmpi slt, %25, %c2 : index
        scf.if %32 {
          %36 = arith.addi %28, %c2 : index
          %37 = memref.load %reinterpret_cast[%28] : memref<256xf32, #gpu.address_space<workgroup>>
          %38 = memref.load %reinterpret_cast[%36] : memref<256xf32, #gpu.address_space<workgroup>>
          %39 = arith.addf %37, %38 : f32
          memref.store %39, %reinterpret_cast[%28] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %33 = arith.cmpi slt, %25, %c1_1 : index
        scf.if %33 {
          %36 = arith.addi %28, %c1_1 : index
          %37 = memref.load %reinterpret_cast[%28] : memref<256xf32, #gpu.address_space<workgroup>>
          %38 = memref.load %reinterpret_cast[%36] : memref<256xf32, #gpu.address_space<workgroup>>
          %39 = arith.addf %37, %38 : f32
          memref.store %39, %reinterpret_cast[%28] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %34 = arith.cmpi eq, %25, %c0_2 : index
        %35 = arith.andi %34, %29 : i1
        scf.if %35 {
          %36 = memref.load %reinterpret_cast[%28] : memref<256xf32, #gpu.address_space<workgroup>>
          %37 = memref.atomic_rmw addf %36, %arg7[%26] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
        }
      }
      gpu.return
    }
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = true, sym_name = "C4", value = 4 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = true, sym_name = "C1", value = 1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S4", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = false, sym_name = "S5", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %c4 = arith.constant 4 : index
    %0 = "disc_shape.dim"() {name = @S2} : () -> index
    %1 = "disc_shape.dim"() {name = @S3} : () -> index
    "disc_shape.tie_product_equal"(%c4, %0, %1) {operand_segment_sizes = array<i32: 2, 1>} : (index, index, index) -> ()
    %2 = "disc_shape.dim"() {name = @S4} : () -> index
    %3 = "disc_shape.dim"() {name = @S5} : () -> index
    "disc_shape.tie_product_equal"(%c4, %2, %3) {operand_segment_sizes = array<i32: 2, 1>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After LhloFusionInlinerPass (lhlo-fusion-inliner) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c512 = arith.constant 512 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c4_i32 = arith.constant 4 : i32
  %c4 = arith.constant 4 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
  %2 = "disc_ral.dispatch"(%arg0, %c1) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
  %dim = memref.dim %2, %c0 : memref<?x4xf32>
  %dim_0 = memref.dim %1, %c0 : memref<?x4xf32>
  %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %reinterpret_cast_1 = memref.reinterpret_cast %2 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %3 = arith.cmpi eq, %dim, %c1 : index
  %4 = arith.select %3, %dim_0, %dim : index
  %alloc = memref.alloc(%dim_0) {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %5 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast, %alloc) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  %alloc_2 = memref.alloc(%dim) {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_1, %alloc_2) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  %6 = arith.index_cast %4 : index to i32
  %7 = arith.muli %6, %c4_i32 : i32
  %8 = arith.index_cast %7 : i32 to index
  %9 = arith.cmpi eq, %dim_0, %4 : index
  %10 = arith.cmpi eq, %dim, %4 : index
  %11 = arith.andi %10, %9 : i1
  %alloc_3 = memref.alloc() : memref<1xf32, #gpu.address_space<global>>
  scf.if %11 {
    %13 = arith.cmpi slt, %8, %c1 : index
    scf.if %13 {
      %c0_6 = arith.constant 0 : index
      %c512_7 = arith.constant 512 : index
      %14 = arith.muli %c1, %c512_7 : index
      %c1_8 = arith.constant 1 : index
      %15 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%c1)[%c0, %14]
      %16 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%14)[%c0_6, %c1]
      gpu.launch_func  @main_kernel::@main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32 blocks in (%15, %c1_8, %c1_8) threads in (%16, %c1_8, %c1_8) args(%14 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
      %17 = arith.cmpi eq, %8, %c0 : index
      %18 = arith.subi %8, %c1 : index
      %19 = arith.divui %18, %c32 : index
      %20 = arith.addi %19, %c1 : index
      %21 = arith.select %17, %c0, %20 : index
      %c0_9 = arith.constant 0 : index
      %c1_10 = arith.constant 1 : index
      %c1_11 = arith.constant 1 : index
      %22 = arith.muli %c1_11, %21 : index
      %23 = arith.muli %22, %c512 : index
      %c0_12 = arith.constant 0 : index
      %c512_13 = arith.constant 512 : index
      %24 = arith.muli %c1_10, %c512_13 : index
      %c1_14 = arith.constant 1 : index
      %25 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%23)[%c0_9, %24]
      %26 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%24)[%c0_12, %c1_10]
      gpu.launch_func  @main_kernel_0::@main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_1 blocks in (%25, %c1_14, %c1_14) threads in (%26, %c1_14, %c1_14) args(%24 : index, %23 : index, %8 : index, %4 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    } else {
      %c0_6 = arith.constant 0 : index
      %c256_7 = arith.constant 256 : index
      %14 = arith.muli %c1, %c256_7 : index
      %c1_8 = arith.constant 1 : index
      %15 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%c1)[%c0, %14]
      %16 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%14)[%c0_6, %c1]
      gpu.launch_func  @main_kernel_1::@main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64 blocks in (%15, %c1_8, %c1_8) threads in (%16, %c1_8, %c1_8) args(%14 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
      %17 = arith.cmpi eq, %8, %c0 : index
      %18 = arith.subi %8, %c1 : index
      %19 = arith.divui %18, %c512 : index
      %20 = arith.addi %19, %c1 : index
      %21 = arith.select %17, %c0, %20 : index
      %c0_9 = arith.constant 0 : index
      %c1_10 = arith.constant 1 : index
      %c1_11 = arith.constant 1 : index
      %22 = arith.muli %c1_11, %21 : index
      %23 = arith.muli %22, %c256 : index
      %c0_12 = arith.constant 0 : index
      %c256_13 = arith.constant 256 : index
      %24 = arith.muli %c1_10, %c256_13 : index
      %c1_14 = arith.constant 1 : index
      %25 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%23)[%c0_9, %24]
      %26 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%24)[%c0_12, %c1_10]
      gpu.launch_func  @main_kernel_2::@main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1 blocks in (%25, %c1_14, %c1_14) threads in (%26, %c1_14, %c1_14) args(%24 : index, %23 : index, %8 : index, %4 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    }
  } else {
    %13 = arith.cmpi slt, %8, %c1 : index
    scf.if %13 {
      %c0_6 = arith.constant 0 : index
      %c512_7 = arith.constant 512 : index
      %14 = arith.muli %c1, %c512_7 : index
      %c1_8 = arith.constant 1 : index
      %15 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%c1)[%c0, %14]
      %16 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%14)[%c0_6, %c1]
      gpu.launch_func  @main_kernel_3::@main_kColReduction_reduce__6_1_0___thread_tile_h32 blocks in (%15, %c1_8, %c1_8) threads in (%16, %c1_8, %c1_8) args(%14 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
      %17 = arith.cmpi eq, %8, %c0 : index
      %18 = arith.subi %8, %c1 : index
      %19 = arith.divui %18, %c32 : index
      %20 = arith.addi %19, %c1 : index
      %21 = arith.select %17, %c0, %20 : index
      %c0_9 = arith.constant 0 : index
      %c1_10 = arith.constant 1 : index
      %c1_11 = arith.constant 1 : index
      %22 = arith.muli %c1_11, %21 : index
      %23 = arith.muli %22, %c512 : index
      %c0_12 = arith.constant 0 : index
      %c512_13 = arith.constant 512 : index
      %24 = arith.muli %c1_10, %c512_13 : index
      %c1_14 = arith.constant 1 : index
      %25 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%23)[%c0_9, %24]
      %26 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%24)[%c0_12, %c1_10]
      gpu.launch_func  @main_kernel_4::@main_kColReduction_reduce__6_1_0___thread_tile_h32_1 blocks in (%25, %c1_14, %c1_14) threads in (%26, %c1_14, %c1_14) args(%dim : index, %dim_0 : index, %24 : index, %23 : index, %8 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    } else {
      %c0_6 = arith.constant 0 : index
      %c256_7 = arith.constant 256 : index
      %14 = arith.muli %c1, %c256_7 : index
      %c1_8 = arith.constant 1 : index
      %15 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%c1)[%c0, %14]
      %16 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%14)[%c0_6, %c1]
      gpu.launch_func  @main_kernel_5::@main_kColReduction_reduce__6_1_0___block_tile_h64 blocks in (%15, %c1_8, %c1_8) threads in (%16, %c1_8, %c1_8) args(%14 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
      %17 = arith.cmpi eq, %8, %c0 : index
      %18 = arith.subi %8, %c1 : index
      %19 = arith.divui %18, %c512 : index
      %20 = arith.addi %19, %c1 : index
      %21 = arith.select %17, %c0, %20 : index
      %c0_9 = arith.constant 0 : index
      %c1_10 = arith.constant 1 : index
      %c1_11 = arith.constant 1 : index
      %22 = arith.muli %c1_11, %21 : index
      %23 = arith.muli %22, %c256 : index
      %c0_12 = arith.constant 0 : index
      %c256_13 = arith.constant 256 : index
      %24 = arith.muli %c1_10, %c256_13 : index
      %c1_14 = arith.constant 1 : index
      %25 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%23)[%c0_9, %24]
      %26 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%24)[%c0_12, %c1_10]
      gpu.launch_func  @main_kernel_6::@main_kColReduction_reduce__6_1_0___block_tile_h64_1 blocks in (%25, %c1_14, %c1_14) threads in (%26, %c1_14, %c1_14) args(%dim : index, %dim_0 : index, %24 : index, %23 : index, %8 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    }
  }
  memref.dealloc %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc : memref<?x4xf32, #gpu.address_space<global>>
  %alloca = memref.alloca() : memref<0xindex>
  %12 = "disc_ral.dispatch"(%arg0, %5, %alloc_3, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1xf32, #gpu.address_space<global>>, memref<0xindex>) -> memref<f32, #gpu.address_space<global>>
  %reinterpret_cast_4 = memref.reinterpret_cast %12 to offset: [0], sizes: [], strides: [] : memref<f32, #gpu.address_space<global>> to memref<f32, #gpu.address_space<global>>
  memref.dealloc %alloc_3 : memref<1xf32, #gpu.address_space<global>>
  %alloc_5 = memref.alloc() : memref<f32>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_4, %alloc_5) {backend_config = "", call_target_name = "d2h", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<f32, #gpu.address_space<global>>, memref<f32>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  memref.dealloc %reinterpret_cast_4 : memref<f32, #gpu.address_space<global>>
  "disc_ral.dispatch"(%arg0, %c0, %alloc_5) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<f32>) -> ()
  return
}

// -----// IR Dump After DiscArgsMutationExpandPass (disc-argsmutation-expand) //----- //
#map = affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>
#map1 = affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>
module attributes {gpu.container_module} {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
    %c256 = arith.constant 256 : index
    %c32 = arith.constant 32 : index
    %c512 = arith.constant 512 : index
    %0 = llvm.mlir.constant(0 : i32) : i32
    %c4_i32 = arith.constant 4 : i32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
    %2 = "disc_ral.dispatch"(%arg0, %c1) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
    %dim = memref.dim %2, %c0 : memref<?x4xf32>
    %dim_0 = memref.dim %1, %c0 : memref<?x4xf32>
    %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
    %reinterpret_cast_1 = memref.reinterpret_cast %2 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
    %3 = arith.cmpi eq, %dim, %c1 : index
    %4 = arith.select %3, %dim_0, %dim : index
    %alloc = memref.alloc(%dim_0) {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
    %5 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
    "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast, %alloc) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
    "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
    %alloc_2 = memref.alloc(%dim) {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
    "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_1, %alloc_2) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
    "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
    %6 = arith.index_cast %4 : index to i32
    %7 = arith.muli %6, %c4_i32 : i32
    %8 = arith.index_cast %7 : i32 to index
    %9 = arith.cmpi eq, %dim_0, %4 : index
    %10 = arith.cmpi eq, %dim, %4 : index
    %11 = arith.andi %10, %9 : i1
    %alloc_3 = memref.alloc() : memref<1xf32, #gpu.address_space<global>>
    scf.if %11 {
      %13 = arith.cmpi slt, %8, %c1 : index
      scf.if %13 {
        gpu.launch_func  @main_kernel::@main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32 blocks in (%c1, %c1, %c1) threads in (%c512, %c1, %c1) args(%c512 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
        %14 = arith.cmpi eq, %8, %c0 : index
        %15 = arith.subi %8, %c1 : index
        %16 = arith.divui %15, %c32 : index
        %17 = arith.addi %16, %c1 : index
        %18 = arith.select %14, %c0, %17 : index
        %19 = arith.muli %18, %c512 : index
        %20 = affine.apply #map(%19)[%c0, %c512]
        gpu.launch_func  @main_kernel_0::@main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_1 blocks in (%20, %c1, %c1) threads in (%c512, %c1, %c1) args(%c512 : index, %19 : index, %8 : index, %4 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
      } else {
        gpu.launch_func  @main_kernel_1::@main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64 blocks in (%c1, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
        %14 = arith.cmpi eq, %8, %c0 : index
        %15 = arith.subi %8, %c1 : index
        %16 = arith.divui %15, %c512 : index
        %17 = arith.addi %16, %c1 : index
        %18 = arith.select %14, %c0, %17 : index
        %19 = arith.muli %18, %c256 : index
        %20 = affine.apply #map(%19)[%c0, %c256]
        gpu.launch_func  @main_kernel_2::@main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1 blocks in (%20, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %19 : index, %8 : index, %4 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
      }
    } else {
      %13 = arith.cmpi slt, %8, %c1 : index
      scf.if %13 {
        gpu.launch_func  @main_kernel_3::@main_kColReduction_reduce__6_1_0___thread_tile_h32 blocks in (%c1, %c1, %c1) threads in (%c512, %c1, %c1) args(%c512 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
        %14 = arith.cmpi eq, %8, %c0 : index
        %15 = arith.subi %8, %c1 : index
        %16 = arith.divui %15, %c32 : index
        %17 = arith.addi %16, %c1 : index
        %18 = arith.select %14, %c0, %17 : index
        %19 = arith.muli %18, %c512 : index
        %20 = affine.apply #map(%19)[%c0, %c512]
        gpu.launch_func  @main_kernel_4::@main_kColReduction_reduce__6_1_0___thread_tile_h32_1 blocks in (%20, %c1, %c1) threads in (%c512, %c1, %c1) args(%dim : index, %dim_0 : index, %c512 : index, %19 : index, %8 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
      } else {
        gpu.launch_func  @main_kernel_5::@main_kColReduction_reduce__6_1_0___block_tile_h64 blocks in (%c1, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
        %14 = arith.cmpi eq, %8, %c0 : index
        %15 = arith.subi %8, %c1 : index
        %16 = arith.divui %15, %c512 : index
        %17 = arith.addi %16, %c1 : index
        %18 = arith.select %14, %c0, %17 : index
        %19 = arith.muli %18, %c256 : index
        %20 = affine.apply #map(%19)[%c0, %c256]
        gpu.launch_func  @main_kernel_6::@main_kColReduction_reduce__6_1_0___block_tile_h64_1 blocks in (%20, %c1, %c1) threads in (%c256, %c1, %c1) args(%dim : index, %dim_0 : index, %c256 : index, %19 : index, %8 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
      }
    }
    memref.dealloc %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>
    memref.dealloc %alloc : memref<?x4xf32, #gpu.address_space<global>>
    %alloca = memref.alloca() : memref<0xindex>
    %12 = "disc_ral.dispatch"(%arg0, %5, %alloc_3, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1xf32, #gpu.address_space<global>>, memref<0xindex>) -> memref<f32, #gpu.address_space<global>>
    %reinterpret_cast_4 = memref.reinterpret_cast %12 to offset: [0], sizes: [], strides: [] : memref<f32, #gpu.address_space<global>> to memref<f32, #gpu.address_space<global>>
    memref.dealloc %alloc_3 : memref<1xf32, #gpu.address_space<global>>
    %alloc_5 = memref.alloc() : memref<f32>
    "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_4, %alloc_5) {backend_config = "", call_target_name = "d2h", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<f32, #gpu.address_space<global>>, memref<f32>) -> ()
    "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
    memref.dealloc %reinterpret_cast_4 : memref<f32, #gpu.address_space<global>>
    "disc_ral.dispatch"(%arg0, %c0, %alloc_5) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<f32>) -> ()
    return
  }
  gpu.module @main_kernel {
    gpu.func @main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32(%arg0: index, %arg1: memref<1xf32, #gpu.address_space<global>>) kernel {
      %cst = arith.constant 0.000000e+00 : f32
      %c1 = arith.constant 1 : index
      %c0 = arith.constant 0 : index
      %0 = gpu.block_id  x
      %1 = gpu.thread_id  x
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %2 = affine.apply #map1(%0)[%arg0, %c0]
      %3 = affine.apply #map1(%1)[%c1, %c0]
      %4 = arith.addi %3, %2 : index
      %5 = arith.addi %3, %2 : index
      %6 = arith.cmpi ult, %5, %c1 : index
      scf.if %6 {
        %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
        memref.store %cst, %reinterpret_cast[%4] : memref<1xf32, #gpu.address_space<global>>
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_0 {
    gpu.func @main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_1(%arg0: index, %arg1: index, %arg2: index, %arg3: index, %arg4: memref<?x4xf32, #gpu.address_space<global>>, %arg5: memref<?x4xf32, #gpu.address_space<global>>, %arg6: memref<1xf32, #gpu.address_space<global>>) kernel {
      %cst = arith.constant 0.000000e+00 : f32
      %c4 = arith.constant 4 : index
      %c32 = arith.constant 32 : index
      %c512 = arith.constant 512 : index
      %c1 = arith.constant 1 : index
      %c0 = arith.constant 0 : index
      %0 = gpu.block_id  x
      %1 = gpu.thread_id  x
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %2 = affine.apply #map1(%0)[%arg0, %c0]
      %3 = affine.apply #map1(%1)[%c1, %c0]
      %4 = arith.addi %3, %2 : index
      %5 = arith.addi %3, %2 : index
      %6 = arith.cmpi ult, %5, %arg1 : index
      scf.if %6 {
        %7 = arith.remsi %4, %c512 : index
        %8 = arith.divsi %4, %c512 : index
        %9 = arith.cmpi ult, %7, %c1 : index
        scf.if %9 {
          %10 = scf.for %arg7 = %c0 to %c32 step %c1 iter_args(%arg8 = %cst) -> (f32) {
            %12 = arith.muli %8, %c32 : index
            %13 = arith.addi %12, %arg7 : index
            %14 = arith.cmpi slt, %13, %arg2 : index
            %15 = scf.if %14 -> (f32) {
              %16 = arith.muli %arg3, %c4 : index
              %reinterpret_cast = memref.reinterpret_cast %arg4 to offset: [%c0], sizes: [%16], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
              %17 = memref.load %reinterpret_cast[%13] : memref<?xf32, #gpu.address_space<global>>
              %reinterpret_cast_0 = memref.reinterpret_cast %arg5 to offset: [%c0], sizes: [%16], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
              %18 = memref.load %reinterpret_cast_0[%13] : memref<?xf32, #gpu.address_space<global>>
              %19 = arith.mulf %17, %18 : f32
              %20 = arith.addf %arg8, %19 : f32
              scf.yield %20 : f32
            } else {
              scf.yield %arg8 : f32
            }
            scf.yield %15 : f32
          }
          %11 = memref.atomic_rmw addf %10, %arg6[%7] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
        }
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_1 {
    gpu.func @main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64(%arg0: index, %arg1: memref<1xf32, #gpu.address_space<global>>) kernel {
      %cst = arith.constant 0.000000e+00 : f32
      %c1 = arith.constant 1 : index
      %c0 = arith.constant 0 : index
      %0 = gpu.block_id  x
      %1 = gpu.thread_id  x
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %2 = affine.apply #map1(%0)[%arg0, %c0]
      %3 = affine.apply #map1(%1)[%c1, %c0]
      %4 = arith.addi %3, %2 : index
      %5 = arith.addi %3, %2 : index
      %6 = arith.cmpi ult, %5, %c1 : index
      scf.if %6 {
        %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
        memref.store %cst, %reinterpret_cast[%4] : memref<1xf32, #gpu.address_space<global>>
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_2 {
    gpu.func @main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1(%arg0: index, %arg1: index, %arg2: index, %arg3: index, %arg4: memref<?x4xf32, #gpu.address_space<global>>, %arg5: memref<?x4xf32, #gpu.address_space<global>>, %arg6: memref<1xf32, #gpu.address_space<global>>) kernel {
      %c2 = arith.constant 2 : index
      %cst = arith.constant 0.000000e+00 : f32
      %c4 = arith.constant 4 : index
      %c64 = arith.constant 64 : index
      %c8 = arith.constant 8 : index
      %c32 = arith.constant 32 : index
      %c256 = arith.constant 256 : index
      %c1 = arith.constant 1 : index
      %c0 = arith.constant 0 : index
      %0 = gpu.block_id  x
      %1 = gpu.thread_id  x
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %2 = affine.apply #map1(%0)[%arg0, %c0]
      %3 = affine.apply #map1(%1)[%c1, %c0]
      %4 = arith.addi %3, %2 : index
      %5 = arith.addi %3, %2 : index
      %6 = arith.cmpi ult, %5, %arg1 : index
      scf.if %6 {
        %7 = arith.remsi %4, %c256 : index
        %8 = arith.divsi %4, %c256 : index
        %9 = arith.divui %7, %c32 : index
        %10 = arith.remui %7, %c32 : index
        %11 = arith.muli %10, %c8 : index
        %12 = arith.addi %9, %11 : index
        %alloc = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
        %13 = arith.cmpi ult, %10, %c1 : index
        %14 = scf.if %13 -> (f32) {
          %20 = scf.for %arg7 = %c0 to %c64 step %c1 iter_args(%arg8 = %cst) -> (f32) {
            %21 = arith.muli %8, %c8 : index
            %22 = arith.addi %9, %21 : index
            %23 = arith.muli %22, %c64 : index
            %24 = arith.addi %arg7, %23 : index
            %25 = arith.cmpi slt, %24, %arg2 : index
            %26 = scf.if %25 -> (f32) {
              %27 = arith.muli %arg3, %c4 : index
              %reinterpret_cast_0 = memref.reinterpret_cast %arg4 to offset: [%c0], sizes: [%27], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
              %28 = memref.load %reinterpret_cast_0[%24] : memref<?xf32, #gpu.address_space<global>>
              %reinterpret_cast_1 = memref.reinterpret_cast %arg5 to offset: [%c0], sizes: [%27], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
              %29 = memref.load %reinterpret_cast_1[%24] : memref<?xf32, #gpu.address_space<global>>
              %30 = arith.mulf %28, %29 : f32
              %31 = arith.addf %arg8, %30 : f32
              scf.yield %31 : f32
            } else {
              scf.yield %arg8 : f32
            }
            scf.yield %26 : f32
          }
          scf.yield %20 : f32
        } else {
          scf.yield %cst : f32
        }
        %reinterpret_cast = memref.reinterpret_cast %alloc to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        memref.store %14, %reinterpret_cast[%12] : memref<256xf32, #gpu.address_space<workgroup>>
        gpu.barrier
        %15 = arith.cmpi slt, %9, %c4 : index
        scf.if %15 {
          %20 = arith.addi %12, %c4 : index
          %21 = memref.load %reinterpret_cast[%12] : memref<256xf32, #gpu.address_space<workgroup>>
          %22 = memref.load %reinterpret_cast[%20] : memref<256xf32, #gpu.address_space<workgroup>>
          %23 = arith.addf %21, %22 : f32
          memref.store %23, %reinterpret_cast[%12] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %16 = arith.cmpi slt, %9, %c2 : index
        scf.if %16 {
          %20 = arith.addi %12, %c2 : index
          %21 = memref.load %reinterpret_cast[%12] : memref<256xf32, #gpu.address_space<workgroup>>
          %22 = memref.load %reinterpret_cast[%20] : memref<256xf32, #gpu.address_space<workgroup>>
          %23 = arith.addf %21, %22 : f32
          memref.store %23, %reinterpret_cast[%12] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %17 = arith.cmpi slt, %9, %c1 : index
        scf.if %17 {
          %20 = arith.addi %12, %c1 : index
          %21 = memref.load %reinterpret_cast[%12] : memref<256xf32, #gpu.address_space<workgroup>>
          %22 = memref.load %reinterpret_cast[%20] : memref<256xf32, #gpu.address_space<workgroup>>
          %23 = arith.addf %21, %22 : f32
          memref.store %23, %reinterpret_cast[%12] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %18 = arith.cmpi eq, %9, %c0 : index
        %19 = arith.andi %18, %13 : i1
        scf.if %19 {
          %20 = memref.load %reinterpret_cast[%12] : memref<256xf32, #gpu.address_space<workgroup>>
          %21 = memref.atomic_rmw addf %20, %arg6[%10] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
        }
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_3 {
    gpu.func @main_kColReduction_reduce__6_1_0___thread_tile_h32(%arg0: index, %arg1: memref<1xf32, #gpu.address_space<global>>) kernel {
      %cst = arith.constant 0.000000e+00 : f32
      %c1 = arith.constant 1 : index
      %c0 = arith.constant 0 : index
      %0 = gpu.block_id  x
      %1 = gpu.thread_id  x
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %2 = affine.apply #map1(%0)[%arg0, %c0]
      %3 = affine.apply #map1(%1)[%c1, %c0]
      %4 = arith.addi %3, %2 : index
      %5 = arith.addi %3, %2 : index
      %6 = arith.cmpi ult, %5, %c1 : index
      scf.if %6 {
        %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
        memref.store %cst, %reinterpret_cast[%4] : memref<1xf32, #gpu.address_space<global>>
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_4 {
    gpu.func @main_kColReduction_reduce__6_1_0___thread_tile_h32_1(%arg0: index, %arg1: index, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?x4xf32, #gpu.address_space<global>>, %arg6: memref<?x4xf32, #gpu.address_space<global>>, %arg7: memref<1xf32, #gpu.address_space<global>>) kernel {
      %cst = arith.constant 0.000000e+00 : f32
      %c4 = arith.constant 4 : index
      %c32 = arith.constant 32 : index
      %c512 = arith.constant 512 : index
      %c1 = arith.constant 1 : index
      %c0 = arith.constant 0 : index
      %0 = gpu.block_id  x
      %1 = gpu.thread_id  x
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %2 = arith.cmpi eq, %arg0, %c1 : index
      %3 = arith.select %2, %arg1, %arg0 : index
      %4 = arith.cmpi eq, %arg1, %3 : index
      %5 = arith.cmpi eq, %arg0, %3 : index
      %6 = affine.apply #map1(%0)[%arg2, %c0]
      %7 = affine.apply #map1(%1)[%c1, %c0]
      %8 = arith.addi %7, %6 : index
      %9 = arith.addi %7, %6 : index
      %10 = arith.cmpi ult, %9, %arg3 : index
      scf.if %10 {
        %11 = arith.remsi %8, %c512 : index
        %12 = arith.divsi %8, %c512 : index
        %13 = arith.cmpi ult, %11, %c1 : index
        scf.if %13 {
          %14 = scf.for %arg8 = %c0 to %c32 step %c1 iter_args(%arg9 = %cst) -> (f32) {
            %16 = arith.muli %12, %c32 : index
            %17 = arith.addi %16, %arg8 : index
            %18 = arith.cmpi slt, %17, %arg4 : index
            %19 = scf.if %18 -> (f32) {
              %20 = arith.remui %17, %c4 : index
              %21 = arith.divui %17, %c4 : index
              %22 = arith.select %4, %21, %c0 : index
              %23 = arith.muli %22, %c4 : index
              %24 = arith.addi %23, %20 : index
              %25 = arith.muli %arg1, %c4 : index
              %reinterpret_cast = memref.reinterpret_cast %arg5 to offset: [%c0], sizes: [%25], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
              %26 = memref.load %reinterpret_cast[%24] : memref<?xf32, #gpu.address_space<global>>
              %27 = arith.select %5, %21, %c0 : index
              %28 = arith.muli %27, %c4 : index
              %29 = arith.addi %28, %20 : index
              %30 = arith.muli %arg0, %c4 : index
              %reinterpret_cast_0 = memref.reinterpret_cast %arg6 to offset: [%c0], sizes: [%30], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
              %31 = memref.load %reinterpret_cast_0[%29] : memref<?xf32, #gpu.address_space<global>>
              %32 = arith.mulf %26, %31 : f32
              %33 = arith.addf %arg9, %32 : f32
              scf.yield %33 : f32
            } else {
              scf.yield %arg9 : f32
            }
            scf.yield %19 : f32
          }
          %15 = memref.atomic_rmw addf %14, %arg7[%11] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
        }
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_5 {
    gpu.func @main_kColReduction_reduce__6_1_0___block_tile_h64(%arg0: index, %arg1: memref<1xf32, #gpu.address_space<global>>) kernel {
      %cst = arith.constant 0.000000e+00 : f32
      %c1 = arith.constant 1 : index
      %c0 = arith.constant 0 : index
      %0 = gpu.block_id  x
      %1 = gpu.thread_id  x
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %2 = affine.apply #map1(%0)[%arg0, %c0]
      %3 = affine.apply #map1(%1)[%c1, %c0]
      %4 = arith.addi %3, %2 : index
      %5 = arith.addi %3, %2 : index
      %6 = arith.cmpi ult, %5, %c1 : index
      scf.if %6 {
        %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
        memref.store %cst, %reinterpret_cast[%4] : memref<1xf32, #gpu.address_space<global>>
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_6 {
    gpu.func @main_kColReduction_reduce__6_1_0___block_tile_h64_1(%arg0: index, %arg1: index, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?x4xf32, #gpu.address_space<global>>, %arg6: memref<?x4xf32, #gpu.address_space<global>>, %arg7: memref<1xf32, #gpu.address_space<global>>) kernel {
      %c2 = arith.constant 2 : index
      %cst = arith.constant 0.000000e+00 : f32
      %c4 = arith.constant 4 : index
      %c64 = arith.constant 64 : index
      %c8 = arith.constant 8 : index
      %c32 = arith.constant 32 : index
      %c256 = arith.constant 256 : index
      %c1 = arith.constant 1 : index
      %c0 = arith.constant 0 : index
      %0 = gpu.block_id  x
      %1 = gpu.thread_id  x
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %2 = arith.cmpi eq, %arg0, %c1 : index
      %3 = arith.select %2, %arg1, %arg0 : index
      %4 = arith.cmpi eq, %arg1, %3 : index
      %5 = arith.cmpi eq, %arg0, %3 : index
      %6 = affine.apply #map1(%0)[%arg2, %c0]
      %7 = affine.apply #map1(%1)[%c1, %c0]
      %8 = arith.addi %7, %6 : index
      %9 = arith.addi %7, %6 : index
      %10 = arith.cmpi ult, %9, %arg3 : index
      scf.if %10 {
        %11 = arith.remsi %8, %c256 : index
        %12 = arith.divsi %8, %c256 : index
        %13 = arith.divui %11, %c32 : index
        %14 = arith.remui %11, %c32 : index
        %15 = arith.muli %14, %c8 : index
        %16 = arith.addi %13, %15 : index
        %alloc = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
        %17 = arith.cmpi ult, %14, %c1 : index
        %18 = scf.if %17 -> (f32) {
          %24 = scf.for %arg8 = %c0 to %c64 step %c1 iter_args(%arg9 = %cst) -> (f32) {
            %25 = arith.muli %12, %c8 : index
            %26 = arith.addi %13, %25 : index
            %27 = arith.muli %26, %c64 : index
            %28 = arith.addi %arg8, %27 : index
            %29 = arith.cmpi slt, %28, %arg4 : index
            %30 = scf.if %29 -> (f32) {
              %31 = arith.remui %28, %c4 : index
              %32 = arith.divui %28, %c4 : index
              %33 = arith.select %4, %32, %c0 : index
              %34 = arith.muli %33, %c4 : index
              %35 = arith.addi %34, %31 : index
              %36 = arith.muli %arg1, %c4 : index
              %reinterpret_cast_0 = memref.reinterpret_cast %arg5 to offset: [%c0], sizes: [%36], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
              %37 = memref.load %reinterpret_cast_0[%35] : memref<?xf32, #gpu.address_space<global>>
              %38 = arith.select %5, %32, %c0 : index
              %39 = arith.muli %38, %c4 : index
              %40 = arith.addi %39, %31 : index
              %41 = arith.muli %arg0, %c4 : index
              %reinterpret_cast_1 = memref.reinterpret_cast %arg6 to offset: [%c0], sizes: [%41], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
              %42 = memref.load %reinterpret_cast_1[%40] : memref<?xf32, #gpu.address_space<global>>
              %43 = arith.mulf %37, %42 : f32
              %44 = arith.addf %arg9, %43 : f32
              scf.yield %44 : f32
            } else {
              scf.yield %arg9 : f32
            }
            scf.yield %30 : f32
          }
          scf.yield %24 : f32
        } else {
          scf.yield %cst : f32
        }
        %reinterpret_cast = memref.reinterpret_cast %alloc to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        memref.store %18, %reinterpret_cast[%16] : memref<256xf32, #gpu.address_space<workgroup>>
        gpu.barrier
        %19 = arith.cmpi slt, %13, %c4 : index
        scf.if %19 {
          %24 = arith.addi %16, %c4 : index
          %25 = memref.load %reinterpret_cast[%16] : memref<256xf32, #gpu.address_space<workgroup>>
          %26 = memref.load %reinterpret_cast[%24] : memref<256xf32, #gpu.address_space<workgroup>>
          %27 = arith.addf %25, %26 : f32
          memref.store %27, %reinterpret_cast[%16] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %20 = arith.cmpi slt, %13, %c2 : index
        scf.if %20 {
          %24 = arith.addi %16, %c2 : index
          %25 = memref.load %reinterpret_cast[%16] : memref<256xf32, #gpu.address_space<workgroup>>
          %26 = memref.load %reinterpret_cast[%24] : memref<256xf32, #gpu.address_space<workgroup>>
          %27 = arith.addf %25, %26 : f32
          memref.store %27, %reinterpret_cast[%16] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %21 = arith.cmpi slt, %13, %c1 : index
        scf.if %21 {
          %24 = arith.addi %16, %c1 : index
          %25 = memref.load %reinterpret_cast[%16] : memref<256xf32, #gpu.address_space<workgroup>>
          %26 = memref.load %reinterpret_cast[%24] : memref<256xf32, #gpu.address_space<workgroup>>
          %27 = arith.addf %25, %26 : f32
          memref.store %27, %reinterpret_cast[%16] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %22 = arith.cmpi eq, %13, %c0 : index
        %23 = arith.andi %22, %17 : i1
        scf.if %23 {
          %24 = memref.load %reinterpret_cast[%16] : memref<256xf32, #gpu.address_space<workgroup>>
          %25 = memref.atomic_rmw addf %24, %arg7[%14] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
        }
      }
      gpu.return
    }
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = true, sym_name = "C4", value = 4 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = true, sym_name = "C1", value = 1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S4", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = false, sym_name = "S5", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %c4 = arith.constant 4 : index
    %0 = "disc_shape.dim"() {name = @S2} : () -> index
    %1 = "disc_shape.dim"() {name = @S3} : () -> index
    "disc_shape.tie_product_equal"(%c4, %0, %1) {operand_segment_sizes = array<i32: 2, 1>} : (index, index, index) -> ()
    %2 = "disc_shape.dim"() {name = @S4} : () -> index
    %3 = "disc_shape.dim"() {name = @S5} : () -> index
    "disc_shape.tie_product_equal"(%c4, %2, %3) {operand_segment_sizes = array<i32: 2, 1>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After ReviseGpuKernelOutliningPass (disc-revise-gpu-kernel-outlining) //----- //
#map = affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>
#map1 = affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>
module attributes {gpu.container_module} {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
    %c256 = arith.constant 256 : index
    %c32 = arith.constant 32 : index
    %c512 = arith.constant 512 : index
    %0 = llvm.mlir.constant(0 : i32) : i32
    %c4_i32 = arith.constant 4 : i32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
    %2 = "disc_ral.dispatch"(%arg0, %c1) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
    %dim = memref.dim %2, %c0 : memref<?x4xf32>
    %dim_0 = memref.dim %1, %c0 : memref<?x4xf32>
    %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
    %reinterpret_cast_1 = memref.reinterpret_cast %2 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
    %3 = arith.cmpi eq, %dim, %c1 : index
    %4 = arith.select %3, %dim_0, %dim : index
    %alloc = memref.alloc(%dim_0) {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
    %5 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
    "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast, %alloc) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
    "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
    %alloc_2 = memref.alloc(%dim) {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
    "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_1, %alloc_2) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
    "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
    %6 = arith.index_cast %4 : index to i32
    %7 = arith.muli %6, %c4_i32 : i32
    %8 = arith.index_cast %7 : i32 to index
    %9 = arith.cmpi eq, %dim_0, %4 : index
    %10 = arith.cmpi eq, %dim, %4 : index
    %11 = arith.andi %10, %9 : i1
    %alloc_3 = memref.alloc() : memref<1xf32, #gpu.address_space<global>>
    scf.if %11 {
      %13 = arith.cmpi slt, %8, %c1 : index
      scf.if %13 {
        gpu.launch_func  @main_kernel::@main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32 blocks in (%c1, %c1, %c1) threads in (%c512, %c1, %c1) args(%c512 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
        %14 = arith.cmpi eq, %8, %c0 : index
        %15 = arith.subi %8, %c1 : index
        %16 = arith.divui %15, %c32 : index
        %17 = arith.addi %16, %c1 : index
        %18 = arith.select %14, %c0, %17 : index
        %19 = arith.muli %18, %c512 : index
        %20 = affine.apply #map(%19)[%c0, %c512]
        gpu.launch_func  @main_kernel_0::@main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_1 blocks in (%20, %c1, %c1) threads in (%c512, %c1, %c1) args(%c512 : index, %19 : index, %8 : index, %4 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
      } else {
        gpu.launch_func  @main_kernel_1::@main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64 blocks in (%c1, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
        %14 = arith.cmpi eq, %8, %c0 : index
        %15 = arith.subi %8, %c1 : index
        %16 = arith.divui %15, %c512 : index
        %17 = arith.addi %16, %c1 : index
        %18 = arith.select %14, %c0, %17 : index
        %19 = arith.muli %18, %c256 : index
        %20 = affine.apply #map(%19)[%c0, %c256]
        gpu.launch_func  @main_kernel_2::@main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1 blocks in (%20, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %19 : index, %8 : index, %4 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
      }
    } else {
      %13 = arith.cmpi slt, %8, %c1 : index
      scf.if %13 {
        gpu.launch_func  @main_kernel_3::@main_kColReduction_reduce__6_1_0___thread_tile_h32 blocks in (%c1, %c1, %c1) threads in (%c512, %c1, %c1) args(%c512 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
        %14 = arith.cmpi eq, %8, %c0 : index
        %15 = arith.subi %8, %c1 : index
        %16 = arith.divui %15, %c32 : index
        %17 = arith.addi %16, %c1 : index
        %18 = arith.select %14, %c0, %17 : index
        %19 = arith.muli %18, %c512 : index
        %20 = affine.apply #map(%19)[%c0, %c512]
        gpu.launch_func  @main_kernel_4::@main_kColReduction_reduce__6_1_0___thread_tile_h32_1 blocks in (%20, %c1, %c1) threads in (%c512, %c1, %c1) args(%dim : index, %dim_0 : index, %c512 : index, %19 : index, %8 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
      } else {
        gpu.launch_func  @main_kernel_5::@main_kColReduction_reduce__6_1_0___block_tile_h64 blocks in (%c1, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
        %14 = arith.cmpi eq, %8, %c0 : index
        %15 = arith.subi %8, %c1 : index
        %16 = arith.divui %15, %c512 : index
        %17 = arith.addi %16, %c1 : index
        %18 = arith.select %14, %c0, %17 : index
        %19 = arith.muli %18, %c256 : index
        %20 = affine.apply #map(%19)[%c0, %c256]
        gpu.launch_func  @main_kernel_6::@main_kColReduction_reduce__6_1_0___block_tile_h64_1 blocks in (%20, %c1, %c1) threads in (%c256, %c1, %c1) args(%dim : index, %dim_0 : index, %c256 : index, %19 : index, %8 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
      }
    }
    memref.dealloc %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>
    memref.dealloc %alloc : memref<?x4xf32, #gpu.address_space<global>>
    %alloca = memref.alloca() : memref<0xindex>
    %12 = "disc_ral.dispatch"(%arg0, %5, %alloc_3, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1xf32, #gpu.address_space<global>>, memref<0xindex>) -> memref<f32, #gpu.address_space<global>>
    %reinterpret_cast_4 = memref.reinterpret_cast %12 to offset: [0], sizes: [], strides: [] : memref<f32, #gpu.address_space<global>> to memref<f32, #gpu.address_space<global>>
    memref.dealloc %alloc_3 : memref<1xf32, #gpu.address_space<global>>
    %alloc_5 = memref.alloc() : memref<f32>
    "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_4, %alloc_5) {backend_config = "", call_target_name = "d2h", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<f32, #gpu.address_space<global>>, memref<f32>) -> ()
    "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
    memref.dealloc %reinterpret_cast_4 : memref<f32, #gpu.address_space<global>>
    "disc_ral.dispatch"(%arg0, %c0, %alloc_5) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<f32>) -> ()
    return
  }
  gpu.module @main_kernel {
    gpu.func @main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32(%arg0: index, %arg1: memref<1xf32, #gpu.address_space<global>>) kernel {
      %cst = arith.constant 0.000000e+00 : f32
      %c1 = arith.constant 1 : index
      %c0 = arith.constant 0 : index
      %0 = gpu.block_id  x
      %1 = gpu.thread_id  x
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %2 = affine.apply #map1(%0)[%arg0, %c0]
      %3 = affine.apply #map1(%1)[%c1, %c0]
      %4 = arith.addi %3, %2 : index
      %5 = arith.addi %3, %2 : index
      %6 = arith.cmpi ult, %5, %c1 : index
      scf.if %6 {
        %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
        memref.store %cst, %reinterpret_cast[%4] : memref<1xf32, #gpu.address_space<global>>
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_0 {
    gpu.func @main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_1(%arg0: index, %arg1: index, %arg2: index, %arg3: index, %arg4: memref<?x4xf32, #gpu.address_space<global>>, %arg5: memref<?x4xf32, #gpu.address_space<global>>, %arg6: memref<1xf32, #gpu.address_space<global>>) kernel {
      %cst = arith.constant 0.000000e+00 : f32
      %c4 = arith.constant 4 : index
      %c32 = arith.constant 32 : index
      %c512 = arith.constant 512 : index
      %c1 = arith.constant 1 : index
      %c0 = arith.constant 0 : index
      %0 = gpu.block_id  x
      %1 = gpu.thread_id  x
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %2 = affine.apply #map1(%0)[%arg0, %c0]
      %3 = affine.apply #map1(%1)[%c1, %c0]
      %4 = arith.addi %3, %2 : index
      %5 = arith.addi %3, %2 : index
      %6 = arith.cmpi ult, %5, %arg1 : index
      scf.if %6 {
        %7 = arith.remsi %4, %c512 : index
        %8 = arith.divsi %4, %c512 : index
        %9 = arith.cmpi ult, %7, %c1 : index
        scf.if %9 {
          %10 = scf.for %arg7 = %c0 to %c32 step %c1 iter_args(%arg8 = %cst) -> (f32) {
            %12 = arith.muli %8, %c32 : index
            %13 = arith.addi %12, %arg7 : index
            %14 = arith.cmpi slt, %13, %arg2 : index
            %15 = scf.if %14 -> (f32) {
              %16 = arith.muli %arg3, %c4 : index
              %reinterpret_cast = memref.reinterpret_cast %arg4 to offset: [%c0], sizes: [%16], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
              %17 = memref.load %reinterpret_cast[%13] : memref<?xf32, #gpu.address_space<global>>
              %reinterpret_cast_0 = memref.reinterpret_cast %arg5 to offset: [%c0], sizes: [%16], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
              %18 = memref.load %reinterpret_cast_0[%13] : memref<?xf32, #gpu.address_space<global>>
              %19 = arith.mulf %17, %18 : f32
              %20 = arith.addf %arg8, %19 : f32
              scf.yield %20 : f32
            } else {
              scf.yield %arg8 : f32
            }
            scf.yield %15 : f32
          }
          %11 = memref.atomic_rmw addf %10, %arg6[%7] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
        }
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_1 {
    gpu.func @main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64(%arg0: index, %arg1: memref<1xf32, #gpu.address_space<global>>) kernel {
      %cst = arith.constant 0.000000e+00 : f32
      %c1 = arith.constant 1 : index
      %c0 = arith.constant 0 : index
      %0 = gpu.block_id  x
      %1 = gpu.thread_id  x
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %2 = affine.apply #map1(%0)[%arg0, %c0]
      %3 = affine.apply #map1(%1)[%c1, %c0]
      %4 = arith.addi %3, %2 : index
      %5 = arith.addi %3, %2 : index
      %6 = arith.cmpi ult, %5, %c1 : index
      scf.if %6 {
        %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
        memref.store %cst, %reinterpret_cast[%4] : memref<1xf32, #gpu.address_space<global>>
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_2 {
    gpu.func @main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1(%arg0: index, %arg1: index, %arg2: index, %arg3: index, %arg4: memref<?x4xf32, #gpu.address_space<global>>, %arg5: memref<?x4xf32, #gpu.address_space<global>>, %arg6: memref<1xf32, #gpu.address_space<global>>) workgroup(%arg7 : memref<256xf32, #gpu.address_space<workgroup>>) kernel {
      %c2 = arith.constant 2 : index
      %cst = arith.constant 0.000000e+00 : f32
      %c4 = arith.constant 4 : index
      %c64 = arith.constant 64 : index
      %c8 = arith.constant 8 : index
      %c32 = arith.constant 32 : index
      %c256 = arith.constant 256 : index
      %c1 = arith.constant 1 : index
      %c0 = arith.constant 0 : index
      %0 = gpu.block_id  x
      %1 = gpu.thread_id  x
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %2 = affine.apply #map1(%0)[%arg0, %c0]
      %3 = affine.apply #map1(%1)[%c1, %c0]
      %4 = arith.addi %3, %2 : index
      %5 = arith.addi %3, %2 : index
      %6 = arith.cmpi ult, %5, %arg1 : index
      scf.if %6 {
        %7 = arith.remsi %4, %c256 : index
        %8 = arith.divsi %4, %c256 : index
        %9 = arith.divui %7, %c32 : index
        %10 = arith.remui %7, %c32 : index
        %11 = arith.muli %10, %c8 : index
        %12 = arith.addi %9, %11 : index
        %13 = arith.cmpi ult, %10, %c1 : index
        %14 = scf.if %13 -> (f32) {
          %20 = scf.for %arg8 = %c0 to %c64 step %c1 iter_args(%arg9 = %cst) -> (f32) {
            %21 = arith.muli %8, %c8 : index
            %22 = arith.addi %9, %21 : index
            %23 = arith.muli %22, %c64 : index
            %24 = arith.addi %arg8, %23 : index
            %25 = arith.cmpi slt, %24, %arg2 : index
            %26 = scf.if %25 -> (f32) {
              %27 = arith.muli %arg3, %c4 : index
              %reinterpret_cast_0 = memref.reinterpret_cast %arg4 to offset: [%c0], sizes: [%27], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
              %28 = memref.load %reinterpret_cast_0[%24] : memref<?xf32, #gpu.address_space<global>>
              %reinterpret_cast_1 = memref.reinterpret_cast %arg5 to offset: [%c0], sizes: [%27], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
              %29 = memref.load %reinterpret_cast_1[%24] : memref<?xf32, #gpu.address_space<global>>
              %30 = arith.mulf %28, %29 : f32
              %31 = arith.addf %arg9, %30 : f32
              scf.yield %31 : f32
            } else {
              scf.yield %arg9 : f32
            }
            scf.yield %26 : f32
          }
          scf.yield %20 : f32
        } else {
          scf.yield %cst : f32
        }
        %reinterpret_cast = memref.reinterpret_cast %arg7 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        memref.store %14, %reinterpret_cast[%12] : memref<256xf32, #gpu.address_space<workgroup>>
        gpu.barrier
        %15 = arith.cmpi slt, %9, %c4 : index
        scf.if %15 {
          %20 = arith.addi %12, %c4 : index
          %21 = memref.load %reinterpret_cast[%12] : memref<256xf32, #gpu.address_space<workgroup>>
          %22 = memref.load %reinterpret_cast[%20] : memref<256xf32, #gpu.address_space<workgroup>>
          %23 = arith.addf %21, %22 : f32
          memref.store %23, %reinterpret_cast[%12] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %16 = arith.cmpi slt, %9, %c2 : index
        scf.if %16 {
          %20 = arith.addi %12, %c2 : index
          %21 = memref.load %reinterpret_cast[%12] : memref<256xf32, #gpu.address_space<workgroup>>
          %22 = memref.load %reinterpret_cast[%20] : memref<256xf32, #gpu.address_space<workgroup>>
          %23 = arith.addf %21, %22 : f32
          memref.store %23, %reinterpret_cast[%12] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %17 = arith.cmpi slt, %9, %c1 : index
        scf.if %17 {
          %20 = arith.addi %12, %c1 : index
          %21 = memref.load %reinterpret_cast[%12] : memref<256xf32, #gpu.address_space<workgroup>>
          %22 = memref.load %reinterpret_cast[%20] : memref<256xf32, #gpu.address_space<workgroup>>
          %23 = arith.addf %21, %22 : f32
          memref.store %23, %reinterpret_cast[%12] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %18 = arith.cmpi eq, %9, %c0 : index
        %19 = arith.andi %18, %13 : i1
        scf.if %19 {
          %20 = memref.load %reinterpret_cast[%12] : memref<256xf32, #gpu.address_space<workgroup>>
          %21 = memref.atomic_rmw addf %20, %arg6[%10] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
        }
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_3 {
    gpu.func @main_kColReduction_reduce__6_1_0___thread_tile_h32(%arg0: index, %arg1: memref<1xf32, #gpu.address_space<global>>) kernel {
      %cst = arith.constant 0.000000e+00 : f32
      %c1 = arith.constant 1 : index
      %c0 = arith.constant 0 : index
      %0 = gpu.block_id  x
      %1 = gpu.thread_id  x
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %2 = affine.apply #map1(%0)[%arg0, %c0]
      %3 = affine.apply #map1(%1)[%c1, %c0]
      %4 = arith.addi %3, %2 : index
      %5 = arith.addi %3, %2 : index
      %6 = arith.cmpi ult, %5, %c1 : index
      scf.if %6 {
        %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
        memref.store %cst, %reinterpret_cast[%4] : memref<1xf32, #gpu.address_space<global>>
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_4 {
    gpu.func @main_kColReduction_reduce__6_1_0___thread_tile_h32_1(%arg0: index, %arg1: index, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?x4xf32, #gpu.address_space<global>>, %arg6: memref<?x4xf32, #gpu.address_space<global>>, %arg7: memref<1xf32, #gpu.address_space<global>>) kernel {
      %cst = arith.constant 0.000000e+00 : f32
      %c4 = arith.constant 4 : index
      %c32 = arith.constant 32 : index
      %c512 = arith.constant 512 : index
      %c1 = arith.constant 1 : index
      %c0 = arith.constant 0 : index
      %0 = gpu.block_id  x
      %1 = gpu.thread_id  x
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %2 = arith.cmpi eq, %arg0, %c1 : index
      %3 = arith.select %2, %arg1, %arg0 : index
      %4 = arith.cmpi eq, %arg1, %3 : index
      %5 = arith.cmpi eq, %arg0, %3 : index
      %6 = affine.apply #map1(%0)[%arg2, %c0]
      %7 = affine.apply #map1(%1)[%c1, %c0]
      %8 = arith.addi %7, %6 : index
      %9 = arith.addi %7, %6 : index
      %10 = arith.cmpi ult, %9, %arg3 : index
      scf.if %10 {
        %11 = arith.remsi %8, %c512 : index
        %12 = arith.divsi %8, %c512 : index
        %13 = arith.cmpi ult, %11, %c1 : index
        scf.if %13 {
          %14 = scf.for %arg8 = %c0 to %c32 step %c1 iter_args(%arg9 = %cst) -> (f32) {
            %16 = arith.muli %12, %c32 : index
            %17 = arith.addi %16, %arg8 : index
            %18 = arith.cmpi slt, %17, %arg4 : index
            %19 = scf.if %18 -> (f32) {
              %20 = arith.remui %17, %c4 : index
              %21 = arith.divui %17, %c4 : index
              %22 = arith.select %4, %21, %c0 : index
              %23 = arith.muli %22, %c4 : index
              %24 = arith.addi %23, %20 : index
              %25 = arith.muli %arg1, %c4 : index
              %reinterpret_cast = memref.reinterpret_cast %arg5 to offset: [%c0], sizes: [%25], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
              %26 = memref.load %reinterpret_cast[%24] : memref<?xf32, #gpu.address_space<global>>
              %27 = arith.select %5, %21, %c0 : index
              %28 = arith.muli %27, %c4 : index
              %29 = arith.addi %28, %20 : index
              %30 = arith.muli %arg0, %c4 : index
              %reinterpret_cast_0 = memref.reinterpret_cast %arg6 to offset: [%c0], sizes: [%30], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
              %31 = memref.load %reinterpret_cast_0[%29] : memref<?xf32, #gpu.address_space<global>>
              %32 = arith.mulf %26, %31 : f32
              %33 = arith.addf %arg9, %32 : f32
              scf.yield %33 : f32
            } else {
              scf.yield %arg9 : f32
            }
            scf.yield %19 : f32
          }
          %15 = memref.atomic_rmw addf %14, %arg7[%11] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
        }
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_5 {
    gpu.func @main_kColReduction_reduce__6_1_0___block_tile_h64(%arg0: index, %arg1: memref<1xf32, #gpu.address_space<global>>) kernel {
      %cst = arith.constant 0.000000e+00 : f32
      %c1 = arith.constant 1 : index
      %c0 = arith.constant 0 : index
      %0 = gpu.block_id  x
      %1 = gpu.thread_id  x
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %2 = affine.apply #map1(%0)[%arg0, %c0]
      %3 = affine.apply #map1(%1)[%c1, %c0]
      %4 = arith.addi %3, %2 : index
      %5 = arith.addi %3, %2 : index
      %6 = arith.cmpi ult, %5, %c1 : index
      scf.if %6 {
        %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
        memref.store %cst, %reinterpret_cast[%4] : memref<1xf32, #gpu.address_space<global>>
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_6 {
    gpu.func @main_kColReduction_reduce__6_1_0___block_tile_h64_1(%arg0: index, %arg1: index, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?x4xf32, #gpu.address_space<global>>, %arg6: memref<?x4xf32, #gpu.address_space<global>>, %arg7: memref<1xf32, #gpu.address_space<global>>) workgroup(%arg8 : memref<256xf32, #gpu.address_space<workgroup>>) kernel {
      %c2 = arith.constant 2 : index
      %cst = arith.constant 0.000000e+00 : f32
      %c4 = arith.constant 4 : index
      %c64 = arith.constant 64 : index
      %c8 = arith.constant 8 : index
      %c32 = arith.constant 32 : index
      %c256 = arith.constant 256 : index
      %c1 = arith.constant 1 : index
      %c0 = arith.constant 0 : index
      %0 = gpu.block_id  x
      %1 = gpu.thread_id  x
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %2 = arith.cmpi eq, %arg0, %c1 : index
      %3 = arith.select %2, %arg1, %arg0 : index
      %4 = arith.cmpi eq, %arg1, %3 : index
      %5 = arith.cmpi eq, %arg0, %3 : index
      %6 = affine.apply #map1(%0)[%arg2, %c0]
      %7 = affine.apply #map1(%1)[%c1, %c0]
      %8 = arith.addi %7, %6 : index
      %9 = arith.addi %7, %6 : index
      %10 = arith.cmpi ult, %9, %arg3 : index
      scf.if %10 {
        %11 = arith.remsi %8, %c256 : index
        %12 = arith.divsi %8, %c256 : index
        %13 = arith.divui %11, %c32 : index
        %14 = arith.remui %11, %c32 : index
        %15 = arith.muli %14, %c8 : index
        %16 = arith.addi %13, %15 : index
        %17 = arith.cmpi ult, %14, %c1 : index
        %18 = scf.if %17 -> (f32) {
          %24 = scf.for %arg9 = %c0 to %c64 step %c1 iter_args(%arg10 = %cst) -> (f32) {
            %25 = arith.muli %12, %c8 : index
            %26 = arith.addi %13, %25 : index
            %27 = arith.muli %26, %c64 : index
            %28 = arith.addi %arg9, %27 : index
            %29 = arith.cmpi slt, %28, %arg4 : index
            %30 = scf.if %29 -> (f32) {
              %31 = arith.remui %28, %c4 : index
              %32 = arith.divui %28, %c4 : index
              %33 = arith.select %4, %32, %c0 : index
              %34 = arith.muli %33, %c4 : index
              %35 = arith.addi %34, %31 : index
              %36 = arith.muli %arg1, %c4 : index
              %reinterpret_cast_0 = memref.reinterpret_cast %arg5 to offset: [%c0], sizes: [%36], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
              %37 = memref.load %reinterpret_cast_0[%35] : memref<?xf32, #gpu.address_space<global>>
              %38 = arith.select %5, %32, %c0 : index
              %39 = arith.muli %38, %c4 : index
              %40 = arith.addi %39, %31 : index
              %41 = arith.muli %arg0, %c4 : index
              %reinterpret_cast_1 = memref.reinterpret_cast %arg6 to offset: [%c0], sizes: [%41], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
              %42 = memref.load %reinterpret_cast_1[%40] : memref<?xf32, #gpu.address_space<global>>
              %43 = arith.mulf %37, %42 : f32
              %44 = arith.addf %arg10, %43 : f32
              scf.yield %44 : f32
            } else {
              scf.yield %arg10 : f32
            }
            scf.yield %30 : f32
          }
          scf.yield %24 : f32
        } else {
          scf.yield %cst : f32
        }
        %reinterpret_cast = memref.reinterpret_cast %arg8 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        memref.store %18, %reinterpret_cast[%16] : memref<256xf32, #gpu.address_space<workgroup>>
        gpu.barrier
        %19 = arith.cmpi slt, %13, %c4 : index
        scf.if %19 {
          %24 = arith.addi %16, %c4 : index
          %25 = memref.load %reinterpret_cast[%16] : memref<256xf32, #gpu.address_space<workgroup>>
          %26 = memref.load %reinterpret_cast[%24] : memref<256xf32, #gpu.address_space<workgroup>>
          %27 = arith.addf %25, %26 : f32
          memref.store %27, %reinterpret_cast[%16] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %20 = arith.cmpi slt, %13, %c2 : index
        scf.if %20 {
          %24 = arith.addi %16, %c2 : index
          %25 = memref.load %reinterpret_cast[%16] : memref<256xf32, #gpu.address_space<workgroup>>
          %26 = memref.load %reinterpret_cast[%24] : memref<256xf32, #gpu.address_space<workgroup>>
          %27 = arith.addf %25, %26 : f32
          memref.store %27, %reinterpret_cast[%16] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %21 = arith.cmpi slt, %13, %c1 : index
        scf.if %21 {
          %24 = arith.addi %16, %c1 : index
          %25 = memref.load %reinterpret_cast[%16] : memref<256xf32, #gpu.address_space<workgroup>>
          %26 = memref.load %reinterpret_cast[%24] : memref<256xf32, #gpu.address_space<workgroup>>
          %27 = arith.addf %25, %26 : f32
          memref.store %27, %reinterpret_cast[%16] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %22 = arith.cmpi eq, %13, %c0 : index
        %23 = arith.andi %22, %17 : i1
        scf.if %23 {
          %24 = memref.load %reinterpret_cast[%16] : memref<256xf32, #gpu.address_space<workgroup>>
          %25 = memref.atomic_rmw addf %24, %arg7[%14] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
        }
      }
      gpu.return
    }
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = true, sym_name = "C4", value = 4 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = true, sym_name = "C1", value = 1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S4", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = false, sym_name = "S5", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %c4 = arith.constant 4 : index
    %0 = "disc_shape.dim"() {name = @S2} : () -> index
    %1 = "disc_shape.dim"() {name = @S3} : () -> index
    "disc_shape.tie_product_equal"(%c4, %0, %1) {operand_segment_sizes = array<i32: 2, 1>} : (index, index, index) -> ()
    %2 = "disc_shape.dim"() {name = @S4} : () -> index
    %3 = "disc_shape.dim"() {name = @S5} : () -> index
    "disc_shape.tie_product_equal"(%c4, %2, %3) {operand_segment_sizes = array<i32: 2, 1>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After CSE (cse) //----- //
gpu.module @main_kernel {
  gpu.func @main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32(%arg0: index, %arg1: memref<1xf32, #gpu.address_space<global>>) kernel {
    %cst = arith.constant 0.000000e+00 : f32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg0, %c0]
    %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %4 = arith.addi %3, %2 : index
    %5 = arith.cmpi ult, %4, %c1 : index
    scf.if %5 {
      %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
      memref.store %cst, %reinterpret_cast[%4] : memref<1xf32, #gpu.address_space<global>>
    }
    gpu.return
  }
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
gpu.module @main_kernel {
  gpu.func @main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32(%arg0: index, %arg1: memref<1xf32, #gpu.address_space<global>>) kernel {
    %cst = arith.constant 0.000000e+00 : f32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg0, %c0]
    %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %4 = arith.addi %3, %2 : index
    %5 = arith.cmpi ult, %4, %c1 : index
    cf.cond_br %5, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
    memref.store %cst, %reinterpret_cast[%4] : memref<1xf32, #gpu.address_space<global>>
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    gpu.return
  }
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
gpu.module @main_kernel {
  gpu.func @main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32(%arg0: index, %arg1: memref<1xf32, #gpu.address_space<global>>) kernel {
    %cst = arith.constant 0.000000e+00 : f32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = arith.muli %0, %arg0 : index
    %3 = arith.addi %2, %c0 : index
    %4 = arith.muli %1, %c1 : index
    %5 = arith.addi %4, %c0 : index
    %6 = arith.addi %5, %3 : index
    %7 = arith.cmpi ult, %6, %c1 : index
    cf.cond_br %7, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
    memref.store %cst, %reinterpret_cast[%6] : memref<1xf32, #gpu.address_space<global>>
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    gpu.return
  }
}

// -----// IR Dump After StripDebugInfo (strip-debuginfo) //----- //
gpu.module @main_kernel {
  gpu.func @main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32(%arg0: index, %arg1: memref<1xf32, #gpu.address_space<global>>) kernel {
    %cst = arith.constant 0.000000e+00 : f32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = arith.muli %0, %arg0 : index
    %3 = arith.addi %2, %c0 : index
    %4 = arith.muli %1, %c1 : index
    %5 = arith.addi %4, %c0 : index
    %6 = arith.addi %5, %3 : index
    %7 = arith.cmpi ult, %6, %c1 : index
    cf.cond_br %7, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
    memref.store %cst, %reinterpret_cast[%6] : memref<1xf32, #gpu.address_space<global>>
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    gpu.return
  }
}

// -----// IR Dump After DiscLowerGpuOpsToNVVMOpsPass (disc-convert-gpu-to-nvvm) //----- //
gpu.module @main_kernel {
  llvm.func @main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32(%arg0: i32, %arg1: !llvm.ptr<1>, %arg2: !llvm.ptr<1>, %arg3: i32, %arg4: i32, %arg5: i32) attributes {gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.undef : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)>
    %1 = llvm.insertvalue %arg1, %0[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %2 = llvm.insertvalue %arg2, %1[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %3 = llvm.insertvalue %arg3, %2[2] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %4 = llvm.insertvalue %arg4, %3[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %5 = llvm.insertvalue %arg5, %4[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %6 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    %7 = llvm.mlir.constant(1 : index) : i32
    %8 = nvvm.read.ptx.sreg.ctaid.x : i32
    %9 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %10 = llvm.mul %8, %arg0  : i32
    %11 = llvm.add %9, %10  : i32
    %12 = llvm.icmp "ult" %11, %7 : i32
    llvm.cond_br %12, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %13 = llvm.mlir.undef : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)>
    %14 = llvm.extractvalue %5[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %15 = llvm.extractvalue %5[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %16 = llvm.insertvalue %14, %13[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %17 = llvm.insertvalue %15, %16[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %18 = llvm.mlir.constant(0 : index) : i32
    %19 = llvm.insertvalue %18, %17[2] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %20 = llvm.mlir.constant(1 : index) : i32
    %21 = llvm.insertvalue %20, %19[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %22 = llvm.mlir.constant(1 : index) : i32
    %23 = llvm.insertvalue %22, %21[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %24 = llvm.extractvalue %23[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %25 = llvm.getelementptr %24[%11] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    llvm.store %6, %25 : f32, !llvm.ptr<1>
    llvm.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    llvm.return
  }
}

// -----// IR Dump After LLVMInsertValueSimplifierPass (disc-llvm-insert-value-simplifier) //----- //
llvm.func @main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32(%arg0: i32, %arg1: !llvm.ptr<1>, %arg2: !llvm.ptr<1>, %arg3: i32, %arg4: i32, %arg5: i32) attributes {gpu.kernel, nvvm.kernel} {
  %0 = llvm.mlir.constant(1 : index) : i32
  %1 = llvm.mlir.constant(0.000000e+00 : f32) : f32
  %2 = nvvm.read.ptx.sreg.ctaid.x : i32
  %3 = nvvm.read.ptx.sreg.tid.x : i32
  llvm.br ^bb1
^bb1:  // pred: ^bb0
  %4 = llvm.mul %2, %arg0  : i32
  %5 = llvm.add %3, %4  : i32
  %6 = llvm.icmp "ult" %5, %0 : i32
  llvm.cond_br %6, ^bb2, ^bb3
^bb2:  // pred: ^bb1
  %7 = llvm.getelementptr %arg2[%5] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
  llvm.store %1, %7 : f32, !llvm.ptr<1>
  llvm.br ^bb3
^bb3:  // 2 preds: ^bb1, ^bb2
  llvm.return
}

// -----// IR Dump After FunctionDeadArgumentEliminationPass (disc-function-dead-argument-elimination) //----- //
gpu.module @main_kernel {
  llvm.func @main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32(%arg0: i32, %arg1: !llvm.ptr<1>) attributes {disc.elimargs = [1 : index, 3 : index, 4 : index, 5 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(1 : index) : i32
    %1 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    %2 = nvvm.read.ptx.sreg.ctaid.x : i32
    %3 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %4 = llvm.mul %2, %arg0  : i32
    %5 = llvm.add %3, %4  : i32
    %6 = llvm.icmp "ult" %5, %0 : i32
    llvm.cond_br %6, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %7 = llvm.getelementptr %arg1[%5] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    llvm.store %1, %7 : f32, !llvm.ptr<1>
    llvm.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    llvm.return
  }
}

// -----// IR Dump After GpuKernelToBlobPass (disc-gpu-kernel-to-blob) //----- //
gpu.module @main_kernel attributes {gpu.binary = "P\EDU\BA\01\00\10\00H\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\08\04\00\00\00\00\00\00\08\04\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00h\0C\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\002\C0\0B\00\01\00\11\09\06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\11e__6_1_0___no_ibXthread_tile_h32H\00\0FB\00+osharedD\00+\9Fconstant0G\00(\FA\01debug_frame\00.rel\11\00!nv\14\00\11aS\00\0Fk\01 \0F\98\00%\0F\A4\01\FAo_param\AB\01\1C\0F\01\00\06\8Ck\00\00\00\03\00\0A\00\01\00 0\01\18\00,\09\00\01\00\11~\18\00,\04\00\01\00\11\9C\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\1C\00\00\00E\00#\04\10\18\00\80/\08\00\05\00\00\00\06\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04x\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C$\00u\01\00\08\00\00\F0!\10\00\01\01\00\C0\F0\11\00\03\1B\FF\00\04\1C\08\00p\B0\04\12\00\01\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00 \01/\05\00\01\00\FF\B0@$v\01\FF\0F\04\A3\FF\00\8E\07\00\C4\0F\00\C3y\EC\03\A2%\00\00\00\22\0E\00\19y\00\01\00\F3\0E!\00\00\00b\0E\00\90r\05?\04\00\00\80?\E0\FF\0F\00\C6\1F\00\B9z\04\00\00X\DE\02p\E4\0F\00\A4r\04\05B\00\B1?\02\8E\0F\00\CC\0F\00\0C|\00>\04\95pR\F0\0B\00\DA/\00Mc\04\F3\0D\80\03\00\EA\0F\00\02z\02\00\00Z\00\00\00\0F\00\00\00\E2\0F\00$v\03\FF\00[\90\00\22\E2\0F`\001F\00\00\B3\04\80\C8\0F\00\86y\00\02\FF\B0\030\19\10\0C \00*MyP\00PGy\00\00\F09\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\80\0F\01\00-\00\07\05.\03\00\01\00\22@\00\01\00=k\01\000\00\08\01\00\1F\0B@\00\04*\AB\01\08\00\0F@\00\05\13\13\0C\04\0C\01\00\13XU\00*\90\000\04\04M\04\22\18\00\01\00.>\01T\00\00\01\00\13\E8@\00/p\00\80\00\0B\1F)'\00\03A\00X\04\00\01\00\04@\06\04\E4\00*\04\00\01\00\1Fq@\00\04\13\88)\00&L\00@\00\1F\0A@\00\00!\\\01D\01\0D@\00\13\D8)\00*\D8\00\01\00\1B\08\08\00%K\01\DB\0A\09\01\00\13\B0]\05\17\10\80\00\17\048\00\04\18\00\13\F7\14\01\0C\84\01*\C0\05(\07\1F\00\C0\00\04\132@\00*\06\00\01\00*\80\07\C0\07\12\03\C0\01:\06\80\00\01\00\13\06\D8\01\05\A8\0B\0B\01\00*\A8\00\08\00\04\F8\00\14\018\00/\05\00\01\00\029@\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00"} {
  llvm.func @main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32(%arg0: i32, %arg1: !llvm.ptr<1>) attributes {disc.elimargs = [1 : index, 3 : index, 4 : index, 5 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(1 : index) : i32
    %1 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    %2 = nvvm.read.ptx.sreg.ctaid.x : i32
    %3 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %4 = llvm.mul %2, %arg0  : i32
    %5 = llvm.add %3, %4  : i32
    %6 = llvm.icmp "ult" %5, %0 : i32
    llvm.cond_br %6, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %7 = llvm.getelementptr %arg1[%5] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    llvm.store %1, %7 : f32, !llvm.ptr<1>
    llvm.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    llvm.return
  }
}

// -----// IR Dump After LoopInvariantCodeMotion (loop-invariant-code-motion) //----- //
gpu.module @main_kernel_0 {
  gpu.func @main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_1(%arg0: index, %arg1: index, %arg2: index, %arg3: index, %arg4: memref<?x4xf32, #gpu.address_space<global>>, %arg5: memref<?x4xf32, #gpu.address_space<global>>, %arg6: memref<1xf32, #gpu.address_space<global>>) kernel {
    %cst = arith.constant 0.000000e+00 : f32
    %c4 = arith.constant 4 : index
    %c32 = arith.constant 32 : index
    %c512 = arith.constant 512 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg0, %c0]
    %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %4 = arith.addi %3, %2 : index
    %5 = arith.addi %3, %2 : index
    %6 = arith.cmpi ult, %5, %arg1 : index
    scf.if %6 {
      %7 = arith.remsi %4, %c512 : index
      %8 = arith.divsi %4, %c512 : index
      %9 = arith.cmpi ult, %7, %c1 : index
      scf.if %9 {
        %10 = arith.muli %8, %c32 : index
        %11 = scf.for %arg7 = %c0 to %c32 step %c1 iter_args(%arg8 = %cst) -> (f32) {
          %13 = arith.addi %10, %arg7 : index
          %14 = arith.cmpi slt, %13, %arg2 : index
          %15 = scf.if %14 -> (f32) {
            %16 = arith.muli %arg3, %c4 : index
            %reinterpret_cast = memref.reinterpret_cast %arg4 to offset: [%c0], sizes: [%16], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
            %17 = memref.load %reinterpret_cast[%13] : memref<?xf32, #gpu.address_space<global>>
            %reinterpret_cast_0 = memref.reinterpret_cast %arg5 to offset: [%c0], sizes: [%16], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
            %18 = memref.load %reinterpret_cast_0[%13] : memref<?xf32, #gpu.address_space<global>>
            %19 = arith.mulf %17, %18 : f32
            %20 = arith.addf %arg8, %19 : f32
            scf.yield %20 : f32
          } else {
            scf.yield %arg8 : f32
          }
          scf.yield %15 : f32
        }
        %12 = memref.atomic_rmw addf %11, %arg6[%7] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
      }
    }
    gpu.return
  }
}

// -----// IR Dump After CSE (cse) //----- //
gpu.module @main_kernel_0 {
  gpu.func @main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_1(%arg0: index, %arg1: index, %arg2: index, %arg3: index, %arg4: memref<?x4xf32, #gpu.address_space<global>>, %arg5: memref<?x4xf32, #gpu.address_space<global>>, %arg6: memref<1xf32, #gpu.address_space<global>>) kernel {
    %cst = arith.constant 0.000000e+00 : f32
    %c4 = arith.constant 4 : index
    %c32 = arith.constant 32 : index
    %c512 = arith.constant 512 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg0, %c0]
    %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %4 = arith.addi %3, %2 : index
    %5 = arith.cmpi ult, %4, %arg1 : index
    scf.if %5 {
      %6 = arith.remsi %4, %c512 : index
      %7 = arith.divsi %4, %c512 : index
      %8 = arith.cmpi ult, %6, %c1 : index
      scf.if %8 {
        %9 = arith.muli %7, %c32 : index
        %10 = scf.for %arg7 = %c0 to %c32 step %c1 iter_args(%arg8 = %cst) -> (f32) {
          %12 = arith.addi %9, %arg7 : index
          %13 = arith.cmpi slt, %12, %arg2 : index
          %14 = scf.if %13 -> (f32) {
            %15 = arith.muli %arg3, %c4 : index
            %reinterpret_cast = memref.reinterpret_cast %arg4 to offset: [%c0], sizes: [%15], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
            %16 = memref.load %reinterpret_cast[%12] : memref<?xf32, #gpu.address_space<global>>
            %reinterpret_cast_0 = memref.reinterpret_cast %arg5 to offset: [%c0], sizes: [%15], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
            %17 = memref.load %reinterpret_cast_0[%12] : memref<?xf32, #gpu.address_space<global>>
            %18 = arith.mulf %16, %17 : f32
            %19 = arith.addf %arg8, %18 : f32
            scf.yield %19 : f32
          } else {
            scf.yield %arg8 : f32
          }
          scf.yield %14 : f32
        }
        %11 = memref.atomic_rmw addf %10, %arg6[%6] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
      }
    }
    gpu.return
  }
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
gpu.module @main_kernel_0 {
  gpu.func @main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_1(%arg0: index, %arg1: index, %arg2: index, %arg3: index, %arg4: memref<?x4xf32, #gpu.address_space<global>>, %arg5: memref<?x4xf32, #gpu.address_space<global>>, %arg6: memref<1xf32, #gpu.address_space<global>>) kernel {
    %cst = arith.constant 0.000000e+00 : f32
    %c4 = arith.constant 4 : index
    %c32 = arith.constant 32 : index
    %c512 = arith.constant 512 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg0, %c0]
    %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %4 = arith.addi %3, %2 : index
    %5 = arith.cmpi ult, %4, %arg1 : index
    cf.cond_br %5, ^bb2, ^bb12
  ^bb2:  // pred: ^bb1
    %6 = arith.remsi %4, %c512 : index
    %7 = arith.divsi %4, %c512 : index
    %8 = arith.cmpi ult, %6, %c1 : index
    cf.cond_br %8, ^bb3, ^bb11
  ^bb3:  // pred: ^bb2
    %9 = arith.muli %7, %c32 : index
    cf.br ^bb4(%c0, %cst : index, f32)
  ^bb4(%10: index, %11: f32):  // 2 preds: ^bb3, ^bb9
    %12 = arith.cmpi slt, %10, %c32 : index
    cf.cond_br %12, ^bb5, ^bb10
  ^bb5:  // pred: ^bb4
    %13 = arith.addi %9, %10 : index
    %14 = arith.cmpi slt, %13, %arg2 : index
    cf.cond_br %14, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %15 = arith.muli %arg3, %c4 : index
    %reinterpret_cast = memref.reinterpret_cast %arg4 to offset: [%c0], sizes: [%15], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
    %16 = memref.load %reinterpret_cast[%13] : memref<?xf32, #gpu.address_space<global>>
    %reinterpret_cast_0 = memref.reinterpret_cast %arg5 to offset: [%c0], sizes: [%15], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
    %17 = memref.load %reinterpret_cast_0[%13] : memref<?xf32, #gpu.address_space<global>>
    %18 = arith.mulf %16, %17 : f32
    %19 = arith.addf %11, %18 : f32
    cf.br ^bb8(%19 : f32)
  ^bb7:  // pred: ^bb5
    cf.br ^bb8(%11 : f32)
  ^bb8(%20: f32):  // 2 preds: ^bb6, ^bb7
    cf.br ^bb9
  ^bb9:  // pred: ^bb8
    %21 = arith.addi %10, %c1 : index
    cf.br ^bb4(%21, %20 : index, f32)
  ^bb10:  // pred: ^bb4
    %22 = memref.atomic_rmw addf %11, %arg6[%6] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
    cf.br ^bb11
  ^bb11:  // 2 preds: ^bb2, ^bb10
    cf.br ^bb12
  ^bb12:  // 2 preds: ^bb1, ^bb11
    gpu.return
  }
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
gpu.module @main_kernel_0 {
  gpu.func @main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_1(%arg0: index, %arg1: index, %arg2: index, %arg3: index, %arg4: memref<?x4xf32, #gpu.address_space<global>>, %arg5: memref<?x4xf32, #gpu.address_space<global>>, %arg6: memref<1xf32, #gpu.address_space<global>>) kernel {
    %cst = arith.constant 0.000000e+00 : f32
    %c4 = arith.constant 4 : index
    %c32 = arith.constant 32 : index
    %c512 = arith.constant 512 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = arith.muli %0, %arg0 : index
    %3 = arith.addi %2, %c0 : index
    %4 = arith.muli %1, %c1 : index
    %5 = arith.addi %4, %c0 : index
    %6 = arith.addi %5, %3 : index
    %7 = arith.cmpi ult, %6, %arg1 : index
    cf.cond_br %7, ^bb2, ^bb12
  ^bb2:  // pred: ^bb1
    %8 = arith.remsi %6, %c512 : index
    %9 = arith.divsi %6, %c512 : index
    %10 = arith.cmpi ult, %8, %c1 : index
    cf.cond_br %10, ^bb3, ^bb11
  ^bb3:  // pred: ^bb2
    %11 = arith.muli %9, %c32 : index
    cf.br ^bb4(%c0, %cst : index, f32)
  ^bb4(%12: index, %13: f32):  // 2 preds: ^bb3, ^bb9
    %14 = arith.cmpi slt, %12, %c32 : index
    cf.cond_br %14, ^bb5, ^bb10
  ^bb5:  // pred: ^bb4
    %15 = arith.addi %11, %12 : index
    %16 = arith.cmpi slt, %15, %arg2 : index
    cf.cond_br %16, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %17 = arith.muli %arg3, %c4 : index
    %reinterpret_cast = memref.reinterpret_cast %arg4 to offset: [%c0], sizes: [%17], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
    %18 = memref.load %reinterpret_cast[%15] : memref<?xf32, #gpu.address_space<global>>
    %reinterpret_cast_0 = memref.reinterpret_cast %arg5 to offset: [%c0], sizes: [%17], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
    %19 = memref.load %reinterpret_cast_0[%15] : memref<?xf32, #gpu.address_space<global>>
    %20 = arith.mulf %18, %19 : f32
    %21 = arith.addf %13, %20 : f32
    cf.br ^bb8(%21 : f32)
  ^bb7:  // pred: ^bb5
    cf.br ^bb8(%13 : f32)
  ^bb8(%22: f32):  // 2 preds: ^bb6, ^bb7
    cf.br ^bb9
  ^bb9:  // pred: ^bb8
    %23 = arith.addi %12, %c1 : index
    cf.br ^bb4(%23, %22 : index, f32)
  ^bb10:  // pred: ^bb4
    %24 = memref.atomic_rmw addf %13, %arg6[%8] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
    cf.br ^bb11
  ^bb11:  // 2 preds: ^bb2, ^bb10
    cf.br ^bb12
  ^bb12:  // 2 preds: ^bb1, ^bb11
    gpu.return
  }
}

// -----// IR Dump After StripDebugInfo (strip-debuginfo) //----- //
gpu.module @main_kernel_0 {
  gpu.func @main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_1(%arg0: index, %arg1: index, %arg2: index, %arg3: index, %arg4: memref<?x4xf32, #gpu.address_space<global>>, %arg5: memref<?x4xf32, #gpu.address_space<global>>, %arg6: memref<1xf32, #gpu.address_space<global>>) kernel {
    %cst = arith.constant 0.000000e+00 : f32
    %c4 = arith.constant 4 : index
    %c32 = arith.constant 32 : index
    %c512 = arith.constant 512 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = arith.muli %0, %arg0 : index
    %3 = arith.addi %2, %c0 : index
    %4 = arith.muli %1, %c1 : index
    %5 = arith.addi %4, %c0 : index
    %6 = arith.addi %5, %3 : index
    %7 = arith.cmpi ult, %6, %arg1 : index
    cf.cond_br %7, ^bb2, ^bb12
  ^bb2:  // pred: ^bb1
    %8 = arith.remsi %6, %c512 : index
    %9 = arith.divsi %6, %c512 : index
    %10 = arith.cmpi ult, %8, %c1 : index
    cf.cond_br %10, ^bb3, ^bb11
  ^bb3:  // pred: ^bb2
    %11 = arith.muli %9, %c32 : index
    cf.br ^bb4(%c0, %cst : index, f32)
  ^bb4(%12: index, %13: f32):  // 2 preds: ^bb3, ^bb9
    %14 = arith.cmpi slt, %12, %c32 : index
    cf.cond_br %14, ^bb5, ^bb10
  ^bb5:  // pred: ^bb4
    %15 = arith.addi %11, %12 : index
    %16 = arith.cmpi slt, %15, %arg2 : index
    cf.cond_br %16, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %17 = arith.muli %arg3, %c4 : index
    %reinterpret_cast = memref.reinterpret_cast %arg4 to offset: [%c0], sizes: [%17], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
    %18 = memref.load %reinterpret_cast[%15] : memref<?xf32, #gpu.address_space<global>>
    %reinterpret_cast_0 = memref.reinterpret_cast %arg5 to offset: [%c0], sizes: [%17], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
    %19 = memref.load %reinterpret_cast_0[%15] : memref<?xf32, #gpu.address_space<global>>
    %20 = arith.mulf %18, %19 : f32
    %21 = arith.addf %13, %20 : f32
    cf.br ^bb8(%21 : f32)
  ^bb7:  // pred: ^bb5
    cf.br ^bb8(%13 : f32)
  ^bb8(%22: f32):  // 2 preds: ^bb6, ^bb7
    cf.br ^bb9
  ^bb9:  // pred: ^bb8
    %23 = arith.addi %12, %c1 : index
    cf.br ^bb4(%23, %22 : index, f32)
  ^bb10:  // pred: ^bb4
    %24 = memref.atomic_rmw addf %13, %arg6[%8] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
    cf.br ^bb11
  ^bb11:  // 2 preds: ^bb2, ^bb10
    cf.br ^bb12
  ^bb12:  // 2 preds: ^bb1, ^bb11
    gpu.return
  }
}

// -----// IR Dump After DiscLowerGpuOpsToNVVMOpsPass (disc-convert-gpu-to-nvvm) //----- //
gpu.module @main_kernel_0 {
  llvm.func @main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_1(%arg0: i32, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: !llvm.ptr<1>, %arg5: !llvm.ptr<1>, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: !llvm.ptr<1>, %arg12: !llvm.ptr<1>, %arg13: i32, %arg14: i32, %arg15: i32, %arg16: i32, %arg17: i32, %arg18: !llvm.ptr<1>, %arg19: !llvm.ptr<1>, %arg20: i32, %arg21: i32, %arg22: i32) attributes {gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.undef : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)>
    %1 = llvm.insertvalue %arg4, %0[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %2 = llvm.insertvalue %arg5, %1[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %3 = llvm.insertvalue %arg6, %2[2] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %4 = llvm.insertvalue %arg7, %3[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %5 = llvm.insertvalue %arg9, %4[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %6 = llvm.insertvalue %arg8, %5[3, 1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %7 = llvm.insertvalue %arg10, %6[4, 1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %8 = llvm.mlir.undef : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)>
    %9 = llvm.insertvalue %arg11, %8[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %10 = llvm.insertvalue %arg12, %9[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %11 = llvm.insertvalue %arg13, %10[2] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %12 = llvm.insertvalue %arg14, %11[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %13 = llvm.insertvalue %arg16, %12[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %14 = llvm.insertvalue %arg15, %13[3, 1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %15 = llvm.insertvalue %arg17, %14[4, 1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %16 = llvm.mlir.undef : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)>
    %17 = llvm.insertvalue %arg18, %16[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %18 = llvm.insertvalue %arg19, %17[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %19 = llvm.insertvalue %arg20, %18[2] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %20 = llvm.insertvalue %arg21, %19[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %21 = llvm.insertvalue %arg22, %20[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %22 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    %23 = llvm.mlir.constant(4 : index) : i32
    %24 = llvm.mlir.constant(32 : index) : i32
    %25 = llvm.mlir.constant(512 : index) : i32
    %26 = llvm.mlir.constant(1 : index) : i32
    %27 = llvm.mlir.constant(0 : index) : i32
    %28 = nvvm.read.ptx.sreg.ctaid.x : i32
    %29 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %30 = llvm.mul %28, %arg0  : i32
    %31 = llvm.add %29, %30  : i32
    %32 = llvm.icmp "ult" %31, %arg1 : i32
    llvm.cond_br %32, ^bb2, ^bb12
  ^bb2:  // pred: ^bb1
    %33 = llvm.srem %31, %25  : i32
    %34 = llvm.sdiv %31, %25  : i32
    %35 = llvm.icmp "ult" %33, %26 : i32
    llvm.cond_br %35, ^bb3, ^bb11
  ^bb3:  // pred: ^bb2
    %36 = llvm.mul %34, %24  : i32
    llvm.br ^bb4(%27, %22 : i32, f32)
  ^bb4(%37: i32, %38: f32):  // 2 preds: ^bb3, ^bb9
    %39 = llvm.icmp "slt" %37, %24 : i32
    llvm.cond_br %39, ^bb5, ^bb10
  ^bb5:  // pred: ^bb4
    %40 = llvm.add %36, %37  : i32
    %41 = llvm.icmp "slt" %40, %arg2 : i32
    llvm.cond_br %41, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %42 = llvm.mul %arg3, %23  : i32
    %43 = llvm.mlir.undef : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)>
    %44 = llvm.extractvalue %7[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %45 = llvm.extractvalue %7[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %46 = llvm.insertvalue %44, %43[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %47 = llvm.insertvalue %45, %46[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %48 = llvm.insertvalue %27, %47[2] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %49 = llvm.insertvalue %42, %48[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %50 = llvm.insertvalue %26, %49[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %51 = llvm.extractvalue %50[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %52 = llvm.getelementptr %51[%40] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    %53 = llvm.load %52 : !llvm.ptr<1> -> f32
    %54 = llvm.mlir.undef : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)>
    %55 = llvm.extractvalue %15[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %56 = llvm.extractvalue %15[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %57 = llvm.insertvalue %55, %54[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %58 = llvm.insertvalue %56, %57[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %59 = llvm.insertvalue %27, %58[2] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %60 = llvm.insertvalue %42, %59[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %61 = llvm.insertvalue %26, %60[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %62 = llvm.extractvalue %61[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %63 = llvm.getelementptr %62[%40] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    %64 = llvm.load %63 : !llvm.ptr<1> -> f32
    %65 = llvm.fmul %53, %64  : f32
    %66 = llvm.fadd %38, %65  : f32
    llvm.br ^bb8(%66 : f32)
  ^bb7:  // pred: ^bb5
    llvm.br ^bb8(%38 : f32)
  ^bb8(%67: f32):  // 2 preds: ^bb6, ^bb7
    llvm.br ^bb9
  ^bb9:  // pred: ^bb8
    %68 = llvm.add %37, %26  : i32
    llvm.br ^bb4(%68, %67 : i32, f32)
  ^bb10:  // pred: ^bb4
    %69 = llvm.extractvalue %21[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %70 = llvm.getelementptr %69[%33] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    %71 = llvm.atomicrmw fadd %70, %38 acq_rel : !llvm.ptr<1>, f32
    llvm.br ^bb11
  ^bb11:  // 2 preds: ^bb2, ^bb10
    llvm.br ^bb12
  ^bb12:  // 2 preds: ^bb1, ^bb11
    llvm.return
  }
}

// -----// IR Dump After LLVMInsertValueSimplifierPass (disc-llvm-insert-value-simplifier) //----- //
llvm.func @main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_1(%arg0: i32, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: !llvm.ptr<1>, %arg5: !llvm.ptr<1>, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: !llvm.ptr<1>, %arg12: !llvm.ptr<1>, %arg13: i32, %arg14: i32, %arg15: i32, %arg16: i32, %arg17: i32, %arg18: !llvm.ptr<1>, %arg19: !llvm.ptr<1>, %arg20: i32, %arg21: i32, %arg22: i32) attributes {gpu.kernel, nvvm.kernel} {
  %0 = llvm.mlir.constant(0 : index) : i32
  %1 = llvm.mlir.constant(1 : index) : i32
  %2 = llvm.mlir.constant(512 : index) : i32
  %3 = llvm.mlir.constant(32 : index) : i32
  %4 = llvm.mlir.constant(0.000000e+00 : f32) : f32
  %5 = nvvm.read.ptx.sreg.ctaid.x : i32
  %6 = nvvm.read.ptx.sreg.tid.x : i32
  llvm.br ^bb1
^bb1:  // pred: ^bb0
  %7 = llvm.mul %5, %arg0  : i32
  %8 = llvm.add %6, %7  : i32
  %9 = llvm.icmp "ult" %8, %arg1 : i32
  llvm.cond_br %9, ^bb2, ^bb12
^bb2:  // pred: ^bb1
  %10 = llvm.srem %8, %2  : i32
  %11 = llvm.sdiv %8, %2  : i32
  %12 = llvm.icmp "ult" %10, %1 : i32
  llvm.cond_br %12, ^bb3, ^bb11
^bb3:  // pred: ^bb2
  %13 = llvm.mul %11, %3  : i32
  llvm.br ^bb4(%0, %4 : i32, f32)
^bb4(%14: i32, %15: f32):  // 2 preds: ^bb3, ^bb9
  %16 = llvm.icmp "slt" %14, %3 : i32
  llvm.cond_br %16, ^bb5, ^bb10
^bb5:  // pred: ^bb4
  %17 = llvm.add %13, %14  : i32
  %18 = llvm.icmp "slt" %17, %arg2 : i32
  llvm.cond_br %18, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  %19 = llvm.getelementptr %arg5[%17] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
  %20 = llvm.load %19 : !llvm.ptr<1> -> f32
  %21 = llvm.getelementptr %arg12[%17] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
  %22 = llvm.load %21 : !llvm.ptr<1> -> f32
  %23 = llvm.fmul %20, %22  : f32
  %24 = llvm.fadd %15, %23  : f32
  llvm.br ^bb8(%24 : f32)
^bb7:  // pred: ^bb5
  llvm.br ^bb8(%15 : f32)
^bb8(%25: f32):  // 2 preds: ^bb6, ^bb7
  llvm.br ^bb9
^bb9:  // pred: ^bb8
  %26 = llvm.add %14, %1  : i32
  llvm.br ^bb4(%26, %25 : i32, f32)
^bb10:  // pred: ^bb4
  %27 = llvm.getelementptr %arg19[%10] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
  %28 = llvm.atomicrmw fadd %27, %15 acq_rel : !llvm.ptr<1>, f32
  llvm.br ^bb11
^bb11:  // 2 preds: ^bb2, ^bb10
  llvm.br ^bb12
^bb12:  // 2 preds: ^bb1, ^bb11
  llvm.return
}

// -----// IR Dump After FunctionDeadArgumentEliminationPass (disc-function-dead-argument-elimination) //----- //
gpu.module @main_kernel_0 {
  llvm.func @main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_1(%arg0: i32, %arg1: i32, %arg2: i32, %arg3: !llvm.ptr<1>, %arg4: !llvm.ptr<1>, %arg5: !llvm.ptr<1>) attributes {disc.elimargs = [3 : index, 4 : index, 6 : index, 7 : index, 8 : index, 9 : index, 10 : index, 11 : index, 13 : index, 14 : index, 15 : index, 16 : index, 17 : index, 18 : index, 20 : index, 21 : index, 22 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(0 : index) : i32
    %1 = llvm.mlir.constant(1 : index) : i32
    %2 = llvm.mlir.constant(512 : index) : i32
    %3 = llvm.mlir.constant(32 : index) : i32
    %4 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    %5 = nvvm.read.ptx.sreg.ctaid.x : i32
    %6 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %7 = llvm.mul %5, %arg0  : i32
    %8 = llvm.add %6, %7  : i32
    %9 = llvm.icmp "ult" %8, %arg1 : i32
    llvm.cond_br %9, ^bb2, ^bb12
  ^bb2:  // pred: ^bb1
    %10 = llvm.srem %8, %2  : i32
    %11 = llvm.sdiv %8, %2  : i32
    %12 = llvm.icmp "ult" %10, %1 : i32
    llvm.cond_br %12, ^bb3, ^bb11
  ^bb3:  // pred: ^bb2
    %13 = llvm.mul %11, %3  : i32
    llvm.br ^bb4(%0, %4 : i32, f32)
  ^bb4(%14: i32, %15: f32):  // 2 preds: ^bb3, ^bb9
    %16 = llvm.icmp "slt" %14, %3 : i32
    llvm.cond_br %16, ^bb5, ^bb10
  ^bb5:  // pred: ^bb4
    %17 = llvm.add %13, %14  : i32
    %18 = llvm.icmp "slt" %17, %arg2 : i32
    llvm.cond_br %18, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %19 = llvm.getelementptr %arg3[%17] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    %20 = llvm.load %19 : !llvm.ptr<1> -> f32
    %21 = llvm.getelementptr %arg4[%17] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    %22 = llvm.load %21 : !llvm.ptr<1> -> f32
    %23 = llvm.fmul %20, %22  : f32
    %24 = llvm.fadd %15, %23  : f32
    llvm.br ^bb8(%24 : f32)
  ^bb7:  // pred: ^bb5
    llvm.br ^bb8(%15 : f32)
  ^bb8(%25: f32):  // 2 preds: ^bb6, ^bb7
    llvm.br ^bb9
  ^bb9:  // pred: ^bb8
    %26 = llvm.add %14, %1  : i32
    llvm.br ^bb4(%26, %25 : i32, f32)
  ^bb10:  // pred: ^bb4
    %27 = llvm.getelementptr %arg5[%10] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    %28 = llvm.atomicrmw fadd %27, %15 acq_rel : !llvm.ptr<1>, f32
    llvm.br ^bb11
  ^bb11:  // 2 preds: ^bb2, ^bb10
    llvm.br ^bb12
  ^bb12:  // 2 preds: ^bb1, ^bb11
    llvm.return
  }
}

// -----// IR Dump After GpuKernelToBlobPass (disc-gpu-kernel-to-blob) //----- //
gpu.module @main_kernel_0 attributes {gpu.binary = "P\EDU\BA\01\00\10\00\00\09\00\00\00\00\00\00\02\00\01\01@\00\00\00\C0\08\00\00\00\00\00\00\BF\08\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00h\17\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\002\C0\16\00\01\00\11\14\06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\13e__6_1_0___no_ibXthread_tile_h32_1J\00\0FD\00-osharedF\00-\9Fconstant0I\00*\FA\01debug_frame\00.rel\11\00!nv\14\00\11aU\00\0Fs\01 \0F\9A\00'\0F\AE\01\FF\03o_param\B5\01\1C\0F\01\00\04\8Cm\00\00\00\03\00\0A\00\01\00 8\01\18\00,\09\00\01\00\11\88\18\00,\04\00\01\00\11\A6\18\00,\07\00\01\00g2\00\00\00\12\10`\00\11\0C\06\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0\0B\07\00 \00\04\9B\00 \04\18i\00\01:\002\04\B8\02\18\00p/\08\00\05\00\00\00\1A\00\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\88\04\80\015\00\00\04\0A\08\00F\00\A2`\01(\00\03\19(\00\04\17\C5\00u\05\00 \00\00\F0!\10\009\04\00\18\10\009\03\00\10\10\00u\02\00\08\00\00\F0\11\10\009\01\00\04\10\00\01\01\00\F3\01\F0\11\00\03\1B\FF\00\04\1C\08\00`\00\00\00P\D8\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\00\D2\05b,\00\00\00H\00\01\00\00`\01/\05\00\01\00\FF\E0@$v\01\FF\7F\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\BB\02Q\0E\00\19y\03\0F\00 \00!-\00\F0\14\0E\00$z\00\00\00X\00\00\03\02\8E\07\00\CA\1F\00\12x\FF\00\FF\01\00\00\FF\C0\80\07\00\C8\0F\00\0C \00\C5Y\00\00pdp\00\00\DA\0F\00M\C3\04\B0\80\03\00\EA\0F\00\19x\03\FF\1F\0F\00\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\00\03\05a\C6\0F\00\11r\03q\001\FFH\8FP\00\000\00\00C\00\10\030\00\93\CA\0F\00$x\07\03 \00\B0\00p\C8\0F\00%x\04\07s\04\10\FF\90\00\F0\09\E2\0F\04\0Cz\00\07\00Z\00\00pb\F4\03\00\E4\0F\04\10x\00\07\01 \00\B0\E0\FF\07\00\E4\0F\00\12x\04\04\01\031\FF\FC\8E\10\00\01\B0\00\010\00\10\F60\00p\00\10z\02\04\00\\0\00!\F1\07@\00Pz\04\04\00^\10\00\11\F3 \01\00P\00\00,\03\04P\00\C2\10z\03\05\00]\00\00\FF\E4\7F\000\00@\05\05\00_\10\00*\FF\00`\00 \F8\03\F0\00\F4\06\81\B9\0B\02\04\FC\FF\FF\00\19\1E\0C\00\A2\0E\00\10x\06\07\03P\00\93\C6\0F\00\81\A9\00\02\04\F8 \00u\E8\0E\00\81\A9\09\04\10\00 \E2\0EP\00\12\06P\00!\FA\030\005\B9\08\04P\00p\A8\0E\00\81\C9\0D\02\10\01\01\10\00y(\0F\00\81\C9\0A\04\10\00S\D9\0F\02\04\04\10\00\10h\10\00%\0C\04\10\00!b\0F\90\00\14\04\90\00\01\F0\006\0E\07\05\F0\00\16\04\80\00\12\FC0\01Jx\10\07\06 \00\12\0E \00\13\F0 \00:\0E\07\07 \00\12\10 \00\11\F2\10\01T$r\06\FF\FF\D0\01\12\E2P\00\14\0A0\00a\E2\0F\04#\A2\06\06\07\00$\00F\00\E2\8F\00`\00\12\F4P\01S\E9\09\02\04\08\C0\00!\E2\0Ep\00\18\08P\018\E9\00\04 \00@#\B2\06\0B\0F\00\11\06P\00\17OP\00\02\00\02P\81\89\0B\02\04\0F\06\05\A0\018\0E\07\09P\008\89\08\04 \00T#\C2\06\0D\0AP\00\93\C6\0F\01\81\99\0D\02\04\10 \00'\22\0F`\00\12\F8\B0\01W\99\0A\04\04\10\90\01X\A9\11\02\04\14\10\00F\0E\04\04\14@\00U#\D2\06\0F\0C\B0\00&\0F\02@\01 \FA\03\A0\01g\81\B9\0F\02\04\18\D0\019\B9\0C\04\10\00X\C9\13\02\04\1C\10\00)\10\04\10\00X\D9\15\02\04 \10\00H\12\04\04 \10\026\14\07\0B\90\01e\00#\E2\06\09\00\90\00\11\8F\10\03\17\0C0\02E\0Cz\00\14 \02\000\00\1D\82p\01\1A\00 \027\00\07\0D \02X\10x\08\07\0Ep\00\17\92`\016\E2\0F\01@\00C\F2\03\00\CA\00\025$\00\00\00\03\00\F0\01\17$\A0\01F\A2\06\11\0E@\00\00\80\00\17\08 \04\02\F0\01\18(@\028\08\07\0F\F0\01\00\D0\01\17,\90\01\1D\B2\90\01\19\08@\02\00\E0\01\070\00\\\10x\0C\07\10@\02\07p\00Z#\C2\06\13\10\A0\00\15\0C0\02\00P\00X\A9\0F\02\040\90\018\10\07\11P\008\A9\0C\04 \00Z#\D2\06\15\12P\00\060\02\00P\00W\B9\11\02\044\F0\01[\B9\0E\04\0440\02\1B80\02\1B80\02\1B<0\02\1A<0\02\1F\120\02\06\11O\F0\01\1F\130\02\16\1A\8F0\02\13\D6\F0\01\18@\A0\03?\00\07\140\02\08\00\D0\01\16\15`\02\19\00@\02\000\00\1F\A2\D0\01\05\11\F4 \00\01P\02\09p\009\08\07\16\B0\027\0A\07\17\80\00\01P\02\19Dp\04\0F\80\02\04\12\F6\C0\03\00\10\02\040\00\00\F0\05\00p\02\17Hp\02\0E \02\19\0A \02\00p\02\070\00\00\B0\04\19\18 \02O\0F\02\04L \02\0A\19\0E \02\00P\02\17L\E0\01\000\02\1BP0\02\1BP0\02\1BT0\02\1BT0\02\1BX0\02\1AX0\02\1F\190\02\05*\C6O \02\030\04\18\1A0\00\0B0\02\00\A0\01\14\1B \00\13\D20\02\16\\\C0\01\0FP\04\0A\03`\00\1A\1C\B0\06\1A\08\B0\048\08\07\1D\80\00\0E@\02\15\00@\02\13\C4@\02\18\\@\028\0A\07\1E\A0\01_\99\0B\02\04`0\02\14\01\F0\01(\08\040\00\00`\04\18\1FP\00_\89\07\02\04d@\02\18X\89\0A\04\04d\80\06O\0D\02\04h0\02\0A\15\0C0\02\13\C40\02\17h\E0\01\00\90\06\1Bl0\02\1Bl \02\18p\10\00K\11\02\04p \02\18t\10\00%\13\02\10\00\1Bb \02 \C8O\D0\01\06\00\02 \C8\8F\10\02%\07\0A\10\00v\0F\01#\A2\06\0D\0C\10\00\10\02\C0\05\08`\01c$v\08\FF\00`\80\08\10\E4\10\00C\09\FF\00a\10\00\11\E2@\01&\11\10@\00\00\10\01\15\13\10\01p\CA\0F\00\8Ey\00\08\0C\00@\84\E7\10\0C0\00*My\F0\0APGy\00\00\F09\0F\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00p\0F\01\00-\11\01h\0F\0E\01\00\22@\00\01\00=s\01\000\00\08\01\00\1F\0B@\00\04\13\B3)\00\1F\B5@\00\0C\13\13\BC\0E\0C\01\00\13hU\00*\90\00\E0\0E\22\08\00\01\00\22\18\00\01\00.F\01T\00\00\01\00\13\F8@\00/p\00\80\00\0B\1F)'\00\03A\00h\04\00\01\00\00\9B\07\17\00\E4\00*\04\00\01\00\1Fs@\00\04\13\98)\00&\8C\00@\00\1F\0A@\00\00!d\01D\01\0D@\001(\05\00\01\00*\D8\00\01\00\1B\08\08\00?S\01\00f\12\00\04\E1\02\10\00B\11\05\80\00\17\048\00\04\18\00\13\FD\14\01\0C\84\01&\10\06\B0\12\04\01\00\0F\C0\00\01\132@\00+\06\00\01\00\1A\08\B0\12\12\03\C0\01>\18\80\00\A7\00\18\05\A8\16\0B\01\00*\A8\00\08\00\04W\00\13\018\00\04\A8\00\0D\D0\12)\0D\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00"} {
  llvm.func @main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_1(%arg0: i32, %arg1: i32, %arg2: i32, %arg3: !llvm.ptr<1>, %arg4: !llvm.ptr<1>, %arg5: !llvm.ptr<1>) attributes {disc.elimargs = [3 : index, 4 : index, 6 : index, 7 : index, 8 : index, 9 : index, 10 : index, 11 : index, 13 : index, 14 : index, 15 : index, 16 : index, 17 : index, 18 : index, 20 : index, 21 : index, 22 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(0 : index) : i32
    %1 = llvm.mlir.constant(1 : index) : i32
    %2 = llvm.mlir.constant(512 : index) : i32
    %3 = llvm.mlir.constant(32 : index) : i32
    %4 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    %5 = nvvm.read.ptx.sreg.ctaid.x : i32
    %6 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %7 = llvm.mul %5, %arg0  : i32
    %8 = llvm.add %6, %7  : i32
    %9 = llvm.icmp "ult" %8, %arg1 : i32
    llvm.cond_br %9, ^bb2, ^bb12
  ^bb2:  // pred: ^bb1
    %10 = llvm.srem %8, %2  : i32
    %11 = llvm.sdiv %8, %2  : i32
    %12 = llvm.icmp "ult" %10, %1 : i32
    llvm.cond_br %12, ^bb3, ^bb11
  ^bb3:  // pred: ^bb2
    %13 = llvm.mul %11, %3  : i32
    llvm.br ^bb4(%0, %4 : i32, f32)
  ^bb4(%14: i32, %15: f32):  // 2 preds: ^bb3, ^bb9
    %16 = llvm.icmp "slt" %14, %3 : i32
    llvm.cond_br %16, ^bb5, ^bb10
  ^bb5:  // pred: ^bb4
    %17 = llvm.add %13, %14  : i32
    %18 = llvm.icmp "slt" %17, %arg2 : i32
    llvm.cond_br %18, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %19 = llvm.getelementptr %arg3[%17] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    %20 = llvm.load %19 : !llvm.ptr<1> -> f32
    %21 = llvm.getelementptr %arg4[%17] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    %22 = llvm.load %21 : !llvm.ptr<1> -> f32
    %23 = llvm.fmul %20, %22  : f32
    %24 = llvm.fadd %15, %23  : f32
    llvm.br ^bb8(%24 : f32)
  ^bb7:  // pred: ^bb5
    llvm.br ^bb8(%15 : f32)
  ^bb8(%25: f32):  // 2 preds: ^bb6, ^bb7
    llvm.br ^bb9
  ^bb9:  // pred: ^bb8
    %26 = llvm.add %14, %1  : i32
    llvm.br ^bb4(%26, %25 : i32, f32)
  ^bb10:  // pred: ^bb4
    %27 = llvm.getelementptr %arg5[%10] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    %28 = llvm.atomicrmw fadd %27, %15 acq_rel : !llvm.ptr<1>, f32
    llvm.br ^bb11
  ^bb11:  // 2 preds: ^bb2, ^bb10
    llvm.br ^bb12
  ^bb12:  // 2 preds: ^bb1, ^bb11
    llvm.return
  }
}

// -----// IR Dump After CSE (cse) //----- //
gpu.module @main_kernel_1 {
  gpu.func @main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64(%arg0: index, %arg1: memref<1xf32, #gpu.address_space<global>>) kernel {
    %cst = arith.constant 0.000000e+00 : f32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg0, %c0]
    %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %4 = arith.addi %3, %2 : index
    %5 = arith.cmpi ult, %4, %c1 : index
    scf.if %5 {
      %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
      memref.store %cst, %reinterpret_cast[%4] : memref<1xf32, #gpu.address_space<global>>
    }
    gpu.return
  }
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
gpu.module @main_kernel_1 {
  gpu.func @main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64(%arg0: index, %arg1: memref<1xf32, #gpu.address_space<global>>) kernel {
    %cst = arith.constant 0.000000e+00 : f32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg0, %c0]
    %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %4 = arith.addi %3, %2 : index
    %5 = arith.cmpi ult, %4, %c1 : index
    cf.cond_br %5, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
    memref.store %cst, %reinterpret_cast[%4] : memref<1xf32, #gpu.address_space<global>>
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    gpu.return
  }
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
gpu.module @main_kernel_1 {
  gpu.func @main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64(%arg0: index, %arg1: memref<1xf32, #gpu.address_space<global>>) kernel {
    %cst = arith.constant 0.000000e+00 : f32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = arith.muli %0, %arg0 : index
    %3 = arith.addi %2, %c0 : index
    %4 = arith.muli %1, %c1 : index
    %5 = arith.addi %4, %c0 : index
    %6 = arith.addi %5, %3 : index
    %7 = arith.cmpi ult, %6, %c1 : index
    cf.cond_br %7, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
    memref.store %cst, %reinterpret_cast[%6] : memref<1xf32, #gpu.address_space<global>>
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    gpu.return
  }
}

// -----// IR Dump After StripDebugInfo (strip-debuginfo) //----- //
gpu.module @main_kernel_1 {
  gpu.func @main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64(%arg0: index, %arg1: memref<1xf32, #gpu.address_space<global>>) kernel {
    %cst = arith.constant 0.000000e+00 : f32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = arith.muli %0, %arg0 : index
    %3 = arith.addi %2, %c0 : index
    %4 = arith.muli %1, %c1 : index
    %5 = arith.addi %4, %c0 : index
    %6 = arith.addi %5, %3 : index
    %7 = arith.cmpi ult, %6, %c1 : index
    cf.cond_br %7, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
    memref.store %cst, %reinterpret_cast[%6] : memref<1xf32, #gpu.address_space<global>>
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    gpu.return
  }
}

// -----// IR Dump After DiscLowerGpuOpsToNVVMOpsPass (disc-convert-gpu-to-nvvm) //----- //
gpu.module @main_kernel_1 {
  llvm.func @main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64(%arg0: i32, %arg1: !llvm.ptr<1>, %arg2: !llvm.ptr<1>, %arg3: i32, %arg4: i32, %arg5: i32) attributes {gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.undef : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)>
    %1 = llvm.insertvalue %arg1, %0[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %2 = llvm.insertvalue %arg2, %1[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %3 = llvm.insertvalue %arg3, %2[2] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %4 = llvm.insertvalue %arg4, %3[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %5 = llvm.insertvalue %arg5, %4[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %6 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    %7 = llvm.mlir.constant(1 : index) : i32
    %8 = nvvm.read.ptx.sreg.ctaid.x : i32
    %9 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %10 = llvm.mul %8, %arg0  : i32
    %11 = llvm.add %9, %10  : i32
    %12 = llvm.icmp "ult" %11, %7 : i32
    llvm.cond_br %12, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %13 = llvm.mlir.undef : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)>
    %14 = llvm.extractvalue %5[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %15 = llvm.extractvalue %5[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %16 = llvm.insertvalue %14, %13[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %17 = llvm.insertvalue %15, %16[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %18 = llvm.mlir.constant(0 : index) : i32
    %19 = llvm.insertvalue %18, %17[2] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %20 = llvm.mlir.constant(1 : index) : i32
    %21 = llvm.insertvalue %20, %19[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %22 = llvm.mlir.constant(1 : index) : i32
    %23 = llvm.insertvalue %22, %21[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %24 = llvm.extractvalue %23[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %25 = llvm.getelementptr %24[%11] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    llvm.store %6, %25 : f32, !llvm.ptr<1>
    llvm.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    llvm.return
  }
}

// -----// IR Dump After LLVMInsertValueSimplifierPass (disc-llvm-insert-value-simplifier) //----- //
llvm.func @main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64(%arg0: i32, %arg1: !llvm.ptr<1>, %arg2: !llvm.ptr<1>, %arg3: i32, %arg4: i32, %arg5: i32) attributes {gpu.kernel, nvvm.kernel} {
  %0 = llvm.mlir.constant(1 : index) : i32
  %1 = llvm.mlir.constant(0.000000e+00 : f32) : f32
  %2 = nvvm.read.ptx.sreg.ctaid.x : i32
  %3 = nvvm.read.ptx.sreg.tid.x : i32
  llvm.br ^bb1
^bb1:  // pred: ^bb0
  %4 = llvm.mul %2, %arg0  : i32
  %5 = llvm.add %3, %4  : i32
  %6 = llvm.icmp "ult" %5, %0 : i32
  llvm.cond_br %6, ^bb2, ^bb3
^bb2:  // pred: ^bb1
  %7 = llvm.getelementptr %arg2[%5] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
  llvm.store %1, %7 : f32, !llvm.ptr<1>
  llvm.br ^bb3
^bb3:  // 2 preds: ^bb1, ^bb2
  llvm.return
}

// -----// IR Dump After FunctionDeadArgumentEliminationPass (disc-function-dead-argument-elimination) //----- //
gpu.module @main_kernel_1 {
  llvm.func @main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64(%arg0: i32, %arg1: !llvm.ptr<1>) attributes {disc.elimargs = [1 : index, 3 : index, 4 : index, 5 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(1 : index) : i32
    %1 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    %2 = nvvm.read.ptx.sreg.ctaid.x : i32
    %3 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %4 = llvm.mul %2, %arg0  : i32
    %5 = llvm.add %3, %4  : i32
    %6 = llvm.icmp "ult" %5, %0 : i32
    llvm.cond_br %6, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %7 = llvm.getelementptr %arg1[%5] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    llvm.store %1, %7 : f32, !llvm.ptr<1>
    llvm.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    llvm.return
  }
}

// -----// IR Dump After GpuKernelToBlobPass (disc-gpu-kernel-to-blob) //----- //
gpu.module @main_kernel_1 attributes {gpu.binary = "P\EDU\BA\01\00\10\00H\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\08\04\00\00\00\00\00\00\06\04\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00h\0C\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\002\C0\0B\00\01\00\11\09\06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\10e__6_1_0___no_ibXblock_tile_h64G\00\0FA\00*osharedC\00*\9Fconstant0F\00'\FA\01debug_frame\00.rel\11\00!nv\14\00\11aR\00\0Fg\01 \0F\97\00$\0F\9F\01\F6o_param\A6\01\1C\0F\01\00\07\8Cj\00\00\00\03\00\0A\00\01\00 ,\01\18\00,\09\00\01\00\11y\18\00,\04\00\01\00\11\97\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\1C\00\00\00E\00#\04\10\18\00\80/\08\00\05\00\00\00\06\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04p\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C$\00u\01\00\08\00\00\F0!\10\00\01\01\00\C0\F0\11\00\03\1B\FF\00\04\1C\08\00p\A8\04\12\00\01\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00 \01/\05\00\01\00\FF\B8@$v\01\FF\17\04\A3\FF\00\8E\07\00\C4\0F\00\C3y\F4\03\A2%\00\00\00\22\0E\00\19y\00\01\00\F3\0E!\00\00\00b\0E\00\90r\05?\04\00\00\80?\E0\FF\0F\00\C6\1F\00\B9z\04\00\00X\E6\02p\E4\0F\00\A4r\04\05B\00\B1?\02\8E\0F\00\CC\0F\00\0C|\00F\04\95pR\F0\0B\00\DA/\00Mk\04\F3\0D\80\03\00\EA\0F\00\02z\02\00\00Z\00\00\00\0F\00\00\00\E2\0F\00$v\03\FF\00[\90\00\22\E2\0F`\001F\00\00\BB\04\80\C8\0F\00\86y\00\02\FF\B8\030\19\10\0C \00*MyP\00PGy\00\00\F0A\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\80\0F\01\00-\00\0F\05.\03\00\01\00\22@\00\01\00=g\01\000\00\08\01\00\1F\0B@\00\04\13\A7)\00\1F\A6@\00\0C\13\13\14\04\0C\01\00\13PU\00*\90\008\04\04U\04\22\18\00\01\00.:\01T\00\00\01\00\13\E0@\00/p\00\80\00\0B\1F)'\00\03A\00P\04\00\01\00\04H\06\04\E4\00*\04\00\01\00\1Fp@\00\04\13\80)\00&L\00@\00\1F\0A@\00\00!X\01D\01\0D@\00\13\D0)\00*\D8\00\01\00\1B\08\08\00%G\01\DB\0A\09\01\00\13\A8e\05\17\10\80\00\17\048\00\04\18\00\13\F4\14\01\0C\84\01*\B8\050\07\1F\00\C0\00\04\132@\00*\06\00\01\00*\80\07\C8\07\12\03\C0\01:\06\80\00\01\00\13\06\D8\01\05\A8\0B\0C\C8\00\1A\00\08\00\04\F8\00\13\018\00\04\A8\00\0C\01\009H\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00\00"} {
  llvm.func @main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64(%arg0: i32, %arg1: !llvm.ptr<1>) attributes {disc.elimargs = [1 : index, 3 : index, 4 : index, 5 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(1 : index) : i32
    %1 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    %2 = nvvm.read.ptx.sreg.ctaid.x : i32
    %3 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %4 = llvm.mul %2, %arg0  : i32
    %5 = llvm.add %3, %4  : i32
    %6 = llvm.icmp "ult" %5, %0 : i32
    llvm.cond_br %6, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %7 = llvm.getelementptr %arg1[%5] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    llvm.store %1, %7 : f32, !llvm.ptr<1>
    llvm.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    llvm.return
  }
}

// -----// IR Dump After LoopInvariantCodeMotion (loop-invariant-code-motion) //----- //
gpu.module @main_kernel_2 {
  gpu.func @main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1(%arg0: index, %arg1: index, %arg2: index, %arg3: index, %arg4: memref<?x4xf32, #gpu.address_space<global>>, %arg5: memref<?x4xf32, #gpu.address_space<global>>, %arg6: memref<1xf32, #gpu.address_space<global>>) workgroup(%arg7 : memref<256xf32, #gpu.address_space<workgroup>>) kernel {
    %c2 = arith.constant 2 : index
    %cst = arith.constant 0.000000e+00 : f32
    %c4 = arith.constant 4 : index
    %c64 = arith.constant 64 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c256 = arith.constant 256 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg0, %c0]
    %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %4 = arith.addi %3, %2 : index
    %5 = arith.addi %3, %2 : index
    %6 = arith.cmpi ult, %5, %arg1 : index
    scf.if %6 {
      %7 = arith.remsi %4, %c256 : index
      %8 = arith.divsi %4, %c256 : index
      %9 = arith.divui %7, %c32 : index
      %10 = arith.remui %7, %c32 : index
      %11 = arith.muli %10, %c8 : index
      %12 = arith.addi %9, %11 : index
      %13 = arith.cmpi ult, %10, %c1 : index
      %14 = scf.if %13 -> (f32) {
        %20 = arith.muli %8, %c8 : index
        %21 = arith.addi %9, %20 : index
        %22 = arith.muli %21, %c64 : index
        %23 = scf.for %arg8 = %c0 to %c64 step %c1 iter_args(%arg9 = %cst) -> (f32) {
          %24 = arith.addi %arg8, %22 : index
          %25 = arith.cmpi slt, %24, %arg2 : index
          %26 = scf.if %25 -> (f32) {
            %27 = arith.muli %arg3, %c4 : index
            %reinterpret_cast_0 = memref.reinterpret_cast %arg4 to offset: [%c0], sizes: [%27], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
            %28 = memref.load %reinterpret_cast_0[%24] : memref<?xf32, #gpu.address_space<global>>
            %reinterpret_cast_1 = memref.reinterpret_cast %arg5 to offset: [%c0], sizes: [%27], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
            %29 = memref.load %reinterpret_cast_1[%24] : memref<?xf32, #gpu.address_space<global>>
            %30 = arith.mulf %28, %29 : f32
            %31 = arith.addf %arg9, %30 : f32
            scf.yield %31 : f32
          } else {
            scf.yield %arg9 : f32
          }
          scf.yield %26 : f32
        }
        scf.yield %23 : f32
      } else {
        scf.yield %cst : f32
      }
      %reinterpret_cast = memref.reinterpret_cast %arg7 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
      memref.store %14, %reinterpret_cast[%12] : memref<256xf32, #gpu.address_space<workgroup>>
      gpu.barrier
      %15 = arith.cmpi slt, %9, %c4 : index
      scf.if %15 {
        %20 = arith.addi %12, %c4 : index
        %21 = memref.load %reinterpret_cast[%12] : memref<256xf32, #gpu.address_space<workgroup>>
        %22 = memref.load %reinterpret_cast[%20] : memref<256xf32, #gpu.address_space<workgroup>>
        %23 = arith.addf %21, %22 : f32
        memref.store %23, %reinterpret_cast[%12] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %16 = arith.cmpi slt, %9, %c2 : index
      scf.if %16 {
        %20 = arith.addi %12, %c2 : index
        %21 = memref.load %reinterpret_cast[%12] : memref<256xf32, #gpu.address_space<workgroup>>
        %22 = memref.load %reinterpret_cast[%20] : memref<256xf32, #gpu.address_space<workgroup>>
        %23 = arith.addf %21, %22 : f32
        memref.store %23, %reinterpret_cast[%12] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %17 = arith.cmpi slt, %9, %c1 : index
      scf.if %17 {
        %20 = arith.addi %12, %c1 : index
        %21 = memref.load %reinterpret_cast[%12] : memref<256xf32, #gpu.address_space<workgroup>>
        %22 = memref.load %reinterpret_cast[%20] : memref<256xf32, #gpu.address_space<workgroup>>
        %23 = arith.addf %21, %22 : f32
        memref.store %23, %reinterpret_cast[%12] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %18 = arith.cmpi eq, %9, %c0 : index
      %19 = arith.andi %18, %13 : i1
      scf.if %19 {
        %20 = memref.load %reinterpret_cast[%12] : memref<256xf32, #gpu.address_space<workgroup>>
        %21 = memref.atomic_rmw addf %20, %arg6[%10] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
      }
    }
    gpu.return
  }
}

// -----// IR Dump After CSE (cse) //----- //
gpu.module @main_kernel_2 {
  gpu.func @main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1(%arg0: index, %arg1: index, %arg2: index, %arg3: index, %arg4: memref<?x4xf32, #gpu.address_space<global>>, %arg5: memref<?x4xf32, #gpu.address_space<global>>, %arg6: memref<1xf32, #gpu.address_space<global>>) workgroup(%arg7 : memref<256xf32, #gpu.address_space<workgroup>>) kernel {
    %c2 = arith.constant 2 : index
    %cst = arith.constant 0.000000e+00 : f32
    %c4 = arith.constant 4 : index
    %c64 = arith.constant 64 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c256 = arith.constant 256 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg0, %c0]
    %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %4 = arith.addi %3, %2 : index
    %5 = arith.cmpi ult, %4, %arg1 : index
    scf.if %5 {
      %6 = arith.remsi %4, %c256 : index
      %7 = arith.divsi %4, %c256 : index
      %8 = arith.divui %6, %c32 : index
      %9 = arith.remui %6, %c32 : index
      %10 = arith.muli %9, %c8 : index
      %11 = arith.addi %8, %10 : index
      %12 = arith.cmpi ult, %9, %c1 : index
      %13 = scf.if %12 -> (f32) {
        %19 = arith.muli %7, %c8 : index
        %20 = arith.addi %8, %19 : index
        %21 = arith.muli %20, %c64 : index
        %22 = scf.for %arg8 = %c0 to %c64 step %c1 iter_args(%arg9 = %cst) -> (f32) {
          %23 = arith.addi %arg8, %21 : index
          %24 = arith.cmpi slt, %23, %arg2 : index
          %25 = scf.if %24 -> (f32) {
            %26 = arith.muli %arg3, %c4 : index
            %reinterpret_cast_0 = memref.reinterpret_cast %arg4 to offset: [%c0], sizes: [%26], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
            %27 = memref.load %reinterpret_cast_0[%23] : memref<?xf32, #gpu.address_space<global>>
            %reinterpret_cast_1 = memref.reinterpret_cast %arg5 to offset: [%c0], sizes: [%26], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
            %28 = memref.load %reinterpret_cast_1[%23] : memref<?xf32, #gpu.address_space<global>>
            %29 = arith.mulf %27, %28 : f32
            %30 = arith.addf %arg9, %29 : f32
            scf.yield %30 : f32
          } else {
            scf.yield %arg9 : f32
          }
          scf.yield %25 : f32
        }
        scf.yield %22 : f32
      } else {
        scf.yield %cst : f32
      }
      %reinterpret_cast = memref.reinterpret_cast %arg7 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
      memref.store %13, %reinterpret_cast[%11] : memref<256xf32, #gpu.address_space<workgroup>>
      gpu.barrier
      %14 = arith.cmpi slt, %8, %c4 : index
      scf.if %14 {
        %19 = arith.addi %11, %c4 : index
        %20 = memref.load %reinterpret_cast[%11] : memref<256xf32, #gpu.address_space<workgroup>>
        %21 = memref.load %reinterpret_cast[%19] : memref<256xf32, #gpu.address_space<workgroup>>
        %22 = arith.addf %20, %21 : f32
        memref.store %22, %reinterpret_cast[%11] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %15 = arith.cmpi slt, %8, %c2 : index
      scf.if %15 {
        %19 = arith.addi %11, %c2 : index
        %20 = memref.load %reinterpret_cast[%11] : memref<256xf32, #gpu.address_space<workgroup>>
        %21 = memref.load %reinterpret_cast[%19] : memref<256xf32, #gpu.address_space<workgroup>>
        %22 = arith.addf %20, %21 : f32
        memref.store %22, %reinterpret_cast[%11] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %16 = arith.cmpi slt, %8, %c1 : index
      scf.if %16 {
        %19 = arith.addi %11, %c1 : index
        %20 = memref.load %reinterpret_cast[%11] : memref<256xf32, #gpu.address_space<workgroup>>
        %21 = memref.load %reinterpret_cast[%19] : memref<256xf32, #gpu.address_space<workgroup>>
        %22 = arith.addf %20, %21 : f32
        memref.store %22, %reinterpret_cast[%11] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %17 = arith.cmpi eq, %8, %c0 : index
      %18 = arith.andi %17, %12 : i1
      scf.if %18 {
        %19 = memref.load %reinterpret_cast[%11] : memref<256xf32, #gpu.address_space<workgroup>>
        %20 = memref.atomic_rmw addf %19, %arg6[%9] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
      }
    }
    gpu.return
  }
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
gpu.module @main_kernel_2 {
  gpu.func @main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1(%arg0: index, %arg1: index, %arg2: index, %arg3: index, %arg4: memref<?x4xf32, #gpu.address_space<global>>, %arg5: memref<?x4xf32, #gpu.address_space<global>>, %arg6: memref<1xf32, #gpu.address_space<global>>) workgroup(%arg7 : memref<256xf32, #gpu.address_space<workgroup>>) kernel {
    %c2 = arith.constant 2 : index
    %cst = arith.constant 0.000000e+00 : f32
    %c4 = arith.constant 4 : index
    %c64 = arith.constant 64 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c256 = arith.constant 256 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg0, %c0]
    %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %4 = arith.addi %3, %2 : index
    %5 = arith.cmpi ult, %4, %arg1 : index
    cf.cond_br %5, ^bb2, ^bb22
  ^bb2:  // pred: ^bb1
    %6 = arith.remsi %4, %c256 : index
    %7 = arith.divsi %4, %c256 : index
    %8 = arith.divui %6, %c32 : index
    %9 = arith.remui %6, %c32 : index
    %10 = arith.muli %9, %c8 : index
    %11 = arith.addi %8, %10 : index
    %12 = arith.cmpi ult, %9, %c1 : index
    cf.cond_br %12, ^bb3, ^bb11
  ^bb3:  // pred: ^bb2
    %13 = arith.muli %7, %c8 : index
    %14 = arith.addi %8, %13 : index
    %15 = arith.muli %14, %c64 : index
    cf.br ^bb4(%c0, %cst : index, f32)
  ^bb4(%16: index, %17: f32):  // 2 preds: ^bb3, ^bb9
    %18 = arith.cmpi slt, %16, %c64 : index
    cf.cond_br %18, ^bb5, ^bb10
  ^bb5:  // pred: ^bb4
    %19 = arith.addi %16, %15 : index
    %20 = arith.cmpi slt, %19, %arg2 : index
    cf.cond_br %20, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %21 = arith.muli %arg3, %c4 : index
    %reinterpret_cast = memref.reinterpret_cast %arg4 to offset: [%c0], sizes: [%21], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
    %22 = memref.load %reinterpret_cast[%19] : memref<?xf32, #gpu.address_space<global>>
    %reinterpret_cast_0 = memref.reinterpret_cast %arg5 to offset: [%c0], sizes: [%21], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
    %23 = memref.load %reinterpret_cast_0[%19] : memref<?xf32, #gpu.address_space<global>>
    %24 = arith.mulf %22, %23 : f32
    %25 = arith.addf %17, %24 : f32
    cf.br ^bb8(%25 : f32)
  ^bb7:  // pred: ^bb5
    cf.br ^bb8(%17 : f32)
  ^bb8(%26: f32):  // 2 preds: ^bb6, ^bb7
    cf.br ^bb9
  ^bb9:  // pred: ^bb8
    %27 = arith.addi %16, %c1 : index
    cf.br ^bb4(%27, %26 : index, f32)
  ^bb10:  // pred: ^bb4
    cf.br ^bb12(%17 : f32)
  ^bb11:  // pred: ^bb2
    cf.br ^bb12(%cst : f32)
  ^bb12(%28: f32):  // 2 preds: ^bb10, ^bb11
    cf.br ^bb13
  ^bb13:  // pred: ^bb12
    %reinterpret_cast_1 = memref.reinterpret_cast %arg7 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
    memref.store %28, %reinterpret_cast_1[%11] : memref<256xf32, #gpu.address_space<workgroup>>
    gpu.barrier
    %29 = arith.cmpi slt, %8, %c4 : index
    cf.cond_br %29, ^bb14, ^bb15
  ^bb14:  // pred: ^bb13
    %30 = arith.addi %11, %c4 : index
    %31 = memref.load %reinterpret_cast_1[%11] : memref<256xf32, #gpu.address_space<workgroup>>
    %32 = memref.load %reinterpret_cast_1[%30] : memref<256xf32, #gpu.address_space<workgroup>>
    %33 = arith.addf %31, %32 : f32
    memref.store %33, %reinterpret_cast_1[%11] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb15
  ^bb15:  // 2 preds: ^bb13, ^bb14
    gpu.barrier
    %34 = arith.cmpi slt, %8, %c2 : index
    cf.cond_br %34, ^bb16, ^bb17
  ^bb16:  // pred: ^bb15
    %35 = arith.addi %11, %c2 : index
    %36 = memref.load %reinterpret_cast_1[%11] : memref<256xf32, #gpu.address_space<workgroup>>
    %37 = memref.load %reinterpret_cast_1[%35] : memref<256xf32, #gpu.address_space<workgroup>>
    %38 = arith.addf %36, %37 : f32
    memref.store %38, %reinterpret_cast_1[%11] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb17
  ^bb17:  // 2 preds: ^bb15, ^bb16
    gpu.barrier
    %39 = arith.cmpi slt, %8, %c1 : index
    cf.cond_br %39, ^bb18, ^bb19
  ^bb18:  // pred: ^bb17
    %40 = arith.addi %11, %c1 : index
    %41 = memref.load %reinterpret_cast_1[%11] : memref<256xf32, #gpu.address_space<workgroup>>
    %42 = memref.load %reinterpret_cast_1[%40] : memref<256xf32, #gpu.address_space<workgroup>>
    %43 = arith.addf %41, %42 : f32
    memref.store %43, %reinterpret_cast_1[%11] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb19
  ^bb19:  // 2 preds: ^bb17, ^bb18
    gpu.barrier
    %44 = arith.cmpi eq, %8, %c0 : index
    %45 = arith.andi %44, %12 : i1
    cf.cond_br %45, ^bb20, ^bb21
  ^bb20:  // pred: ^bb19
    %46 = memref.load %reinterpret_cast_1[%11] : memref<256xf32, #gpu.address_space<workgroup>>
    %47 = memref.atomic_rmw addf %46, %arg6[%9] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
    cf.br ^bb21
  ^bb21:  // 2 preds: ^bb19, ^bb20
    cf.br ^bb22
  ^bb22:  // 2 preds: ^bb1, ^bb21
    gpu.return
  }
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
gpu.module @main_kernel_2 {
  gpu.func @main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1(%arg0: index, %arg1: index, %arg2: index, %arg3: index, %arg4: memref<?x4xf32, #gpu.address_space<global>>, %arg5: memref<?x4xf32, #gpu.address_space<global>>, %arg6: memref<1xf32, #gpu.address_space<global>>) workgroup(%arg7 : memref<256xf32, #gpu.address_space<workgroup>>) kernel {
    %c2 = arith.constant 2 : index
    %cst = arith.constant 0.000000e+00 : f32
    %c4 = arith.constant 4 : index
    %c64 = arith.constant 64 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c256 = arith.constant 256 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = arith.muli %0, %arg0 : index
    %3 = arith.addi %2, %c0 : index
    %4 = arith.muli %1, %c1 : index
    %5 = arith.addi %4, %c0 : index
    %6 = arith.addi %5, %3 : index
    %7 = arith.cmpi ult, %6, %arg1 : index
    cf.cond_br %7, ^bb2, ^bb22
  ^bb2:  // pred: ^bb1
    %8 = arith.remsi %6, %c256 : index
    %9 = arith.divsi %6, %c256 : index
    %10 = arith.divui %8, %c32 : index
    %11 = arith.remui %8, %c32 : index
    %12 = arith.muli %11, %c8 : index
    %13 = arith.addi %10, %12 : index
    %14 = arith.cmpi ult, %11, %c1 : index
    cf.cond_br %14, ^bb3, ^bb11
  ^bb3:  // pred: ^bb2
    %15 = arith.muli %9, %c8 : index
    %16 = arith.addi %10, %15 : index
    %17 = arith.muli %16, %c64 : index
    cf.br ^bb4(%c0, %cst : index, f32)
  ^bb4(%18: index, %19: f32):  // 2 preds: ^bb3, ^bb9
    %20 = arith.cmpi slt, %18, %c64 : index
    cf.cond_br %20, ^bb5, ^bb10
  ^bb5:  // pred: ^bb4
    %21 = arith.addi %18, %17 : index
    %22 = arith.cmpi slt, %21, %arg2 : index
    cf.cond_br %22, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %23 = arith.muli %arg3, %c4 : index
    %reinterpret_cast = memref.reinterpret_cast %arg4 to offset: [%c0], sizes: [%23], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
    %24 = memref.load %reinterpret_cast[%21] : memref<?xf32, #gpu.address_space<global>>
    %reinterpret_cast_0 = memref.reinterpret_cast %arg5 to offset: [%c0], sizes: [%23], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
    %25 = memref.load %reinterpret_cast_0[%21] : memref<?xf32, #gpu.address_space<global>>
    %26 = arith.mulf %24, %25 : f32
    %27 = arith.addf %19, %26 : f32
    cf.br ^bb8(%27 : f32)
  ^bb7:  // pred: ^bb5
    cf.br ^bb8(%19 : f32)
  ^bb8(%28: f32):  // 2 preds: ^bb6, ^bb7
    cf.br ^bb9
  ^bb9:  // pred: ^bb8
    %29 = arith.addi %18, %c1 : index
    cf.br ^bb4(%29, %28 : index, f32)
  ^bb10:  // pred: ^bb4
    cf.br ^bb12(%19 : f32)
  ^bb11:  // pred: ^bb2
    cf.br ^bb12(%cst : f32)
  ^bb12(%30: f32):  // 2 preds: ^bb10, ^bb11
    cf.br ^bb13
  ^bb13:  // pred: ^bb12
    %reinterpret_cast_1 = memref.reinterpret_cast %arg7 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
    memref.store %30, %reinterpret_cast_1[%13] : memref<256xf32, #gpu.address_space<workgroup>>
    gpu.barrier
    %31 = arith.cmpi slt, %10, %c4 : index
    cf.cond_br %31, ^bb14, ^bb15
  ^bb14:  // pred: ^bb13
    %32 = arith.addi %13, %c4 : index
    %33 = memref.load %reinterpret_cast_1[%13] : memref<256xf32, #gpu.address_space<workgroup>>
    %34 = memref.load %reinterpret_cast_1[%32] : memref<256xf32, #gpu.address_space<workgroup>>
    %35 = arith.addf %33, %34 : f32
    memref.store %35, %reinterpret_cast_1[%13] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb15
  ^bb15:  // 2 preds: ^bb13, ^bb14
    gpu.barrier
    %36 = arith.cmpi slt, %10, %c2 : index
    cf.cond_br %36, ^bb16, ^bb17
  ^bb16:  // pred: ^bb15
    %37 = arith.addi %13, %c2 : index
    %38 = memref.load %reinterpret_cast_1[%13] : memref<256xf32, #gpu.address_space<workgroup>>
    %39 = memref.load %reinterpret_cast_1[%37] : memref<256xf32, #gpu.address_space<workgroup>>
    %40 = arith.addf %38, %39 : f32
    memref.store %40, %reinterpret_cast_1[%13] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb17
  ^bb17:  // 2 preds: ^bb15, ^bb16
    gpu.barrier
    %41 = arith.cmpi slt, %10, %c1 : index
    cf.cond_br %41, ^bb18, ^bb19
  ^bb18:  // pred: ^bb17
    %42 = arith.addi %13, %c1 : index
    %43 = memref.load %reinterpret_cast_1[%13] : memref<256xf32, #gpu.address_space<workgroup>>
    %44 = memref.load %reinterpret_cast_1[%42] : memref<256xf32, #gpu.address_space<workgroup>>
    %45 = arith.addf %43, %44 : f32
    memref.store %45, %reinterpret_cast_1[%13] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb19
  ^bb19:  // 2 preds: ^bb17, ^bb18
    gpu.barrier
    %46 = arith.cmpi eq, %10, %c0 : index
    %47 = arith.andi %46, %14 : i1
    cf.cond_br %47, ^bb20, ^bb21
  ^bb20:  // pred: ^bb19
    %48 = memref.load %reinterpret_cast_1[%13] : memref<256xf32, #gpu.address_space<workgroup>>
    %49 = memref.atomic_rmw addf %48, %arg6[%11] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
    cf.br ^bb21
  ^bb21:  // 2 preds: ^bb19, ^bb20
    cf.br ^bb22
  ^bb22:  // 2 preds: ^bb1, ^bb21
    gpu.return
  }
}

// -----// IR Dump After StripDebugInfo (strip-debuginfo) //----- //
gpu.module @main_kernel_2 {
  gpu.func @main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1(%arg0: index, %arg1: index, %arg2: index, %arg3: index, %arg4: memref<?x4xf32, #gpu.address_space<global>>, %arg5: memref<?x4xf32, #gpu.address_space<global>>, %arg6: memref<1xf32, #gpu.address_space<global>>) workgroup(%arg7 : memref<256xf32, #gpu.address_space<workgroup>>) kernel {
    %c2 = arith.constant 2 : index
    %cst = arith.constant 0.000000e+00 : f32
    %c4 = arith.constant 4 : index
    %c64 = arith.constant 64 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c256 = arith.constant 256 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = arith.muli %0, %arg0 : index
    %3 = arith.addi %2, %c0 : index
    %4 = arith.muli %1, %c1 : index
    %5 = arith.addi %4, %c0 : index
    %6 = arith.addi %5, %3 : index
    %7 = arith.cmpi ult, %6, %arg1 : index
    cf.cond_br %7, ^bb2, ^bb22
  ^bb2:  // pred: ^bb1
    %8 = arith.remsi %6, %c256 : index
    %9 = arith.divsi %6, %c256 : index
    %10 = arith.divui %8, %c32 : index
    %11 = arith.remui %8, %c32 : index
    %12 = arith.muli %11, %c8 : index
    %13 = arith.addi %10, %12 : index
    %14 = arith.cmpi ult, %11, %c1 : index
    cf.cond_br %14, ^bb3, ^bb11
  ^bb3:  // pred: ^bb2
    %15 = arith.muli %9, %c8 : index
    %16 = arith.addi %10, %15 : index
    %17 = arith.muli %16, %c64 : index
    cf.br ^bb4(%c0, %cst : index, f32)
  ^bb4(%18: index, %19: f32):  // 2 preds: ^bb3, ^bb9
    %20 = arith.cmpi slt, %18, %c64 : index
    cf.cond_br %20, ^bb5, ^bb10
  ^bb5:  // pred: ^bb4
    %21 = arith.addi %18, %17 : index
    %22 = arith.cmpi slt, %21, %arg2 : index
    cf.cond_br %22, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %23 = arith.muli %arg3, %c4 : index
    %reinterpret_cast = memref.reinterpret_cast %arg4 to offset: [%c0], sizes: [%23], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
    %24 = memref.load %reinterpret_cast[%21] : memref<?xf32, #gpu.address_space<global>>
    %reinterpret_cast_0 = memref.reinterpret_cast %arg5 to offset: [%c0], sizes: [%23], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
    %25 = memref.load %reinterpret_cast_0[%21] : memref<?xf32, #gpu.address_space<global>>
    %26 = arith.mulf %24, %25 : f32
    %27 = arith.addf %19, %26 : f32
    cf.br ^bb8(%27 : f32)
  ^bb7:  // pred: ^bb5
    cf.br ^bb8(%19 : f32)
  ^bb8(%28: f32):  // 2 preds: ^bb6, ^bb7
    cf.br ^bb9
  ^bb9:  // pred: ^bb8
    %29 = arith.addi %18, %c1 : index
    cf.br ^bb4(%29, %28 : index, f32)
  ^bb10:  // pred: ^bb4
    cf.br ^bb12(%19 : f32)
  ^bb11:  // pred: ^bb2
    cf.br ^bb12(%cst : f32)
  ^bb12(%30: f32):  // 2 preds: ^bb10, ^bb11
    cf.br ^bb13
  ^bb13:  // pred: ^bb12
    %reinterpret_cast_1 = memref.reinterpret_cast %arg7 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
    memref.store %30, %reinterpret_cast_1[%13] : memref<256xf32, #gpu.address_space<workgroup>>
    gpu.barrier
    %31 = arith.cmpi slt, %10, %c4 : index
    cf.cond_br %31, ^bb14, ^bb15
  ^bb14:  // pred: ^bb13
    %32 = arith.addi %13, %c4 : index
    %33 = memref.load %reinterpret_cast_1[%13] : memref<256xf32, #gpu.address_space<workgroup>>
    %34 = memref.load %reinterpret_cast_1[%32] : memref<256xf32, #gpu.address_space<workgroup>>
    %35 = arith.addf %33, %34 : f32
    memref.store %35, %reinterpret_cast_1[%13] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb15
  ^bb15:  // 2 preds: ^bb13, ^bb14
    gpu.barrier
    %36 = arith.cmpi slt, %10, %c2 : index
    cf.cond_br %36, ^bb16, ^bb17
  ^bb16:  // pred: ^bb15
    %37 = arith.addi %13, %c2 : index
    %38 = memref.load %reinterpret_cast_1[%13] : memref<256xf32, #gpu.address_space<workgroup>>
    %39 = memref.load %reinterpret_cast_1[%37] : memref<256xf32, #gpu.address_space<workgroup>>
    %40 = arith.addf %38, %39 : f32
    memref.store %40, %reinterpret_cast_1[%13] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb17
  ^bb17:  // 2 preds: ^bb15, ^bb16
    gpu.barrier
    %41 = arith.cmpi slt, %10, %c1 : index
    cf.cond_br %41, ^bb18, ^bb19
  ^bb18:  // pred: ^bb17
    %42 = arith.addi %13, %c1 : index
    %43 = memref.load %reinterpret_cast_1[%13] : memref<256xf32, #gpu.address_space<workgroup>>
    %44 = memref.load %reinterpret_cast_1[%42] : memref<256xf32, #gpu.address_space<workgroup>>
    %45 = arith.addf %43, %44 : f32
    memref.store %45, %reinterpret_cast_1[%13] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb19
  ^bb19:  // 2 preds: ^bb17, ^bb18
    gpu.barrier
    %46 = arith.cmpi eq, %10, %c0 : index
    %47 = arith.andi %46, %14 : i1
    cf.cond_br %47, ^bb20, ^bb21
  ^bb20:  // pred: ^bb19
    %48 = memref.load %reinterpret_cast_1[%13] : memref<256xf32, #gpu.address_space<workgroup>>
    %49 = memref.atomic_rmw addf %48, %arg6[%11] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
    cf.br ^bb21
  ^bb21:  // 2 preds: ^bb19, ^bb20
    cf.br ^bb22
  ^bb22:  // 2 preds: ^bb1, ^bb21
    gpu.return
  }
}

// -----// IR Dump After DiscLowerGpuOpsToNVVMOpsPass (disc-convert-gpu-to-nvvm) //----- //
gpu.module @main_kernel_2 {
  llvm.mlir.global internal @__wg_main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1_0() {addr_space = 3 : i32} : !llvm.array<256 x f32>
  llvm.func @main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1(%arg0: i32, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: !llvm.ptr<1>, %arg5: !llvm.ptr<1>, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: !llvm.ptr<1>, %arg12: !llvm.ptr<1>, %arg13: i32, %arg14: i32, %arg15: i32, %arg16: i32, %arg17: i32, %arg18: !llvm.ptr<1>, %arg19: !llvm.ptr<1>, %arg20: i32, %arg21: i32, %arg22: i32) attributes {gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.undef : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)>
    %1 = llvm.insertvalue %arg4, %0[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %2 = llvm.insertvalue %arg5, %1[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %3 = llvm.insertvalue %arg6, %2[2] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %4 = llvm.insertvalue %arg7, %3[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %5 = llvm.insertvalue %arg9, %4[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %6 = llvm.insertvalue %arg8, %5[3, 1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %7 = llvm.insertvalue %arg10, %6[4, 1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %8 = llvm.mlir.undef : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)>
    %9 = llvm.insertvalue %arg11, %8[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %10 = llvm.insertvalue %arg12, %9[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %11 = llvm.insertvalue %arg13, %10[2] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %12 = llvm.insertvalue %arg14, %11[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %13 = llvm.insertvalue %arg16, %12[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %14 = llvm.insertvalue %arg15, %13[3, 1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %15 = llvm.insertvalue %arg17, %14[4, 1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %16 = llvm.mlir.undef : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)>
    %17 = llvm.insertvalue %arg18, %16[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %18 = llvm.insertvalue %arg19, %17[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %19 = llvm.insertvalue %arg20, %18[2] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %20 = llvm.insertvalue %arg21, %19[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %21 = llvm.insertvalue %arg22, %20[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %22 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1_0 : !llvm.ptr<3>
    %23 = llvm.getelementptr %22[0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<256 x f32>
    %24 = llvm.mlir.undef : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)>
    %25 = llvm.insertvalue %23, %24[0] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %26 = llvm.insertvalue %23, %25[1] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %27 = llvm.mlir.constant(0 : index) : i32
    %28 = llvm.insertvalue %27, %26[2] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %29 = llvm.mlir.constant(256 : index) : i32
    %30 = llvm.insertvalue %29, %28[3, 0] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %31 = llvm.mlir.constant(1 : index) : i32
    %32 = llvm.insertvalue %31, %30[4, 0] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %33 = llvm.mlir.constant(2 : index) : i32
    %34 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    %35 = llvm.mlir.constant(4 : index) : i32
    %36 = llvm.mlir.constant(64 : index) : i32
    %37 = llvm.mlir.constant(8 : index) : i32
    %38 = llvm.mlir.constant(32 : index) : i32
    %39 = llvm.mlir.constant(256 : index) : i32
    %40 = llvm.mlir.constant(1 : index) : i32
    %41 = llvm.mlir.constant(0 : index) : i32
    %42 = nvvm.read.ptx.sreg.ctaid.x : i32
    %43 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %44 = llvm.mul %42, %arg0  : i32
    %45 = llvm.add %43, %44  : i32
    %46 = llvm.icmp "ult" %45, %arg1 : i32
    llvm.cond_br %46, ^bb2, ^bb21
  ^bb2:  // pred: ^bb1
    %47 = llvm.srem %45, %39  : i32
    %48 = llvm.sdiv %45, %39  : i32
    %49 = llvm.udiv %47, %38  : i32
    %50 = llvm.urem %47, %38  : i32
    %51 = llvm.mul %50, %37  : i32
    %52 = llvm.add %49, %51  : i32
    %53 = llvm.icmp "ult" %50, %40 : i32
    llvm.cond_br %53, ^bb3, ^bb10(%34 : f32)
  ^bb3:  // pred: ^bb2
    %54 = llvm.mul %48, %37  : i32
    %55 = llvm.add %49, %54  : i32
    %56 = llvm.mul %55, %36  : i32
    llvm.br ^bb4(%41, %34 : i32, f32)
  ^bb4(%57: i32, %58: f32):  // 2 preds: ^bb3, ^bb9
    %59 = llvm.icmp "slt" %57, %36 : i32
    llvm.cond_br %59, ^bb5, ^bb10(%58 : f32)
  ^bb5:  // pred: ^bb4
    %60 = llvm.add %57, %56  : i32
    %61 = llvm.icmp "slt" %60, %arg2 : i32
    llvm.cond_br %61, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %62 = llvm.mul %arg3, %35  : i32
    %63 = llvm.mlir.undef : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)>
    %64 = llvm.extractvalue %7[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %65 = llvm.extractvalue %7[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %66 = llvm.insertvalue %64, %63[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %67 = llvm.insertvalue %65, %66[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %68 = llvm.insertvalue %41, %67[2] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %69 = llvm.insertvalue %62, %68[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %70 = llvm.insertvalue %40, %69[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %71 = llvm.extractvalue %70[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %72 = llvm.getelementptr %71[%60] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    %73 = llvm.load %72 : !llvm.ptr<1> -> f32
    %74 = llvm.mlir.undef : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)>
    %75 = llvm.extractvalue %15[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %76 = llvm.extractvalue %15[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %77 = llvm.insertvalue %75, %74[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %78 = llvm.insertvalue %76, %77[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %79 = llvm.insertvalue %41, %78[2] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %80 = llvm.insertvalue %62, %79[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %81 = llvm.insertvalue %40, %80[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %82 = llvm.extractvalue %81[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %83 = llvm.getelementptr %82[%60] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    %84 = llvm.load %83 : !llvm.ptr<1> -> f32
    %85 = llvm.fmul %73, %84  : f32
    %86 = llvm.fadd %58, %85  : f32
    llvm.br ^bb8(%86 : f32)
  ^bb7:  // pred: ^bb5
    llvm.br ^bb8(%58 : f32)
  ^bb8(%87: f32):  // 2 preds: ^bb6, ^bb7
    llvm.br ^bb9
  ^bb9:  // pred: ^bb8
    %88 = llvm.add %57, %40  : i32
    llvm.br ^bb4(%88, %87 : i32, f32)
  ^bb10(%89: f32):  // 2 preds: ^bb2, ^bb4
    llvm.br ^bb11(%89 : f32)
  ^bb11(%90: f32):  // pred: ^bb10
    llvm.br ^bb12
  ^bb12:  // pred: ^bb11
    %91 = llvm.mlir.undef : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)>
    %92 = llvm.extractvalue %32[0] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %93 = llvm.extractvalue %32[1] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %94 = llvm.insertvalue %92, %91[0] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %95 = llvm.insertvalue %93, %94[1] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %96 = llvm.mlir.constant(0 : index) : i32
    %97 = llvm.insertvalue %96, %95[2] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %98 = llvm.mlir.constant(256 : index) : i32
    %99 = llvm.insertvalue %98, %97[3, 0] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %100 = llvm.mlir.constant(1 : index) : i32
    %101 = llvm.insertvalue %100, %99[4, 0] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %102 = llvm.extractvalue %101[1] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %103 = llvm.getelementptr %102[%52] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    llvm.store %90, %103 : f32, !llvm.ptr<3>
    nvvm.barrier0
    %104 = llvm.icmp "slt" %49, %35 : i32
    llvm.cond_br %104, ^bb13, ^bb14
  ^bb13:  // pred: ^bb12
    %105 = llvm.add %52, %35  : i32
    %106 = llvm.extractvalue %101[1] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %107 = llvm.getelementptr %106[%52] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %108 = llvm.load %107 : !llvm.ptr<3> -> f32
    %109 = llvm.extractvalue %101[1] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %110 = llvm.getelementptr %109[%105] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %111 = llvm.load %110 : !llvm.ptr<3> -> f32
    %112 = llvm.fadd %108, %111  : f32
    %113 = llvm.extractvalue %101[1] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %114 = llvm.getelementptr %113[%52] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    llvm.store %112, %114 : f32, !llvm.ptr<3>
    llvm.br ^bb14
  ^bb14:  // 2 preds: ^bb12, ^bb13
    nvvm.barrier0
    %115 = llvm.icmp "slt" %49, %33 : i32
    llvm.cond_br %115, ^bb15, ^bb16
  ^bb15:  // pred: ^bb14
    %116 = llvm.add %52, %33  : i32
    %117 = llvm.extractvalue %101[1] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %118 = llvm.getelementptr %117[%52] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %119 = llvm.load %118 : !llvm.ptr<3> -> f32
    %120 = llvm.extractvalue %101[1] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %121 = llvm.getelementptr %120[%116] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %122 = llvm.load %121 : !llvm.ptr<3> -> f32
    %123 = llvm.fadd %119, %122  : f32
    %124 = llvm.extractvalue %101[1] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %125 = llvm.getelementptr %124[%52] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    llvm.store %123, %125 : f32, !llvm.ptr<3>
    llvm.br ^bb16
  ^bb16:  // 2 preds: ^bb14, ^bb15
    nvvm.barrier0
    %126 = llvm.icmp "slt" %49, %40 : i32
    llvm.cond_br %126, ^bb17, ^bb18
  ^bb17:  // pred: ^bb16
    %127 = llvm.add %52, %40  : i32
    %128 = llvm.extractvalue %101[1] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %129 = llvm.getelementptr %128[%52] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %130 = llvm.load %129 : !llvm.ptr<3> -> f32
    %131 = llvm.extractvalue %101[1] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %132 = llvm.getelementptr %131[%127] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %133 = llvm.load %132 : !llvm.ptr<3> -> f32
    %134 = llvm.fadd %130, %133  : f32
    %135 = llvm.extractvalue %101[1] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %136 = llvm.getelementptr %135[%52] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    llvm.store %134, %136 : f32, !llvm.ptr<3>
    llvm.br ^bb18
  ^bb18:  // 2 preds: ^bb16, ^bb17
    nvvm.barrier0
    %137 = llvm.icmp "eq" %49, %41 : i32
    %138 = llvm.and %137, %53  : i1
    llvm.cond_br %138, ^bb19, ^bb20
  ^bb19:  // pred: ^bb18
    %139 = llvm.extractvalue %101[1] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %140 = llvm.getelementptr %139[%52] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %141 = llvm.load %140 : !llvm.ptr<3> -> f32
    %142 = llvm.extractvalue %21[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %143 = llvm.getelementptr %142[%50] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    %144 = llvm.atomicrmw fadd %143, %141 acq_rel : !llvm.ptr<1>, f32
    llvm.br ^bb20
  ^bb20:  // 2 preds: ^bb18, ^bb19
    llvm.br ^bb21
  ^bb21:  // 2 preds: ^bb1, ^bb20
    llvm.return
  }
}

// -----// IR Dump After LLVMInsertValueSimplifierPass (disc-llvm-insert-value-simplifier) //----- //
llvm.func @main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1(%arg0: i32, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: !llvm.ptr<1>, %arg5: !llvm.ptr<1>, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: !llvm.ptr<1>, %arg12: !llvm.ptr<1>, %arg13: i32, %arg14: i32, %arg15: i32, %arg16: i32, %arg17: i32, %arg18: !llvm.ptr<1>, %arg19: !llvm.ptr<1>, %arg20: i32, %arg21: i32, %arg22: i32) attributes {gpu.kernel, nvvm.kernel} {
  %0 = llvm.mlir.constant(32 : index) : i32
  %1 = llvm.mlir.constant(8 : index) : i32
  %2 = llvm.mlir.constant(64 : index) : i32
  %3 = llvm.mlir.constant(4 : index) : i32
  %4 = llvm.mlir.constant(0.000000e+00 : f32) : f32
  %5 = llvm.mlir.constant(2 : index) : i32
  %6 = llvm.mlir.constant(1 : index) : i32
  %7 = llvm.mlir.constant(256 : index) : i32
  %8 = llvm.mlir.constant(0 : index) : i32
  %9 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1_0 : !llvm.ptr<3>
  %10 = llvm.getelementptr %9[0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<256 x f32>
  %11 = nvvm.read.ptx.sreg.ctaid.x : i32
  %12 = nvvm.read.ptx.sreg.tid.x : i32
  llvm.br ^bb1
^bb1:  // pred: ^bb0
  %13 = llvm.mul %11, %arg0  : i32
  %14 = llvm.add %12, %13  : i32
  %15 = llvm.icmp "ult" %14, %arg1 : i32
  llvm.cond_br %15, ^bb2, ^bb21
^bb2:  // pred: ^bb1
  %16 = llvm.srem %14, %7  : i32
  %17 = llvm.sdiv %14, %7  : i32
  %18 = llvm.udiv %16, %0  : i32
  %19 = llvm.urem %16, %0  : i32
  %20 = llvm.mul %19, %1  : i32
  %21 = llvm.add %18, %20  : i32
  %22 = llvm.icmp "ult" %19, %6 : i32
  llvm.cond_br %22, ^bb3, ^bb10(%4 : f32)
^bb3:  // pred: ^bb2
  %23 = llvm.mul %17, %1  : i32
  %24 = llvm.add %18, %23  : i32
  %25 = llvm.mul %24, %2  : i32
  llvm.br ^bb4(%8, %4 : i32, f32)
^bb4(%26: i32, %27: f32):  // 2 preds: ^bb3, ^bb9
  %28 = llvm.icmp "slt" %26, %2 : i32
  llvm.cond_br %28, ^bb5, ^bb10(%27 : f32)
^bb5:  // pred: ^bb4
  %29 = llvm.add %26, %25  : i32
  %30 = llvm.icmp "slt" %29, %arg2 : i32
  llvm.cond_br %30, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  %31 = llvm.getelementptr %arg5[%29] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
  %32 = llvm.load %31 : !llvm.ptr<1> -> f32
  %33 = llvm.getelementptr %arg12[%29] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
  %34 = llvm.load %33 : !llvm.ptr<1> -> f32
  %35 = llvm.fmul %32, %34  : f32
  %36 = llvm.fadd %27, %35  : f32
  llvm.br ^bb8(%36 : f32)
^bb7:  // pred: ^bb5
  llvm.br ^bb8(%27 : f32)
^bb8(%37: f32):  // 2 preds: ^bb6, ^bb7
  llvm.br ^bb9
^bb9:  // pred: ^bb8
  %38 = llvm.add %26, %6  : i32
  llvm.br ^bb4(%38, %37 : i32, f32)
^bb10(%39: f32):  // 2 preds: ^bb2, ^bb4
  llvm.br ^bb11(%39 : f32)
^bb11(%40: f32):  // pred: ^bb10
  llvm.br ^bb12
^bb12:  // pred: ^bb11
  %41 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
  llvm.store %40, %41 : f32, !llvm.ptr<3>
  nvvm.barrier0
  %42 = llvm.icmp "slt" %18, %3 : i32
  llvm.cond_br %42, ^bb13, ^bb14
^bb13:  // pred: ^bb12
  %43 = llvm.add %21, %3  : i32
  %44 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
  %45 = llvm.load %44 : !llvm.ptr<3> -> f32
  %46 = llvm.getelementptr %10[%43] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
  %47 = llvm.load %46 : !llvm.ptr<3> -> f32
  %48 = llvm.fadd %45, %47  : f32
  %49 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
  llvm.store %48, %49 : f32, !llvm.ptr<3>
  llvm.br ^bb14
^bb14:  // 2 preds: ^bb12, ^bb13
  nvvm.barrier0
  %50 = llvm.icmp "slt" %18, %5 : i32
  llvm.cond_br %50, ^bb15, ^bb16
^bb15:  // pred: ^bb14
  %51 = llvm.add %21, %5  : i32
  %52 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
  %53 = llvm.load %52 : !llvm.ptr<3> -> f32
  %54 = llvm.getelementptr %10[%51] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
  %55 = llvm.load %54 : !llvm.ptr<3> -> f32
  %56 = llvm.fadd %53, %55  : f32
  %57 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
  llvm.store %56, %57 : f32, !llvm.ptr<3>
  llvm.br ^bb16
^bb16:  // 2 preds: ^bb14, ^bb15
  nvvm.barrier0
  %58 = llvm.icmp "slt" %18, %6 : i32
  llvm.cond_br %58, ^bb17, ^bb18
^bb17:  // pred: ^bb16
  %59 = llvm.add %21, %6  : i32
  %60 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
  %61 = llvm.load %60 : !llvm.ptr<3> -> f32
  %62 = llvm.getelementptr %10[%59] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
  %63 = llvm.load %62 : !llvm.ptr<3> -> f32
  %64 = llvm.fadd %61, %63  : f32
  %65 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
  llvm.store %64, %65 : f32, !llvm.ptr<3>
  llvm.br ^bb18
^bb18:  // 2 preds: ^bb16, ^bb17
  nvvm.barrier0
  %66 = llvm.icmp "eq" %18, %8 : i32
  %67 = llvm.and %66, %22  : i1
  llvm.cond_br %67, ^bb19, ^bb20
^bb19:  // pred: ^bb18
  %68 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
  %69 = llvm.load %68 : !llvm.ptr<3> -> f32
  %70 = llvm.getelementptr %arg19[%19] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
  %71 = llvm.atomicrmw fadd %70, %69 acq_rel : !llvm.ptr<1>, f32
  llvm.br ^bb20
^bb20:  // 2 preds: ^bb18, ^bb19
  llvm.br ^bb21
^bb21:  // 2 preds: ^bb1, ^bb20
  llvm.return
}

// -----// IR Dump After FunctionDeadArgumentEliminationPass (disc-function-dead-argument-elimination) //----- //
gpu.module @main_kernel_2 {
  llvm.mlir.global internal @__wg_main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1_0() {addr_space = 3 : i32} : !llvm.array<256 x f32>
  llvm.func @main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1(%arg0: i32, %arg1: i32, %arg2: i32, %arg3: !llvm.ptr<1>, %arg4: !llvm.ptr<1>, %arg5: !llvm.ptr<1>) attributes {disc.elimargs = [3 : index, 4 : index, 6 : index, 7 : index, 8 : index, 9 : index, 10 : index, 11 : index, 13 : index, 14 : index, 15 : index, 16 : index, 17 : index, 18 : index, 20 : index, 21 : index, 22 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(32 : index) : i32
    %1 = llvm.mlir.constant(8 : index) : i32
    %2 = llvm.mlir.constant(64 : index) : i32
    %3 = llvm.mlir.constant(4 : index) : i32
    %4 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    %5 = llvm.mlir.constant(2 : index) : i32
    %6 = llvm.mlir.constant(1 : index) : i32
    %7 = llvm.mlir.constant(256 : index) : i32
    %8 = llvm.mlir.constant(0 : index) : i32
    %9 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1_0 : !llvm.ptr<3>
    %10 = llvm.getelementptr %9[0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<256 x f32>
    %11 = nvvm.read.ptx.sreg.ctaid.x : i32
    %12 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %13 = llvm.mul %11, %arg0  : i32
    %14 = llvm.add %12, %13  : i32
    %15 = llvm.icmp "ult" %14, %arg1 : i32
    llvm.cond_br %15, ^bb2, ^bb21
  ^bb2:  // pred: ^bb1
    %16 = llvm.srem %14, %7  : i32
    %17 = llvm.sdiv %14, %7  : i32
    %18 = llvm.udiv %16, %0  : i32
    %19 = llvm.urem %16, %0  : i32
    %20 = llvm.mul %19, %1  : i32
    %21 = llvm.add %18, %20  : i32
    %22 = llvm.icmp "ult" %19, %6 : i32
    llvm.cond_br %22, ^bb3, ^bb10(%4 : f32)
  ^bb3:  // pred: ^bb2
    %23 = llvm.mul %17, %1  : i32
    %24 = llvm.add %18, %23  : i32
    %25 = llvm.mul %24, %2  : i32
    llvm.br ^bb4(%8, %4 : i32, f32)
  ^bb4(%26: i32, %27: f32):  // 2 preds: ^bb3, ^bb9
    %28 = llvm.icmp "slt" %26, %2 : i32
    llvm.cond_br %28, ^bb5, ^bb10(%27 : f32)
  ^bb5:  // pred: ^bb4
    %29 = llvm.add %26, %25  : i32
    %30 = llvm.icmp "slt" %29, %arg2 : i32
    llvm.cond_br %30, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %31 = llvm.getelementptr %arg3[%29] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    %32 = llvm.load %31 : !llvm.ptr<1> -> f32
    %33 = llvm.getelementptr %arg4[%29] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    %34 = llvm.load %33 : !llvm.ptr<1> -> f32
    %35 = llvm.fmul %32, %34  : f32
    %36 = llvm.fadd %27, %35  : f32
    llvm.br ^bb8(%36 : f32)
  ^bb7:  // pred: ^bb5
    llvm.br ^bb8(%27 : f32)
  ^bb8(%37: f32):  // 2 preds: ^bb6, ^bb7
    llvm.br ^bb9
  ^bb9:  // pred: ^bb8
    %38 = llvm.add %26, %6  : i32
    llvm.br ^bb4(%38, %37 : i32, f32)
  ^bb10(%39: f32):  // 2 preds: ^bb2, ^bb4
    llvm.br ^bb11(%39 : f32)
  ^bb11(%40: f32):  // pred: ^bb10
    llvm.br ^bb12
  ^bb12:  // pred: ^bb11
    %41 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    llvm.store %40, %41 : f32, !llvm.ptr<3>
    nvvm.barrier0
    %42 = llvm.icmp "slt" %18, %3 : i32
    llvm.cond_br %42, ^bb13, ^bb14
  ^bb13:  // pred: ^bb12
    %43 = llvm.add %21, %3  : i32
    %44 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %45 = llvm.load %44 : !llvm.ptr<3> -> f32
    %46 = llvm.getelementptr %10[%43] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %47 = llvm.load %46 : !llvm.ptr<3> -> f32
    %48 = llvm.fadd %45, %47  : f32
    %49 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    llvm.store %48, %49 : f32, !llvm.ptr<3>
    llvm.br ^bb14
  ^bb14:  // 2 preds: ^bb12, ^bb13
    nvvm.barrier0
    %50 = llvm.icmp "slt" %18, %5 : i32
    llvm.cond_br %50, ^bb15, ^bb16
  ^bb15:  // pred: ^bb14
    %51 = llvm.add %21, %5  : i32
    %52 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %53 = llvm.load %52 : !llvm.ptr<3> -> f32
    %54 = llvm.getelementptr %10[%51] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %55 = llvm.load %54 : !llvm.ptr<3> -> f32
    %56 = llvm.fadd %53, %55  : f32
    %57 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    llvm.store %56, %57 : f32, !llvm.ptr<3>
    llvm.br ^bb16
  ^bb16:  // 2 preds: ^bb14, ^bb15
    nvvm.barrier0
    %58 = llvm.icmp "slt" %18, %6 : i32
    llvm.cond_br %58, ^bb17, ^bb18
  ^bb17:  // pred: ^bb16
    %59 = llvm.add %21, %6  : i32
    %60 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %61 = llvm.load %60 : !llvm.ptr<3> -> f32
    %62 = llvm.getelementptr %10[%59] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %63 = llvm.load %62 : !llvm.ptr<3> -> f32
    %64 = llvm.fadd %61, %63  : f32
    %65 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    llvm.store %64, %65 : f32, !llvm.ptr<3>
    llvm.br ^bb18
  ^bb18:  // 2 preds: ^bb16, ^bb17
    nvvm.barrier0
    %66 = llvm.icmp "eq" %18, %8 : i32
    %67 = llvm.and %66, %22  : i1
    llvm.cond_br %67, ^bb19, ^bb20
  ^bb19:  // pred: ^bb18
    %68 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %69 = llvm.load %68 : !llvm.ptr<3> -> f32
    %70 = llvm.getelementptr %arg5[%19] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    %71 = llvm.atomicrmw fadd %70, %69 acq_rel : !llvm.ptr<1>, f32
    llvm.br ^bb20
  ^bb20:  // 2 preds: ^bb18, ^bb19
    llvm.br ^bb21
  ^bb21:  // 2 preds: ^bb1, ^bb20
    llvm.return
  }
}

// -----// IR Dump After GpuKernelToBlobPass (disc-gpu-kernel-to-blob) //----- //
gpu.module @main_kernel_2 attributes {gpu.binary = "P\EDU\BA\01\00\10\00\A8\0B\00\00\00\00\00\00\02\00\01\01@\00\00\00h\0B\00\00\00\00\00\00c\0B\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\A8#\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00g\02\00\BE\00r\00\01\00\22#\00\01\00\11 \06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\12e__6_1_0___no_ibXblock_tile_h64_1I\00\0FC\00,osharedE\00,\9Fconstant0H\00)\FA\01debug_frame\00.rel\11\00!nv\14\00\11aT\00\0Fo\01 \0F\99\00&\0F\A9\01\B6\8F$____wg_B\00&\00\1B\00/26\F1\016o_param\F8\01\1C\0F\01\00\05\8Cl\00\00\00\03\00\0A\00\01\00\11\EF\18\00,\0B\00\01\00 |\01\18\00,\09\00\01\00\11\CB\18\00,\04\00\01\00\11\E9\18\00,\07\00\01\00g2\00\00\00\12\10x\00\11\18\06\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0\17\07\00 \00\04\9B\00R\04\14\00\00\00E\002\04\C0\05\18\00C/\08\00\06\7F\00\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E0\04\F1\08\015\00\00\04\0A\08\00\03\00\00\00`\01(\00\03\19(\00\04\17\0C$\00\10\05\ED\04%\F0!\10\009\04\00\18\10\009\03\00\10\10\00u\02\00\08\00\00\F0\11\10\009\01\00\04\10\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\00\10\17\00\00`\17\00\00\04\1Et\01#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\005\02\00\00\84\01\0F\01\00\FFz@$v\01\FF?\04\A3\FF\00\8E\07\00\C4\0F\00\19y\A6\01\10%[\02a\0E\00\19y\03\00\01\00\10!-\00\F5\14\0E\00$z\06\06\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\06\00Y\00\00p`\F0\03\00\DA\0F\00M[\04\B0\80\03\00\EA\0F\00\19x\03\FF\1F\FC\01\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\00\B3\04\93\E2\0F\00Ey\00\00@\150\00\93\E2\0F\00$r\08\FF\FF\00\90\00p\E2\0F\00\11r\03\03\92\00\C1\FF@\8F\07\00\C8\0F\00\12x\05\031\04\10\C0\80\00p\0F\00$x\06\06\01\B4\03\12\0A\10\00@\12x\05\06p\00\01 \00p\E4\0F\04\19x\00\FF*\04\C0\06\16\01\00\00\E4\0F\00\0Cr\00\05`\00@pR\F2\03 \00P\0Cx\00\06\7F\10\00\22@\F0\80\002x\07\05\C9\02\11\8Ep\00T$x\07\07\04\90\00\9A\CC\0F\00G\19\00\00\80\14\E0\00\000\00\10\03\E0\00\01\90\00*\03\03@\004\09\03@@\00q\C8\0F\00%x\04\09P\00\11\02\E0\00P\04\10x\00\09\C0\001\FF\E0\FF\B0\00\B0\0Cz\00\09\00Z\00\00pb\F4\A0\00P\00\12x\04\04P\00!\FF\FC\D0\00\00p\01\12\00 \00\11\F6 \00`\10z\02\04\00\\@\00\11\F3@\00`\10z\04\04\00^\10\00\11\F9\D0\01\00`\00\00|\03\03`\00\D2\00\10z\03\05\00]\00\00\FF\E4\FF\000\00@\05\05\00_\10\00*\7F\02`\00\11\F8\10\01\F4\06\81\B9\0D\02\04\FC\FF\FF\00\19\1E\0C\00\A2\0E\00\10x\0A\09\03P\00\93\C6\0F\00\81\A9\00\02\04\F8 \00 \E2\0E@\00\12\0A@\00\22\FA\03 \00%\0B\04 \00u\E8\0E\00\81\B9\0A\04P\00p\A8\0E\00\81\C9\0F\02\10\01\01\10\00y(\0F\00\81\C9\0C\04\10\00S\D9\11\02\04\04\10\00\10h\10\00%\0E\04\10\00\10b\E0\004\10\09\04\90\00\01\F0\00:\12\09\05P\01\12\10\90\00#\FC\03\E0\00\12\12\10\00\12\F2@\01Dx\10\09\060\00a\E2\0F\04#\A2\086\07\00$\021\00\E2\8F@\01\14\07 \00q\CE\0F\00\81\E9\0B\02\91\01\06\F0\00\15\10\B0\01\93\E2\0F\00#\B2\08\0D\0A\00=\05!\E2O0\01\16\08`\00`\00\81\99\0D\02\04\17\06\03P\01\09\D0\01\000\018\E9\00\04`\00U#\C2\08\0F\0CP\00&\0F\01`\01\11\F8\C0\00H\81\99\0A\04P\00\00\D0\00\19\09\A0\01C\0F\02\04\10 \00\86\22\0F\00#\D2\08\11\0EP\00\16\02\C0\00\12\FAP\00W\A9\0C\04\04\10\80\01W\B9\11\02\04\14\80\019\B9\0E\04\10\00X\C9\13\02\04\18\10\00)\10\04\10\00X\D9\15\02\04\1C\10\00H\12\04\04\1C\C0\017\14\09\0A \01U#\E2\08\0B\00\A0\00\02\80\01\17\0B\E0\01E\0Cz\00\14\D0\01\000\00\1A\92p\01\06P\01\04\E0\017\00\09\0C\10\02\00\90\01\18\0Dp\00\1D\A2`\01\15\00\D0\01\13\CA\F0\01\18 `\018\0C\09\0F`\01\00\B0\01\07 \00-#\B2`\01\19\0A\B0\03Lx\0A\09\0E\10\02\19$\F0\01$\13\10@\00\02\B0\02\06\F0\01\13\E2\D0\01\17(\A0\01\00\00\02\08@\00F\D2\08\15\12\80\00\00@\00\15\0C\E0\01\13\C4\D0\01\17,\90\01\00\F0\01\08P\00\00\E0\01\1B,\E0\01\1B0\E0\01\1B0\E0\01\1B4\E0\01\1A4\E0\01\1F\10\E0\01\06\11O\A0\01\1F\11\E0\01\16\1A\8F\E0\01\13\D6\A0\01\188\A0\01;\00\09\12\F0\01\1F\13\F0\01\15\13\E4\D0\01\1F8\D0\01\1B\16\14`\00\11\04\D0\01\16<\90\01X\10x\0E\09\15\80\00\08\E0\01\1B\E2\E0\01\04\B0\03\1B@\E0\01\1F<\E0\01\0A\1D\0E\C0\03\1B@\C0\03\1BD\C0\03\1BD\E0\01\1BH\E0\01\1BH\E0\01\1BL\E0\01\1AL\E0\01\1F\16\E0\01\0C\1F\17\E0\01-\1AP\E0\01\1F\18\D0\01\08\00\90\01\16\19\00\02\1F\00\E0\01\02\1FP\E0\01\17\01\D0\01\18T\D0\017\0A\09\1A\D0\00\00\10\04\1F\1B\E0\01\15\13\E4\E0\01\1BX\E0\01\1FT\C0\03\1C\1B\\\C0\03\1BX\C0\03\1B\\\E0\01\1B`\E0\01\1B`\E0\01\1Bd\E0\01\1Ad\E0\01\1F\1C\E0\01\0C\1F\1D\E0\01\05\13\DA\C0\01\17hp\01\0F\F0\01\09\01P\03;\00\09\1E\C0\03\1F\1F\C0\03\1D\1Fh\E0\01\14\01\C0\01<\0A\09 \C0\03\1Al\C0\03\1F!\C0\03\1D\1Bp\E0\01\1Fl\C0\03\1C\1Bp\C0\03\1Bt\C0\03\1Bt\E0\01\1Bx\E0\01\1Bx\E0\01\1B|\E0\01\1A|\E0\01\1F\22\E0\01\0C\1F#\C0\03-\16\80\90\01\00P\00\1F$\C0\03\0C\1F%\C0\03\0D\1F\80\C0\03\1C\18\84\D0\017\0A\09&\D0\00\00\C0\03\1F'\C0\03\1D\1B\88\E0\01\1F\84\C0\03\1C\1B\8C\C0\03\1B\88\C0\03\1B\8C\E0\01\1B\90\E0\01\1B\90\E0\01\1B\94\E0\01\1A\94\E0\01\1F(\E0\01\0C\1F)\C0\03\0D\1F\98\C0\03\1B\1B*\C0\03\1F+\C0\03\1D\1F\98\C0\03\1B\1C,\C0\03\1A\9C\C0\03\1F-\C0\03\1D\1B\A0\E0\01\1F\9C\C0\03\1C\1B\A0\C0\03\1B\A4\C0\03\1B\A4\E0\01\1B\A8\E0\01\1B\A8\E0\01\1B\AC\E0\01\1A\AC\E0\01\1F.\E0\01\0C\1F/\C0\03-\16\B0\90\01\00P\00\1F0\C0\03\0C\1F1\C0\03\0D\1F\B0\C0\03\1C\18\B4\D0\017\0A\092\D0\00\00\C0\03\1F3\C0\03\1D\1B\B8\E0\01\1F\B4\C0\03\1C\1B\BC\C0\03\1B\B8\C0\03\1B\BC\E0\01\1B\C0\E0\01\1B\C0\E0\01\1B\C4\E0\01\1A\C4\E0\01\1F4\E0\01\0C\1F5\C0\03\0D\1F\C8\C0\03\1B\1B6\C0\03\1F7\C0\03\1D\1F\C8\C0\03\1B\1C8\C0\03\1A\CC\C0\03\1F9\C0\03\1D\1B\D0\E0\01\1F\CC\C0\03\1C\1B\D0\C0\03\1B\D4\C0\03\1B\D4\E0\01\1B\D8\E0\01\1B\D8\E0\01\1B\DC\E0\01\1A\DC\E0\01\1F:\E0\01\0C\1F;\C0\03-\16\E0\90\01\00P\00\1F<\C0\03\0C\1F=\C0\03\0D\1F\E0\C0\03\17\00P\00\17>\C0\00\00\B0\03\17?`\00g\81\99\09\02\04\E4\A0\01\0F\C0\03\0D\00\D0\01\040\00\00P\12Y\A9\0D\02\04\E8\E0\10\0F\C0\03\0B\00\E0\01\18\E8\E0\01K\0F\02\04\EC\E0\01\1B\EC\D0\01\18\F0\10\00K\11\02\04\F0\D0\01\18\F4\10\00%\13\02\10\00\1Bb\D0\01 \C8O\B0\01\15\09\B0\01 \C8\8F\80\01\15\0D\80\01\86\C8\0F\01#\B2\08\0F\0E\10\00f\02#\C2\08\11\10\10\00\00\E0\00\15\13\E0\00P\C4\0F\00Ay\09\00\06\90\14c\88s\00\07\08\00!\00f\E8\0F\00\1D{\00\01\00S\EC\0F\00\84\89\CD\19\13\08 \01Ax\00\06?\00\15\11\F2@\03c\84\89\03\07\00\10 \00s$\0E\00!\82\00\00\02\16\10\00\F0\15'\88\83@\00\0F`\00\01-\99\02`\00\14\1F`\15\00`\00W\99\03\07\00\08`\009\92\02\02`\00O\93\00\07\02\C0\00\0A\1A\03`\005r\00\06\D0\15\01\C0\00H\04\07\00\04\C0\00J\04\03\04\00\C0\00\1F\04`\00\089M\19\00P\016\84y\07p\00\93\22\0E\00$v\02\FF\00`\D0\15\93\C4\0F\00$v\03\FF\00a\10\00a\CA\0F\00\8Ey\00\01\01\9B\84\E7\10\0C\00\E2\1F\00M\A0\01PGy\00\00\F0\C0\16\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00`\0F\01\00-'\01\00 \02\07\01\00\22@\00\01\00=o\01\000\00\08\01\00\1F\0B@\00\04\13\AF)\00\1F\F8@\00\0C\13\13\\\1A\0C\01\00\13\A8U\00*\A8\00\80\1A\22\08\00\01\00\22\18\00\01\00.B\01T\00\00\01\00\13P5\02/p\00\80\00\0B/)\00'\00\02#\00\C0@\00\00,\09\17\00\E4\00*\04\00\01\00\1Fr@\00\04\13\F0)\00&\98\00@\00\1F\0A@\00\00!`\01D\01\0D@\001\88\05\00\01\00*\D8\00\01\00\1B\08\08\00?O\01\00\0E\1E\00Q\00\00`\06\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\FA\14\01\0C\84\01\13p@\00\17\881\01\0F\C0\00\01\132@\00\15\06R\00\0B\A1\00\16\18\80\00j\06\00\00\18\80\00\01\00\13\B5+\00+\03\00\01\00\00\D5\0F\1A\00q\00\0F\80\00\01\11\06\F0\1D\07\E8\22\0CH\02\1A\00\08\00\04\97\00\13\018\00\04\E8\00\0C\01\00*\90\19\08\00\088\00\18\06\A0\00\0F\01\00\05\03\A9\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00"} {
  llvm.mlir.global internal @__wg_main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1_0() {addr_space = 3 : i32} : !llvm.array<256 x f32>
  llvm.func @main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1(%arg0: i32, %arg1: i32, %arg2: i32, %arg3: !llvm.ptr<1>, %arg4: !llvm.ptr<1>, %arg5: !llvm.ptr<1>) attributes {disc.elimargs = [3 : index, 4 : index, 6 : index, 7 : index, 8 : index, 9 : index, 10 : index, 11 : index, 13 : index, 14 : index, 15 : index, 16 : index, 17 : index, 18 : index, 20 : index, 21 : index, 22 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(32 : index) : i32
    %1 = llvm.mlir.constant(8 : index) : i32
    %2 = llvm.mlir.constant(64 : index) : i32
    %3 = llvm.mlir.constant(4 : index) : i32
    %4 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    %5 = llvm.mlir.constant(2 : index) : i32
    %6 = llvm.mlir.constant(1 : index) : i32
    %7 = llvm.mlir.constant(256 : index) : i32
    %8 = llvm.mlir.constant(0 : index) : i32
    %9 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1_0 : !llvm.ptr<3>
    %10 = llvm.getelementptr %9[0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<256 x f32>
    %11 = nvvm.read.ptx.sreg.ctaid.x : i32
    %12 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %13 = llvm.mul %11, %arg0  : i32
    %14 = llvm.add %12, %13  : i32
    %15 = llvm.icmp "ult" %14, %arg1 : i32
    llvm.cond_br %15, ^bb2, ^bb21
  ^bb2:  // pred: ^bb1
    %16 = llvm.srem %14, %7  : i32
    %17 = llvm.sdiv %14, %7  : i32
    %18 = llvm.udiv %16, %0  : i32
    %19 = llvm.urem %16, %0  : i32
    %20 = llvm.mul %19, %1  : i32
    %21 = llvm.add %18, %20  : i32
    %22 = llvm.icmp "ult" %19, %6 : i32
    llvm.cond_br %22, ^bb3, ^bb10(%4 : f32)
  ^bb3:  // pred: ^bb2
    %23 = llvm.mul %17, %1  : i32
    %24 = llvm.add %18, %23  : i32
    %25 = llvm.mul %24, %2  : i32
    llvm.br ^bb4(%8, %4 : i32, f32)
  ^bb4(%26: i32, %27: f32):  // 2 preds: ^bb3, ^bb9
    %28 = llvm.icmp "slt" %26, %2 : i32
    llvm.cond_br %28, ^bb5, ^bb10(%27 : f32)
  ^bb5:  // pred: ^bb4
    %29 = llvm.add %26, %25  : i32
    %30 = llvm.icmp "slt" %29, %arg2 : i32
    llvm.cond_br %30, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %31 = llvm.getelementptr %arg3[%29] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    %32 = llvm.load %31 : !llvm.ptr<1> -> f32
    %33 = llvm.getelementptr %arg4[%29] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    %34 = llvm.load %33 : !llvm.ptr<1> -> f32
    %35 = llvm.fmul %32, %34  : f32
    %36 = llvm.fadd %27, %35  : f32
    llvm.br ^bb8(%36 : f32)
  ^bb7:  // pred: ^bb5
    llvm.br ^bb8(%27 : f32)
  ^bb8(%37: f32):  // 2 preds: ^bb6, ^bb7
    llvm.br ^bb9
  ^bb9:  // pred: ^bb8
    %38 = llvm.add %26, %6  : i32
    llvm.br ^bb4(%38, %37 : i32, f32)
  ^bb10(%39: f32):  // 2 preds: ^bb2, ^bb4
    llvm.br ^bb11(%39 : f32)
  ^bb11(%40: f32):  // pred: ^bb10
    llvm.br ^bb12
  ^bb12:  // pred: ^bb11
    %41 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    llvm.store %40, %41 : f32, !llvm.ptr<3>
    nvvm.barrier0
    %42 = llvm.icmp "slt" %18, %3 : i32
    llvm.cond_br %42, ^bb13, ^bb14
  ^bb13:  // pred: ^bb12
    %43 = llvm.add %21, %3  : i32
    %44 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %45 = llvm.load %44 : !llvm.ptr<3> -> f32
    %46 = llvm.getelementptr %10[%43] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %47 = llvm.load %46 : !llvm.ptr<3> -> f32
    %48 = llvm.fadd %45, %47  : f32
    %49 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    llvm.store %48, %49 : f32, !llvm.ptr<3>
    llvm.br ^bb14
  ^bb14:  // 2 preds: ^bb12, ^bb13
    nvvm.barrier0
    %50 = llvm.icmp "slt" %18, %5 : i32
    llvm.cond_br %50, ^bb15, ^bb16
  ^bb15:  // pred: ^bb14
    %51 = llvm.add %21, %5  : i32
    %52 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %53 = llvm.load %52 : !llvm.ptr<3> -> f32
    %54 = llvm.getelementptr %10[%51] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %55 = llvm.load %54 : !llvm.ptr<3> -> f32
    %56 = llvm.fadd %53, %55  : f32
    %57 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    llvm.store %56, %57 : f32, !llvm.ptr<3>
    llvm.br ^bb16
  ^bb16:  // 2 preds: ^bb14, ^bb15
    nvvm.barrier0
    %58 = llvm.icmp "slt" %18, %6 : i32
    llvm.cond_br %58, ^bb17, ^bb18
  ^bb17:  // pred: ^bb16
    %59 = llvm.add %21, %6  : i32
    %60 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %61 = llvm.load %60 : !llvm.ptr<3> -> f32
    %62 = llvm.getelementptr %10[%59] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %63 = llvm.load %62 : !llvm.ptr<3> -> f32
    %64 = llvm.fadd %61, %63  : f32
    %65 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    llvm.store %64, %65 : f32, !llvm.ptr<3>
    llvm.br ^bb18
  ^bb18:  // 2 preds: ^bb16, ^bb17
    nvvm.barrier0
    %66 = llvm.icmp "eq" %18, %8 : i32
    %67 = llvm.and %66, %22  : i1
    llvm.cond_br %67, ^bb19, ^bb20
  ^bb19:  // pred: ^bb18
    %68 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %69 = llvm.load %68 : !llvm.ptr<3> -> f32
    %70 = llvm.getelementptr %arg5[%19] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    %71 = llvm.atomicrmw fadd %70, %69 acq_rel : !llvm.ptr<1>, f32
    llvm.br ^bb20
  ^bb20:  // 2 preds: ^bb18, ^bb19
    llvm.br ^bb21
  ^bb21:  // 2 preds: ^bb1, ^bb20
    llvm.return
  }
}

// -----// IR Dump After CSE (cse) //----- //
gpu.module @main_kernel_3 {
  gpu.func @main_kColReduction_reduce__6_1_0___thread_tile_h32(%arg0: index, %arg1: memref<1xf32, #gpu.address_space<global>>) kernel {
    %cst = arith.constant 0.000000e+00 : f32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg0, %c0]
    %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %4 = arith.addi %3, %2 : index
    %5 = arith.cmpi ult, %4, %c1 : index
    scf.if %5 {
      %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
      memref.store %cst, %reinterpret_cast[%4] : memref<1xf32, #gpu.address_space<global>>
    }
    gpu.return
  }
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
gpu.module @main_kernel_3 {
  gpu.func @main_kColReduction_reduce__6_1_0___thread_tile_h32(%arg0: index, %arg1: memref<1xf32, #gpu.address_space<global>>) kernel {
    %cst = arith.constant 0.000000e+00 : f32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg0, %c0]
    %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %4 = arith.addi %3, %2 : index
    %5 = arith.cmpi ult, %4, %c1 : index
    cf.cond_br %5, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
    memref.store %cst, %reinterpret_cast[%4] : memref<1xf32, #gpu.address_space<global>>
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    gpu.return
  }
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
gpu.module @main_kernel_3 {
  gpu.func @main_kColReduction_reduce__6_1_0___thread_tile_h32(%arg0: index, %arg1: memref<1xf32, #gpu.address_space<global>>) kernel {
    %cst = arith.constant 0.000000e+00 : f32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = arith.muli %0, %arg0 : index
    %3 = arith.addi %2, %c0 : index
    %4 = arith.muli %1, %c1 : index
    %5 = arith.addi %4, %c0 : index
    %6 = arith.addi %5, %3 : index
    %7 = arith.cmpi ult, %6, %c1 : index
    cf.cond_br %7, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
    memref.store %cst, %reinterpret_cast[%6] : memref<1xf32, #gpu.address_space<global>>
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    gpu.return
  }
}

// -----// IR Dump After StripDebugInfo (strip-debuginfo) //----- //
gpu.module @main_kernel_3 {
  gpu.func @main_kColReduction_reduce__6_1_0___thread_tile_h32(%arg0: index, %arg1: memref<1xf32, #gpu.address_space<global>>) kernel {
    %cst = arith.constant 0.000000e+00 : f32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = arith.muli %0, %arg0 : index
    %3 = arith.addi %2, %c0 : index
    %4 = arith.muli %1, %c1 : index
    %5 = arith.addi %4, %c0 : index
    %6 = arith.addi %5, %3 : index
    %7 = arith.cmpi ult, %6, %c1 : index
    cf.cond_br %7, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
    memref.store %cst, %reinterpret_cast[%6] : memref<1xf32, #gpu.address_space<global>>
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    gpu.return
  }
}

// -----// IR Dump After DiscLowerGpuOpsToNVVMOpsPass (disc-convert-gpu-to-nvvm) //----- //
gpu.module @main_kernel_3 {
  llvm.func @main_kColReduction_reduce__6_1_0___thread_tile_h32(%arg0: i32, %arg1: !llvm.ptr<1>, %arg2: !llvm.ptr<1>, %arg3: i32, %arg4: i32, %arg5: i32) attributes {gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.undef : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)>
    %1 = llvm.insertvalue %arg1, %0[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %2 = llvm.insertvalue %arg2, %1[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %3 = llvm.insertvalue %arg3, %2[2] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %4 = llvm.insertvalue %arg4, %3[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %5 = llvm.insertvalue %arg5, %4[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %6 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    %7 = llvm.mlir.constant(1 : index) : i32
    %8 = nvvm.read.ptx.sreg.ctaid.x : i32
    %9 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %10 = llvm.mul %8, %arg0  : i32
    %11 = llvm.add %9, %10  : i32
    %12 = llvm.icmp "ult" %11, %7 : i32
    llvm.cond_br %12, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %13 = llvm.mlir.undef : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)>
    %14 = llvm.extractvalue %5[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %15 = llvm.extractvalue %5[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %16 = llvm.insertvalue %14, %13[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %17 = llvm.insertvalue %15, %16[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %18 = llvm.mlir.constant(0 : index) : i32
    %19 = llvm.insertvalue %18, %17[2] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %20 = llvm.mlir.constant(1 : index) : i32
    %21 = llvm.insertvalue %20, %19[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %22 = llvm.mlir.constant(1 : index) : i32
    %23 = llvm.insertvalue %22, %21[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %24 = llvm.extractvalue %23[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %25 = llvm.getelementptr %24[%11] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    llvm.store %6, %25 : f32, !llvm.ptr<1>
    llvm.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    llvm.return
  }
}

// -----// IR Dump After LLVMInsertValueSimplifierPass (disc-llvm-insert-value-simplifier) //----- //
llvm.func @main_kColReduction_reduce__6_1_0___thread_tile_h32(%arg0: i32, %arg1: !llvm.ptr<1>, %arg2: !llvm.ptr<1>, %arg3: i32, %arg4: i32, %arg5: i32) attributes {gpu.kernel, nvvm.kernel} {
  %0 = llvm.mlir.constant(1 : index) : i32
  %1 = llvm.mlir.constant(0.000000e+00 : f32) : f32
  %2 = nvvm.read.ptx.sreg.ctaid.x : i32
  %3 = nvvm.read.ptx.sreg.tid.x : i32
  llvm.br ^bb1
^bb1:  // pred: ^bb0
  %4 = llvm.mul %2, %arg0  : i32
  %5 = llvm.add %3, %4  : i32
  %6 = llvm.icmp "ult" %5, %0 : i32
  llvm.cond_br %6, ^bb2, ^bb3
^bb2:  // pred: ^bb1
  %7 = llvm.getelementptr %arg2[%5] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
  llvm.store %1, %7 : f32, !llvm.ptr<1>
  llvm.br ^bb3
^bb3:  // 2 preds: ^bb1, ^bb2
  llvm.return
}

// -----// IR Dump After FunctionDeadArgumentEliminationPass (disc-function-dead-argument-elimination) //----- //
gpu.module @main_kernel_3 {
  llvm.func @main_kColReduction_reduce__6_1_0___thread_tile_h32(%arg0: i32, %arg1: !llvm.ptr<1>) attributes {disc.elimargs = [1 : index, 3 : index, 4 : index, 5 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(1 : index) : i32
    %1 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    %2 = nvvm.read.ptx.sreg.ctaid.x : i32
    %3 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %4 = llvm.mul %2, %arg0  : i32
    %5 = llvm.add %3, %4  : i32
    %6 = llvm.icmp "ult" %5, %0 : i32
    llvm.cond_br %6, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %7 = llvm.getelementptr %arg1[%5] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    llvm.store %1, %7 : f32, !llvm.ptr<1>
    llvm.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    llvm.return
  }
}

// -----// IR Dump After GpuKernelToBlobPass (disc-gpu-kernel-to-blob) //----- //
gpu.module @main_kernel_3 attributes {gpu.binary = "P\EDU\BA\01\00\10\00H\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\08\04\00\00\00\00\00\00\01\04\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\0Be__6_1_0___thread_tile_h32B\00\0F<\00%oshared>\00%\9Fconstant0A\00\22\FA\01debug_frame\00.rel\11\00!nv\14\00\11aM\00\0FS\01 \0F\92\00\1F\0F\86\01\E2o_param\8D\01\1C\0F\01\00\04\8Ce\00\00\00\03\00\0A\00\01\00 \18\01\18\00,\09\00\01\00\11`\18\00,\04\00\01\00\11~\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\1C\00\00\00E\00#\04\10\18\00\80/\08\00\05\00\00\00\06\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04@\04\91\015\00\00\04\0A\08\00\02\FC\00\91\10\00\03\19\10\00\04\17\0C$\00u\01\00\08\00\00\F0!\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00p\00\00\00\C0\00\01\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00 \01/\05\00\01\00\FFh@$v\01\FF\C7\03\A3\FF\00\8E\07\00\C4\0F\00\C3y\A4\03\A2%\00\00\00\22\0E\00\19y\00\01\00\F3\0E!\00\00\00b\0E\00\90r\05?\04\00\00\80?\E0\FF\0F\00\C6\1F\00\B9z\04\00\00X\96\02p\E4\0F\00\A4r\04\05B\00\B1?\02\8E\0F\00\CC\0F\00\0C|\00\F6\03\95pR\F0\0B\00\DA/\00M\1B\04\F3\0D\80\03\00\EA\0F\00\02z\02\00\00Z\00\00\00\0F\00\00\00\E2\0F\00$v\03\FF\00[\90\00\22\E2\0F`\001F\00\00k\04\80\C8\0F\00\86y\00\02\FFh\030\19\10\0C \00*MyP\00PGy\00\00\F0\F1\03\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\80\0F\01\00-\00\BF\04.\03\00\01\00\22@\00\01\00=S\01\000\00\08\01\00\1F\0B@\00\04\13\93)\00\1F\8D@\00\0C\13\13\C4\03\0C\01\00\13 U\00*\90\00\E8\03\04\05\04\22\18\00\01\00.&\01T\00\00\01\00\13\B0@\00/p\00\80\00\0B\1F)'\00\03A\00 \04\00\01\00\04\F8\05\04\E4\00*\04\00\01\00\1Fk@\00\04\13P)\00&L\00@\00\1F\0A@\00\00!D\01D\01\0D@\00\13\A0)\00*\D8\00\01\00\1B\08\08\00?3\01\00.\07\003\00\00x\15\05\17\10\80\00\17\048\00\04\18\00\13\E5\14\01\0C\84\01*\88\05\E0\06\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07x\07\12\03\C0\01:\06\80\00\01\00\13\06\D8\01\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\13\018\00\04\A8\00\0C\01\00*\F8\02\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00\00\00\00\00\00\00"} {
  llvm.func @main_kColReduction_reduce__6_1_0___thread_tile_h32(%arg0: i32, %arg1: !llvm.ptr<1>) attributes {disc.elimargs = [1 : index, 3 : index, 4 : index, 5 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(1 : index) : i32
    %1 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    %2 = nvvm.read.ptx.sreg.ctaid.x : i32
    %3 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %4 = llvm.mul %2, %arg0  : i32
    %5 = llvm.add %3, %4  : i32
    %6 = llvm.icmp "ult" %5, %0 : i32
    llvm.cond_br %6, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %7 = llvm.getelementptr %arg1[%5] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    llvm.store %1, %7 : f32, !llvm.ptr<1>
    llvm.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    llvm.return
  }
}

// -----// IR Dump After LoopInvariantCodeMotion (loop-invariant-code-motion) //----- //
gpu.module @main_kernel_4 {
  gpu.func @main_kColReduction_reduce__6_1_0___thread_tile_h32_1(%arg0: index, %arg1: index, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?x4xf32, #gpu.address_space<global>>, %arg6: memref<?x4xf32, #gpu.address_space<global>>, %arg7: memref<1xf32, #gpu.address_space<global>>) kernel {
    %cst = arith.constant 0.000000e+00 : f32
    %c4 = arith.constant 4 : index
    %c32 = arith.constant 32 : index
    %c512 = arith.constant 512 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = arith.cmpi eq, %arg0, %c1 : index
    %3 = arith.select %2, %arg1, %arg0 : index
    %4 = arith.cmpi eq, %arg1, %3 : index
    %5 = arith.cmpi eq, %arg0, %3 : index
    %6 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg2, %c0]
    %7 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %8 = arith.addi %7, %6 : index
    %9 = arith.addi %7, %6 : index
    %10 = arith.cmpi ult, %9, %arg3 : index
    scf.if %10 {
      %11 = arith.remsi %8, %c512 : index
      %12 = arith.divsi %8, %c512 : index
      %13 = arith.cmpi ult, %11, %c1 : index
      scf.if %13 {
        %14 = arith.muli %12, %c32 : index
        %15 = scf.for %arg8 = %c0 to %c32 step %c1 iter_args(%arg9 = %cst) -> (f32) {
          %17 = arith.addi %14, %arg8 : index
          %18 = arith.cmpi slt, %17, %arg4 : index
          %19 = scf.if %18 -> (f32) {
            %20 = arith.remui %17, %c4 : index
            %21 = arith.divui %17, %c4 : index
            %22 = arith.select %4, %21, %c0 : index
            %23 = arith.muli %22, %c4 : index
            %24 = arith.addi %23, %20 : index
            %25 = arith.muli %arg1, %c4 : index
            %reinterpret_cast = memref.reinterpret_cast %arg5 to offset: [%c0], sizes: [%25], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
            %26 = memref.load %reinterpret_cast[%24] : memref<?xf32, #gpu.address_space<global>>
            %27 = arith.select %5, %21, %c0 : index
            %28 = arith.muli %27, %c4 : index
            %29 = arith.addi %28, %20 : index
            %30 = arith.muli %arg0, %c4 : index
            %reinterpret_cast_0 = memref.reinterpret_cast %arg6 to offset: [%c0], sizes: [%30], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
            %31 = memref.load %reinterpret_cast_0[%29] : memref<?xf32, #gpu.address_space<global>>
            %32 = arith.mulf %26, %31 : f32
            %33 = arith.addf %arg9, %32 : f32
            scf.yield %33 : f32
          } else {
            scf.yield %arg9 : f32
          }
          scf.yield %19 : f32
        }
        %16 = memref.atomic_rmw addf %15, %arg7[%11] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
      }
    }
    gpu.return
  }
}

// -----// IR Dump After CSE (cse) //----- //
gpu.module @main_kernel_4 {
  gpu.func @main_kColReduction_reduce__6_1_0___thread_tile_h32_1(%arg0: index, %arg1: index, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?x4xf32, #gpu.address_space<global>>, %arg6: memref<?x4xf32, #gpu.address_space<global>>, %arg7: memref<1xf32, #gpu.address_space<global>>) kernel {
    %cst = arith.constant 0.000000e+00 : f32
    %c4 = arith.constant 4 : index
    %c32 = arith.constant 32 : index
    %c512 = arith.constant 512 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = arith.cmpi eq, %arg0, %c1 : index
    %3 = arith.select %2, %arg1, %arg0 : index
    %4 = arith.cmpi eq, %arg1, %3 : index
    %5 = arith.cmpi eq, %arg0, %3 : index
    %6 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg2, %c0]
    %7 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %8 = arith.addi %7, %6 : index
    %9 = arith.cmpi ult, %8, %arg3 : index
    scf.if %9 {
      %10 = arith.remsi %8, %c512 : index
      %11 = arith.divsi %8, %c512 : index
      %12 = arith.cmpi ult, %10, %c1 : index
      scf.if %12 {
        %13 = arith.muli %11, %c32 : index
        %14 = scf.for %arg8 = %c0 to %c32 step %c1 iter_args(%arg9 = %cst) -> (f32) {
          %16 = arith.addi %13, %arg8 : index
          %17 = arith.cmpi slt, %16, %arg4 : index
          %18 = scf.if %17 -> (f32) {
            %19 = arith.remui %16, %c4 : index
            %20 = arith.divui %16, %c4 : index
            %21 = arith.select %4, %20, %c0 : index
            %22 = arith.muli %21, %c4 : index
            %23 = arith.addi %22, %19 : index
            %24 = arith.muli %arg1, %c4 : index
            %reinterpret_cast = memref.reinterpret_cast %arg5 to offset: [%c0], sizes: [%24], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
            %25 = memref.load %reinterpret_cast[%23] : memref<?xf32, #gpu.address_space<global>>
            %26 = arith.select %5, %20, %c0 : index
            %27 = arith.muli %26, %c4 : index
            %28 = arith.addi %27, %19 : index
            %29 = arith.muli %arg0, %c4 : index
            %reinterpret_cast_0 = memref.reinterpret_cast %arg6 to offset: [%c0], sizes: [%29], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
            %30 = memref.load %reinterpret_cast_0[%28] : memref<?xf32, #gpu.address_space<global>>
            %31 = arith.mulf %25, %30 : f32
            %32 = arith.addf %arg9, %31 : f32
            scf.yield %32 : f32
          } else {
            scf.yield %arg9 : f32
          }
          scf.yield %18 : f32
        }
        %15 = memref.atomic_rmw addf %14, %arg7[%10] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
      }
    }
    gpu.return
  }
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
gpu.module @main_kernel_4 {
  gpu.func @main_kColReduction_reduce__6_1_0___thread_tile_h32_1(%arg0: index, %arg1: index, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?x4xf32, #gpu.address_space<global>>, %arg6: memref<?x4xf32, #gpu.address_space<global>>, %arg7: memref<1xf32, #gpu.address_space<global>>) kernel {
    %cst = arith.constant 0.000000e+00 : f32
    %c4 = arith.constant 4 : index
    %c32 = arith.constant 32 : index
    %c512 = arith.constant 512 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = arith.cmpi eq, %arg0, %c1 : index
    %3 = arith.select %2, %arg1, %arg0 : index
    %4 = arith.cmpi eq, %arg1, %3 : index
    %5 = arith.cmpi eq, %arg0, %3 : index
    %6 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg2, %c0]
    %7 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %8 = arith.addi %7, %6 : index
    %9 = arith.cmpi ult, %8, %arg3 : index
    cf.cond_br %9, ^bb2, ^bb12
  ^bb2:  // pred: ^bb1
    %10 = arith.remsi %8, %c512 : index
    %11 = arith.divsi %8, %c512 : index
    %12 = arith.cmpi ult, %10, %c1 : index
    cf.cond_br %12, ^bb3, ^bb11
  ^bb3:  // pred: ^bb2
    %13 = arith.muli %11, %c32 : index
    cf.br ^bb4(%c0, %cst : index, f32)
  ^bb4(%14: index, %15: f32):  // 2 preds: ^bb3, ^bb9
    %16 = arith.cmpi slt, %14, %c32 : index
    cf.cond_br %16, ^bb5, ^bb10
  ^bb5:  // pred: ^bb4
    %17 = arith.addi %13, %14 : index
    %18 = arith.cmpi slt, %17, %arg4 : index
    cf.cond_br %18, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %19 = arith.remui %17, %c4 : index
    %20 = arith.divui %17, %c4 : index
    %21 = arith.select %4, %20, %c0 : index
    %22 = arith.muli %21, %c4 : index
    %23 = arith.addi %22, %19 : index
    %24 = arith.muli %arg1, %c4 : index
    %reinterpret_cast = memref.reinterpret_cast %arg5 to offset: [%c0], sizes: [%24], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
    %25 = memref.load %reinterpret_cast[%23] : memref<?xf32, #gpu.address_space<global>>
    %26 = arith.select %5, %20, %c0 : index
    %27 = arith.muli %26, %c4 : index
    %28 = arith.addi %27, %19 : index
    %29 = arith.muli %arg0, %c4 : index
    %reinterpret_cast_0 = memref.reinterpret_cast %arg6 to offset: [%c0], sizes: [%29], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
    %30 = memref.load %reinterpret_cast_0[%28] : memref<?xf32, #gpu.address_space<global>>
    %31 = arith.mulf %25, %30 : f32
    %32 = arith.addf %15, %31 : f32
    cf.br ^bb8(%32 : f32)
  ^bb7:  // pred: ^bb5
    cf.br ^bb8(%15 : f32)
  ^bb8(%33: f32):  // 2 preds: ^bb6, ^bb7
    cf.br ^bb9
  ^bb9:  // pred: ^bb8
    %34 = arith.addi %14, %c1 : index
    cf.br ^bb4(%34, %33 : index, f32)
  ^bb10:  // pred: ^bb4
    %35 = memref.atomic_rmw addf %15, %arg7[%10] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
    cf.br ^bb11
  ^bb11:  // 2 preds: ^bb2, ^bb10
    cf.br ^bb12
  ^bb12:  // 2 preds: ^bb1, ^bb11
    gpu.return
  }
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
gpu.module @main_kernel_4 {
  gpu.func @main_kColReduction_reduce__6_1_0___thread_tile_h32_1(%arg0: index, %arg1: index, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?x4xf32, #gpu.address_space<global>>, %arg6: memref<?x4xf32, #gpu.address_space<global>>, %arg7: memref<1xf32, #gpu.address_space<global>>) kernel {
    %cst = arith.constant 0.000000e+00 : f32
    %c4 = arith.constant 4 : index
    %c32 = arith.constant 32 : index
    %c512 = arith.constant 512 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = arith.cmpi eq, %arg0, %c1 : index
    %3 = arith.select %2, %arg1, %arg0 : index
    %4 = arith.cmpi eq, %arg1, %3 : index
    %5 = arith.cmpi eq, %arg0, %3 : index
    %6 = arith.muli %0, %arg2 : index
    %7 = arith.addi %6, %c0 : index
    %8 = arith.muli %1, %c1 : index
    %9 = arith.addi %8, %c0 : index
    %10 = arith.addi %9, %7 : index
    %11 = arith.cmpi ult, %10, %arg3 : index
    cf.cond_br %11, ^bb2, ^bb12
  ^bb2:  // pred: ^bb1
    %12 = arith.remsi %10, %c512 : index
    %13 = arith.divsi %10, %c512 : index
    %14 = arith.cmpi ult, %12, %c1 : index
    cf.cond_br %14, ^bb3, ^bb11
  ^bb3:  // pred: ^bb2
    %15 = arith.muli %13, %c32 : index
    cf.br ^bb4(%c0, %cst : index, f32)
  ^bb4(%16: index, %17: f32):  // 2 preds: ^bb3, ^bb9
    %18 = arith.cmpi slt, %16, %c32 : index
    cf.cond_br %18, ^bb5, ^bb10
  ^bb5:  // pred: ^bb4
    %19 = arith.addi %15, %16 : index
    %20 = arith.cmpi slt, %19, %arg4 : index
    cf.cond_br %20, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %21 = arith.remui %19, %c4 : index
    %22 = arith.divui %19, %c4 : index
    %23 = arith.select %4, %22, %c0 : index
    %24 = arith.muli %23, %c4 : index
    %25 = arith.addi %24, %21 : index
    %26 = arith.muli %arg1, %c4 : index
    %reinterpret_cast = memref.reinterpret_cast %arg5 to offset: [%c0], sizes: [%26], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
    %27 = memref.load %reinterpret_cast[%25] : memref<?xf32, #gpu.address_space<global>>
    %28 = arith.select %5, %22, %c0 : index
    %29 = arith.muli %28, %c4 : index
    %30 = arith.addi %29, %21 : index
    %31 = arith.muli %arg0, %c4 : index
    %reinterpret_cast_0 = memref.reinterpret_cast %arg6 to offset: [%c0], sizes: [%31], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
    %32 = memref.load %reinterpret_cast_0[%30] : memref<?xf32, #gpu.address_space<global>>
    %33 = arith.mulf %27, %32 : f32
    %34 = arith.addf %17, %33 : f32
    cf.br ^bb8(%34 : f32)
  ^bb7:  // pred: ^bb5
    cf.br ^bb8(%17 : f32)
  ^bb8(%35: f32):  // 2 preds: ^bb6, ^bb7
    cf.br ^bb9
  ^bb9:  // pred: ^bb8
    %36 = arith.addi %16, %c1 : index
    cf.br ^bb4(%36, %35 : index, f32)
  ^bb10:  // pred: ^bb4
    %37 = memref.atomic_rmw addf %17, %arg7[%12] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
    cf.br ^bb11
  ^bb11:  // 2 preds: ^bb2, ^bb10
    cf.br ^bb12
  ^bb12:  // 2 preds: ^bb1, ^bb11
    gpu.return
  }
}

// -----// IR Dump After StripDebugInfo (strip-debuginfo) //----- //
gpu.module @main_kernel_4 {
  gpu.func @main_kColReduction_reduce__6_1_0___thread_tile_h32_1(%arg0: index, %arg1: index, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?x4xf32, #gpu.address_space<global>>, %arg6: memref<?x4xf32, #gpu.address_space<global>>, %arg7: memref<1xf32, #gpu.address_space<global>>) kernel {
    %cst = arith.constant 0.000000e+00 : f32
    %c4 = arith.constant 4 : index
    %c32 = arith.constant 32 : index
    %c512 = arith.constant 512 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = arith.cmpi eq, %arg0, %c1 : index
    %3 = arith.select %2, %arg1, %arg0 : index
    %4 = arith.cmpi eq, %arg1, %3 : index
    %5 = arith.cmpi eq, %arg0, %3 : index
    %6 = arith.muli %0, %arg2 : index
    %7 = arith.addi %6, %c0 : index
    %8 = arith.muli %1, %c1 : index
    %9 = arith.addi %8, %c0 : index
    %10 = arith.addi %9, %7 : index
    %11 = arith.cmpi ult, %10, %arg3 : index
    cf.cond_br %11, ^bb2, ^bb12
  ^bb2:  // pred: ^bb1
    %12 = arith.remsi %10, %c512 : index
    %13 = arith.divsi %10, %c512 : index
    %14 = arith.cmpi ult, %12, %c1 : index
    cf.cond_br %14, ^bb3, ^bb11
  ^bb3:  // pred: ^bb2
    %15 = arith.muli %13, %c32 : index
    cf.br ^bb4(%c0, %cst : index, f32)
  ^bb4(%16: index, %17: f32):  // 2 preds: ^bb3, ^bb9
    %18 = arith.cmpi slt, %16, %c32 : index
    cf.cond_br %18, ^bb5, ^bb10
  ^bb5:  // pred: ^bb4
    %19 = arith.addi %15, %16 : index
    %20 = arith.cmpi slt, %19, %arg4 : index
    cf.cond_br %20, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %21 = arith.remui %19, %c4 : index
    %22 = arith.divui %19, %c4 : index
    %23 = arith.select %4, %22, %c0 : index
    %24 = arith.muli %23, %c4 : index
    %25 = arith.addi %24, %21 : index
    %26 = arith.muli %arg1, %c4 : index
    %reinterpret_cast = memref.reinterpret_cast %arg5 to offset: [%c0], sizes: [%26], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
    %27 = memref.load %reinterpret_cast[%25] : memref<?xf32, #gpu.address_space<global>>
    %28 = arith.select %5, %22, %c0 : index
    %29 = arith.muli %28, %c4 : index
    %30 = arith.addi %29, %21 : index
    %31 = arith.muli %arg0, %c4 : index
    %reinterpret_cast_0 = memref.reinterpret_cast %arg6 to offset: [%c0], sizes: [%31], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
    %32 = memref.load %reinterpret_cast_0[%30] : memref<?xf32, #gpu.address_space<global>>
    %33 = arith.mulf %27, %32 : f32
    %34 = arith.addf %17, %33 : f32
    cf.br ^bb8(%34 : f32)
  ^bb7:  // pred: ^bb5
    cf.br ^bb8(%17 : f32)
  ^bb8(%35: f32):  // 2 preds: ^bb6, ^bb7
    cf.br ^bb9
  ^bb9:  // pred: ^bb8
    %36 = arith.addi %16, %c1 : index
    cf.br ^bb4(%36, %35 : index, f32)
  ^bb10:  // pred: ^bb4
    %37 = memref.atomic_rmw addf %17, %arg7[%12] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
    cf.br ^bb11
  ^bb11:  // 2 preds: ^bb2, ^bb10
    cf.br ^bb12
  ^bb12:  // 2 preds: ^bb1, ^bb11
    gpu.return
  }
}

// -----// IR Dump After DiscLowerGpuOpsToNVVMOpsPass (disc-convert-gpu-to-nvvm) //----- //
gpu.module @main_kernel_4 {
  llvm.func @main_kColReduction_reduce__6_1_0___thread_tile_h32_1(%arg0: i32, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: !llvm.ptr<1>, %arg6: !llvm.ptr<1>, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: !llvm.ptr<1>, %arg13: !llvm.ptr<1>, %arg14: i32, %arg15: i32, %arg16: i32, %arg17: i32, %arg18: i32, %arg19: !llvm.ptr<1>, %arg20: !llvm.ptr<1>, %arg21: i32, %arg22: i32, %arg23: i32) attributes {gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.undef : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)>
    %1 = llvm.insertvalue %arg5, %0[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %2 = llvm.insertvalue %arg6, %1[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %3 = llvm.insertvalue %arg7, %2[2] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %4 = llvm.insertvalue %arg8, %3[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %5 = llvm.insertvalue %arg10, %4[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %6 = llvm.insertvalue %arg9, %5[3, 1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %7 = llvm.insertvalue %arg11, %6[4, 1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %8 = llvm.mlir.undef : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)>
    %9 = llvm.insertvalue %arg12, %8[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %10 = llvm.insertvalue %arg13, %9[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %11 = llvm.insertvalue %arg14, %10[2] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %12 = llvm.insertvalue %arg15, %11[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %13 = llvm.insertvalue %arg17, %12[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %14 = llvm.insertvalue %arg16, %13[3, 1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %15 = llvm.insertvalue %arg18, %14[4, 1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %16 = llvm.mlir.undef : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)>
    %17 = llvm.insertvalue %arg19, %16[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %18 = llvm.insertvalue %arg20, %17[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %19 = llvm.insertvalue %arg21, %18[2] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %20 = llvm.insertvalue %arg22, %19[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %21 = llvm.insertvalue %arg23, %20[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %22 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    %23 = llvm.mlir.constant(4 : index) : i32
    %24 = llvm.mlir.constant(32 : index) : i32
    %25 = llvm.mlir.constant(512 : index) : i32
    %26 = llvm.mlir.constant(1 : index) : i32
    %27 = llvm.mlir.constant(0 : index) : i32
    %28 = nvvm.read.ptx.sreg.ctaid.x : i32
    %29 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %30 = llvm.icmp "eq" %arg0, %26 : i32
    %31 = llvm.select %30, %arg1, %arg0 : i1, i32
    %32 = llvm.icmp "eq" %arg1, %31 : i32
    %33 = llvm.icmp "eq" %arg0, %31 : i32
    %34 = llvm.mul %28, %arg2  : i32
    %35 = llvm.add %29, %34  : i32
    %36 = llvm.icmp "ult" %35, %arg3 : i32
    llvm.cond_br %36, ^bb2, ^bb12
  ^bb2:  // pred: ^bb1
    %37 = llvm.srem %35, %25  : i32
    %38 = llvm.sdiv %35, %25  : i32
    %39 = llvm.icmp "ult" %37, %26 : i32
    llvm.cond_br %39, ^bb3, ^bb11
  ^bb3:  // pred: ^bb2
    %40 = llvm.mul %38, %24  : i32
    llvm.br ^bb4(%27, %22 : i32, f32)
  ^bb4(%41: i32, %42: f32):  // 2 preds: ^bb3, ^bb9
    %43 = llvm.icmp "slt" %41, %24 : i32
    llvm.cond_br %43, ^bb5, ^bb10
  ^bb5:  // pred: ^bb4
    %44 = llvm.add %40, %41  : i32
    %45 = llvm.icmp "slt" %44, %arg4 : i32
    llvm.cond_br %45, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %46 = llvm.urem %44, %23  : i32
    %47 = llvm.udiv %44, %23  : i32
    %48 = llvm.select %32, %47, %27 : i1, i32
    %49 = llvm.mul %48, %23  : i32
    %50 = llvm.add %49, %46  : i32
    %51 = llvm.mul %arg1, %23  : i32
    %52 = llvm.mlir.undef : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)>
    %53 = llvm.extractvalue %7[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %54 = llvm.extractvalue %7[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %55 = llvm.insertvalue %53, %52[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %56 = llvm.insertvalue %54, %55[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %57 = llvm.insertvalue %27, %56[2] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %58 = llvm.insertvalue %51, %57[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %59 = llvm.insertvalue %26, %58[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %60 = llvm.extractvalue %59[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %61 = llvm.getelementptr %60[%50] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    %62 = llvm.load %61 : !llvm.ptr<1> -> f32
    %63 = llvm.select %33, %47, %27 : i1, i32
    %64 = llvm.mul %63, %23  : i32
    %65 = llvm.add %64, %46  : i32
    %66 = llvm.mul %arg0, %23  : i32
    %67 = llvm.mlir.undef : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)>
    %68 = llvm.extractvalue %15[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %69 = llvm.extractvalue %15[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %70 = llvm.insertvalue %68, %67[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %71 = llvm.insertvalue %69, %70[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %72 = llvm.insertvalue %27, %71[2] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %73 = llvm.insertvalue %66, %72[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %74 = llvm.insertvalue %26, %73[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %75 = llvm.extractvalue %74[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %76 = llvm.getelementptr %75[%65] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    %77 = llvm.load %76 : !llvm.ptr<1> -> f32
    %78 = llvm.fmul %62, %77  : f32
    %79 = llvm.fadd %42, %78  : f32
    llvm.br ^bb8(%79 : f32)
  ^bb7:  // pred: ^bb5
    llvm.br ^bb8(%42 : f32)
  ^bb8(%80: f32):  // 2 preds: ^bb6, ^bb7
    llvm.br ^bb9
  ^bb9:  // pred: ^bb8
    %81 = llvm.add %41, %26  : i32
    llvm.br ^bb4(%81, %80 : i32, f32)
  ^bb10:  // pred: ^bb4
    %82 = llvm.extractvalue %21[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %83 = llvm.getelementptr %82[%37] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    %84 = llvm.atomicrmw fadd %83, %42 acq_rel : !llvm.ptr<1>, f32
    llvm.br ^bb11
  ^bb11:  // 2 preds: ^bb2, ^bb10
    llvm.br ^bb12
  ^bb12:  // 2 preds: ^bb1, ^bb11
    llvm.return
  }
}

// -----// IR Dump After LLVMInsertValueSimplifierPass (disc-llvm-insert-value-simplifier) //----- //
llvm.func @main_kColReduction_reduce__6_1_0___thread_tile_h32_1(%arg0: i32, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: !llvm.ptr<1>, %arg6: !llvm.ptr<1>, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: !llvm.ptr<1>, %arg13: !llvm.ptr<1>, %arg14: i32, %arg15: i32, %arg16: i32, %arg17: i32, %arg18: i32, %arg19: !llvm.ptr<1>, %arg20: !llvm.ptr<1>, %arg21: i32, %arg22: i32, %arg23: i32) attributes {gpu.kernel, nvvm.kernel} {
  %0 = llvm.mlir.constant(0 : index) : i32
  %1 = llvm.mlir.constant(1 : index) : i32
  %2 = llvm.mlir.constant(512 : index) : i32
  %3 = llvm.mlir.constant(32 : index) : i32
  %4 = llvm.mlir.constant(4 : index) : i32
  %5 = llvm.mlir.constant(0.000000e+00 : f32) : f32
  %6 = nvvm.read.ptx.sreg.ctaid.x : i32
  %7 = nvvm.read.ptx.sreg.tid.x : i32
  llvm.br ^bb1
^bb1:  // pred: ^bb0
  %8 = llvm.icmp "eq" %arg0, %1 : i32
  %9 = llvm.select %8, %arg1, %arg0 : i1, i32
  %10 = llvm.icmp "eq" %arg1, %9 : i32
  %11 = llvm.icmp "eq" %arg0, %9 : i32
  %12 = llvm.mul %6, %arg2  : i32
  %13 = llvm.add %7, %12  : i32
  %14 = llvm.icmp "ult" %13, %arg3 : i32
  llvm.cond_br %14, ^bb2, ^bb12
^bb2:  // pred: ^bb1
  %15 = llvm.srem %13, %2  : i32
  %16 = llvm.sdiv %13, %2  : i32
  %17 = llvm.icmp "ult" %15, %1 : i32
  llvm.cond_br %17, ^bb3, ^bb11
^bb3:  // pred: ^bb2
  %18 = llvm.mul %16, %3  : i32
  llvm.br ^bb4(%0, %5 : i32, f32)
^bb4(%19: i32, %20: f32):  // 2 preds: ^bb3, ^bb9
  %21 = llvm.icmp "slt" %19, %3 : i32
  llvm.cond_br %21, ^bb5, ^bb10
^bb5:  // pred: ^bb4
  %22 = llvm.add %18, %19  : i32
  %23 = llvm.icmp "slt" %22, %arg4 : i32
  llvm.cond_br %23, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  %24 = llvm.urem %22, %4  : i32
  %25 = llvm.udiv %22, %4  : i32
  %26 = llvm.select %10, %25, %0 : i1, i32
  %27 = llvm.mul %26, %4  : i32
  %28 = llvm.add %27, %24  : i32
  %29 = llvm.getelementptr %arg6[%28] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
  %30 = llvm.load %29 : !llvm.ptr<1> -> f32
  %31 = llvm.select %11, %25, %0 : i1, i32
  %32 = llvm.mul %31, %4  : i32
  %33 = llvm.add %32, %24  : i32
  %34 = llvm.getelementptr %arg13[%33] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
  %35 = llvm.load %34 : !llvm.ptr<1> -> f32
  %36 = llvm.fmul %30, %35  : f32
  %37 = llvm.fadd %20, %36  : f32
  llvm.br ^bb8(%37 : f32)
^bb7:  // pred: ^bb5
  llvm.br ^bb8(%20 : f32)
^bb8(%38: f32):  // 2 preds: ^bb6, ^bb7
  llvm.br ^bb9
^bb9:  // pred: ^bb8
  %39 = llvm.add %19, %1  : i32
  llvm.br ^bb4(%39, %38 : i32, f32)
^bb10:  // pred: ^bb4
  %40 = llvm.getelementptr %arg20[%15] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
  %41 = llvm.atomicrmw fadd %40, %20 acq_rel : !llvm.ptr<1>, f32
  llvm.br ^bb11
^bb11:  // 2 preds: ^bb2, ^bb10
  llvm.br ^bb12
^bb12:  // 2 preds: ^bb1, ^bb11
  llvm.return
}

// -----// IR Dump After FunctionDeadArgumentEliminationPass (disc-function-dead-argument-elimination) //----- //
gpu.module @main_kernel_4 {
  llvm.func @main_kColReduction_reduce__6_1_0___thread_tile_h32_1(%arg0: i32, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: !llvm.ptr<1>, %arg6: !llvm.ptr<1>, %arg7: !llvm.ptr<1>) attributes {disc.elimargs = [5 : index, 7 : index, 8 : index, 9 : index, 10 : index, 11 : index, 12 : index, 14 : index, 15 : index, 16 : index, 17 : index, 18 : index, 19 : index, 21 : index, 22 : index, 23 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(0 : index) : i32
    %1 = llvm.mlir.constant(1 : index) : i32
    %2 = llvm.mlir.constant(512 : index) : i32
    %3 = llvm.mlir.constant(32 : index) : i32
    %4 = llvm.mlir.constant(4 : index) : i32
    %5 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    %6 = nvvm.read.ptx.sreg.ctaid.x : i32
    %7 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %8 = llvm.icmp "eq" %arg0, %1 : i32
    %9 = llvm.select %8, %arg1, %arg0 : i1, i32
    %10 = llvm.icmp "eq" %arg1, %9 : i32
    %11 = llvm.icmp "eq" %arg0, %9 : i32
    %12 = llvm.mul %6, %arg2  : i32
    %13 = llvm.add %7, %12  : i32
    %14 = llvm.icmp "ult" %13, %arg3 : i32
    llvm.cond_br %14, ^bb2, ^bb12
  ^bb2:  // pred: ^bb1
    %15 = llvm.srem %13, %2  : i32
    %16 = llvm.sdiv %13, %2  : i32
    %17 = llvm.icmp "ult" %15, %1 : i32
    llvm.cond_br %17, ^bb3, ^bb11
  ^bb3:  // pred: ^bb2
    %18 = llvm.mul %16, %3  : i32
    llvm.br ^bb4(%0, %5 : i32, f32)
  ^bb4(%19: i32, %20: f32):  // 2 preds: ^bb3, ^bb9
    %21 = llvm.icmp "slt" %19, %3 : i32
    llvm.cond_br %21, ^bb5, ^bb10
  ^bb5:  // pred: ^bb4
    %22 = llvm.add %18, %19  : i32
    %23 = llvm.icmp "slt" %22, %arg4 : i32
    llvm.cond_br %23, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %24 = llvm.urem %22, %4  : i32
    %25 = llvm.udiv %22, %4  : i32
    %26 = llvm.select %10, %25, %0 : i1, i32
    %27 = llvm.mul %26, %4  : i32
    %28 = llvm.add %27, %24  : i32
    %29 = llvm.getelementptr %arg5[%28] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    %30 = llvm.load %29 : !llvm.ptr<1> -> f32
    %31 = llvm.select %11, %25, %0 : i1, i32
    %32 = llvm.mul %31, %4  : i32
    %33 = llvm.add %32, %24  : i32
    %34 = llvm.getelementptr %arg6[%33] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    %35 = llvm.load %34 : !llvm.ptr<1> -> f32
    %36 = llvm.fmul %30, %35  : f32
    %37 = llvm.fadd %20, %36  : f32
    llvm.br ^bb8(%37 : f32)
  ^bb7:  // pred: ^bb5
    llvm.br ^bb8(%20 : f32)
  ^bb8(%38: f32):  // 2 preds: ^bb6, ^bb7
    llvm.br ^bb9
  ^bb9:  // pred: ^bb8
    %39 = llvm.add %19, %1  : i32
    llvm.br ^bb4(%39, %38 : i32, f32)
  ^bb10:  // pred: ^bb4
    %40 = llvm.getelementptr %arg7[%15] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    %41 = llvm.atomicrmw fadd %40, %20 acq_rel : !llvm.ptr<1>, f32
    llvm.br ^bb11
  ^bb11:  // 2 preds: ^bb2, ^bb10
    llvm.br ^bb12
  ^bb12:  // 2 preds: ^bb1, ^bb11
    llvm.return
  }
}

// -----// IR Dump After GpuKernelToBlobPass (disc-gpu-kernel-to-blob) //----- //
gpu.module @main_kernel_4 attributes {gpu.binary = "P\EDU\BA\01\00\10\00\18\0D\00\00\00\00\00\00\02\00\01\01@\00\00\00\D8\0C\00\00\00\00\00\00\D8\0C\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8!\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\00!@!\07\001\00\80\1E\07\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\0De__6_1_0___thread_tile_h32_1D\00\0F>\00'oshared@\00'\9Fconstant0C\00$\FA\01debug_frame\00.rel\11\00!nv\14\00\11aO\00\0F[\01 \0F\94\00!\0F\90\01\EAo_param\97\01\1C\0F\01\00\0A\8Cg\00\00\00\03\00\0A\00\01\00  \01\18\00,\09\00\01\00\11j\18\00,\04\00\01\00\11\88\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\16\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\18\00\00\00E\002\04H\05\18\000/\08\00\0A\00\10\22\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04X\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\010\00\03\190\00\04\17\0C$\00u\07\00(\00\00\F0!\10\009\06\00 \10\009\05\00\18\10\00u\04\00\10\00\00\F0\11\10\009\03\00\0C\10\009\02\00\08\10\009\01\00\04\10\00\01\01\00\F1\02\F0\11\00\03\1B\FF\00\04\1C\08\00`\00\00\00\90\15\16\003\00K\00\01\00`\02\02\08\10\0A/\E7\00\11\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00\80\01/\05\00\01\00\FF\F0@$v\01\FF\AF\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\CB\02Q\0E\00\19y\03\0F\00\10\00\08\08\F0\15$\0E\00$z\00\00\00Z\00\00\03\02\8E\07\00\CA\1F\00\12x\FF\00\FF\01\00\00\FF\C0\80\07\00\C8\0F\00\0C \00\C5[\00\00pdp\00\00\DA\0F\00M\F3\04\B0\80\03\00\EA\0F\00\19x\03\FF\1F\0F\00\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\003\05a\C6\0F\00\11r\03q\00@\FFH\8F\07 \00c$v\00\FF\00X\A0\00\12\C6@\00\00S\00\10\03@\00P\E4\0F\00\0Cx$\03`\00\00pR\F0\03 \00c$x\03\03 \000\00@\E2\0F\00\07\90\00\22Y\00\01\00p\C8\0F\00\10x\0A\030\00\F0\05\FF\E0\FF\07\00\E4\0F\04\0Cz\00\03\00\\\00\00pb\F4\03\10\00Z\10x\0C\03\02 \00\12\0A \00\10\FC \00f\00\10x\12\03\03 \00\00\F0\00\12\0C \00C\FA\03\00\C4\00\01\13X\90\00\02@\00\01\80\00ApR\F2\03\D0\00G\A4\09\FF\04\A0\00B\0Cz\00\12@\00\13\F6`\004\02\03\04`\00i\E2\0F\04$\E4\0B0\00c\07\A2\04\03\FF\00\FF\04c\E4\0F\04\07\A2\08\10\00 \80\04 \00<$\D4\0D`\00\12\02`\00\11\F8\80\00@%\A6\04\04\E9\04#\09\02P\00D\E8\06\0A\01@\00\10\C4\10\004\0A\0A\01`\00\010\00G\08\08\00^0\00F\D8\0E\0C\02p\00A\04\81\A9\05#\06\D8\00\19\1E\0C\00\A2\00\00\07\D8\0C\0C\02@\000\E6\06\06@\00\12\0B@\00f\08\07\B8\10\12\03@\00U\00\81\A9\00\08@\00\95\A4\02\00%\E6\0A\0A\00`0\00g\00\07\B8\12\12\03P\00E\81\E9\07\060\00p\E4\0E\00%\D6\0E\0E`\00\14\0D0\008\C2\13\02\10\01F\81\E9\14\0A0\00V\08\00$\B4\15 \01\96\C4\0F\00%\D6\0C\0C\00`@\00E\81\D9\0F\0E0\00pf\0F\00%\B6\10\10`\00\14\15\C0\00F\C2\0A\02\FF\90\00U\01\81\D9\0C\0C0\00\10d0\00C\08\12\00`0\00u\E4/\00\81\B9\11\10 \00i$\03\00$\C4\0B\80\006\81\B9\08\00\01u$\0F\00%\C6\12\130\01\10\C8\10\00\07\10\01u\E4\0F\00\81\C9\13\120\00\10(\10\00\16\0A\E0\00\84\22\0F\00$r\02\FF\FF`\00\10\E2P\025\04\03\05\B0\02u\1F\04\10x\10\03\07`\02Q/\04#\A2\02\08\06\01\D4\00\84\E2O\00\10x\05\03\06 \00\02\D0\02\15\04 \03\83\E2\0F\00#\E2\02\07\14@\060\00\E2\8F \00\15\05 \03\84\C6\0F\00#\D2\02\0F\0C \00\85\C8\0F\02#\B2\02\11\080\00j\0F\01\0Cz\00\10\F0\028\11\03\08\F0\02\0B \03W\07\A8\08\04\01\F0\02V\10x\00\03\090\00f\00#\C2\02\13\0A`\00\00\90\00\15\11\F0\02\10\E4@\007\0A\04\01\10\02*$\E4 \03Y\07\E8\06\05\02\00\039\0C\05\02\00\03'\0A\0A\00\03\11\08\D0\03\04\F0\03\00`\008\B8\0E\10\D0\02\010\03\08`\03G\B8\10\10\03P\006\81\A9\05\B0\01%\A4\00 \03\07\C0\02)\11\FF \03\19\04 \039$\B4\17@\02*%\E6\C0\02_\07\C2\12\11\FF0\03\09\12\B60\03\12\17P\02Y\08\81\E9\14\0C \03&\C4\16`\00\10\E4\D0\02C\0A\10\00`0\00W\E2\1F\00\81\B9 \03\10b\10\04\11\15\80\05\04@\011%\C6\10\C0\02\14\16`\009\B9\0A\0A \03\12\C6 \03\01 \00 \E2/0\04(\00\01P\03\18\C90\03\1A\01\D0\04\01\00\03\0B0\035\D6\12\15@\01\1B\C8\D0\03\000\00\1B\D90\03\08\B0\03\10\22\80\029\10\03\0C \034\00\03\0E\10\00\00`\02E\A2\02\05\04`\02\11OP\03\17\0A \03\000\03\14\0B\10\00\1F\E40\03\0Cw\C8\8F\00#\B2\02\0F\B0\02\16\02@\03\12\F6@\03\1F\C20\03\05\02\D0\02\000\03\17\0D\A0\00\0CP\06\00\F0\02\14\02\90\01\02\80\06\18\11\C0\06\00P\03\17\02 \02Z#\D2\02\13\0C@\03\06\E0\02\00`\00\1B\B40\037\B8\06\05\F0\02\1B\04 \03d\00\07\B8\0C\05\03`\00\00P\03O\C2\0E\10\FF \03\09O\C2\10\10\FF \03\09\19\B6 \03i\08\07\E8\13\11\01\80\06\0B \03\17\C4 \03\01\C0\02\0B \03H\E8\12\11\01p\00\1B\B9 \03\17\C6 \03\12\E2\E0\02\17\02\C0\00+\81\B90\03*\E4\16\90\03\1B\C60\03\1B\C9P\06\17\E6 \03\00\D0\00I\D8\0C\00\02\00\03\0A0\03\09\D0\07\00\D0\00\1A\E6@\03'\81\E90\03*&\01\10\03\00`\06\18\E90\03\1F\03 \03\22\1B\11 \03\14\13\10\00\0F \03\04\14\0F \00\03 \03\14\10\10\00\0F \03\01\1F\B2P\06\05\05\10\03\0C0\03\06\10\03\1C\CE\00\03\11/\00\03\08\A0\02*#\E2P\03\00@\03\17\12\90\00\00\10\03\16\03\80\01\0F\10\03\01\1F\11\10\03\0A:\B2\06\05\E0\02\090\06\07P\03\11\FC \0A3\07\B2\0C0\00\1F\00\10\03\03:\C8\0E\10\E0\02)\05\0A`\09H\C8\10\10\01@\00\08 \03\00p\05\0F\10\03\1DH\D8\13\11\02p\00\08\10\03\88\E2\0E\00\07\D8\12\11\02p\00\08 \03\1F\C4\10\03\00\1B\D4@\06\0B\10\03H\07\E8\15\00\A0\01\09 \03\10d\C0\05\0B \03O\E8\0C\00\03 \03\08'%\D6\10\03[\C4/\00\81\D9@\06)\E4\0D\90\00+\81\D9@\06\1B\E6@\06\08P\07\000\00\1B\E9 \03\1E\E9 \03\1B\16 \03\14\18\10\00\0F \03\04\1B\14 \03\14\15\10\00\0F \03=\11\C6\D0\02\0D\00\03\14\17p\00\1F\C6@\06\00O\A2\0A\04\FF@\06\02\03\F0\08G\A2\08\04\FF\10\02/#\E2@\06\05\1F\FC@\06\06\1F\01@\06\0C\18\01@\06F\C8\0E\10\020\00\0F0\03\00?\10\10\02@\06\19O\D8\13\11\03@\06)O\D8\12\11\03@\06\05.$\07@\06G\E2\15\00\FF\C0\00\090\03*$\0B0\03\0F@\06\00H\07\E2\00\00p\01\090\03*$\010\03\00\00\04\08 \03\10$\F0\0A\17\06\D0\00\1A\8F0\03\1B\E20\03%\E6\0E\10\03\22\06\02\90\00\090\03\00 \07S\E6\0C\00\00` \009\E4\0F\02 \03\1Bh \03\10b\E0\026\00\03\1A\E0\02d\04\10x\06\03\1B\10\00\01\00\034\0E\03\1C\10\00/\E2\1F0\03\007\05\03\19\B0\02\09\10\03;\C8\0F\01\00\03\00\80\02\15\06@\03\0D\00\03\17\8F\A0\02\12\FA \00\0A\C0\02\16\02`\03\04\F0\0F9\0A\03\1D\C0\009\08\03\1E\10\007\07\03\1F\A0\002\07\D8\16L\00\00\B0\01\02\D0\03\15\0E\B0\03\02\10\00\15\0A\A0\03\00P\03G\E8\10\05\010\00W\07\A8\0D\06\03\10\001\03x\0C\C0\01\01\01\00\02\90\0D\18\08\90\03H\0Cz\00\07\B0\00\06\C0\00\12\F4`\00H\B2\0B\0E\FF`\00&\C8\09\E0\0F\10\CA\B0\00(\04\08\B0\00H\07\E8\03\07\80\00\060\01\01\D0\10d\00\07\A8\14\05\01\D0\02\00 \000r\00\0C`\00$pRp\00%\0E\0E\F0\02\01\C0\03(\0A\0A0\00T\07\D8\08\08\02\10\00\000\01H\88\12\00\02 \00H\E8\00\07\03\10\008\A8\18\06\10\00\06\E0\00\03P\11\1A\84\B0\039$\A4\1A\10\00E%\86\12\12\90\03f\D0\0F\00$\94\11 \00\00P\03&\89\07\C0\02p\A6\00\00%\96\14\140\00\14\11\10\06!\96\10\90\03#\11\020\005\99\05\140\00v\E6\02\00%\86\16\16\B0\03\00 \00\17\06P\03`\08\00%\A6\18\18P\00\14\1A \005\89\0C\16 \00f\A6\0A\00$\B4\1C\90\00\00\00\04S\A6\1A\0D\00`0\00\00\80\06:\A9\0D\18\D0\04\02`\04\15\1C\F0\07&\1A\1A \00V\0E\00$\C4\1DP\00\01\C0\04 \14\0BP\00\14\1C\F0\03\08\C0\0Dl\A6\0E\00$\D4\1F\80\04\01\90\05\13\1D0\05H\81\B9\14\140\00c%\C6\10\09\00` \00\10\E4\90\07\19\0Bp\041%\D6\120\12\11\1F \00\00\00\0B*\10\10\90\04\090\0BA\02%\D6\08\90\12\14\1F \01\08\90\0A\010\0B)\18\00\E0\04\08\90\04\00\00\08U\E6\16\03\00` \00F\00\81\E9\190\01\01\90\04(\16\16\90\04c$v\04\FF\00b\80\00\00\C0\06H\92\02\05\06\A0\0D5\82\02\07\10\04 \C8O\90\04%\0D\1A\10\00\01\80\04\16\0F\90\07\00P\00C\05\FF\00cP\00\02\90\07(\0B\10\A0\04H\D2\02\13\08\10\11D\E2\02\19\16\10\00a\CA\0F\00\8Ey\00\90\0D@\84\E7\10\0CP\00\14M\80\15\030\15PGy\00\00\F0\A9\19\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\B0\0F\01\00-\00\\\04.\03\00\01\00\22@\00\01\00=[\01\000\00\08\01\00\1F\0B@\00\04\13\9B)\00\1F\97@\00\0C\14\13\CC\01\0B\01\00\138U\00\11\90\06\00\06p\19\22\08\00\01\00\22\18\00\01\00..\01T\00\00\01\00\13\C8@\00/p\00\80\00\0B\1F)'\00\03!\008\F5\02$\00\00\E0\1B\04\E4\00*\04\00\01\00\1Fm@\00\04\13h)\00&\AC\00@\00\1F\0A@\00\00!L\01D\01\0D@\001\18\05\00\01\00*\D8\00\01\00\1B\08\08\00?;\01\00\16\1D\003\00\00\F0@\00&\10\00\80\00\17\048\00\04\18\00\13\EB\14\01\0D\84\01!\06\00\01\00\17\901\01\0F\C0\00\01\132@\00+\06\00\01\00\1A\08`\1D\12\03\C0\01>\22\80\00g\00\17\05(!\0C\01\00*\A8\00\08\00\04\C0\00\13\018\00\0Ey\00\03x\00\1A\18\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00"} {
  llvm.func @main_kColReduction_reduce__6_1_0___thread_tile_h32_1(%arg0: i32, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: !llvm.ptr<1>, %arg6: !llvm.ptr<1>, %arg7: !llvm.ptr<1>) attributes {disc.elimargs = [5 : index, 7 : index, 8 : index, 9 : index, 10 : index, 11 : index, 12 : index, 14 : index, 15 : index, 16 : index, 17 : index, 18 : index, 19 : index, 21 : index, 22 : index, 23 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(0 : index) : i32
    %1 = llvm.mlir.constant(1 : index) : i32
    %2 = llvm.mlir.constant(512 : index) : i32
    %3 = llvm.mlir.constant(32 : index) : i32
    %4 = llvm.mlir.constant(4 : index) : i32
    %5 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    %6 = nvvm.read.ptx.sreg.ctaid.x : i32
    %7 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %8 = llvm.icmp "eq" %arg0, %1 : i32
    %9 = llvm.select %8, %arg1, %arg0 : i1, i32
    %10 = llvm.icmp "eq" %arg1, %9 : i32
    %11 = llvm.icmp "eq" %arg0, %9 : i32
    %12 = llvm.mul %6, %arg2  : i32
    %13 = llvm.add %7, %12  : i32
    %14 = llvm.icmp "ult" %13, %arg3 : i32
    llvm.cond_br %14, ^bb2, ^bb12
  ^bb2:  // pred: ^bb1
    %15 = llvm.srem %13, %2  : i32
    %16 = llvm.sdiv %13, %2  : i32
    %17 = llvm.icmp "ult" %15, %1 : i32
    llvm.cond_br %17, ^bb3, ^bb11
  ^bb3:  // pred: ^bb2
    %18 = llvm.mul %16, %3  : i32
    llvm.br ^bb4(%0, %5 : i32, f32)
  ^bb4(%19: i32, %20: f32):  // 2 preds: ^bb3, ^bb9
    %21 = llvm.icmp "slt" %19, %3 : i32
    llvm.cond_br %21, ^bb5, ^bb10
  ^bb5:  // pred: ^bb4
    %22 = llvm.add %18, %19  : i32
    %23 = llvm.icmp "slt" %22, %arg4 : i32
    llvm.cond_br %23, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %24 = llvm.urem %22, %4  : i32
    %25 = llvm.udiv %22, %4  : i32
    %26 = llvm.select %10, %25, %0 : i1, i32
    %27 = llvm.mul %26, %4  : i32
    %28 = llvm.add %27, %24  : i32
    %29 = llvm.getelementptr %arg5[%28] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    %30 = llvm.load %29 : !llvm.ptr<1> -> f32
    %31 = llvm.select %11, %25, %0 : i1, i32
    %32 = llvm.mul %31, %4  : i32
    %33 = llvm.add %32, %24  : i32
    %34 = llvm.getelementptr %arg6[%33] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    %35 = llvm.load %34 : !llvm.ptr<1> -> f32
    %36 = llvm.fmul %30, %35  : f32
    %37 = llvm.fadd %20, %36  : f32
    llvm.br ^bb8(%37 : f32)
  ^bb7:  // pred: ^bb5
    llvm.br ^bb8(%20 : f32)
  ^bb8(%38: f32):  // 2 preds: ^bb6, ^bb7
    llvm.br ^bb9
  ^bb9:  // pred: ^bb8
    %39 = llvm.add %19, %1  : i32
    llvm.br ^bb4(%39, %38 : i32, f32)
  ^bb10:  // pred: ^bb4
    %40 = llvm.getelementptr %arg7[%15] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    %41 = llvm.atomicrmw fadd %40, %20 acq_rel : !llvm.ptr<1>, f32
    llvm.br ^bb11
  ^bb11:  // 2 preds: ^bb2, ^bb10
    llvm.br ^bb12
  ^bb12:  // 2 preds: ^bb1, ^bb11
    llvm.return
  }
}

// -----// IR Dump After CSE (cse) //----- //
gpu.module @main_kernel_5 {
  gpu.func @main_kColReduction_reduce__6_1_0___block_tile_h64(%arg0: index, %arg1: memref<1xf32, #gpu.address_space<global>>) kernel {
    %cst = arith.constant 0.000000e+00 : f32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg0, %c0]
    %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %4 = arith.addi %3, %2 : index
    %5 = arith.cmpi ult, %4, %c1 : index
    scf.if %5 {
      %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
      memref.store %cst, %reinterpret_cast[%4] : memref<1xf32, #gpu.address_space<global>>
    }
    gpu.return
  }
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
gpu.module @main_kernel_5 {
  gpu.func @main_kColReduction_reduce__6_1_0___block_tile_h64(%arg0: index, %arg1: memref<1xf32, #gpu.address_space<global>>) kernel {
    %cst = arith.constant 0.000000e+00 : f32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg0, %c0]
    %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %4 = arith.addi %3, %2 : index
    %5 = arith.cmpi ult, %4, %c1 : index
    cf.cond_br %5, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
    memref.store %cst, %reinterpret_cast[%4] : memref<1xf32, #gpu.address_space<global>>
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    gpu.return
  }
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
gpu.module @main_kernel_5 {
  gpu.func @main_kColReduction_reduce__6_1_0___block_tile_h64(%arg0: index, %arg1: memref<1xf32, #gpu.address_space<global>>) kernel {
    %cst = arith.constant 0.000000e+00 : f32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = arith.muli %0, %arg0 : index
    %3 = arith.addi %2, %c0 : index
    %4 = arith.muli %1, %c1 : index
    %5 = arith.addi %4, %c0 : index
    %6 = arith.addi %5, %3 : index
    %7 = arith.cmpi ult, %6, %c1 : index
    cf.cond_br %7, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
    memref.store %cst, %reinterpret_cast[%6] : memref<1xf32, #gpu.address_space<global>>
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    gpu.return
  }
}

// -----// IR Dump After StripDebugInfo (strip-debuginfo) //----- //
gpu.module @main_kernel_5 {
  gpu.func @main_kColReduction_reduce__6_1_0___block_tile_h64(%arg0: index, %arg1: memref<1xf32, #gpu.address_space<global>>) kernel {
    %cst = arith.constant 0.000000e+00 : f32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = arith.muli %0, %arg0 : index
    %3 = arith.addi %2, %c0 : index
    %4 = arith.muli %1, %c1 : index
    %5 = arith.addi %4, %c0 : index
    %6 = arith.addi %5, %3 : index
    %7 = arith.cmpi ult, %6, %c1 : index
    cf.cond_br %7, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1], strides: [1] : memref<1xf32, #gpu.address_space<global>> to memref<1xf32, #gpu.address_space<global>>
    memref.store %cst, %reinterpret_cast[%6] : memref<1xf32, #gpu.address_space<global>>
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    gpu.return
  }
}

// -----// IR Dump After DiscLowerGpuOpsToNVVMOpsPass (disc-convert-gpu-to-nvvm) //----- //
gpu.module @main_kernel_5 {
  llvm.func @main_kColReduction_reduce__6_1_0___block_tile_h64(%arg0: i32, %arg1: !llvm.ptr<1>, %arg2: !llvm.ptr<1>, %arg3: i32, %arg4: i32, %arg5: i32) attributes {gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.undef : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)>
    %1 = llvm.insertvalue %arg1, %0[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %2 = llvm.insertvalue %arg2, %1[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %3 = llvm.insertvalue %arg3, %2[2] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %4 = llvm.insertvalue %arg4, %3[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %5 = llvm.insertvalue %arg5, %4[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %6 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    %7 = llvm.mlir.constant(1 : index) : i32
    %8 = nvvm.read.ptx.sreg.ctaid.x : i32
    %9 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %10 = llvm.mul %8, %arg0  : i32
    %11 = llvm.add %9, %10  : i32
    %12 = llvm.icmp "ult" %11, %7 : i32
    llvm.cond_br %12, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %13 = llvm.mlir.undef : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)>
    %14 = llvm.extractvalue %5[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %15 = llvm.extractvalue %5[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %16 = llvm.insertvalue %14, %13[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %17 = llvm.insertvalue %15, %16[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %18 = llvm.mlir.constant(0 : index) : i32
    %19 = llvm.insertvalue %18, %17[2] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %20 = llvm.mlir.constant(1 : index) : i32
    %21 = llvm.insertvalue %20, %19[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %22 = llvm.mlir.constant(1 : index) : i32
    %23 = llvm.insertvalue %22, %21[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %24 = llvm.extractvalue %23[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %25 = llvm.getelementptr %24[%11] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    llvm.store %6, %25 : f32, !llvm.ptr<1>
    llvm.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    llvm.return
  }
}

// -----// IR Dump After LLVMInsertValueSimplifierPass (disc-llvm-insert-value-simplifier) //----- //
llvm.func @main_kColReduction_reduce__6_1_0___block_tile_h64(%arg0: i32, %arg1: !llvm.ptr<1>, %arg2: !llvm.ptr<1>, %arg3: i32, %arg4: i32, %arg5: i32) attributes {gpu.kernel, nvvm.kernel} {
  %0 = llvm.mlir.constant(1 : index) : i32
  %1 = llvm.mlir.constant(0.000000e+00 : f32) : f32
  %2 = nvvm.read.ptx.sreg.ctaid.x : i32
  %3 = nvvm.read.ptx.sreg.tid.x : i32
  llvm.br ^bb1
^bb1:  // pred: ^bb0
  %4 = llvm.mul %2, %arg0  : i32
  %5 = llvm.add %3, %4  : i32
  %6 = llvm.icmp "ult" %5, %0 : i32
  llvm.cond_br %6, ^bb2, ^bb3
^bb2:  // pred: ^bb1
  %7 = llvm.getelementptr %arg2[%5] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
  llvm.store %1, %7 : f32, !llvm.ptr<1>
  llvm.br ^bb3
^bb3:  // 2 preds: ^bb1, ^bb2
  llvm.return
}

// -----// IR Dump After FunctionDeadArgumentEliminationPass (disc-function-dead-argument-elimination) //----- //
gpu.module @main_kernel_5 {
  llvm.func @main_kColReduction_reduce__6_1_0___block_tile_h64(%arg0: i32, %arg1: !llvm.ptr<1>) attributes {disc.elimargs = [1 : index, 3 : index, 4 : index, 5 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(1 : index) : i32
    %1 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    %2 = nvvm.read.ptx.sreg.ctaid.x : i32
    %3 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %4 = llvm.mul %2, %arg0  : i32
    %5 = llvm.add %3, %4  : i32
    %6 = llvm.icmp "ult" %5, %0 : i32
    llvm.cond_br %6, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %7 = llvm.getelementptr %arg1[%5] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    llvm.store %1, %7 : f32, !llvm.ptr<1>
    llvm.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    llvm.return
  }
}

// -----// IR Dump After GpuKernelToBlobPass (disc-gpu-kernel-to-blob) //----- //
gpu.module @main_kernel_5 attributes {gpu.binary = "P\EDU\BA\01\00\10\00H\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\08\04\00\00\00\00\00\00\01\04\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\0Ae__6_1_0___block_tile_h64A\00\0F;\00$oshared=\00$\9Fconstant0@\00!\FA\01debug_frame\00.rel\11\00!nv\14\00\11aL\00\0FO\01 \0F\91\00\1E\0F\81\01\DEo_param\88\01\1C\0F\01\00\05\8Cd\00\00\00\03\00\0A\00\01\00 \14\01\18\00,\09\00\01\00\11[\18\00,\04\00\01\00\11y\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\1C\00\00\00E\00#\04\10\18\00\80/\08\00\05\00\00\00\06\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\048\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C$\00u\01\00\08\00\00\F0!\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00p\00\00\00\C0\00\01\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\00#\02b,\00\00\00H\00\01\00\00 \01/\05\00\01\00\FFp@$v\01\FF\CF\03\A3\FF\00\8E\07\00\C4\0F\00\C3y\AC\03\A2%\00\00\00\22\0E\00\19y\00\01\00\F3\0E!\00\00\00b\0E\00\90r\05?\04\00\00\80?\E0\FF\0F\00\C6\1F\00\B9z\04\00\00X\9E\02p\E4\0F\00\A4r\04\05B\00\B1?\02\8E\0F\00\CC\0F\00\0C|\00\FE\03\95pR\F0\0B\00\DA/\00M#\04\F3\0D\80\03\00\EA\0F\00\02z\02\00\00Z\00\00\00\0F\00\00\00\E2\0F\00$v\03\FF\00[\90\00\22\E2\0F`\001F\00\00s\04\80\C8\0F\00\86y\00\02\FFp\030\19\10\0C \00*MyP\00PGy\00\00\F0\F9\03\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\80\0F\01\00-\00\C7\04.\03\00\01\00\22@\00\01\00=O\01\000\00\08\01\00\1F\0B@\00\04\13\8F)\00\1F\88@\00\0C\13\13\CC\03\0C\01\00\13\18U\00*\90\00\F0\03\04\0D\04\22\18\00\01\00.\22\01T\00\00\01\00\13\A8@\00/p\00\80\00\0B\1F)'\00\03A\00\18\04\00\01\00\04\00\06\04\E4\00*\04\00\01\00\1Fj@\00\04\13H)\00&L\00@\00\1F\0A@\00\00!@\01D\01\0D@\00\13\98)\00*\D8\00\01\00\1B\08\08\00?/\01\006\07\003\00\00p\1D\05\17\10\80\00\17\048\00\04\18\00\13\E2\14\01\0C\84\01*\80\05\E8\06\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\80\07\12\03\C0\01:\06\80\00\01\00\13\06\D8\01\04(\0B\0D\C8\01\1A\00\08\00\04\F8\00\13\018\00\04\A8\00\0D\01\00)\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00\00\00\00\00\00\00"} {
  llvm.func @main_kColReduction_reduce__6_1_0___block_tile_h64(%arg0: i32, %arg1: !llvm.ptr<1>) attributes {disc.elimargs = [1 : index, 3 : index, 4 : index, 5 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(1 : index) : i32
    %1 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    %2 = nvvm.read.ptx.sreg.ctaid.x : i32
    %3 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %4 = llvm.mul %2, %arg0  : i32
    %5 = llvm.add %3, %4  : i32
    %6 = llvm.icmp "ult" %5, %0 : i32
    llvm.cond_br %6, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %7 = llvm.getelementptr %arg1[%5] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    llvm.store %1, %7 : f32, !llvm.ptr<1>
    llvm.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    llvm.return
  }
}

// -----// IR Dump After LoopInvariantCodeMotion (loop-invariant-code-motion) //----- //
gpu.module @main_kernel_6 {
  gpu.func @main_kColReduction_reduce__6_1_0___block_tile_h64_1(%arg0: index, %arg1: index, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?x4xf32, #gpu.address_space<global>>, %arg6: memref<?x4xf32, #gpu.address_space<global>>, %arg7: memref<1xf32, #gpu.address_space<global>>) workgroup(%arg8 : memref<256xf32, #gpu.address_space<workgroup>>) kernel {
    %c2 = arith.constant 2 : index
    %cst = arith.constant 0.000000e+00 : f32
    %c4 = arith.constant 4 : index
    %c64 = arith.constant 64 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c256 = arith.constant 256 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = arith.cmpi eq, %arg0, %c1 : index
    %3 = arith.select %2, %arg1, %arg0 : index
    %4 = arith.cmpi eq, %arg1, %3 : index
    %5 = arith.cmpi eq, %arg0, %3 : index
    %6 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg2, %c0]
    %7 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %8 = arith.addi %7, %6 : index
    %9 = arith.addi %7, %6 : index
    %10 = arith.cmpi ult, %9, %arg3 : index
    scf.if %10 {
      %11 = arith.remsi %8, %c256 : index
      %12 = arith.divsi %8, %c256 : index
      %13 = arith.divui %11, %c32 : index
      %14 = arith.remui %11, %c32 : index
      %15 = arith.muli %14, %c8 : index
      %16 = arith.addi %13, %15 : index
      %17 = arith.cmpi ult, %14, %c1 : index
      %18 = scf.if %17 -> (f32) {
        %24 = arith.muli %12, %c8 : index
        %25 = arith.addi %13, %24 : index
        %26 = arith.muli %25, %c64 : index
        %27 = scf.for %arg9 = %c0 to %c64 step %c1 iter_args(%arg10 = %cst) -> (f32) {
          %28 = arith.addi %arg9, %26 : index
          %29 = arith.cmpi slt, %28, %arg4 : index
          %30 = scf.if %29 -> (f32) {
            %31 = arith.remui %28, %c4 : index
            %32 = arith.divui %28, %c4 : index
            %33 = arith.select %4, %32, %c0 : index
            %34 = arith.muli %33, %c4 : index
            %35 = arith.addi %34, %31 : index
            %36 = arith.muli %arg1, %c4 : index
            %reinterpret_cast_0 = memref.reinterpret_cast %arg5 to offset: [%c0], sizes: [%36], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
            %37 = memref.load %reinterpret_cast_0[%35] : memref<?xf32, #gpu.address_space<global>>
            %38 = arith.select %5, %32, %c0 : index
            %39 = arith.muli %38, %c4 : index
            %40 = arith.addi %39, %31 : index
            %41 = arith.muli %arg0, %c4 : index
            %reinterpret_cast_1 = memref.reinterpret_cast %arg6 to offset: [%c0], sizes: [%41], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
            %42 = memref.load %reinterpret_cast_1[%40] : memref<?xf32, #gpu.address_space<global>>
            %43 = arith.mulf %37, %42 : f32
            %44 = arith.addf %arg10, %43 : f32
            scf.yield %44 : f32
          } else {
            scf.yield %arg10 : f32
          }
          scf.yield %30 : f32
        }
        scf.yield %27 : f32
      } else {
        scf.yield %cst : f32
      }
      %reinterpret_cast = memref.reinterpret_cast %arg8 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
      memref.store %18, %reinterpret_cast[%16] : memref<256xf32, #gpu.address_space<workgroup>>
      gpu.barrier
      %19 = arith.cmpi slt, %13, %c4 : index
      scf.if %19 {
        %24 = arith.addi %16, %c4 : index
        %25 = memref.load %reinterpret_cast[%16] : memref<256xf32, #gpu.address_space<workgroup>>
        %26 = memref.load %reinterpret_cast[%24] : memref<256xf32, #gpu.address_space<workgroup>>
        %27 = arith.addf %25, %26 : f32
        memref.store %27, %reinterpret_cast[%16] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %20 = arith.cmpi slt, %13, %c2 : index
      scf.if %20 {
        %24 = arith.addi %16, %c2 : index
        %25 = memref.load %reinterpret_cast[%16] : memref<256xf32, #gpu.address_space<workgroup>>
        %26 = memref.load %reinterpret_cast[%24] : memref<256xf32, #gpu.address_space<workgroup>>
        %27 = arith.addf %25, %26 : f32
        memref.store %27, %reinterpret_cast[%16] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %21 = arith.cmpi slt, %13, %c1 : index
      scf.if %21 {
        %24 = arith.addi %16, %c1 : index
        %25 = memref.load %reinterpret_cast[%16] : memref<256xf32, #gpu.address_space<workgroup>>
        %26 = memref.load %reinterpret_cast[%24] : memref<256xf32, #gpu.address_space<workgroup>>
        %27 = arith.addf %25, %26 : f32
        memref.store %27, %reinterpret_cast[%16] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %22 = arith.cmpi eq, %13, %c0 : index
      %23 = arith.andi %22, %17 : i1
      scf.if %23 {
        %24 = memref.load %reinterpret_cast[%16] : memref<256xf32, #gpu.address_space<workgroup>>
        %25 = memref.atomic_rmw addf %24, %arg7[%14] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
      }
    }
    gpu.return
  }
}

// -----// IR Dump After CSE (cse) //----- //
gpu.module @main_kernel_6 {
  gpu.func @main_kColReduction_reduce__6_1_0___block_tile_h64_1(%arg0: index, %arg1: index, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?x4xf32, #gpu.address_space<global>>, %arg6: memref<?x4xf32, #gpu.address_space<global>>, %arg7: memref<1xf32, #gpu.address_space<global>>) workgroup(%arg8 : memref<256xf32, #gpu.address_space<workgroup>>) kernel {
    %c2 = arith.constant 2 : index
    %cst = arith.constant 0.000000e+00 : f32
    %c4 = arith.constant 4 : index
    %c64 = arith.constant 64 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c256 = arith.constant 256 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = arith.cmpi eq, %arg0, %c1 : index
    %3 = arith.select %2, %arg1, %arg0 : index
    %4 = arith.cmpi eq, %arg1, %3 : index
    %5 = arith.cmpi eq, %arg0, %3 : index
    %6 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg2, %c0]
    %7 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %8 = arith.addi %7, %6 : index
    %9 = arith.cmpi ult, %8, %arg3 : index
    scf.if %9 {
      %10 = arith.remsi %8, %c256 : index
      %11 = arith.divsi %8, %c256 : index
      %12 = arith.divui %10, %c32 : index
      %13 = arith.remui %10, %c32 : index
      %14 = arith.muli %13, %c8 : index
      %15 = arith.addi %12, %14 : index
      %16 = arith.cmpi ult, %13, %c1 : index
      %17 = scf.if %16 -> (f32) {
        %23 = arith.muli %11, %c8 : index
        %24 = arith.addi %12, %23 : index
        %25 = arith.muli %24, %c64 : index
        %26 = scf.for %arg9 = %c0 to %c64 step %c1 iter_args(%arg10 = %cst) -> (f32) {
          %27 = arith.addi %arg9, %25 : index
          %28 = arith.cmpi slt, %27, %arg4 : index
          %29 = scf.if %28 -> (f32) {
            %30 = arith.remui %27, %c4 : index
            %31 = arith.divui %27, %c4 : index
            %32 = arith.select %4, %31, %c0 : index
            %33 = arith.muli %32, %c4 : index
            %34 = arith.addi %33, %30 : index
            %35 = arith.muli %arg1, %c4 : index
            %reinterpret_cast_0 = memref.reinterpret_cast %arg5 to offset: [%c0], sizes: [%35], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
            %36 = memref.load %reinterpret_cast_0[%34] : memref<?xf32, #gpu.address_space<global>>
            %37 = arith.select %5, %31, %c0 : index
            %38 = arith.muli %37, %c4 : index
            %39 = arith.addi %38, %30 : index
            %40 = arith.muli %arg0, %c4 : index
            %reinterpret_cast_1 = memref.reinterpret_cast %arg6 to offset: [%c0], sizes: [%40], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
            %41 = memref.load %reinterpret_cast_1[%39] : memref<?xf32, #gpu.address_space<global>>
            %42 = arith.mulf %36, %41 : f32
            %43 = arith.addf %arg10, %42 : f32
            scf.yield %43 : f32
          } else {
            scf.yield %arg10 : f32
          }
          scf.yield %29 : f32
        }
        scf.yield %26 : f32
      } else {
        scf.yield %cst : f32
      }
      %reinterpret_cast = memref.reinterpret_cast %arg8 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
      memref.store %17, %reinterpret_cast[%15] : memref<256xf32, #gpu.address_space<workgroup>>
      gpu.barrier
      %18 = arith.cmpi slt, %12, %c4 : index
      scf.if %18 {
        %23 = arith.addi %15, %c4 : index
        %24 = memref.load %reinterpret_cast[%15] : memref<256xf32, #gpu.address_space<workgroup>>
        %25 = memref.load %reinterpret_cast[%23] : memref<256xf32, #gpu.address_space<workgroup>>
        %26 = arith.addf %24, %25 : f32
        memref.store %26, %reinterpret_cast[%15] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %19 = arith.cmpi slt, %12, %c2 : index
      scf.if %19 {
        %23 = arith.addi %15, %c2 : index
        %24 = memref.load %reinterpret_cast[%15] : memref<256xf32, #gpu.address_space<workgroup>>
        %25 = memref.load %reinterpret_cast[%23] : memref<256xf32, #gpu.address_space<workgroup>>
        %26 = arith.addf %24, %25 : f32
        memref.store %26, %reinterpret_cast[%15] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %20 = arith.cmpi slt, %12, %c1 : index
      scf.if %20 {
        %23 = arith.addi %15, %c1 : index
        %24 = memref.load %reinterpret_cast[%15] : memref<256xf32, #gpu.address_space<workgroup>>
        %25 = memref.load %reinterpret_cast[%23] : memref<256xf32, #gpu.address_space<workgroup>>
        %26 = arith.addf %24, %25 : f32
        memref.store %26, %reinterpret_cast[%15] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %21 = arith.cmpi eq, %12, %c0 : index
      %22 = arith.andi %21, %16 : i1
      scf.if %22 {
        %23 = memref.load %reinterpret_cast[%15] : memref<256xf32, #gpu.address_space<workgroup>>
        %24 = memref.atomic_rmw addf %23, %arg7[%13] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
      }
    }
    gpu.return
  }
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
gpu.module @main_kernel_6 {
  gpu.func @main_kColReduction_reduce__6_1_0___block_tile_h64_1(%arg0: index, %arg1: index, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?x4xf32, #gpu.address_space<global>>, %arg6: memref<?x4xf32, #gpu.address_space<global>>, %arg7: memref<1xf32, #gpu.address_space<global>>) workgroup(%arg8 : memref<256xf32, #gpu.address_space<workgroup>>) kernel {
    %c2 = arith.constant 2 : index
    %cst = arith.constant 0.000000e+00 : f32
    %c4 = arith.constant 4 : index
    %c64 = arith.constant 64 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c256 = arith.constant 256 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = arith.cmpi eq, %arg0, %c1 : index
    %3 = arith.select %2, %arg1, %arg0 : index
    %4 = arith.cmpi eq, %arg1, %3 : index
    %5 = arith.cmpi eq, %arg0, %3 : index
    %6 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg2, %c0]
    %7 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %8 = arith.addi %7, %6 : index
    %9 = arith.cmpi ult, %8, %arg3 : index
    cf.cond_br %9, ^bb2, ^bb22
  ^bb2:  // pred: ^bb1
    %10 = arith.remsi %8, %c256 : index
    %11 = arith.divsi %8, %c256 : index
    %12 = arith.divui %10, %c32 : index
    %13 = arith.remui %10, %c32 : index
    %14 = arith.muli %13, %c8 : index
    %15 = arith.addi %12, %14 : index
    %16 = arith.cmpi ult, %13, %c1 : index
    cf.cond_br %16, ^bb3, ^bb11
  ^bb3:  // pred: ^bb2
    %17 = arith.muli %11, %c8 : index
    %18 = arith.addi %12, %17 : index
    %19 = arith.muli %18, %c64 : index
    cf.br ^bb4(%c0, %cst : index, f32)
  ^bb4(%20: index, %21: f32):  // 2 preds: ^bb3, ^bb9
    %22 = arith.cmpi slt, %20, %c64 : index
    cf.cond_br %22, ^bb5, ^bb10
  ^bb5:  // pred: ^bb4
    %23 = arith.addi %20, %19 : index
    %24 = arith.cmpi slt, %23, %arg4 : index
    cf.cond_br %24, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %25 = arith.remui %23, %c4 : index
    %26 = arith.divui %23, %c4 : index
    %27 = arith.select %4, %26, %c0 : index
    %28 = arith.muli %27, %c4 : index
    %29 = arith.addi %28, %25 : index
    %30 = arith.muli %arg1, %c4 : index
    %reinterpret_cast = memref.reinterpret_cast %arg5 to offset: [%c0], sizes: [%30], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
    %31 = memref.load %reinterpret_cast[%29] : memref<?xf32, #gpu.address_space<global>>
    %32 = arith.select %5, %26, %c0 : index
    %33 = arith.muli %32, %c4 : index
    %34 = arith.addi %33, %25 : index
    %35 = arith.muli %arg0, %c4 : index
    %reinterpret_cast_0 = memref.reinterpret_cast %arg6 to offset: [%c0], sizes: [%35], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
    %36 = memref.load %reinterpret_cast_0[%34] : memref<?xf32, #gpu.address_space<global>>
    %37 = arith.mulf %31, %36 : f32
    %38 = arith.addf %21, %37 : f32
    cf.br ^bb8(%38 : f32)
  ^bb7:  // pred: ^bb5
    cf.br ^bb8(%21 : f32)
  ^bb8(%39: f32):  // 2 preds: ^bb6, ^bb7
    cf.br ^bb9
  ^bb9:  // pred: ^bb8
    %40 = arith.addi %20, %c1 : index
    cf.br ^bb4(%40, %39 : index, f32)
  ^bb10:  // pred: ^bb4
    cf.br ^bb12(%21 : f32)
  ^bb11:  // pred: ^bb2
    cf.br ^bb12(%cst : f32)
  ^bb12(%41: f32):  // 2 preds: ^bb10, ^bb11
    cf.br ^bb13
  ^bb13:  // pred: ^bb12
    %reinterpret_cast_1 = memref.reinterpret_cast %arg8 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
    memref.store %41, %reinterpret_cast_1[%15] : memref<256xf32, #gpu.address_space<workgroup>>
    gpu.barrier
    %42 = arith.cmpi slt, %12, %c4 : index
    cf.cond_br %42, ^bb14, ^bb15
  ^bb14:  // pred: ^bb13
    %43 = arith.addi %15, %c4 : index
    %44 = memref.load %reinterpret_cast_1[%15] : memref<256xf32, #gpu.address_space<workgroup>>
    %45 = memref.load %reinterpret_cast_1[%43] : memref<256xf32, #gpu.address_space<workgroup>>
    %46 = arith.addf %44, %45 : f32
    memref.store %46, %reinterpret_cast_1[%15] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb15
  ^bb15:  // 2 preds: ^bb13, ^bb14
    gpu.barrier
    %47 = arith.cmpi slt, %12, %c2 : index
    cf.cond_br %47, ^bb16, ^bb17
  ^bb16:  // pred: ^bb15
    %48 = arith.addi %15, %c2 : index
    %49 = memref.load %reinterpret_cast_1[%15] : memref<256xf32, #gpu.address_space<workgroup>>
    %50 = memref.load %reinterpret_cast_1[%48] : memref<256xf32, #gpu.address_space<workgroup>>
    %51 = arith.addf %49, %50 : f32
    memref.store %51, %reinterpret_cast_1[%15] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb17
  ^bb17:  // 2 preds: ^bb15, ^bb16
    gpu.barrier
    %52 = arith.cmpi slt, %12, %c1 : index
    cf.cond_br %52, ^bb18, ^bb19
  ^bb18:  // pred: ^bb17
    %53 = arith.addi %15, %c1 : index
    %54 = memref.load %reinterpret_cast_1[%15] : memref<256xf32, #gpu.address_space<workgroup>>
    %55 = memref.load %reinterpret_cast_1[%53] : memref<256xf32, #gpu.address_space<workgroup>>
    %56 = arith.addf %54, %55 : f32
    memref.store %56, %reinterpret_cast_1[%15] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb19
  ^bb19:  // 2 preds: ^bb17, ^bb18
    gpu.barrier
    %57 = arith.cmpi eq, %12, %c0 : index
    %58 = arith.andi %57, %16 : i1
    cf.cond_br %58, ^bb20, ^bb21
  ^bb20:  // pred: ^bb19
    %59 = memref.load %reinterpret_cast_1[%15] : memref<256xf32, #gpu.address_space<workgroup>>
    %60 = memref.atomic_rmw addf %59, %arg7[%13] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
    cf.br ^bb21
  ^bb21:  // 2 preds: ^bb19, ^bb20
    cf.br ^bb22
  ^bb22:  // 2 preds: ^bb1, ^bb21
    gpu.return
  }
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
gpu.module @main_kernel_6 {
  gpu.func @main_kColReduction_reduce__6_1_0___block_tile_h64_1(%arg0: index, %arg1: index, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?x4xf32, #gpu.address_space<global>>, %arg6: memref<?x4xf32, #gpu.address_space<global>>, %arg7: memref<1xf32, #gpu.address_space<global>>) workgroup(%arg8 : memref<256xf32, #gpu.address_space<workgroup>>) kernel {
    %c2 = arith.constant 2 : index
    %cst = arith.constant 0.000000e+00 : f32
    %c4 = arith.constant 4 : index
    %c64 = arith.constant 64 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c256 = arith.constant 256 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = arith.cmpi eq, %arg0, %c1 : index
    %3 = arith.select %2, %arg1, %arg0 : index
    %4 = arith.cmpi eq, %arg1, %3 : index
    %5 = arith.cmpi eq, %arg0, %3 : index
    %6 = arith.muli %0, %arg2 : index
    %7 = arith.addi %6, %c0 : index
    %8 = arith.muli %1, %c1 : index
    %9 = arith.addi %8, %c0 : index
    %10 = arith.addi %9, %7 : index
    %11 = arith.cmpi ult, %10, %arg3 : index
    cf.cond_br %11, ^bb2, ^bb22
  ^bb2:  // pred: ^bb1
    %12 = arith.remsi %10, %c256 : index
    %13 = arith.divsi %10, %c256 : index
    %14 = arith.divui %12, %c32 : index
    %15 = arith.remui %12, %c32 : index
    %16 = arith.muli %15, %c8 : index
    %17 = arith.addi %14, %16 : index
    %18 = arith.cmpi ult, %15, %c1 : index
    cf.cond_br %18, ^bb3, ^bb11
  ^bb3:  // pred: ^bb2
    %19 = arith.muli %13, %c8 : index
    %20 = arith.addi %14, %19 : index
    %21 = arith.muli %20, %c64 : index
    cf.br ^bb4(%c0, %cst : index, f32)
  ^bb4(%22: index, %23: f32):  // 2 preds: ^bb3, ^bb9
    %24 = arith.cmpi slt, %22, %c64 : index
    cf.cond_br %24, ^bb5, ^bb10
  ^bb5:  // pred: ^bb4
    %25 = arith.addi %22, %21 : index
    %26 = arith.cmpi slt, %25, %arg4 : index
    cf.cond_br %26, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %27 = arith.remui %25, %c4 : index
    %28 = arith.divui %25, %c4 : index
    %29 = arith.select %4, %28, %c0 : index
    %30 = arith.muli %29, %c4 : index
    %31 = arith.addi %30, %27 : index
    %32 = arith.muli %arg1, %c4 : index
    %reinterpret_cast = memref.reinterpret_cast %arg5 to offset: [%c0], sizes: [%32], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
    %33 = memref.load %reinterpret_cast[%31] : memref<?xf32, #gpu.address_space<global>>
    %34 = arith.select %5, %28, %c0 : index
    %35 = arith.muli %34, %c4 : index
    %36 = arith.addi %35, %27 : index
    %37 = arith.muli %arg0, %c4 : index
    %reinterpret_cast_0 = memref.reinterpret_cast %arg6 to offset: [%c0], sizes: [%37], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
    %38 = memref.load %reinterpret_cast_0[%36] : memref<?xf32, #gpu.address_space<global>>
    %39 = arith.mulf %33, %38 : f32
    %40 = arith.addf %23, %39 : f32
    cf.br ^bb8(%40 : f32)
  ^bb7:  // pred: ^bb5
    cf.br ^bb8(%23 : f32)
  ^bb8(%41: f32):  // 2 preds: ^bb6, ^bb7
    cf.br ^bb9
  ^bb9:  // pred: ^bb8
    %42 = arith.addi %22, %c1 : index
    cf.br ^bb4(%42, %41 : index, f32)
  ^bb10:  // pred: ^bb4
    cf.br ^bb12(%23 : f32)
  ^bb11:  // pred: ^bb2
    cf.br ^bb12(%cst : f32)
  ^bb12(%43: f32):  // 2 preds: ^bb10, ^bb11
    cf.br ^bb13
  ^bb13:  // pred: ^bb12
    %reinterpret_cast_1 = memref.reinterpret_cast %arg8 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
    memref.store %43, %reinterpret_cast_1[%17] : memref<256xf32, #gpu.address_space<workgroup>>
    gpu.barrier
    %44 = arith.cmpi slt, %14, %c4 : index
    cf.cond_br %44, ^bb14, ^bb15
  ^bb14:  // pred: ^bb13
    %45 = arith.addi %17, %c4 : index
    %46 = memref.load %reinterpret_cast_1[%17] : memref<256xf32, #gpu.address_space<workgroup>>
    %47 = memref.load %reinterpret_cast_1[%45] : memref<256xf32, #gpu.address_space<workgroup>>
    %48 = arith.addf %46, %47 : f32
    memref.store %48, %reinterpret_cast_1[%17] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb15
  ^bb15:  // 2 preds: ^bb13, ^bb14
    gpu.barrier
    %49 = arith.cmpi slt, %14, %c2 : index
    cf.cond_br %49, ^bb16, ^bb17
  ^bb16:  // pred: ^bb15
    %50 = arith.addi %17, %c2 : index
    %51 = memref.load %reinterpret_cast_1[%17] : memref<256xf32, #gpu.address_space<workgroup>>
    %52 = memref.load %reinterpret_cast_1[%50] : memref<256xf32, #gpu.address_space<workgroup>>
    %53 = arith.addf %51, %52 : f32
    memref.store %53, %reinterpret_cast_1[%17] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb17
  ^bb17:  // 2 preds: ^bb15, ^bb16
    gpu.barrier
    %54 = arith.cmpi slt, %14, %c1 : index
    cf.cond_br %54, ^bb18, ^bb19
  ^bb18:  // pred: ^bb17
    %55 = arith.addi %17, %c1 : index
    %56 = memref.load %reinterpret_cast_1[%17] : memref<256xf32, #gpu.address_space<workgroup>>
    %57 = memref.load %reinterpret_cast_1[%55] : memref<256xf32, #gpu.address_space<workgroup>>
    %58 = arith.addf %56, %57 : f32
    memref.store %58, %reinterpret_cast_1[%17] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb19
  ^bb19:  // 2 preds: ^bb17, ^bb18
    gpu.barrier
    %59 = arith.cmpi eq, %14, %c0 : index
    %60 = arith.andi %59, %18 : i1
    cf.cond_br %60, ^bb20, ^bb21
  ^bb20:  // pred: ^bb19
    %61 = memref.load %reinterpret_cast_1[%17] : memref<256xf32, #gpu.address_space<workgroup>>
    %62 = memref.atomic_rmw addf %61, %arg7[%15] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
    cf.br ^bb21
  ^bb21:  // 2 preds: ^bb19, ^bb20
    cf.br ^bb22
  ^bb22:  // 2 preds: ^bb1, ^bb21
    gpu.return
  }
}

// -----// IR Dump After StripDebugInfo (strip-debuginfo) //----- //
gpu.module @main_kernel_6 {
  gpu.func @main_kColReduction_reduce__6_1_0___block_tile_h64_1(%arg0: index, %arg1: index, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?x4xf32, #gpu.address_space<global>>, %arg6: memref<?x4xf32, #gpu.address_space<global>>, %arg7: memref<1xf32, #gpu.address_space<global>>) workgroup(%arg8 : memref<256xf32, #gpu.address_space<workgroup>>) kernel {
    %c2 = arith.constant 2 : index
    %cst = arith.constant 0.000000e+00 : f32
    %c4 = arith.constant 4 : index
    %c64 = arith.constant 64 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c256 = arith.constant 256 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = arith.cmpi eq, %arg0, %c1 : index
    %3 = arith.select %2, %arg1, %arg0 : index
    %4 = arith.cmpi eq, %arg1, %3 : index
    %5 = arith.cmpi eq, %arg0, %3 : index
    %6 = arith.muli %0, %arg2 : index
    %7 = arith.addi %6, %c0 : index
    %8 = arith.muli %1, %c1 : index
    %9 = arith.addi %8, %c0 : index
    %10 = arith.addi %9, %7 : index
    %11 = arith.cmpi ult, %10, %arg3 : index
    cf.cond_br %11, ^bb2, ^bb22
  ^bb2:  // pred: ^bb1
    %12 = arith.remsi %10, %c256 : index
    %13 = arith.divsi %10, %c256 : index
    %14 = arith.divui %12, %c32 : index
    %15 = arith.remui %12, %c32 : index
    %16 = arith.muli %15, %c8 : index
    %17 = arith.addi %14, %16 : index
    %18 = arith.cmpi ult, %15, %c1 : index
    cf.cond_br %18, ^bb3, ^bb11
  ^bb3:  // pred: ^bb2
    %19 = arith.muli %13, %c8 : index
    %20 = arith.addi %14, %19 : index
    %21 = arith.muli %20, %c64 : index
    cf.br ^bb4(%c0, %cst : index, f32)
  ^bb4(%22: index, %23: f32):  // 2 preds: ^bb3, ^bb9
    %24 = arith.cmpi slt, %22, %c64 : index
    cf.cond_br %24, ^bb5, ^bb10
  ^bb5:  // pred: ^bb4
    %25 = arith.addi %22, %21 : index
    %26 = arith.cmpi slt, %25, %arg4 : index
    cf.cond_br %26, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %27 = arith.remui %25, %c4 : index
    %28 = arith.divui %25, %c4 : index
    %29 = arith.select %4, %28, %c0 : index
    %30 = arith.muli %29, %c4 : index
    %31 = arith.addi %30, %27 : index
    %32 = arith.muli %arg1, %c4 : index
    %reinterpret_cast = memref.reinterpret_cast %arg5 to offset: [%c0], sizes: [%32], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
    %33 = memref.load %reinterpret_cast[%31] : memref<?xf32, #gpu.address_space<global>>
    %34 = arith.select %5, %28, %c0 : index
    %35 = arith.muli %34, %c4 : index
    %36 = arith.addi %35, %27 : index
    %37 = arith.muli %arg0, %c4 : index
    %reinterpret_cast_0 = memref.reinterpret_cast %arg6 to offset: [%c0], sizes: [%37], strides: [%c1] : memref<?x4xf32, #gpu.address_space<global>> to memref<?xf32, #gpu.address_space<global>>
    %38 = memref.load %reinterpret_cast_0[%36] : memref<?xf32, #gpu.address_space<global>>
    %39 = arith.mulf %33, %38 : f32
    %40 = arith.addf %23, %39 : f32
    cf.br ^bb8(%40 : f32)
  ^bb7:  // pred: ^bb5
    cf.br ^bb8(%23 : f32)
  ^bb8(%41: f32):  // 2 preds: ^bb6, ^bb7
    cf.br ^bb9
  ^bb9:  // pred: ^bb8
    %42 = arith.addi %22, %c1 : index
    cf.br ^bb4(%42, %41 : index, f32)
  ^bb10:  // pred: ^bb4
    cf.br ^bb12(%23 : f32)
  ^bb11:  // pred: ^bb2
    cf.br ^bb12(%cst : f32)
  ^bb12(%43: f32):  // 2 preds: ^bb10, ^bb11
    cf.br ^bb13
  ^bb13:  // pred: ^bb12
    %reinterpret_cast_1 = memref.reinterpret_cast %arg8 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
    memref.store %43, %reinterpret_cast_1[%17] : memref<256xf32, #gpu.address_space<workgroup>>
    gpu.barrier
    %44 = arith.cmpi slt, %14, %c4 : index
    cf.cond_br %44, ^bb14, ^bb15
  ^bb14:  // pred: ^bb13
    %45 = arith.addi %17, %c4 : index
    %46 = memref.load %reinterpret_cast_1[%17] : memref<256xf32, #gpu.address_space<workgroup>>
    %47 = memref.load %reinterpret_cast_1[%45] : memref<256xf32, #gpu.address_space<workgroup>>
    %48 = arith.addf %46, %47 : f32
    memref.store %48, %reinterpret_cast_1[%17] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb15
  ^bb15:  // 2 preds: ^bb13, ^bb14
    gpu.barrier
    %49 = arith.cmpi slt, %14, %c2 : index
    cf.cond_br %49, ^bb16, ^bb17
  ^bb16:  // pred: ^bb15
    %50 = arith.addi %17, %c2 : index
    %51 = memref.load %reinterpret_cast_1[%17] : memref<256xf32, #gpu.address_space<workgroup>>
    %52 = memref.load %reinterpret_cast_1[%50] : memref<256xf32, #gpu.address_space<workgroup>>
    %53 = arith.addf %51, %52 : f32
    memref.store %53, %reinterpret_cast_1[%17] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb17
  ^bb17:  // 2 preds: ^bb15, ^bb16
    gpu.barrier
    %54 = arith.cmpi slt, %14, %c1 : index
    cf.cond_br %54, ^bb18, ^bb19
  ^bb18:  // pred: ^bb17
    %55 = arith.addi %17, %c1 : index
    %56 = memref.load %reinterpret_cast_1[%17] : memref<256xf32, #gpu.address_space<workgroup>>
    %57 = memref.load %reinterpret_cast_1[%55] : memref<256xf32, #gpu.address_space<workgroup>>
    %58 = arith.addf %56, %57 : f32
    memref.store %58, %reinterpret_cast_1[%17] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb19
  ^bb19:  // 2 preds: ^bb17, ^bb18
    gpu.barrier
    %59 = arith.cmpi eq, %14, %c0 : index
    %60 = arith.andi %59, %18 : i1
    cf.cond_br %60, ^bb20, ^bb21
  ^bb20:  // pred: ^bb19
    %61 = memref.load %reinterpret_cast_1[%17] : memref<256xf32, #gpu.address_space<workgroup>>
    %62 = memref.atomic_rmw addf %61, %arg7[%15] : (f32, memref<1xf32, #gpu.address_space<global>>) -> f32
    cf.br ^bb21
  ^bb21:  // 2 preds: ^bb19, ^bb20
    cf.br ^bb22
  ^bb22:  // 2 preds: ^bb1, ^bb21
    gpu.return
  }
}

// -----// IR Dump After DiscLowerGpuOpsToNVVMOpsPass (disc-convert-gpu-to-nvvm) //----- //
gpu.module @main_kernel_6 {
  llvm.mlir.global internal @__wg_main_kColReduction_reduce__6_1_0___block_tile_h64_1_0() {addr_space = 3 : i32} : !llvm.array<256 x f32>
  llvm.func @main_kColReduction_reduce__6_1_0___block_tile_h64_1(%arg0: i32, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: !llvm.ptr<1>, %arg6: !llvm.ptr<1>, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: !llvm.ptr<1>, %arg13: !llvm.ptr<1>, %arg14: i32, %arg15: i32, %arg16: i32, %arg17: i32, %arg18: i32, %arg19: !llvm.ptr<1>, %arg20: !llvm.ptr<1>, %arg21: i32, %arg22: i32, %arg23: i32) attributes {gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.undef : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)>
    %1 = llvm.insertvalue %arg5, %0[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %2 = llvm.insertvalue %arg6, %1[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %3 = llvm.insertvalue %arg7, %2[2] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %4 = llvm.insertvalue %arg8, %3[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %5 = llvm.insertvalue %arg10, %4[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %6 = llvm.insertvalue %arg9, %5[3, 1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %7 = llvm.insertvalue %arg11, %6[4, 1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %8 = llvm.mlir.undef : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)>
    %9 = llvm.insertvalue %arg12, %8[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %10 = llvm.insertvalue %arg13, %9[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %11 = llvm.insertvalue %arg14, %10[2] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %12 = llvm.insertvalue %arg15, %11[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %13 = llvm.insertvalue %arg17, %12[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %14 = llvm.insertvalue %arg16, %13[3, 1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %15 = llvm.insertvalue %arg18, %14[4, 1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %16 = llvm.mlir.undef : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)>
    %17 = llvm.insertvalue %arg19, %16[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %18 = llvm.insertvalue %arg20, %17[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %19 = llvm.insertvalue %arg21, %18[2] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %20 = llvm.insertvalue %arg22, %19[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %21 = llvm.insertvalue %arg23, %20[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %22 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__6_1_0___block_tile_h64_1_0 : !llvm.ptr<3>
    %23 = llvm.getelementptr %22[0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<256 x f32>
    %24 = llvm.mlir.undef : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)>
    %25 = llvm.insertvalue %23, %24[0] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %26 = llvm.insertvalue %23, %25[1] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %27 = llvm.mlir.constant(0 : index) : i32
    %28 = llvm.insertvalue %27, %26[2] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %29 = llvm.mlir.constant(256 : index) : i32
    %30 = llvm.insertvalue %29, %28[3, 0] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %31 = llvm.mlir.constant(1 : index) : i32
    %32 = llvm.insertvalue %31, %30[4, 0] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %33 = llvm.mlir.constant(2 : index) : i32
    %34 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    %35 = llvm.mlir.constant(4 : index) : i32
    %36 = llvm.mlir.constant(64 : index) : i32
    %37 = llvm.mlir.constant(8 : index) : i32
    %38 = llvm.mlir.constant(32 : index) : i32
    %39 = llvm.mlir.constant(256 : index) : i32
    %40 = llvm.mlir.constant(1 : index) : i32
    %41 = llvm.mlir.constant(0 : index) : i32
    %42 = nvvm.read.ptx.sreg.ctaid.x : i32
    %43 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %44 = llvm.icmp "eq" %arg0, %40 : i32
    %45 = llvm.select %44, %arg1, %arg0 : i1, i32
    %46 = llvm.icmp "eq" %arg1, %45 : i32
    %47 = llvm.icmp "eq" %arg0, %45 : i32
    %48 = llvm.mul %42, %arg2  : i32
    %49 = llvm.add %43, %48  : i32
    %50 = llvm.icmp "ult" %49, %arg3 : i32
    llvm.cond_br %50, ^bb2, ^bb21
  ^bb2:  // pred: ^bb1
    %51 = llvm.srem %49, %39  : i32
    %52 = llvm.sdiv %49, %39  : i32
    %53 = llvm.udiv %51, %38  : i32
    %54 = llvm.urem %51, %38  : i32
    %55 = llvm.mul %54, %37  : i32
    %56 = llvm.add %53, %55  : i32
    %57 = llvm.icmp "ult" %54, %40 : i32
    llvm.cond_br %57, ^bb3, ^bb10(%34 : f32)
  ^bb3:  // pred: ^bb2
    %58 = llvm.mul %52, %37  : i32
    %59 = llvm.add %53, %58  : i32
    %60 = llvm.mul %59, %36  : i32
    llvm.br ^bb4(%41, %34 : i32, f32)
  ^bb4(%61: i32, %62: f32):  // 2 preds: ^bb3, ^bb9
    %63 = llvm.icmp "slt" %61, %36 : i32
    llvm.cond_br %63, ^bb5, ^bb10(%62 : f32)
  ^bb5:  // pred: ^bb4
    %64 = llvm.add %61, %60  : i32
    %65 = llvm.icmp "slt" %64, %arg4 : i32
    llvm.cond_br %65, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %66 = llvm.urem %64, %35  : i32
    %67 = llvm.udiv %64, %35  : i32
    %68 = llvm.select %46, %67, %41 : i1, i32
    %69 = llvm.mul %68, %35  : i32
    %70 = llvm.add %69, %66  : i32
    %71 = llvm.mul %arg1, %35  : i32
    %72 = llvm.mlir.undef : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)>
    %73 = llvm.extractvalue %7[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %74 = llvm.extractvalue %7[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %75 = llvm.insertvalue %73, %72[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %76 = llvm.insertvalue %74, %75[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %77 = llvm.insertvalue %41, %76[2] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %78 = llvm.insertvalue %71, %77[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %79 = llvm.insertvalue %40, %78[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %80 = llvm.extractvalue %79[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %81 = llvm.getelementptr %80[%70] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    %82 = llvm.load %81 : !llvm.ptr<1> -> f32
    %83 = llvm.select %47, %67, %41 : i1, i32
    %84 = llvm.mul %83, %35  : i32
    %85 = llvm.add %84, %66  : i32
    %86 = llvm.mul %arg0, %35  : i32
    %87 = llvm.mlir.undef : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)>
    %88 = llvm.extractvalue %15[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %89 = llvm.extractvalue %15[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<2 x i32>, array<2 x i32>)> 
    %90 = llvm.insertvalue %88, %87[0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %91 = llvm.insertvalue %89, %90[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %92 = llvm.insertvalue %41, %91[2] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %93 = llvm.insertvalue %86, %92[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %94 = llvm.insertvalue %40, %93[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %95 = llvm.extractvalue %94[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %96 = llvm.getelementptr %95[%85] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    %97 = llvm.load %96 : !llvm.ptr<1> -> f32
    %98 = llvm.fmul %82, %97  : f32
    %99 = llvm.fadd %62, %98  : f32
    llvm.br ^bb8(%99 : f32)
  ^bb7:  // pred: ^bb5
    llvm.br ^bb8(%62 : f32)
  ^bb8(%100: f32):  // 2 preds: ^bb6, ^bb7
    llvm.br ^bb9
  ^bb9:  // pred: ^bb8
    %101 = llvm.add %61, %40  : i32
    llvm.br ^bb4(%101, %100 : i32, f32)
  ^bb10(%102: f32):  // 2 preds: ^bb2, ^bb4
    llvm.br ^bb11(%102 : f32)
  ^bb11(%103: f32):  // pred: ^bb10
    llvm.br ^bb12
  ^bb12:  // pred: ^bb11
    %104 = llvm.mlir.undef : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)>
    %105 = llvm.extractvalue %32[0] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %106 = llvm.extractvalue %32[1] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %107 = llvm.insertvalue %105, %104[0] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %108 = llvm.insertvalue %106, %107[1] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %109 = llvm.mlir.constant(0 : index) : i32
    %110 = llvm.insertvalue %109, %108[2] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %111 = llvm.mlir.constant(256 : index) : i32
    %112 = llvm.insertvalue %111, %110[3, 0] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %113 = llvm.mlir.constant(1 : index) : i32
    %114 = llvm.insertvalue %113, %112[4, 0] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %115 = llvm.extractvalue %114[1] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %116 = llvm.getelementptr %115[%56] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    llvm.store %103, %116 : f32, !llvm.ptr<3>
    nvvm.barrier0
    %117 = llvm.icmp "slt" %53, %35 : i32
    llvm.cond_br %117, ^bb13, ^bb14
  ^bb13:  // pred: ^bb12
    %118 = llvm.add %56, %35  : i32
    %119 = llvm.extractvalue %114[1] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %120 = llvm.getelementptr %119[%56] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %121 = llvm.load %120 : !llvm.ptr<3> -> f32
    %122 = llvm.extractvalue %114[1] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %123 = llvm.getelementptr %122[%118] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %124 = llvm.load %123 : !llvm.ptr<3> -> f32
    %125 = llvm.fadd %121, %124  : f32
    %126 = llvm.extractvalue %114[1] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %127 = llvm.getelementptr %126[%56] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    llvm.store %125, %127 : f32, !llvm.ptr<3>
    llvm.br ^bb14
  ^bb14:  // 2 preds: ^bb12, ^bb13
    nvvm.barrier0
    %128 = llvm.icmp "slt" %53, %33 : i32
    llvm.cond_br %128, ^bb15, ^bb16
  ^bb15:  // pred: ^bb14
    %129 = llvm.add %56, %33  : i32
    %130 = llvm.extractvalue %114[1] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %131 = llvm.getelementptr %130[%56] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %132 = llvm.load %131 : !llvm.ptr<3> -> f32
    %133 = llvm.extractvalue %114[1] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %134 = llvm.getelementptr %133[%129] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %135 = llvm.load %134 : !llvm.ptr<3> -> f32
    %136 = llvm.fadd %132, %135  : f32
    %137 = llvm.extractvalue %114[1] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %138 = llvm.getelementptr %137[%56] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    llvm.store %136, %138 : f32, !llvm.ptr<3>
    llvm.br ^bb16
  ^bb16:  // 2 preds: ^bb14, ^bb15
    nvvm.barrier0
    %139 = llvm.icmp "slt" %53, %40 : i32
    llvm.cond_br %139, ^bb17, ^bb18
  ^bb17:  // pred: ^bb16
    %140 = llvm.add %56, %40  : i32
    %141 = llvm.extractvalue %114[1] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %142 = llvm.getelementptr %141[%56] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %143 = llvm.load %142 : !llvm.ptr<3> -> f32
    %144 = llvm.extractvalue %114[1] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %145 = llvm.getelementptr %144[%140] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %146 = llvm.load %145 : !llvm.ptr<3> -> f32
    %147 = llvm.fadd %143, %146  : f32
    %148 = llvm.extractvalue %114[1] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %149 = llvm.getelementptr %148[%56] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    llvm.store %147, %149 : f32, !llvm.ptr<3>
    llvm.br ^bb18
  ^bb18:  // 2 preds: ^bb16, ^bb17
    nvvm.barrier0
    %150 = llvm.icmp "eq" %53, %41 : i32
    %151 = llvm.and %150, %57  : i1
    llvm.cond_br %151, ^bb19, ^bb20
  ^bb19:  // pred: ^bb18
    %152 = llvm.extractvalue %114[1] : !llvm.struct<(ptr<3>, ptr<3>, i32, array<1 x i32>, array<1 x i32>)> 
    %153 = llvm.getelementptr %152[%56] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %154 = llvm.load %153 : !llvm.ptr<3> -> f32
    %155 = llvm.extractvalue %21[1] : !llvm.struct<(ptr<1>, ptr<1>, i32, array<1 x i32>, array<1 x i32>)> 
    %156 = llvm.getelementptr %155[%54] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    %157 = llvm.atomicrmw fadd %156, %154 acq_rel : !llvm.ptr<1>, f32
    llvm.br ^bb20
  ^bb20:  // 2 preds: ^bb18, ^bb19
    llvm.br ^bb21
  ^bb21:  // 2 preds: ^bb1, ^bb20
    llvm.return
  }
}

// -----// IR Dump After LLVMInsertValueSimplifierPass (disc-llvm-insert-value-simplifier) //----- //
llvm.func @main_kColReduction_reduce__6_1_0___block_tile_h64_1(%arg0: i32, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: !llvm.ptr<1>, %arg6: !llvm.ptr<1>, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: !llvm.ptr<1>, %arg13: !llvm.ptr<1>, %arg14: i32, %arg15: i32, %arg16: i32, %arg17: i32, %arg18: i32, %arg19: !llvm.ptr<1>, %arg20: !llvm.ptr<1>, %arg21: i32, %arg22: i32, %arg23: i32) attributes {gpu.kernel, nvvm.kernel} {
  %0 = llvm.mlir.constant(32 : index) : i32
  %1 = llvm.mlir.constant(8 : index) : i32
  %2 = llvm.mlir.constant(64 : index) : i32
  %3 = llvm.mlir.constant(4 : index) : i32
  %4 = llvm.mlir.constant(0.000000e+00 : f32) : f32
  %5 = llvm.mlir.constant(2 : index) : i32
  %6 = llvm.mlir.constant(1 : index) : i32
  %7 = llvm.mlir.constant(256 : index) : i32
  %8 = llvm.mlir.constant(0 : index) : i32
  %9 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__6_1_0___block_tile_h64_1_0 : !llvm.ptr<3>
  %10 = llvm.getelementptr %9[0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<256 x f32>
  %11 = nvvm.read.ptx.sreg.ctaid.x : i32
  %12 = nvvm.read.ptx.sreg.tid.x : i32
  llvm.br ^bb1
^bb1:  // pred: ^bb0
  %13 = llvm.icmp "eq" %arg0, %6 : i32
  %14 = llvm.select %13, %arg1, %arg0 : i1, i32
  %15 = llvm.icmp "eq" %arg1, %14 : i32
  %16 = llvm.icmp "eq" %arg0, %14 : i32
  %17 = llvm.mul %11, %arg2  : i32
  %18 = llvm.add %12, %17  : i32
  %19 = llvm.icmp "ult" %18, %arg3 : i32
  llvm.cond_br %19, ^bb2, ^bb21
^bb2:  // pred: ^bb1
  %20 = llvm.srem %18, %7  : i32
  %21 = llvm.sdiv %18, %7  : i32
  %22 = llvm.udiv %20, %0  : i32
  %23 = llvm.urem %20, %0  : i32
  %24 = llvm.mul %23, %1  : i32
  %25 = llvm.add %22, %24  : i32
  %26 = llvm.icmp "ult" %23, %6 : i32
  llvm.cond_br %26, ^bb3, ^bb10(%4 : f32)
^bb3:  // pred: ^bb2
  %27 = llvm.mul %21, %1  : i32
  %28 = llvm.add %22, %27  : i32
  %29 = llvm.mul %28, %2  : i32
  llvm.br ^bb4(%8, %4 : i32, f32)
^bb4(%30: i32, %31: f32):  // 2 preds: ^bb3, ^bb9
  %32 = llvm.icmp "slt" %30, %2 : i32
  llvm.cond_br %32, ^bb5, ^bb10(%31 : f32)
^bb5:  // pred: ^bb4
  %33 = llvm.add %30, %29  : i32
  %34 = llvm.icmp "slt" %33, %arg4 : i32
  llvm.cond_br %34, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  %35 = llvm.urem %33, %3  : i32
  %36 = llvm.udiv %33, %3  : i32
  %37 = llvm.select %15, %36, %8 : i1, i32
  %38 = llvm.mul %37, %3  : i32
  %39 = llvm.add %38, %35  : i32
  %40 = llvm.getelementptr %arg6[%39] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
  %41 = llvm.load %40 : !llvm.ptr<1> -> f32
  %42 = llvm.select %16, %36, %8 : i1, i32
  %43 = llvm.mul %42, %3  : i32
  %44 = llvm.add %43, %35  : i32
  %45 = llvm.getelementptr %arg13[%44] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
  %46 = llvm.load %45 : !llvm.ptr<1> -> f32
  %47 = llvm.fmul %41, %46  : f32
  %48 = llvm.fadd %31, %47  : f32
  llvm.br ^bb8(%48 : f32)
^bb7:  // pred: ^bb5
  llvm.br ^bb8(%31 : f32)
^bb8(%49: f32):  // 2 preds: ^bb6, ^bb7
  llvm.br ^bb9
^bb9:  // pred: ^bb8
  %50 = llvm.add %30, %6  : i32
  llvm.br ^bb4(%50, %49 : i32, f32)
^bb10(%51: f32):  // 2 preds: ^bb2, ^bb4
  llvm.br ^bb11(%51 : f32)
^bb11(%52: f32):  // pred: ^bb10
  llvm.br ^bb12
^bb12:  // pred: ^bb11
  %53 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
  llvm.store %52, %53 : f32, !llvm.ptr<3>
  nvvm.barrier0
  %54 = llvm.icmp "slt" %22, %3 : i32
  llvm.cond_br %54, ^bb13, ^bb14
^bb13:  // pred: ^bb12
  %55 = llvm.add %25, %3  : i32
  %56 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
  %57 = llvm.load %56 : !llvm.ptr<3> -> f32
  %58 = llvm.getelementptr %10[%55] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
  %59 = llvm.load %58 : !llvm.ptr<3> -> f32
  %60 = llvm.fadd %57, %59  : f32
  %61 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
  llvm.store %60, %61 : f32, !llvm.ptr<3>
  llvm.br ^bb14
^bb14:  // 2 preds: ^bb12, ^bb13
  nvvm.barrier0
  %62 = llvm.icmp "slt" %22, %5 : i32
  llvm.cond_br %62, ^bb15, ^bb16
^bb15:  // pred: ^bb14
  %63 = llvm.add %25, %5  : i32
  %64 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
  %65 = llvm.load %64 : !llvm.ptr<3> -> f32
  %66 = llvm.getelementptr %10[%63] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
  %67 = llvm.load %66 : !llvm.ptr<3> -> f32
  %68 = llvm.fadd %65, %67  : f32
  %69 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
  llvm.store %68, %69 : f32, !llvm.ptr<3>
  llvm.br ^bb16
^bb16:  // 2 preds: ^bb14, ^bb15
  nvvm.barrier0
  %70 = llvm.icmp "slt" %22, %6 : i32
  llvm.cond_br %70, ^bb17, ^bb18
^bb17:  // pred: ^bb16
  %71 = llvm.add %25, %6  : i32
  %72 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
  %73 = llvm.load %72 : !llvm.ptr<3> -> f32
  %74 = llvm.getelementptr %10[%71] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
  %75 = llvm.load %74 : !llvm.ptr<3> -> f32
  %76 = llvm.fadd %73, %75  : f32
  %77 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
  llvm.store %76, %77 : f32, !llvm.ptr<3>
  llvm.br ^bb18
^bb18:  // 2 preds: ^bb16, ^bb17
  nvvm.barrier0
  %78 = llvm.icmp "eq" %22, %8 : i32
  %79 = llvm.and %78, %26  : i1
  llvm.cond_br %79, ^bb19, ^bb20
^bb19:  // pred: ^bb18
  %80 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
  %81 = llvm.load %80 : !llvm.ptr<3> -> f32
  %82 = llvm.getelementptr %arg20[%23] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
  %83 = llvm.atomicrmw fadd %82, %81 acq_rel : !llvm.ptr<1>, f32
  llvm.br ^bb20
^bb20:  // 2 preds: ^bb18, ^bb19
  llvm.br ^bb21
^bb21:  // 2 preds: ^bb1, ^bb20
  llvm.return
}

// -----// IR Dump After FunctionDeadArgumentEliminationPass (disc-function-dead-argument-elimination) //----- //
gpu.module @main_kernel_6 {
  llvm.mlir.global internal @__wg_main_kColReduction_reduce__6_1_0___block_tile_h64_1_0() {addr_space = 3 : i32} : !llvm.array<256 x f32>
  llvm.func @main_kColReduction_reduce__6_1_0___block_tile_h64_1(%arg0: i32, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: !llvm.ptr<1>, %arg6: !llvm.ptr<1>, %arg7: !llvm.ptr<1>) attributes {disc.elimargs = [5 : index, 7 : index, 8 : index, 9 : index, 10 : index, 11 : index, 12 : index, 14 : index, 15 : index, 16 : index, 17 : index, 18 : index, 19 : index, 21 : index, 22 : index, 23 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(32 : index) : i32
    %1 = llvm.mlir.constant(8 : index) : i32
    %2 = llvm.mlir.constant(64 : index) : i32
    %3 = llvm.mlir.constant(4 : index) : i32
    %4 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    %5 = llvm.mlir.constant(2 : index) : i32
    %6 = llvm.mlir.constant(1 : index) : i32
    %7 = llvm.mlir.constant(256 : index) : i32
    %8 = llvm.mlir.constant(0 : index) : i32
    %9 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__6_1_0___block_tile_h64_1_0 : !llvm.ptr<3>
    %10 = llvm.getelementptr %9[0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<256 x f32>
    %11 = nvvm.read.ptx.sreg.ctaid.x : i32
    %12 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %13 = llvm.icmp "eq" %arg0, %6 : i32
    %14 = llvm.select %13, %arg1, %arg0 : i1, i32
    %15 = llvm.icmp "eq" %arg1, %14 : i32
    %16 = llvm.icmp "eq" %arg0, %14 : i32
    %17 = llvm.mul %11, %arg2  : i32
    %18 = llvm.add %12, %17  : i32
    %19 = llvm.icmp "ult" %18, %arg3 : i32
    llvm.cond_br %19, ^bb2, ^bb21
  ^bb2:  // pred: ^bb1
    %20 = llvm.srem %18, %7  : i32
    %21 = llvm.sdiv %18, %7  : i32
    %22 = llvm.udiv %20, %0  : i32
    %23 = llvm.urem %20, %0  : i32
    %24 = llvm.mul %23, %1  : i32
    %25 = llvm.add %22, %24  : i32
    %26 = llvm.icmp "ult" %23, %6 : i32
    llvm.cond_br %26, ^bb3, ^bb10(%4 : f32)
  ^bb3:  // pred: ^bb2
    %27 = llvm.mul %21, %1  : i32
    %28 = llvm.add %22, %27  : i32
    %29 = llvm.mul %28, %2  : i32
    llvm.br ^bb4(%8, %4 : i32, f32)
  ^bb4(%30: i32, %31: f32):  // 2 preds: ^bb3, ^bb9
    %32 = llvm.icmp "slt" %30, %2 : i32
    llvm.cond_br %32, ^bb5, ^bb10(%31 : f32)
  ^bb5:  // pred: ^bb4
    %33 = llvm.add %30, %29  : i32
    %34 = llvm.icmp "slt" %33, %arg4 : i32
    llvm.cond_br %34, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %35 = llvm.urem %33, %3  : i32
    %36 = llvm.udiv %33, %3  : i32
    %37 = llvm.select %15, %36, %8 : i1, i32
    %38 = llvm.mul %37, %3  : i32
    %39 = llvm.add %38, %35  : i32
    %40 = llvm.getelementptr %arg5[%39] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    %41 = llvm.load %40 : !llvm.ptr<1> -> f32
    %42 = llvm.select %16, %36, %8 : i1, i32
    %43 = llvm.mul %42, %3  : i32
    %44 = llvm.add %43, %35  : i32
    %45 = llvm.getelementptr %arg6[%44] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    %46 = llvm.load %45 : !llvm.ptr<1> -> f32
    %47 = llvm.fmul %41, %46  : f32
    %48 = llvm.fadd %31, %47  : f32
    llvm.br ^bb8(%48 : f32)
  ^bb7:  // pred: ^bb5
    llvm.br ^bb8(%31 : f32)
  ^bb8(%49: f32):  // 2 preds: ^bb6, ^bb7
    llvm.br ^bb9
  ^bb9:  // pred: ^bb8
    %50 = llvm.add %30, %6  : i32
    llvm.br ^bb4(%50, %49 : i32, f32)
  ^bb10(%51: f32):  // 2 preds: ^bb2, ^bb4
    llvm.br ^bb11(%51 : f32)
  ^bb11(%52: f32):  // pred: ^bb10
    llvm.br ^bb12
  ^bb12:  // pred: ^bb11
    %53 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    llvm.store %52, %53 : f32, !llvm.ptr<3>
    nvvm.barrier0
    %54 = llvm.icmp "slt" %22, %3 : i32
    llvm.cond_br %54, ^bb13, ^bb14
  ^bb13:  // pred: ^bb12
    %55 = llvm.add %25, %3  : i32
    %56 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %57 = llvm.load %56 : !llvm.ptr<3> -> f32
    %58 = llvm.getelementptr %10[%55] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %59 = llvm.load %58 : !llvm.ptr<3> -> f32
    %60 = llvm.fadd %57, %59  : f32
    %61 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    llvm.store %60, %61 : f32, !llvm.ptr<3>
    llvm.br ^bb14
  ^bb14:  // 2 preds: ^bb12, ^bb13
    nvvm.barrier0
    %62 = llvm.icmp "slt" %22, %5 : i32
    llvm.cond_br %62, ^bb15, ^bb16
  ^bb15:  // pred: ^bb14
    %63 = llvm.add %25, %5  : i32
    %64 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %65 = llvm.load %64 : !llvm.ptr<3> -> f32
    %66 = llvm.getelementptr %10[%63] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %67 = llvm.load %66 : !llvm.ptr<3> -> f32
    %68 = llvm.fadd %65, %67  : f32
    %69 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    llvm.store %68, %69 : f32, !llvm.ptr<3>
    llvm.br ^bb16
  ^bb16:  // 2 preds: ^bb14, ^bb15
    nvvm.barrier0
    %70 = llvm.icmp "slt" %22, %6 : i32
    llvm.cond_br %70, ^bb17, ^bb18
  ^bb17:  // pred: ^bb16
    %71 = llvm.add %25, %6  : i32
    %72 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %73 = llvm.load %72 : !llvm.ptr<3> -> f32
    %74 = llvm.getelementptr %10[%71] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %75 = llvm.load %74 : !llvm.ptr<3> -> f32
    %76 = llvm.fadd %73, %75  : f32
    %77 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    llvm.store %76, %77 : f32, !llvm.ptr<3>
    llvm.br ^bb18
  ^bb18:  // 2 preds: ^bb16, ^bb17
    nvvm.barrier0
    %78 = llvm.icmp "eq" %22, %8 : i32
    %79 = llvm.and %78, %26  : i1
    llvm.cond_br %79, ^bb19, ^bb20
  ^bb19:  // pred: ^bb18
    %80 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %81 = llvm.load %80 : !llvm.ptr<3> -> f32
    %82 = llvm.getelementptr %arg7[%23] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    %83 = llvm.atomicrmw fadd %82, %81 acq_rel : !llvm.ptr<1>, f32
    llvm.br ^bb20
  ^bb20:  // 2 preds: ^bb18, ^bb19
    llvm.br ^bb21
  ^bb21:  // 2 preds: ^bb1, ^bb20
    llvm.return
  }
}

// -----// IR Dump After GpuKernelToBlobPass (disc-gpu-kernel-to-blob) //----- //
gpu.module @main_kernel_6 attributes {gpu.binary = "P\EDU\BA\01\00\10\00\D8\07\00\00\00\00\00\00\02\00\01\01@\00\00\00\98\07\00\00\00\00\00\00\93\07\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\A8\12\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00g\02\00\BE\00r\00\01\00\22\12\00\01\00\11\0F\06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\0Ce__6_1_0___block_tile_h64_1C\00\0F=\00&oshared?\00&\9Fconstant0B\00#\FA\01debug_frame\00.rel\11\00!nv\14\00\11aN\00\0FW\01 \0F\93\00 \0F\8B\01\A4\8F$____wg_<\00 \00\15\00/28\CD\010o_param\D4\01\1C\0F\01\00\09\8Cf\00\00\00\03\00\0A\00\01\00\11\DD\18\00,\0B\00\01\00 ^\01\18\00,\09\00\01\00\11\A7\18\00,\04\00\01\00\11\C5\18\00,\07\00\01\00\102\E3\03\17\10x\00\03#\00f\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0\06\07\00 \00\04\9B\00R\04\14\00\00\00E\002\04p\01\18\000/\08\00#\00\10\19\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\A8\04\F3\08\015\00\00\04\0A\08\00\03\00\00\00`\010\00\03\190\00\04\17\0C\CB\00U(\00\00\F0!\10\009\06\00 \10\009\05\00\18\10\00u\04\00\10\00\00\F0\11\10\009\03\00\0C\10\009\02\00\08\10\009\01\00\04\10\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\00\D0\05\00\00 \06\00\00\04\1E\94\01#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\11\02h\01\0F\01\00\FF\98@$v\01\FFw\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%s\02a\0E\00\19y\03\00\01\00\10!-\00\F0\04\0E\00$z\00\00\00Z\00\00\03\02\8E\07\00\CA\1F\00\0C\10\00\C5[\00\00p`\F0\03\00\DA\0F\00M\93\04\B0\80\03\00\EA\0F\00\19x\03\FF\1F\0F\00\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\00\EB\04Q\E2\0F\00Eyy\03\020\00a\E4\0F\00\11r\04q\00\C1\FF@\8F\07\00\C8\0F\00\12x\03\04Y\04\10\C0p\00@\0F\00$x\BC\02`\00\00\03\0A\8E\070\00c$r\03\FF\FF\00\C0\00\10\C60\00 \02\00p\00\22\FF\C0 \00P\19x\05\FF\05Q\000\16\01\00\10\00@\0Cr\00\020\001pR\F2p\00P\0Cx\00\00\7F\10\000@\F0\03\90\00@$x\02\02\D9\02\10\05\E0\00\12\C8\10\00\14\04`\00\96\CC\0F\00G\19\00\00@\03\E0\00c$v\03\FF\00X \00\10\E2p\001\04\FF\08\E3\04A\01\00\00\C8`\00$\03\01p\00u\E2\0F\04$x\04\04`\00\01\C0\00\16\05\C0\00\91\E2\0F\00\07z\03\03\00Y`\00\02@\00Uz\00\03\00X\B0\00\11\04\10\00\10Y\10\00\12\F4\B0\00\08\00\01\10\C4\B0\004\08\04@`\00\11\CA@\00\81\08\00\\\00\00pb\FA@\00@\10x\0A\08\90\00B\FF\E0\FF\07\10\006\10\08\02\10\00\000\00\12\0A0\00#\FC\03\10\00\12\10\10\00\11\F8\10\00T\10x\13\08\030\00\91\C6\0F\00\07\D2\06\08\FF\00\B0\00\11\04\E0\00H\D4\09\FF\04\D0\00#\D2\08 \00#\00\05P\00\12\13P\00\11\F6\C0\00\A3%\D6\06\06\00`\00\00\09\020\00S\E8\0C\0A\01\000\00\10\C6 \00H\08\08\00^ \006\0A\0A\01p\00P\00\81\D9\06\06p\00\BA\00\19\1E\0C\00\A4\00\00$\E4\0B\80\00S\C8\0E\10\02\00P\00u\E2\0F\04\81\D9\12\080\00\87\A2\02\00\07\C8\10\10\02P\00@%\E6\0C\0Cp\00\14\0Bp\001\B8\11\13\EF\01\03\90\00:$\C4\15`\00E\B8\07\13\03@\00\86\1F\00%\E6\0A\0A\00`@\00E\81\E9\14\0Cp\00p\E6\00\00%\C6\0E\0E`\001\15\02\8Ep\01E\81\E9\0A\0A \00\87\E4\0E\00$\B4\16\FF\04\D0\01c%\C6\08\10\00`0\00u\E2/\00\81\C9\0E\0E0\00p&\0F\00%\B6\10\11P\00\12\16P\00F\08\81\C9\08\E0\00\10$ \00D\0C\07\00` \00e\1F\00\81\B9\10\10 \00fh\0F\00\81\B9\0C\A0\00\10b\E0\014\05\05\04\E0\01\81\E2\0F\00#\D2\03\12\06\0C\07P\00\00\00\C6O\D0\02\10\05`\021pR\FA\C0\01T#\E2\03\14\0A \00\85\C8\8F\00#\C2\03\0E\08\10\00v\0F\01#\B2\03\10\0C\10\00p\02GY\00\00P\FD\D1\03\11\83@\03\14Ap\04\03P\03A\88s\00\02,\00\00\96\06f\E8\0F\00\1D{\00\01\00\84\EC\0F\00\84\89\04\02\00 \00\12\E2\C0\03\10?\90\000@\F2\03\A0\01c\84\89\05\02\00\10 \00\93$\0E\00!\82\05\04\05\00\01\00\8F\CA\1F\00\88\83\00\02\05`\00\09\1E\99`\00\14\1F \04\00`\00!\99\07\07\08\05`\00H\92\07\04\07`\00O\93\00\02\07\C0\00\0A\1A\03`\00\10r\EC\01\03\E0\03\13\C6\E0\00\18\04\C0\00J\03\03\04\00\C0\00\0F \01\099M\19\00P\01'\84yp\00\96\22\0E\00$v\04\FF\00b`\02c$v\05\FF\00c\10\00a\CA\0F\00\8Ey\00\81\05@\84\E7\10\0C\D0\02\1BM\A0\01cGy\00\00\F0\FF\C0\01f\C0\0F\00\18y\00\01\00\0F\10\00\A0\0F\01\00-\14\01\DC\02\0B\01\00\22@\00\01\00=W\01\000\00\08\01\00\1F\0B@\00\04\13\97)\00\1F\D4@\00\0C\13\13t\09\0C\01\00\13pU\00*\A8\00\98\09\22\08\00\01\00\22\18\00\01\00.*\01T\00\00\01\00\13\18u\02/p\00\80\00\0B\1F)'\00\03#\00\88@\00\04\10\0C\04\E4\00*\04\00\01\00\1Fl@\00\04\13\B8)\00&\B8\00@\00\1F\0A@\00\00!H\01D\01\0D@\00\13p\F5\03*\D8\00\01\00\1B\08\08\00?7\01\00F\0D\00Q\00\00H\06\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\E8\14\01\0C\84\01\13X@\00\17\901\01\0F\C0\00\01\132@\00\15\06R\00\0B\A1\00\16\07\80\00j\06\00\00\19\80\00\01\00\13\A9+\00+\03\00\01\00\04\DB\02\1F\04\80\00\0B\11\06\D9\06\07\E8\11\0B\01\00\1B\A8\08\00\04\97\00\13\018\00\04\E8\00\0D8\00\1A\08\08\00\088\00\18\06\A0\00\0F\01\00\05\03\A9\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00"} {
  llvm.mlir.global internal @__wg_main_kColReduction_reduce__6_1_0___block_tile_h64_1_0() {addr_space = 3 : i32} : !llvm.array<256 x f32>
  llvm.func @main_kColReduction_reduce__6_1_0___block_tile_h64_1(%arg0: i32, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: !llvm.ptr<1>, %arg6: !llvm.ptr<1>, %arg7: !llvm.ptr<1>) attributes {disc.elimargs = [5 : index, 7 : index, 8 : index, 9 : index, 10 : index, 11 : index, 12 : index, 14 : index, 15 : index, 16 : index, 17 : index, 18 : index, 19 : index, 21 : index, 22 : index, 23 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(32 : index) : i32
    %1 = llvm.mlir.constant(8 : index) : i32
    %2 = llvm.mlir.constant(64 : index) : i32
    %3 = llvm.mlir.constant(4 : index) : i32
    %4 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    %5 = llvm.mlir.constant(2 : index) : i32
    %6 = llvm.mlir.constant(1 : index) : i32
    %7 = llvm.mlir.constant(256 : index) : i32
    %8 = llvm.mlir.constant(0 : index) : i32
    %9 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__6_1_0___block_tile_h64_1_0 : !llvm.ptr<3>
    %10 = llvm.getelementptr %9[0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<256 x f32>
    %11 = nvvm.read.ptx.sreg.ctaid.x : i32
    %12 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %13 = llvm.icmp "eq" %arg0, %6 : i32
    %14 = llvm.select %13, %arg1, %arg0 : i1, i32
    %15 = llvm.icmp "eq" %arg1, %14 : i32
    %16 = llvm.icmp "eq" %arg0, %14 : i32
    %17 = llvm.mul %11, %arg2  : i32
    %18 = llvm.add %12, %17  : i32
    %19 = llvm.icmp "ult" %18, %arg3 : i32
    llvm.cond_br %19, ^bb2, ^bb21
  ^bb2:  // pred: ^bb1
    %20 = llvm.srem %18, %7  : i32
    %21 = llvm.sdiv %18, %7  : i32
    %22 = llvm.udiv %20, %0  : i32
    %23 = llvm.urem %20, %0  : i32
    %24 = llvm.mul %23, %1  : i32
    %25 = llvm.add %22, %24  : i32
    %26 = llvm.icmp "ult" %23, %6 : i32
    llvm.cond_br %26, ^bb3, ^bb10(%4 : f32)
  ^bb3:  // pred: ^bb2
    %27 = llvm.mul %21, %1  : i32
    %28 = llvm.add %22, %27  : i32
    %29 = llvm.mul %28, %2  : i32
    llvm.br ^bb4(%8, %4 : i32, f32)
  ^bb4(%30: i32, %31: f32):  // 2 preds: ^bb3, ^bb9
    %32 = llvm.icmp "slt" %30, %2 : i32
    llvm.cond_br %32, ^bb5, ^bb10(%31 : f32)
  ^bb5:  // pred: ^bb4
    %33 = llvm.add %30, %29  : i32
    %34 = llvm.icmp "slt" %33, %arg4 : i32
    llvm.cond_br %34, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %35 = llvm.urem %33, %3  : i32
    %36 = llvm.udiv %33, %3  : i32
    %37 = llvm.select %15, %36, %8 : i1, i32
    %38 = llvm.mul %37, %3  : i32
    %39 = llvm.add %38, %35  : i32
    %40 = llvm.getelementptr %arg5[%39] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    %41 = llvm.load %40 : !llvm.ptr<1> -> f32
    %42 = llvm.select %16, %36, %8 : i1, i32
    %43 = llvm.mul %42, %3  : i32
    %44 = llvm.add %43, %35  : i32
    %45 = llvm.getelementptr %arg6[%44] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    %46 = llvm.load %45 : !llvm.ptr<1> -> f32
    %47 = llvm.fmul %41, %46  : f32
    %48 = llvm.fadd %31, %47  : f32
    llvm.br ^bb8(%48 : f32)
  ^bb7:  // pred: ^bb5
    llvm.br ^bb8(%31 : f32)
  ^bb8(%49: f32):  // 2 preds: ^bb6, ^bb7
    llvm.br ^bb9
  ^bb9:  // pred: ^bb8
    %50 = llvm.add %30, %6  : i32
    llvm.br ^bb4(%50, %49 : i32, f32)
  ^bb10(%51: f32):  // 2 preds: ^bb2, ^bb4
    llvm.br ^bb11(%51 : f32)
  ^bb11(%52: f32):  // pred: ^bb10
    llvm.br ^bb12
  ^bb12:  // pred: ^bb11
    %53 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    llvm.store %52, %53 : f32, !llvm.ptr<3>
    nvvm.barrier0
    %54 = llvm.icmp "slt" %22, %3 : i32
    llvm.cond_br %54, ^bb13, ^bb14
  ^bb13:  // pred: ^bb12
    %55 = llvm.add %25, %3  : i32
    %56 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %57 = llvm.load %56 : !llvm.ptr<3> -> f32
    %58 = llvm.getelementptr %10[%55] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %59 = llvm.load %58 : !llvm.ptr<3> -> f32
    %60 = llvm.fadd %57, %59  : f32
    %61 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    llvm.store %60, %61 : f32, !llvm.ptr<3>
    llvm.br ^bb14
  ^bb14:  // 2 preds: ^bb12, ^bb13
    nvvm.barrier0
    %62 = llvm.icmp "slt" %22, %5 : i32
    llvm.cond_br %62, ^bb15, ^bb16
  ^bb15:  // pred: ^bb14
    %63 = llvm.add %25, %5  : i32
    %64 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %65 = llvm.load %64 : !llvm.ptr<3> -> f32
    %66 = llvm.getelementptr %10[%63] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %67 = llvm.load %66 : !llvm.ptr<3> -> f32
    %68 = llvm.fadd %65, %67  : f32
    %69 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    llvm.store %68, %69 : f32, !llvm.ptr<3>
    llvm.br ^bb16
  ^bb16:  // 2 preds: ^bb14, ^bb15
    nvvm.barrier0
    %70 = llvm.icmp "slt" %22, %6 : i32
    llvm.cond_br %70, ^bb17, ^bb18
  ^bb17:  // pred: ^bb16
    %71 = llvm.add %25, %6  : i32
    %72 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %73 = llvm.load %72 : !llvm.ptr<3> -> f32
    %74 = llvm.getelementptr %10[%71] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %75 = llvm.load %74 : !llvm.ptr<3> -> f32
    %76 = llvm.fadd %73, %75  : f32
    %77 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    llvm.store %76, %77 : f32, !llvm.ptr<3>
    llvm.br ^bb18
  ^bb18:  // 2 preds: ^bb16, ^bb17
    nvvm.barrier0
    %78 = llvm.icmp "eq" %22, %8 : i32
    %79 = llvm.and %78, %26  : i1
    llvm.cond_br %79, ^bb19, ^bb20
  ^bb19:  // pred: ^bb18
    %80 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
    %81 = llvm.load %80 : !llvm.ptr<3> -> f32
    %82 = llvm.getelementptr %arg7[%23] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
    %83 = llvm.atomicrmw fadd %82, %81 acq_rel : !llvm.ptr<1>, f32
    llvm.br ^bb20
  ^bb20:  // 2 preds: ^bb18, ^bb19
    llvm.br ^bb21
  ^bb21:  // 2 preds: ^bb1, ^bb20
    llvm.return
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
  %c256 = arith.constant 256 : index
  %c32 = arith.constant 32 : index
  %c512 = arith.constant 512 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c4_i32 = arith.constant 4 : i32
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
  %2 = "disc_ral.dispatch"(%arg0, %c1) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
  %dim = memref.dim %2, %c0 : memref<?x4xf32>
  %dim_0 = memref.dim %1, %c0 : memref<?x4xf32>
  %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %reinterpret_cast_1 = memref.reinterpret_cast %2 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %3 = arith.cmpi eq, %dim, %c1 : index
  %4 = arith.select %3, %dim_0, %dim : index
  %alloc = memref.alloc(%dim_0) {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %5 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast, %alloc) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  %alloc_2 = memref.alloc(%dim) {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_1, %alloc_2) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  %6 = arith.index_cast %4 : index to i32
  %7 = arith.muli %6, %c4_i32 : i32
  %8 = arith.index_cast %7 : i32 to index
  %9 = arith.cmpi eq, %dim_0, %4 : index
  %10 = arith.cmpi eq, %dim, %4 : index
  %11 = arith.andi %10, %9 : i1
  %alloc_3 = memref.alloc() : memref<1xf32, #gpu.address_space<global>>
  scf.if %11 {
    %13 = arith.cmpi slt, %8, %c1 : index
    scf.if %13 {
      gpu.launch_func  @main_kernel::@main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32 blocks in (%c1, %c1, %c1) threads in (%c512, %c1, %c1) args(%c512 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
      %14 = arith.cmpi eq, %8, %c0 : index
      %15 = arith.subi %8, %c1 : index
      %16 = arith.divui %15, %c32 : index
      %17 = arith.addi %16, %c1 : index
      %18 = arith.select %14, %c0, %17 : index
      %19 = arith.muli %18, %c512 : index
      %20 = affine.apply affine_map<(d0) -> (d0 ceildiv 512)>(%19)
      gpu.launch_func  @main_kernel_0::@main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_1 blocks in (%20, %c1, %c1) threads in (%c512, %c1, %c1) args(%c512 : index, %19 : index, %8 : index, %4 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    } else {
      gpu.launch_func  @main_kernel_1::@main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64 blocks in (%c1, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
      %14 = arith.cmpi eq, %8, %c0 : index
      %15 = arith.subi %8, %c1 : index
      %16 = arith.divui %15, %c512 : index
      %17 = arith.addi %16, %c1 : index
      %18 = arith.select %14, %c0, %17 : index
      %19 = arith.muli %18, %c256 : index
      %20 = affine.apply affine_map<(d0) -> (d0 ceildiv 256)>(%19)
      gpu.launch_func  @main_kernel_2::@main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1 blocks in (%20, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %19 : index, %8 : index, %4 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    }
  } else {
    %13 = arith.cmpi slt, %8, %c1 : index
    scf.if %13 {
      gpu.launch_func  @main_kernel_3::@main_kColReduction_reduce__6_1_0___thread_tile_h32 blocks in (%c1, %c1, %c1) threads in (%c512, %c1, %c1) args(%c512 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
      %14 = arith.cmpi eq, %8, %c0 : index
      %15 = arith.subi %8, %c1 : index
      %16 = arith.divui %15, %c32 : index
      %17 = arith.addi %16, %c1 : index
      %18 = arith.select %14, %c0, %17 : index
      %19 = arith.muli %18, %c512 : index
      %20 = affine.apply affine_map<(d0) -> (d0 ceildiv 512)>(%19)
      gpu.launch_func  @main_kernel_4::@main_kColReduction_reduce__6_1_0___thread_tile_h32_1 blocks in (%20, %c1, %c1) threads in (%c512, %c1, %c1) args(%dim : index, %dim_0 : index, %c512 : index, %19 : index, %8 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    } else {
      gpu.launch_func  @main_kernel_5::@main_kColReduction_reduce__6_1_0___block_tile_h64 blocks in (%c1, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
      %14 = arith.cmpi eq, %8, %c0 : index
      %15 = arith.subi %8, %c1 : index
      %16 = arith.divui %15, %c512 : index
      %17 = arith.addi %16, %c1 : index
      %18 = arith.select %14, %c0, %17 : index
      %19 = arith.muli %18, %c256 : index
      %20 = affine.apply affine_map<(d0) -> (d0 ceildiv 256)>(%19)
      gpu.launch_func  @main_kernel_6::@main_kColReduction_reduce__6_1_0___block_tile_h64_1 blocks in (%20, %c1, %c1) threads in (%c256, %c1, %c1) args(%dim : index, %dim_0 : index, %c256 : index, %19 : index, %8 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    }
  }
  memref.dealloc %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc : memref<?x4xf32, #gpu.address_space<global>>
  %alloca = memref.alloca() : memref<0xindex>
  %12 = "disc_ral.dispatch"(%arg0, %5, %alloc_3, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1xf32, #gpu.address_space<global>>, memref<0xindex>) -> memref<f32, #gpu.address_space<global>>
  %reinterpret_cast_4 = memref.reinterpret_cast %12 to offset: [0], sizes: [], strides: [] : memref<f32, #gpu.address_space<global>> to memref<f32, #gpu.address_space<global>>
  memref.dealloc %alloc_3 : memref<1xf32, #gpu.address_space<global>>
  %alloc_5 = memref.alloc() : memref<f32>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_4, %alloc_5) {backend_config = "", call_target_name = "d2h", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<f32, #gpu.address_space<global>>, memref<f32>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  memref.dealloc %reinterpret_cast_4 : memref<f32, #gpu.address_space<global>>
  "disc_ral.dispatch"(%arg0, %c0, %alloc_5) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<f32>) -> ()
  return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
#map = affine_map<(d0) -> (d0 ceildiv 512)>
#map1 = affine_map<(d0) -> (d0 ceildiv 256)>
module attributes {gpu.container_module} {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
    %c256 = arith.constant 256 : index
    %c32 = arith.constant 32 : index
    %c512 = arith.constant 512 : index
    %0 = llvm.mlir.constant(0 : i32) : i32
    %c4_i32 = arith.constant 4 : i32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
    %2 = "disc_ral.dispatch"(%arg0, %c1) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
    %dim = memref.dim %2, %c0 : memref<?x4xf32>
    %dim_0 = memref.dim %1, %c0 : memref<?x4xf32>
    %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
    %reinterpret_cast_1 = memref.reinterpret_cast %2 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
    %3 = arith.cmpi eq, %dim, %c1 : index
    %4 = arith.select %3, %dim_0, %dim : index
    %alloc = memref.alloc(%dim_0) {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
    %5 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
    "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast, %alloc) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
    "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
    %alloc_2 = memref.alloc(%dim) {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
    "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_1, %alloc_2) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
    "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
    %6 = arith.index_cast %4 : index to i32
    %7 = arith.muli %6, %c4_i32 : i32
    %8 = arith.index_cast %7 : i32 to index
    %9 = arith.cmpi eq, %dim_0, %4 : index
    %10 = arith.cmpi eq, %dim, %4 : index
    %11 = arith.andi %10, %9 : i1
    %alloc_3 = memref.alloc() : memref<1xf32, #gpu.address_space<global>>
    cf.cond_br %11, ^bb1, ^bb5
  ^bb1:  // pred: ^bb0
    %12 = arith.cmpi slt, %8, %c1 : index
    cf.cond_br %12, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    gpu.launch_func  @main_kernel::@main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32 blocks in (%c1, %c1, %c1) threads in (%c512, %c1, %c1) args(%c512 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    %13 = arith.cmpi eq, %8, %c0 : index
    %14 = arith.subi %8, %c1 : index
    %15 = arith.divui %14, %c32 : index
    %16 = arith.addi %15, %c1 : index
    %17 = arith.select %13, %c0, %16 : index
    %18 = arith.muli %17, %c512 : index
    %19 = affine.apply #map(%18)
    gpu.launch_func  @main_kernel_0::@main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_1 blocks in (%19, %c1, %c1) threads in (%c512, %c1, %c1) args(%c512 : index, %18 : index, %8 : index, %4 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    cf.br ^bb4
  ^bb3:  // pred: ^bb1
    gpu.launch_func  @main_kernel_1::@main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64 blocks in (%c1, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    %20 = arith.cmpi eq, %8, %c0 : index
    %21 = arith.subi %8, %c1 : index
    %22 = arith.divui %21, %c512 : index
    %23 = arith.addi %22, %c1 : index
    %24 = arith.select %20, %c0, %23 : index
    %25 = arith.muli %24, %c256 : index
    %26 = affine.apply #map1(%25)
    gpu.launch_func  @main_kernel_2::@main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1 blocks in (%26, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %25 : index, %8 : index, %4 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    cf.br ^bb4
  ^bb4:  // 2 preds: ^bb2, ^bb3
    cf.br ^bb9
  ^bb5:  // pred: ^bb0
    %27 = arith.cmpi slt, %8, %c1 : index
    cf.cond_br %27, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    gpu.launch_func  @main_kernel_3::@main_kColReduction_reduce__6_1_0___thread_tile_h32 blocks in (%c1, %c1, %c1) threads in (%c512, %c1, %c1) args(%c512 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    %28 = arith.cmpi eq, %8, %c0 : index
    %29 = arith.subi %8, %c1 : index
    %30 = arith.divui %29, %c32 : index
    %31 = arith.addi %30, %c1 : index
    %32 = arith.select %28, %c0, %31 : index
    %33 = arith.muli %32, %c512 : index
    %34 = affine.apply #map(%33)
    gpu.launch_func  @main_kernel_4::@main_kColReduction_reduce__6_1_0___thread_tile_h32_1 blocks in (%34, %c1, %c1) threads in (%c512, %c1, %c1) args(%dim : index, %dim_0 : index, %c512 : index, %33 : index, %8 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    cf.br ^bb8
  ^bb7:  // pred: ^bb5
    gpu.launch_func  @main_kernel_5::@main_kColReduction_reduce__6_1_0___block_tile_h64 blocks in (%c1, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    %35 = arith.cmpi eq, %8, %c0 : index
    %36 = arith.subi %8, %c1 : index
    %37 = arith.divui %36, %c512 : index
    %38 = arith.addi %37, %c1 : index
    %39 = arith.select %35, %c0, %38 : index
    %40 = arith.muli %39, %c256 : index
    %41 = affine.apply #map1(%40)
    gpu.launch_func  @main_kernel_6::@main_kColReduction_reduce__6_1_0___block_tile_h64_1 blocks in (%41, %c1, %c1) threads in (%c256, %c1, %c1) args(%dim : index, %dim_0 : index, %c256 : index, %40 : index, %8 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    cf.br ^bb8
  ^bb8:  // 2 preds: ^bb6, ^bb7
    cf.br ^bb9
  ^bb9:  // 2 preds: ^bb4, ^bb8
    memref.dealloc %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>
    memref.dealloc %alloc : memref<?x4xf32, #gpu.address_space<global>>
    %alloca = memref.alloca() : memref<0xindex>
    %42 = "disc_ral.dispatch"(%arg0, %5, %alloc_3, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1xf32, #gpu.address_space<global>>, memref<0xindex>) -> memref<f32, #gpu.address_space<global>>
    %reinterpret_cast_4 = memref.reinterpret_cast %42 to offset: [0], sizes: [], strides: [] : memref<f32, #gpu.address_space<global>> to memref<f32, #gpu.address_space<global>>
    memref.dealloc %alloc_3 : memref<1xf32, #gpu.address_space<global>>
    %alloc_5 = memref.alloc() : memref<f32>
    "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_4, %alloc_5) {backend_config = "", call_target_name = "d2h", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<f32, #gpu.address_space<global>>, memref<f32>) -> ()
    "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
    memref.dealloc %reinterpret_cast_4 : memref<f32, #gpu.address_space<global>>
    "disc_ral.dispatch"(%arg0, %c0, %alloc_5) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<f32>) -> ()
    return
  }
  gpu.module @main_kernel attributes {gpu.binary = "P\EDU\BA\01\00\10\00H\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\08\04\00\00\00\00\00\00\08\04\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00h\0C\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\002\C0\0B\00\01\00\11\09\06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\11e__6_1_0___no_ibXthread_tile_h32H\00\0FB\00+osharedD\00+\9Fconstant0G\00(\FA\01debug_frame\00.rel\11\00!nv\14\00\11aS\00\0Fk\01 \0F\98\00%\0F\A4\01\FAo_param\AB\01\1C\0F\01\00\06\8Ck\00\00\00\03\00\0A\00\01\00 0\01\18\00,\09\00\01\00\11~\18\00,\04\00\01\00\11\9C\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\1C\00\00\00E\00#\04\10\18\00\80/\08\00\05\00\00\00\06\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04x\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C$\00u\01\00\08\00\00\F0!\10\00\01\01\00\C0\F0\11\00\03\1B\FF\00\04\1C\08\00p\B0\04\12\00\01\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00 \01/\05\00\01\00\FF\B0@$v\01\FF\0F\04\A3\FF\00\8E\07\00\C4\0F\00\C3y\EC\03\A2%\00\00\00\22\0E\00\19y\00\01\00\F3\0E!\00\00\00b\0E\00\90r\05?\04\00\00\80?\E0\FF\0F\00\C6\1F\00\B9z\04\00\00X\DE\02p\E4\0F\00\A4r\04\05B\00\B1?\02\8E\0F\00\CC\0F\00\0C|\00>\04\95pR\F0\0B\00\DA/\00Mc\04\F3\0D\80\03\00\EA\0F\00\02z\02\00\00Z\00\00\00\0F\00\00\00\E2\0F\00$v\03\FF\00[\90\00\22\E2\0F`\001F\00\00\B3\04\80\C8\0F\00\86y\00\02\FF\B0\030\19\10\0C \00*MyP\00PGy\00\00\F09\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\80\0F\01\00-\00\07\05.\03\00\01\00\22@\00\01\00=k\01\000\00\08\01\00\1F\0B@\00\04*\AB\01\08\00\0F@\00\05\13\13\0C\04\0C\01\00\13XU\00*\90\000\04\04M\04\22\18\00\01\00.>\01T\00\00\01\00\13\E8@\00/p\00\80\00\0B\1F)'\00\03A\00X\04\00\01\00\04@\06\04\E4\00*\04\00\01\00\1Fq@\00\04\13\88)\00&L\00@\00\1F\0A@\00\00!\\\01D\01\0D@\00\13\D8)\00*\D8\00\01\00\1B\08\08\00%K\01\DB\0A\09\01\00\13\B0]\05\17\10\80\00\17\048\00\04\18\00\13\F7\14\01\0C\84\01*\C0\05(\07\1F\00\C0\00\04\132@\00*\06\00\01\00*\80\07\C0\07\12\03\C0\01:\06\80\00\01\00\13\06\D8\01\05\A8\0B\0B\01\00*\A8\00\08\00\04\F8\00\14\018\00/\05\00\01\00\029@\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32(%arg0: i32, %arg1: !llvm.ptr<1>) attributes {disc.elimargs = [1 : index, 3 : index, 4 : index, 5 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(1 : index) : i32
      %1 = llvm.mlir.constant(0.000000e+00 : f32) : f32
      %2 = nvvm.read.ptx.sreg.ctaid.x : i32
      %3 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %4 = llvm.mul %2, %arg0  : i32
      %5 = llvm.add %3, %4  : i32
      %6 = llvm.icmp "ult" %5, %0 : i32
      llvm.cond_br %6, ^bb2, ^bb3
    ^bb2:  // pred: ^bb1
      %7 = llvm.getelementptr %arg1[%5] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      llvm.store %1, %7 : f32, !llvm.ptr<1>
      llvm.br ^bb3
    ^bb3:  // 2 preds: ^bb1, ^bb2
      llvm.return
    }
  }
  gpu.module @main_kernel_0 attributes {gpu.binary = "P\EDU\BA\01\00\10\00\00\09\00\00\00\00\00\00\02\00\01\01@\00\00\00\C0\08\00\00\00\00\00\00\BF\08\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00h\17\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\002\C0\16\00\01\00\11\14\06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\13e__6_1_0___no_ibXthread_tile_h32_1J\00\0FD\00-osharedF\00-\9Fconstant0I\00*\FA\01debug_frame\00.rel\11\00!nv\14\00\11aU\00\0Fs\01 \0F\9A\00'\0F\AE\01\FF\03o_param\B5\01\1C\0F\01\00\04\8Cm\00\00\00\03\00\0A\00\01\00 8\01\18\00,\09\00\01\00\11\88\18\00,\04\00\01\00\11\A6\18\00,\07\00\01\00g2\00\00\00\12\10`\00\11\0C\06\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0\0B\07\00 \00\04\9B\00 \04\18i\00\01:\002\04\B8\02\18\00p/\08\00\05\00\00\00\1A\00\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\88\04\80\015\00\00\04\0A\08\00F\00\A2`\01(\00\03\19(\00\04\17\C5\00u\05\00 \00\00\F0!\10\009\04\00\18\10\009\03\00\10\10\00u\02\00\08\00\00\F0\11\10\009\01\00\04\10\00\01\01\00\F3\01\F0\11\00\03\1B\FF\00\04\1C\08\00`\00\00\00P\D8\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\00\D2\05b,\00\00\00H\00\01\00\00`\01/\05\00\01\00\FF\E0@$v\01\FF\7F\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\BB\02Q\0E\00\19y\03\0F\00 \00!-\00\F0\14\0E\00$z\00\00\00X\00\00\03\02\8E\07\00\CA\1F\00\12x\FF\00\FF\01\00\00\FF\C0\80\07\00\C8\0F\00\0C \00\C5Y\00\00pdp\00\00\DA\0F\00M\C3\04\B0\80\03\00\EA\0F\00\19x\03\FF\1F\0F\00\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\00\03\05a\C6\0F\00\11r\03q\001\FFH\8FP\00\000\00\00C\00\10\030\00\93\CA\0F\00$x\07\03 \00\B0\00p\C8\0F\00%x\04\07s\04\10\FF\90\00\F0\09\E2\0F\04\0Cz\00\07\00Z\00\00pb\F4\03\00\E4\0F\04\10x\00\07\01 \00\B0\E0\FF\07\00\E4\0F\00\12x\04\04\01\031\FF\FC\8E\10\00\01\B0\00\010\00\10\F60\00p\00\10z\02\04\00\\0\00!\F1\07@\00Pz\04\04\00^\10\00\11\F3 \01\00P\00\00,\03\04P\00\C2\10z\03\05\00]\00\00\FF\E4\7F\000\00@\05\05\00_\10\00*\FF\00`\00 \F8\03\F0\00\F4\06\81\B9\0B\02\04\FC\FF\FF\00\19\1E\0C\00\A2\0E\00\10x\06\07\03P\00\93\C6\0F\00\81\A9\00\02\04\F8 \00u\E8\0E\00\81\A9\09\04\10\00 \E2\0EP\00\12\06P\00!\FA\030\005\B9\08\04P\00p\A8\0E\00\81\C9\0D\02\10\01\01\10\00y(\0F\00\81\C9\0A\04\10\00S\D9\0F\02\04\04\10\00\10h\10\00%\0C\04\10\00!b\0F\90\00\14\04\90\00\01\F0\006\0E\07\05\F0\00\16\04\80\00\12\FC0\01Jx\10\07\06 \00\12\0E \00\13\F0 \00:\0E\07\07 \00\12\10 \00\11\F2\10\01T$r\06\FF\FF\D0\01\12\E2P\00\14\0A0\00a\E2\0F\04#\A2\06\06\07\00$\00F\00\E2\8F\00`\00\12\F4P\01S\E9\09\02\04\08\C0\00!\E2\0Ep\00\18\08P\018\E9\00\04 \00@#\B2\06\0B\0F\00\11\06P\00\17OP\00\02\00\02P\81\89\0B\02\04\0F\06\05\A0\018\0E\07\09P\008\89\08\04 \00T#\C2\06\0D\0AP\00\93\C6\0F\01\81\99\0D\02\04\10 \00'\22\0F`\00\12\F8\B0\01W\99\0A\04\04\10\90\01X\A9\11\02\04\14\10\00F\0E\04\04\14@\00U#\D2\06\0F\0C\B0\00&\0F\02@\01 \FA\03\A0\01g\81\B9\0F\02\04\18\D0\019\B9\0C\04\10\00X\C9\13\02\04\1C\10\00)\10\04\10\00X\D9\15\02\04 \10\00H\12\04\04 \10\026\14\07\0B\90\01e\00#\E2\06\09\00\90\00\11\8F\10\03\17\0C0\02E\0Cz\00\14 \02\000\00\1D\82p\01\1A\00 \027\00\07\0D \02X\10x\08\07\0Ep\00\17\92`\016\E2\0F\01@\00C\F2\03\00\CA\00\025$\00\00\00\03\00\F0\01\17$\A0\01F\A2\06\11\0E@\00\00\80\00\17\08 \04\02\F0\01\18(@\028\08\07\0F\F0\01\00\D0\01\17,\90\01\1D\B2\90\01\19\08@\02\00\E0\01\070\00\\\10x\0C\07\10@\02\07p\00Z#\C2\06\13\10\A0\00\15\0C0\02\00P\00X\A9\0F\02\040\90\018\10\07\11P\008\A9\0C\04 \00Z#\D2\06\15\12P\00\060\02\00P\00W\B9\11\02\044\F0\01[\B9\0E\04\0440\02\1B80\02\1B80\02\1B<0\02\1A<0\02\1F\120\02\06\11O\F0\01\1F\130\02\16\1A\8F0\02\13\D6\F0\01\18@\A0\03?\00\07\140\02\08\00\D0\01\16\15`\02\19\00@\02\000\00\1F\A2\D0\01\05\11\F4 \00\01P\02\09p\009\08\07\16\B0\027\0A\07\17\80\00\01P\02\19Dp\04\0F\80\02\04\12\F6\C0\03\00\10\02\040\00\00\F0\05\00p\02\17Hp\02\0E \02\19\0A \02\00p\02\070\00\00\B0\04\19\18 \02O\0F\02\04L \02\0A\19\0E \02\00P\02\17L\E0\01\000\02\1BP0\02\1BP0\02\1BT0\02\1BT0\02\1BX0\02\1AX0\02\1F\190\02\05*\C6O \02\030\04\18\1A0\00\0B0\02\00\A0\01\14\1B \00\13\D20\02\16\\\C0\01\0FP\04\0A\03`\00\1A\1C\B0\06\1A\08\B0\048\08\07\1D\80\00\0E@\02\15\00@\02\13\C4@\02\18\\@\028\0A\07\1E\A0\01_\99\0B\02\04`0\02\14\01\F0\01(\08\040\00\00`\04\18\1FP\00_\89\07\02\04d@\02\18X\89\0A\04\04d\80\06O\0D\02\04h0\02\0A\15\0C0\02\13\C40\02\17h\E0\01\00\90\06\1Bl0\02\1Bl \02\18p\10\00K\11\02\04p \02\18t\10\00%\13\02\10\00\1Bb \02 \C8O\D0\01\06\00\02 \C8\8F\10\02%\07\0A\10\00v\0F\01#\A2\06\0D\0C\10\00\10\02\C0\05\08`\01c$v\08\FF\00`\80\08\10\E4\10\00C\09\FF\00a\10\00\11\E2@\01&\11\10@\00\00\10\01\15\13\10\01p\CA\0F\00\8Ey\00\08\0C\00@\84\E7\10\0C0\00*My\F0\0APGy\00\00\F09\0F\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00p\0F\01\00-\11\01h\0F\0E\01\00\22@\00\01\00=s\01\000\00\08\01\00\1F\0B@\00\04\13\B3)\00\1F\B5@\00\0C\13\13\BC\0E\0C\01\00\13hU\00*\90\00\E0\0E\22\08\00\01\00\22\18\00\01\00.F\01T\00\00\01\00\13\F8@\00/p\00\80\00\0B\1F)'\00\03A\00h\04\00\01\00\00\9B\07\17\00\E4\00*\04\00\01\00\1Fs@\00\04\13\98)\00&\8C\00@\00\1F\0A@\00\00!d\01D\01\0D@\001(\05\00\01\00*\D8\00\01\00\1B\08\08\00?S\01\00f\12\00\04\E1\02\10\00B\11\05\80\00\17\048\00\04\18\00\13\FD\14\01\0C\84\01&\10\06\B0\12\04\01\00\0F\C0\00\01\132@\00+\06\00\01\00\1A\08\B0\12\12\03\C0\01>\18\80\00\A7\00\18\05\A8\16\0B\01\00*\A8\00\08\00\04W\00\13\018\00\04\A8\00\0D\D0\12)\0D\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_1(%arg0: i32, %arg1: i32, %arg2: i32, %arg3: !llvm.ptr<1>, %arg4: !llvm.ptr<1>, %arg5: !llvm.ptr<1>) attributes {disc.elimargs = [3 : index, 4 : index, 6 : index, 7 : index, 8 : index, 9 : index, 10 : index, 11 : index, 13 : index, 14 : index, 15 : index, 16 : index, 17 : index, 18 : index, 20 : index, 21 : index, 22 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(0 : index) : i32
      %1 = llvm.mlir.constant(1 : index) : i32
      %2 = llvm.mlir.constant(512 : index) : i32
      %3 = llvm.mlir.constant(32 : index) : i32
      %4 = llvm.mlir.constant(0.000000e+00 : f32) : f32
      %5 = nvvm.read.ptx.sreg.ctaid.x : i32
      %6 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %7 = llvm.mul %5, %arg0  : i32
      %8 = llvm.add %6, %7  : i32
      %9 = llvm.icmp "ult" %8, %arg1 : i32
      llvm.cond_br %9, ^bb2, ^bb12
    ^bb2:  // pred: ^bb1
      %10 = llvm.srem %8, %2  : i32
      %11 = llvm.sdiv %8, %2  : i32
      %12 = llvm.icmp "ult" %10, %1 : i32
      llvm.cond_br %12, ^bb3, ^bb11
    ^bb3:  // pred: ^bb2
      %13 = llvm.mul %11, %3  : i32
      llvm.br ^bb4(%0, %4 : i32, f32)
    ^bb4(%14: i32, %15: f32):  // 2 preds: ^bb3, ^bb9
      %16 = llvm.icmp "slt" %14, %3 : i32
      llvm.cond_br %16, ^bb5, ^bb10
    ^bb5:  // pred: ^bb4
      %17 = llvm.add %13, %14  : i32
      %18 = llvm.icmp "slt" %17, %arg2 : i32
      llvm.cond_br %18, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %19 = llvm.getelementptr %arg3[%17] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %20 = llvm.load %19 : !llvm.ptr<1> -> f32
      %21 = llvm.getelementptr %arg4[%17] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %22 = llvm.load %21 : !llvm.ptr<1> -> f32
      %23 = llvm.fmul %20, %22  : f32
      %24 = llvm.fadd %15, %23  : f32
      llvm.br ^bb8(%24 : f32)
    ^bb7:  // pred: ^bb5
      llvm.br ^bb8(%15 : f32)
    ^bb8(%25: f32):  // 2 preds: ^bb6, ^bb7
      llvm.br ^bb9
    ^bb9:  // pred: ^bb8
      %26 = llvm.add %14, %1  : i32
      llvm.br ^bb4(%26, %25 : i32, f32)
    ^bb10:  // pred: ^bb4
      %27 = llvm.getelementptr %arg5[%10] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %28 = llvm.atomicrmw fadd %27, %15 acq_rel : !llvm.ptr<1>, f32
      llvm.br ^bb11
    ^bb11:  // 2 preds: ^bb2, ^bb10
      llvm.br ^bb12
    ^bb12:  // 2 preds: ^bb1, ^bb11
      llvm.return
    }
  }
  gpu.module @main_kernel_1 attributes {gpu.binary = "P\EDU\BA\01\00\10\00H\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\08\04\00\00\00\00\00\00\06\04\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00h\0C\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\002\C0\0B\00\01\00\11\09\06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\10e__6_1_0___no_ibXblock_tile_h64G\00\0FA\00*osharedC\00*\9Fconstant0F\00'\FA\01debug_frame\00.rel\11\00!nv\14\00\11aR\00\0Fg\01 \0F\97\00$\0F\9F\01\F6o_param\A6\01\1C\0F\01\00\07\8Cj\00\00\00\03\00\0A\00\01\00 ,\01\18\00,\09\00\01\00\11y\18\00,\04\00\01\00\11\97\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\1C\00\00\00E\00#\04\10\18\00\80/\08\00\05\00\00\00\06\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04p\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C$\00u\01\00\08\00\00\F0!\10\00\01\01\00\C0\F0\11\00\03\1B\FF\00\04\1C\08\00p\A8\04\12\00\01\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00 \01/\05\00\01\00\FF\B8@$v\01\FF\17\04\A3\FF\00\8E\07\00\C4\0F\00\C3y\F4\03\A2%\00\00\00\22\0E\00\19y\00\01\00\F3\0E!\00\00\00b\0E\00\90r\05?\04\00\00\80?\E0\FF\0F\00\C6\1F\00\B9z\04\00\00X\E6\02p\E4\0F\00\A4r\04\05B\00\B1?\02\8E\0F\00\CC\0F\00\0C|\00F\04\95pR\F0\0B\00\DA/\00Mk\04\F3\0D\80\03\00\EA\0F\00\02z\02\00\00Z\00\00\00\0F\00\00\00\E2\0F\00$v\03\FF\00[\90\00\22\E2\0F`\001F\00\00\BB\04\80\C8\0F\00\86y\00\02\FF\B8\030\19\10\0C \00*MyP\00PGy\00\00\F0A\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\80\0F\01\00-\00\0F\05.\03\00\01\00\22@\00\01\00=g\01\000\00\08\01\00\1F\0B@\00\04\13\A7)\00\1F\A6@\00\0C\13\13\14\04\0C\01\00\13PU\00*\90\008\04\04U\04\22\18\00\01\00.:\01T\00\00\01\00\13\E0@\00/p\00\80\00\0B\1F)'\00\03A\00P\04\00\01\00\04H\06\04\E4\00*\04\00\01\00\1Fp@\00\04\13\80)\00&L\00@\00\1F\0A@\00\00!X\01D\01\0D@\00\13\D0)\00*\D8\00\01\00\1B\08\08\00%G\01\DB\0A\09\01\00\13\A8e\05\17\10\80\00\17\048\00\04\18\00\13\F4\14\01\0C\84\01*\B8\050\07\1F\00\C0\00\04\132@\00*\06\00\01\00*\80\07\C8\07\12\03\C0\01:\06\80\00\01\00\13\06\D8\01\05\A8\0B\0C\C8\00\1A\00\08\00\04\F8\00\13\018\00\04\A8\00\0C\01\009H\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64(%arg0: i32, %arg1: !llvm.ptr<1>) attributes {disc.elimargs = [1 : index, 3 : index, 4 : index, 5 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(1 : index) : i32
      %1 = llvm.mlir.constant(0.000000e+00 : f32) : f32
      %2 = nvvm.read.ptx.sreg.ctaid.x : i32
      %3 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %4 = llvm.mul %2, %arg0  : i32
      %5 = llvm.add %3, %4  : i32
      %6 = llvm.icmp "ult" %5, %0 : i32
      llvm.cond_br %6, ^bb2, ^bb3
    ^bb2:  // pred: ^bb1
      %7 = llvm.getelementptr %arg1[%5] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      llvm.store %1, %7 : f32, !llvm.ptr<1>
      llvm.br ^bb3
    ^bb3:  // 2 preds: ^bb1, ^bb2
      llvm.return
    }
  }
  gpu.module @main_kernel_2 attributes {gpu.binary = "P\EDU\BA\01\00\10\00\A8\0B\00\00\00\00\00\00\02\00\01\01@\00\00\00h\0B\00\00\00\00\00\00c\0B\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\A8#\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00g\02\00\BE\00r\00\01\00\22#\00\01\00\11 \06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\12e__6_1_0___no_ibXblock_tile_h64_1I\00\0FC\00,osharedE\00,\9Fconstant0H\00)\FA\01debug_frame\00.rel\11\00!nv\14\00\11aT\00\0Fo\01 \0F\99\00&\0F\A9\01\B6\8F$____wg_B\00&\00\1B\00/26\F1\016o_param\F8\01\1C\0F\01\00\05\8Cl\00\00\00\03\00\0A\00\01\00\11\EF\18\00,\0B\00\01\00 |\01\18\00,\09\00\01\00\11\CB\18\00,\04\00\01\00\11\E9\18\00,\07\00\01\00g2\00\00\00\12\10x\00\11\18\06\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0\17\07\00 \00\04\9B\00R\04\14\00\00\00E\002\04\C0\05\18\00C/\08\00\06\7F\00\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E0\04\F1\08\015\00\00\04\0A\08\00\03\00\00\00`\01(\00\03\19(\00\04\17\0C$\00\10\05\ED\04%\F0!\10\009\04\00\18\10\009\03\00\10\10\00u\02\00\08\00\00\F0\11\10\009\01\00\04\10\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\00\10\17\00\00`\17\00\00\04\1Et\01#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\005\02\00\00\84\01\0F\01\00\FFz@$v\01\FF?\04\A3\FF\00\8E\07\00\C4\0F\00\19y\A6\01\10%[\02a\0E\00\19y\03\00\01\00\10!-\00\F5\14\0E\00$z\06\06\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\06\00Y\00\00p`\F0\03\00\DA\0F\00M[\04\B0\80\03\00\EA\0F\00\19x\03\FF\1F\FC\01\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\00\B3\04\93\E2\0F\00Ey\00\00@\150\00\93\E2\0F\00$r\08\FF\FF\00\90\00p\E2\0F\00\11r\03\03\92\00\C1\FF@\8F\07\00\C8\0F\00\12x\05\031\04\10\C0\80\00p\0F\00$x\06\06\01\B4\03\12\0A\10\00@\12x\05\06p\00\01 \00p\E4\0F\04\19x\00\FF*\04\C0\06\16\01\00\00\E4\0F\00\0Cr\00\05`\00@pR\F2\03 \00P\0Cx\00\06\7F\10\00\22@\F0\80\002x\07\05\C9\02\11\8Ep\00T$x\07\07\04\90\00\9A\CC\0F\00G\19\00\00\80\14\E0\00\000\00\10\03\E0\00\01\90\00*\03\03@\004\09\03@@\00q\C8\0F\00%x\04\09P\00\11\02\E0\00P\04\10x\00\09\C0\001\FF\E0\FF\B0\00\B0\0Cz\00\09\00Z\00\00pb\F4\A0\00P\00\12x\04\04P\00!\FF\FC\D0\00\00p\01\12\00 \00\11\F6 \00`\10z\02\04\00\\@\00\11\F3@\00`\10z\04\04\00^\10\00\11\F9\D0\01\00`\00\00|\03\03`\00\D2\00\10z\03\05\00]\00\00\FF\E4\FF\000\00@\05\05\00_\10\00*\7F\02`\00\11\F8\10\01\F4\06\81\B9\0D\02\04\FC\FF\FF\00\19\1E\0C\00\A2\0E\00\10x\0A\09\03P\00\93\C6\0F\00\81\A9\00\02\04\F8 \00 \E2\0E@\00\12\0A@\00\22\FA\03 \00%\0B\04 \00u\E8\0E\00\81\B9\0A\04P\00p\A8\0E\00\81\C9\0F\02\10\01\01\10\00y(\0F\00\81\C9\0C\04\10\00S\D9\11\02\04\04\10\00\10h\10\00%\0E\04\10\00\10b\E0\004\10\09\04\90\00\01\F0\00:\12\09\05P\01\12\10\90\00#\FC\03\E0\00\12\12\10\00\12\F2@\01Dx\10\09\060\00a\E2\0F\04#\A2\086\07\00$\021\00\E2\8F@\01\14\07 \00q\CE\0F\00\81\E9\0B\02\91\01\06\F0\00\15\10\B0\01\93\E2\0F\00#\B2\08\0D\0A\00=\05!\E2O0\01\16\08`\00`\00\81\99\0D\02\04\17\06\03P\01\09\D0\01\000\018\E9\00\04`\00U#\C2\08\0F\0CP\00&\0F\01`\01\11\F8\C0\00H\81\99\0A\04P\00\00\D0\00\19\09\A0\01C\0F\02\04\10 \00\86\22\0F\00#\D2\08\11\0EP\00\16\02\C0\00\12\FAP\00W\A9\0C\04\04\10\80\01W\B9\11\02\04\14\80\019\B9\0E\04\10\00X\C9\13\02\04\18\10\00)\10\04\10\00X\D9\15\02\04\1C\10\00H\12\04\04\1C\C0\017\14\09\0A \01U#\E2\08\0B\00\A0\00\02\80\01\17\0B\E0\01E\0Cz\00\14\D0\01\000\00\1A\92p\01\06P\01\04\E0\017\00\09\0C\10\02\00\90\01\18\0Dp\00\1D\A2`\01\15\00\D0\01\13\CA\F0\01\18 `\018\0C\09\0F`\01\00\B0\01\07 \00-#\B2`\01\19\0A\B0\03Lx\0A\09\0E\10\02\19$\F0\01$\13\10@\00\02\B0\02\06\F0\01\13\E2\D0\01\17(\A0\01\00\00\02\08@\00F\D2\08\15\12\80\00\00@\00\15\0C\E0\01\13\C4\D0\01\17,\90\01\00\F0\01\08P\00\00\E0\01\1B,\E0\01\1B0\E0\01\1B0\E0\01\1B4\E0\01\1A4\E0\01\1F\10\E0\01\06\11O\A0\01\1F\11\E0\01\16\1A\8F\E0\01\13\D6\A0\01\188\A0\01;\00\09\12\F0\01\1F\13\F0\01\15\13\E4\D0\01\1F8\D0\01\1B\16\14`\00\11\04\D0\01\16<\90\01X\10x\0E\09\15\80\00\08\E0\01\1B\E2\E0\01\04\B0\03\1B@\E0\01\1F<\E0\01\0A\1D\0E\C0\03\1B@\C0\03\1BD\C0\03\1BD\E0\01\1BH\E0\01\1BH\E0\01\1BL\E0\01\1AL\E0\01\1F\16\E0\01\0C\1F\17\E0\01-\1AP\E0\01\1F\18\D0\01\08\00\90\01\16\19\00\02\1F\00\E0\01\02\1FP\E0\01\17\01\D0\01\18T\D0\017\0A\09\1A\D0\00\00\10\04\1F\1B\E0\01\15\13\E4\E0\01\1BX\E0\01\1FT\C0\03\1C\1B\\\C0\03\1BX\C0\03\1B\\\E0\01\1B`\E0\01\1B`\E0\01\1Bd\E0\01\1Ad\E0\01\1F\1C\E0\01\0C\1F\1D\E0\01\05\13\DA\C0\01\17hp\01\0F\F0\01\09\01P\03;\00\09\1E\C0\03\1F\1F\C0\03\1D\1Fh\E0\01\14\01\C0\01<\0A\09 \C0\03\1Al\C0\03\1F!\C0\03\1D\1Bp\E0\01\1Fl\C0\03\1C\1Bp\C0\03\1Bt\C0\03\1Bt\E0\01\1Bx\E0\01\1Bx\E0\01\1B|\E0\01\1A|\E0\01\1F\22\E0\01\0C\1F#\C0\03-\16\80\90\01\00P\00\1F$\C0\03\0C\1F%\C0\03\0D\1F\80\C0\03\1C\18\84\D0\017\0A\09&\D0\00\00\C0\03\1F'\C0\03\1D\1B\88\E0\01\1F\84\C0\03\1C\1B\8C\C0\03\1B\88\C0\03\1B\8C\E0\01\1B\90\E0\01\1B\90\E0\01\1B\94\E0\01\1A\94\E0\01\1F(\E0\01\0C\1F)\C0\03\0D\1F\98\C0\03\1B\1B*\C0\03\1F+\C0\03\1D\1F\98\C0\03\1B\1C,\C0\03\1A\9C\C0\03\1F-\C0\03\1D\1B\A0\E0\01\1F\9C\C0\03\1C\1B\A0\C0\03\1B\A4\C0\03\1B\A4\E0\01\1B\A8\E0\01\1B\A8\E0\01\1B\AC\E0\01\1A\AC\E0\01\1F.\E0\01\0C\1F/\C0\03-\16\B0\90\01\00P\00\1F0\C0\03\0C\1F1\C0\03\0D\1F\B0\C0\03\1C\18\B4\D0\017\0A\092\D0\00\00\C0\03\1F3\C0\03\1D\1B\B8\E0\01\1F\B4\C0\03\1C\1B\BC\C0\03\1B\B8\C0\03\1B\BC\E0\01\1B\C0\E0\01\1B\C0\E0\01\1B\C4\E0\01\1A\C4\E0\01\1F4\E0\01\0C\1F5\C0\03\0D\1F\C8\C0\03\1B\1B6\C0\03\1F7\C0\03\1D\1F\C8\C0\03\1B\1C8\C0\03\1A\CC\C0\03\1F9\C0\03\1D\1B\D0\E0\01\1F\CC\C0\03\1C\1B\D0\C0\03\1B\D4\C0\03\1B\D4\E0\01\1B\D8\E0\01\1B\D8\E0\01\1B\DC\E0\01\1A\DC\E0\01\1F:\E0\01\0C\1F;\C0\03-\16\E0\90\01\00P\00\1F<\C0\03\0C\1F=\C0\03\0D\1F\E0\C0\03\17\00P\00\17>\C0\00\00\B0\03\17?`\00g\81\99\09\02\04\E4\A0\01\0F\C0\03\0D\00\D0\01\040\00\00P\12Y\A9\0D\02\04\E8\E0\10\0F\C0\03\0B\00\E0\01\18\E8\E0\01K\0F\02\04\EC\E0\01\1B\EC\D0\01\18\F0\10\00K\11\02\04\F0\D0\01\18\F4\10\00%\13\02\10\00\1Bb\D0\01 \C8O\B0\01\15\09\B0\01 \C8\8F\80\01\15\0D\80\01\86\C8\0F\01#\B2\08\0F\0E\10\00f\02#\C2\08\11\10\10\00\00\E0\00\15\13\E0\00P\C4\0F\00Ay\09\00\06\90\14c\88s\00\07\08\00!\00f\E8\0F\00\1D{\00\01\00S\EC\0F\00\84\89\CD\19\13\08 \01Ax\00\06?\00\15\11\F2@\03c\84\89\03\07\00\10 \00s$\0E\00!\82\00\00\02\16\10\00\F0\15'\88\83@\00\0F`\00\01-\99\02`\00\14\1F`\15\00`\00W\99\03\07\00\08`\009\92\02\02`\00O\93\00\07\02\C0\00\0A\1A\03`\005r\00\06\D0\15\01\C0\00H\04\07\00\04\C0\00J\04\03\04\00\C0\00\1F\04`\00\089M\19\00P\016\84y\07p\00\93\22\0E\00$v\02\FF\00`\D0\15\93\C4\0F\00$v\03\FF\00a\10\00a\CA\0F\00\8Ey\00\01\01\9B\84\E7\10\0C\00\E2\1F\00M\A0\01PGy\00\00\F0\C0\16\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00`\0F\01\00-'\01\00 \02\07\01\00\22@\00\01\00=o\01\000\00\08\01\00\1F\0B@\00\04\13\AF)\00\1F\F8@\00\0C\13\13\\\1A\0C\01\00\13\A8U\00*\A8\00\80\1A\22\08\00\01\00\22\18\00\01\00.B\01T\00\00\01\00\13P5\02/p\00\80\00\0B/)\00'\00\02#\00\C0@\00\00,\09\17\00\E4\00*\04\00\01\00\1Fr@\00\04\13\F0)\00&\98\00@\00\1F\0A@\00\00!`\01D\01\0D@\001\88\05\00\01\00*\D8\00\01\00\1B\08\08\00?O\01\00\0E\1E\00Q\00\00`\06\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\FA\14\01\0C\84\01\13p@\00\17\881\01\0F\C0\00\01\132@\00\15\06R\00\0B\A1\00\16\18\80\00j\06\00\00\18\80\00\01\00\13\B5+\00+\03\00\01\00\00\D5\0F\1A\00q\00\0F\80\00\01\11\06\F0\1D\07\E8\22\0CH\02\1A\00\08\00\04\97\00\13\018\00\04\E8\00\0C\01\00*\90\19\08\00\088\00\18\06\A0\00\0F\01\00\05\03\A9\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00"} {
    llvm.mlir.global internal @__wg_main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1_0() {addr_space = 3 : i32} : !llvm.array<256 x f32>
    llvm.func @main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1(%arg0: i32, %arg1: i32, %arg2: i32, %arg3: !llvm.ptr<1>, %arg4: !llvm.ptr<1>, %arg5: !llvm.ptr<1>) attributes {disc.elimargs = [3 : index, 4 : index, 6 : index, 7 : index, 8 : index, 9 : index, 10 : index, 11 : index, 13 : index, 14 : index, 15 : index, 16 : index, 17 : index, 18 : index, 20 : index, 21 : index, 22 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(32 : index) : i32
      %1 = llvm.mlir.constant(8 : index) : i32
      %2 = llvm.mlir.constant(64 : index) : i32
      %3 = llvm.mlir.constant(4 : index) : i32
      %4 = llvm.mlir.constant(0.000000e+00 : f32) : f32
      %5 = llvm.mlir.constant(2 : index) : i32
      %6 = llvm.mlir.constant(1 : index) : i32
      %7 = llvm.mlir.constant(256 : index) : i32
      %8 = llvm.mlir.constant(0 : index) : i32
      %9 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1_0 : !llvm.ptr<3>
      %10 = llvm.getelementptr %9[0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<256 x f32>
      %11 = nvvm.read.ptx.sreg.ctaid.x : i32
      %12 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %13 = llvm.mul %11, %arg0  : i32
      %14 = llvm.add %12, %13  : i32
      %15 = llvm.icmp "ult" %14, %arg1 : i32
      llvm.cond_br %15, ^bb2, ^bb21
    ^bb2:  // pred: ^bb1
      %16 = llvm.srem %14, %7  : i32
      %17 = llvm.sdiv %14, %7  : i32
      %18 = llvm.udiv %16, %0  : i32
      %19 = llvm.urem %16, %0  : i32
      %20 = llvm.mul %19, %1  : i32
      %21 = llvm.add %18, %20  : i32
      %22 = llvm.icmp "ult" %19, %6 : i32
      llvm.cond_br %22, ^bb3, ^bb10(%4 : f32)
    ^bb3:  // pred: ^bb2
      %23 = llvm.mul %17, %1  : i32
      %24 = llvm.add %18, %23  : i32
      %25 = llvm.mul %24, %2  : i32
      llvm.br ^bb4(%8, %4 : i32, f32)
    ^bb4(%26: i32, %27: f32):  // 2 preds: ^bb3, ^bb9
      %28 = llvm.icmp "slt" %26, %2 : i32
      llvm.cond_br %28, ^bb5, ^bb10(%27 : f32)
    ^bb5:  // pred: ^bb4
      %29 = llvm.add %26, %25  : i32
      %30 = llvm.icmp "slt" %29, %arg2 : i32
      llvm.cond_br %30, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %31 = llvm.getelementptr %arg3[%29] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %32 = llvm.load %31 : !llvm.ptr<1> -> f32
      %33 = llvm.getelementptr %arg4[%29] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %34 = llvm.load %33 : !llvm.ptr<1> -> f32
      %35 = llvm.fmul %32, %34  : f32
      %36 = llvm.fadd %27, %35  : f32
      llvm.br ^bb8(%36 : f32)
    ^bb7:  // pred: ^bb5
      llvm.br ^bb8(%27 : f32)
    ^bb8(%37: f32):  // 2 preds: ^bb6, ^bb7
      llvm.br ^bb9
    ^bb9:  // pred: ^bb8
      %38 = llvm.add %26, %6  : i32
      llvm.br ^bb4(%38, %37 : i32, f32)
    ^bb10(%39: f32):  // 2 preds: ^bb2, ^bb4
      llvm.br ^bb11(%39 : f32)
    ^bb11(%40: f32):  // pred: ^bb10
      llvm.br ^bb12
    ^bb12:  // pred: ^bb11
      %41 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      llvm.store %40, %41 : f32, !llvm.ptr<3>
      nvvm.barrier0
      %42 = llvm.icmp "slt" %18, %3 : i32
      llvm.cond_br %42, ^bb13, ^bb14
    ^bb13:  // pred: ^bb12
      %43 = llvm.add %21, %3  : i32
      %44 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %45 = llvm.load %44 : !llvm.ptr<3> -> f32
      %46 = llvm.getelementptr %10[%43] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %47 = llvm.load %46 : !llvm.ptr<3> -> f32
      %48 = llvm.fadd %45, %47  : f32
      %49 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      llvm.store %48, %49 : f32, !llvm.ptr<3>
      llvm.br ^bb14
    ^bb14:  // 2 preds: ^bb12, ^bb13
      nvvm.barrier0
      %50 = llvm.icmp "slt" %18, %5 : i32
      llvm.cond_br %50, ^bb15, ^bb16
    ^bb15:  // pred: ^bb14
      %51 = llvm.add %21, %5  : i32
      %52 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %53 = llvm.load %52 : !llvm.ptr<3> -> f32
      %54 = llvm.getelementptr %10[%51] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %55 = llvm.load %54 : !llvm.ptr<3> -> f32
      %56 = llvm.fadd %53, %55  : f32
      %57 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      llvm.store %56, %57 : f32, !llvm.ptr<3>
      llvm.br ^bb16
    ^bb16:  // 2 preds: ^bb14, ^bb15
      nvvm.barrier0
      %58 = llvm.icmp "slt" %18, %6 : i32
      llvm.cond_br %58, ^bb17, ^bb18
    ^bb17:  // pred: ^bb16
      %59 = llvm.add %21, %6  : i32
      %60 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %61 = llvm.load %60 : !llvm.ptr<3> -> f32
      %62 = llvm.getelementptr %10[%59] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %63 = llvm.load %62 : !llvm.ptr<3> -> f32
      %64 = llvm.fadd %61, %63  : f32
      %65 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      llvm.store %64, %65 : f32, !llvm.ptr<3>
      llvm.br ^bb18
    ^bb18:  // 2 preds: ^bb16, ^bb17
      nvvm.barrier0
      %66 = llvm.icmp "eq" %18, %8 : i32
      %67 = llvm.and %66, %22  : i1
      llvm.cond_br %67, ^bb19, ^bb20
    ^bb19:  // pred: ^bb18
      %68 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %69 = llvm.load %68 : !llvm.ptr<3> -> f32
      %70 = llvm.getelementptr %arg5[%19] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %71 = llvm.atomicrmw fadd %70, %69 acq_rel : !llvm.ptr<1>, f32
      llvm.br ^bb20
    ^bb20:  // 2 preds: ^bb18, ^bb19
      llvm.br ^bb21
    ^bb21:  // 2 preds: ^bb1, ^bb20
      llvm.return
    }
  }
  gpu.module @main_kernel_3 attributes {gpu.binary = "P\EDU\BA\01\00\10\00H\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\08\04\00\00\00\00\00\00\01\04\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\0Be__6_1_0___thread_tile_h32B\00\0F<\00%oshared>\00%\9Fconstant0A\00\22\FA\01debug_frame\00.rel\11\00!nv\14\00\11aM\00\0FS\01 \0F\92\00\1F\0F\86\01\E2o_param\8D\01\1C\0F\01\00\04\8Ce\00\00\00\03\00\0A\00\01\00 \18\01\18\00,\09\00\01\00\11`\18\00,\04\00\01\00\11~\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\1C\00\00\00E\00#\04\10\18\00\80/\08\00\05\00\00\00\06\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04@\04\91\015\00\00\04\0A\08\00\02\FC\00\91\10\00\03\19\10\00\04\17\0C$\00u\01\00\08\00\00\F0!\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00p\00\00\00\C0\00\01\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00 \01/\05\00\01\00\FFh@$v\01\FF\C7\03\A3\FF\00\8E\07\00\C4\0F\00\C3y\A4\03\A2%\00\00\00\22\0E\00\19y\00\01\00\F3\0E!\00\00\00b\0E\00\90r\05?\04\00\00\80?\E0\FF\0F\00\C6\1F\00\B9z\04\00\00X\96\02p\E4\0F\00\A4r\04\05B\00\B1?\02\8E\0F\00\CC\0F\00\0C|\00\F6\03\95pR\F0\0B\00\DA/\00M\1B\04\F3\0D\80\03\00\EA\0F\00\02z\02\00\00Z\00\00\00\0F\00\00\00\E2\0F\00$v\03\FF\00[\90\00\22\E2\0F`\001F\00\00k\04\80\C8\0F\00\86y\00\02\FFh\030\19\10\0C \00*MyP\00PGy\00\00\F0\F1\03\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\80\0F\01\00-\00\BF\04.\03\00\01\00\22@\00\01\00=S\01\000\00\08\01\00\1F\0B@\00\04\13\93)\00\1F\8D@\00\0C\13\13\C4\03\0C\01\00\13 U\00*\90\00\E8\03\04\05\04\22\18\00\01\00.&\01T\00\00\01\00\13\B0@\00/p\00\80\00\0B\1F)'\00\03A\00 \04\00\01\00\04\F8\05\04\E4\00*\04\00\01\00\1Fk@\00\04\13P)\00&L\00@\00\1F\0A@\00\00!D\01D\01\0D@\00\13\A0)\00*\D8\00\01\00\1B\08\08\00?3\01\00.\07\003\00\00x\15\05\17\10\80\00\17\048\00\04\18\00\13\E5\14\01\0C\84\01*\88\05\E0\06\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07x\07\12\03\C0\01:\06\80\00\01\00\13\06\D8\01\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\13\018\00\04\A8\00\0C\01\00*\F8\02\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__6_1_0___thread_tile_h32(%arg0: i32, %arg1: !llvm.ptr<1>) attributes {disc.elimargs = [1 : index, 3 : index, 4 : index, 5 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(1 : index) : i32
      %1 = llvm.mlir.constant(0.000000e+00 : f32) : f32
      %2 = nvvm.read.ptx.sreg.ctaid.x : i32
      %3 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %4 = llvm.mul %2, %arg0  : i32
      %5 = llvm.add %3, %4  : i32
      %6 = llvm.icmp "ult" %5, %0 : i32
      llvm.cond_br %6, ^bb2, ^bb3
    ^bb2:  // pred: ^bb1
      %7 = llvm.getelementptr %arg1[%5] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      llvm.store %1, %7 : f32, !llvm.ptr<1>
      llvm.br ^bb3
    ^bb3:  // 2 preds: ^bb1, ^bb2
      llvm.return
    }
  }
  gpu.module @main_kernel_4 attributes {gpu.binary = "P\EDU\BA\01\00\10\00\18\0D\00\00\00\00\00\00\02\00\01\01@\00\00\00\D8\0C\00\00\00\00\00\00\D8\0C\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8!\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\00!@!\07\001\00\80\1E\07\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\0De__6_1_0___thread_tile_h32_1D\00\0F>\00'oshared@\00'\9Fconstant0C\00$\FA\01debug_frame\00.rel\11\00!nv\14\00\11aO\00\0F[\01 \0F\94\00!\0F\90\01\EAo_param\97\01\1C\0F\01\00\0A\8Cg\00\00\00\03\00\0A\00\01\00  \01\18\00,\09\00\01\00\11j\18\00,\04\00\01\00\11\88\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\16\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\18\00\00\00E\002\04H\05\18\000/\08\00\0A\00\10\22\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04X\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\010\00\03\190\00\04\17\0C$\00u\07\00(\00\00\F0!\10\009\06\00 \10\009\05\00\18\10\00u\04\00\10\00\00\F0\11\10\009\03\00\0C\10\009\02\00\08\10\009\01\00\04\10\00\01\01\00\F1\02\F0\11\00\03\1B\FF\00\04\1C\08\00`\00\00\00\90\15\16\003\00K\00\01\00`\02\02\08\10\0A/\E7\00\11\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00\80\01/\05\00\01\00\FF\F0@$v\01\FF\AF\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\CB\02Q\0E\00\19y\03\0F\00\10\00\08\08\F0\15$\0E\00$z\00\00\00Z\00\00\03\02\8E\07\00\CA\1F\00\12x\FF\00\FF\01\00\00\FF\C0\80\07\00\C8\0F\00\0C \00\C5[\00\00pdp\00\00\DA\0F\00M\F3\04\B0\80\03\00\EA\0F\00\19x\03\FF\1F\0F\00\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\003\05a\C6\0F\00\11r\03q\00@\FFH\8F\07 \00c$v\00\FF\00X\A0\00\12\C6@\00\00S\00\10\03@\00P\E4\0F\00\0Cx$\03`\00\00pR\F0\03 \00c$x\03\03 \000\00@\E2\0F\00\07\90\00\22Y\00\01\00p\C8\0F\00\10x\0A\030\00\F0\05\FF\E0\FF\07\00\E4\0F\04\0Cz\00\03\00\\\00\00pb\F4\03\10\00Z\10x\0C\03\02 \00\12\0A \00\10\FC \00f\00\10x\12\03\03 \00\00\F0\00\12\0C \00C\FA\03\00\C4\00\01\13X\90\00\02@\00\01\80\00ApR\F2\03\D0\00G\A4\09\FF\04\A0\00B\0Cz\00\12@\00\13\F6`\004\02\03\04`\00i\E2\0F\04$\E4\0B0\00c\07\A2\04\03\FF\00\FF\04c\E4\0F\04\07\A2\08\10\00 \80\04 \00<$\D4\0D`\00\12\02`\00\11\F8\80\00@%\A6\04\04\E9\04#\09\02P\00D\E8\06\0A\01@\00\10\C4\10\004\0A\0A\01`\00\010\00G\08\08\00^0\00F\D8\0E\0C\02p\00A\04\81\A9\05#\06\D8\00\19\1E\0C\00\A2\00\00\07\D8\0C\0C\02@\000\E6\06\06@\00\12\0B@\00f\08\07\B8\10\12\03@\00U\00\81\A9\00\08@\00\95\A4\02\00%\E6\0A\0A\00`0\00g\00\07\B8\12\12\03P\00E\81\E9\07\060\00p\E4\0E\00%\D6\0E\0E`\00\14\0D0\008\C2\13\02\10\01F\81\E9\14\0A0\00V\08\00$\B4\15 \01\96\C4\0F\00%\D6\0C\0C\00`@\00E\81\D9\0F\0E0\00pf\0F\00%\B6\10\10`\00\14\15\C0\00F\C2\0A\02\FF\90\00U\01\81\D9\0C\0C0\00\10d0\00C\08\12\00`0\00u\E4/\00\81\B9\11\10 \00i$\03\00$\C4\0B\80\006\81\B9\08\00\01u$\0F\00%\C6\12\130\01\10\C8\10\00\07\10\01u\E4\0F\00\81\C9\13\120\00\10(\10\00\16\0A\E0\00\84\22\0F\00$r\02\FF\FF`\00\10\E2P\025\04\03\05\B0\02u\1F\04\10x\10\03\07`\02Q/\04#\A2\02\08\06\01\D4\00\84\E2O\00\10x\05\03\06 \00\02\D0\02\15\04 \03\83\E2\0F\00#\E2\02\07\14@\060\00\E2\8F \00\15\05 \03\84\C6\0F\00#\D2\02\0F\0C \00\85\C8\0F\02#\B2\02\11\080\00j\0F\01\0Cz\00\10\F0\028\11\03\08\F0\02\0B \03W\07\A8\08\04\01\F0\02V\10x\00\03\090\00f\00#\C2\02\13\0A`\00\00\90\00\15\11\F0\02\10\E4@\007\0A\04\01\10\02*$\E4 \03Y\07\E8\06\05\02\00\039\0C\05\02\00\03'\0A\0A\00\03\11\08\D0\03\04\F0\03\00`\008\B8\0E\10\D0\02\010\03\08`\03G\B8\10\10\03P\006\81\A9\05\B0\01%\A4\00 \03\07\C0\02)\11\FF \03\19\04 \039$\B4\17@\02*%\E6\C0\02_\07\C2\12\11\FF0\03\09\12\B60\03\12\17P\02Y\08\81\E9\14\0C \03&\C4\16`\00\10\E4\D0\02C\0A\10\00`0\00W\E2\1F\00\81\B9 \03\10b\10\04\11\15\80\05\04@\011%\C6\10\C0\02\14\16`\009\B9\0A\0A \03\12\C6 \03\01 \00 \E2/0\04(\00\01P\03\18\C90\03\1A\01\D0\04\01\00\03\0B0\035\D6\12\15@\01\1B\C8\D0\03\000\00\1B\D90\03\08\B0\03\10\22\80\029\10\03\0C \034\00\03\0E\10\00\00`\02E\A2\02\05\04`\02\11OP\03\17\0A \03\000\03\14\0B\10\00\1F\E40\03\0Cw\C8\8F\00#\B2\02\0F\B0\02\16\02@\03\12\F6@\03\1F\C20\03\05\02\D0\02\000\03\17\0D\A0\00\0CP\06\00\F0\02\14\02\90\01\02\80\06\18\11\C0\06\00P\03\17\02 \02Z#\D2\02\13\0C@\03\06\E0\02\00`\00\1B\B40\037\B8\06\05\F0\02\1B\04 \03d\00\07\B8\0C\05\03`\00\00P\03O\C2\0E\10\FF \03\09O\C2\10\10\FF \03\09\19\B6 \03i\08\07\E8\13\11\01\80\06\0B \03\17\C4 \03\01\C0\02\0B \03H\E8\12\11\01p\00\1B\B9 \03\17\C6 \03\12\E2\E0\02\17\02\C0\00+\81\B90\03*\E4\16\90\03\1B\C60\03\1B\C9P\06\17\E6 \03\00\D0\00I\D8\0C\00\02\00\03\0A0\03\09\D0\07\00\D0\00\1A\E6@\03'\81\E90\03*&\01\10\03\00`\06\18\E90\03\1F\03 \03\22\1B\11 \03\14\13\10\00\0F \03\04\14\0F \00\03 \03\14\10\10\00\0F \03\01\1F\B2P\06\05\05\10\03\0C0\03\06\10\03\1C\CE\00\03\11/\00\03\08\A0\02*#\E2P\03\00@\03\17\12\90\00\00\10\03\16\03\80\01\0F\10\03\01\1F\11\10\03\0A:\B2\06\05\E0\02\090\06\07P\03\11\FC \0A3\07\B2\0C0\00\1F\00\10\03\03:\C8\0E\10\E0\02)\05\0A`\09H\C8\10\10\01@\00\08 \03\00p\05\0F\10\03\1DH\D8\13\11\02p\00\08\10\03\88\E2\0E\00\07\D8\12\11\02p\00\08 \03\1F\C4\10\03\00\1B\D4@\06\0B\10\03H\07\E8\15\00\A0\01\09 \03\10d\C0\05\0B \03O\E8\0C\00\03 \03\08'%\D6\10\03[\C4/\00\81\D9@\06)\E4\0D\90\00+\81\D9@\06\1B\E6@\06\08P\07\000\00\1B\E9 \03\1E\E9 \03\1B\16 \03\14\18\10\00\0F \03\04\1B\14 \03\14\15\10\00\0F \03=\11\C6\D0\02\0D\00\03\14\17p\00\1F\C6@\06\00O\A2\0A\04\FF@\06\02\03\F0\08G\A2\08\04\FF\10\02/#\E2@\06\05\1F\FC@\06\06\1F\01@\06\0C\18\01@\06F\C8\0E\10\020\00\0F0\03\00?\10\10\02@\06\19O\D8\13\11\03@\06)O\D8\12\11\03@\06\05.$\07@\06G\E2\15\00\FF\C0\00\090\03*$\0B0\03\0F@\06\00H\07\E2\00\00p\01\090\03*$\010\03\00\00\04\08 \03\10$\F0\0A\17\06\D0\00\1A\8F0\03\1B\E20\03%\E6\0E\10\03\22\06\02\90\00\090\03\00 \07S\E6\0C\00\00` \009\E4\0F\02 \03\1Bh \03\10b\E0\026\00\03\1A\E0\02d\04\10x\06\03\1B\10\00\01\00\034\0E\03\1C\10\00/\E2\1F0\03\007\05\03\19\B0\02\09\10\03;\C8\0F\01\00\03\00\80\02\15\06@\03\0D\00\03\17\8F\A0\02\12\FA \00\0A\C0\02\16\02`\03\04\F0\0F9\0A\03\1D\C0\009\08\03\1E\10\007\07\03\1F\A0\002\07\D8\16L\00\00\B0\01\02\D0\03\15\0E\B0\03\02\10\00\15\0A\A0\03\00P\03G\E8\10\05\010\00W\07\A8\0D\06\03\10\001\03x\0C\C0\01\01\01\00\02\90\0D\18\08\90\03H\0Cz\00\07\B0\00\06\C0\00\12\F4`\00H\B2\0B\0E\FF`\00&\C8\09\E0\0F\10\CA\B0\00(\04\08\B0\00H\07\E8\03\07\80\00\060\01\01\D0\10d\00\07\A8\14\05\01\D0\02\00 \000r\00\0C`\00$pRp\00%\0E\0E\F0\02\01\C0\03(\0A\0A0\00T\07\D8\08\08\02\10\00\000\01H\88\12\00\02 \00H\E8\00\07\03\10\008\A8\18\06\10\00\06\E0\00\03P\11\1A\84\B0\039$\A4\1A\10\00E%\86\12\12\90\03f\D0\0F\00$\94\11 \00\00P\03&\89\07\C0\02p\A6\00\00%\96\14\140\00\14\11\10\06!\96\10\90\03#\11\020\005\99\05\140\00v\E6\02\00%\86\16\16\B0\03\00 \00\17\06P\03`\08\00%\A6\18\18P\00\14\1A \005\89\0C\16 \00f\A6\0A\00$\B4\1C\90\00\00\00\04S\A6\1A\0D\00`0\00\00\80\06:\A9\0D\18\D0\04\02`\04\15\1C\F0\07&\1A\1A \00V\0E\00$\C4\1DP\00\01\C0\04 \14\0BP\00\14\1C\F0\03\08\C0\0Dl\A6\0E\00$\D4\1F\80\04\01\90\05\13\1D0\05H\81\B9\14\140\00c%\C6\10\09\00` \00\10\E4\90\07\19\0Bp\041%\D6\120\12\11\1F \00\00\00\0B*\10\10\90\04\090\0BA\02%\D6\08\90\12\14\1F \01\08\90\0A\010\0B)\18\00\E0\04\08\90\04\00\00\08U\E6\16\03\00` \00F\00\81\E9\190\01\01\90\04(\16\16\90\04c$v\04\FF\00b\80\00\00\C0\06H\92\02\05\06\A0\0D5\82\02\07\10\04 \C8O\90\04%\0D\1A\10\00\01\80\04\16\0F\90\07\00P\00C\05\FF\00cP\00\02\90\07(\0B\10\A0\04H\D2\02\13\08\10\11D\E2\02\19\16\10\00a\CA\0F\00\8Ey\00\90\0D@\84\E7\10\0CP\00\14M\80\15\030\15PGy\00\00\F0\A9\19\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\B0\0F\01\00-\00\\\04.\03\00\01\00\22@\00\01\00=[\01\000\00\08\01\00\1F\0B@\00\04\13\9B)\00\1F\97@\00\0C\14\13\CC\01\0B\01\00\138U\00\11\90\06\00\06p\19\22\08\00\01\00\22\18\00\01\00..\01T\00\00\01\00\13\C8@\00/p\00\80\00\0B\1F)'\00\03!\008\F5\02$\00\00\E0\1B\04\E4\00*\04\00\01\00\1Fm@\00\04\13h)\00&\AC\00@\00\1F\0A@\00\00!L\01D\01\0D@\001\18\05\00\01\00*\D8\00\01\00\1B\08\08\00?;\01\00\16\1D\003\00\00\F0@\00&\10\00\80\00\17\048\00\04\18\00\13\EB\14\01\0D\84\01!\06\00\01\00\17\901\01\0F\C0\00\01\132@\00+\06\00\01\00\1A\08`\1D\12\03\C0\01>\22\80\00g\00\17\05(!\0C\01\00*\A8\00\08\00\04\C0\00\13\018\00\0Ey\00\03x\00\1A\18\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__6_1_0___thread_tile_h32_1(%arg0: i32, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: !llvm.ptr<1>, %arg6: !llvm.ptr<1>, %arg7: !llvm.ptr<1>) attributes {disc.elimargs = [5 : index, 7 : index, 8 : index, 9 : index, 10 : index, 11 : index, 12 : index, 14 : index, 15 : index, 16 : index, 17 : index, 18 : index, 19 : index, 21 : index, 22 : index, 23 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(0 : index) : i32
      %1 = llvm.mlir.constant(1 : index) : i32
      %2 = llvm.mlir.constant(512 : index) : i32
      %3 = llvm.mlir.constant(32 : index) : i32
      %4 = llvm.mlir.constant(4 : index) : i32
      %5 = llvm.mlir.constant(0.000000e+00 : f32) : f32
      %6 = nvvm.read.ptx.sreg.ctaid.x : i32
      %7 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %8 = llvm.icmp "eq" %arg0, %1 : i32
      %9 = llvm.select %8, %arg1, %arg0 : i1, i32
      %10 = llvm.icmp "eq" %arg1, %9 : i32
      %11 = llvm.icmp "eq" %arg0, %9 : i32
      %12 = llvm.mul %6, %arg2  : i32
      %13 = llvm.add %7, %12  : i32
      %14 = llvm.icmp "ult" %13, %arg3 : i32
      llvm.cond_br %14, ^bb2, ^bb12
    ^bb2:  // pred: ^bb1
      %15 = llvm.srem %13, %2  : i32
      %16 = llvm.sdiv %13, %2  : i32
      %17 = llvm.icmp "ult" %15, %1 : i32
      llvm.cond_br %17, ^bb3, ^bb11
    ^bb3:  // pred: ^bb2
      %18 = llvm.mul %16, %3  : i32
      llvm.br ^bb4(%0, %5 : i32, f32)
    ^bb4(%19: i32, %20: f32):  // 2 preds: ^bb3, ^bb9
      %21 = llvm.icmp "slt" %19, %3 : i32
      llvm.cond_br %21, ^bb5, ^bb10
    ^bb5:  // pred: ^bb4
      %22 = llvm.add %18, %19  : i32
      %23 = llvm.icmp "slt" %22, %arg4 : i32
      llvm.cond_br %23, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %24 = llvm.urem %22, %4  : i32
      %25 = llvm.udiv %22, %4  : i32
      %26 = llvm.select %10, %25, %0 : i1, i32
      %27 = llvm.mul %26, %4  : i32
      %28 = llvm.add %27, %24  : i32
      %29 = llvm.getelementptr %arg5[%28] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %30 = llvm.load %29 : !llvm.ptr<1> -> f32
      %31 = llvm.select %11, %25, %0 : i1, i32
      %32 = llvm.mul %31, %4  : i32
      %33 = llvm.add %32, %24  : i32
      %34 = llvm.getelementptr %arg6[%33] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %35 = llvm.load %34 : !llvm.ptr<1> -> f32
      %36 = llvm.fmul %30, %35  : f32
      %37 = llvm.fadd %20, %36  : f32
      llvm.br ^bb8(%37 : f32)
    ^bb7:  // pred: ^bb5
      llvm.br ^bb8(%20 : f32)
    ^bb8(%38: f32):  // 2 preds: ^bb6, ^bb7
      llvm.br ^bb9
    ^bb9:  // pred: ^bb8
      %39 = llvm.add %19, %1  : i32
      llvm.br ^bb4(%39, %38 : i32, f32)
    ^bb10:  // pred: ^bb4
      %40 = llvm.getelementptr %arg7[%15] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %41 = llvm.atomicrmw fadd %40, %20 acq_rel : !llvm.ptr<1>, f32
      llvm.br ^bb11
    ^bb11:  // 2 preds: ^bb2, ^bb10
      llvm.br ^bb12
    ^bb12:  // 2 preds: ^bb1, ^bb11
      llvm.return
    }
  }
  gpu.module @main_kernel_5 attributes {gpu.binary = "P\EDU\BA\01\00\10\00H\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\08\04\00\00\00\00\00\00\01\04\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\0Ae__6_1_0___block_tile_h64A\00\0F;\00$oshared=\00$\9Fconstant0@\00!\FA\01debug_frame\00.rel\11\00!nv\14\00\11aL\00\0FO\01 \0F\91\00\1E\0F\81\01\DEo_param\88\01\1C\0F\01\00\05\8Cd\00\00\00\03\00\0A\00\01\00 \14\01\18\00,\09\00\01\00\11[\18\00,\04\00\01\00\11y\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\1C\00\00\00E\00#\04\10\18\00\80/\08\00\05\00\00\00\06\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\048\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C$\00u\01\00\08\00\00\F0!\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00p\00\00\00\C0\00\01\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\00#\02b,\00\00\00H\00\01\00\00 \01/\05\00\01\00\FFp@$v\01\FF\CF\03\A3\FF\00\8E\07\00\C4\0F\00\C3y\AC\03\A2%\00\00\00\22\0E\00\19y\00\01\00\F3\0E!\00\00\00b\0E\00\90r\05?\04\00\00\80?\E0\FF\0F\00\C6\1F\00\B9z\04\00\00X\9E\02p\E4\0F\00\A4r\04\05B\00\B1?\02\8E\0F\00\CC\0F\00\0C|\00\FE\03\95pR\F0\0B\00\DA/\00M#\04\F3\0D\80\03\00\EA\0F\00\02z\02\00\00Z\00\00\00\0F\00\00\00\E2\0F\00$v\03\FF\00[\90\00\22\E2\0F`\001F\00\00s\04\80\C8\0F\00\86y\00\02\FFp\030\19\10\0C \00*MyP\00PGy\00\00\F0\F9\03\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\80\0F\01\00-\00\C7\04.\03\00\01\00\22@\00\01\00=O\01\000\00\08\01\00\1F\0B@\00\04\13\8F)\00\1F\88@\00\0C\13\13\CC\03\0C\01\00\13\18U\00*\90\00\F0\03\04\0D\04\22\18\00\01\00.\22\01T\00\00\01\00\13\A8@\00/p\00\80\00\0B\1F)'\00\03A\00\18\04\00\01\00\04\00\06\04\E4\00*\04\00\01\00\1Fj@\00\04\13H)\00&L\00@\00\1F\0A@\00\00!@\01D\01\0D@\00\13\98)\00*\D8\00\01\00\1B\08\08\00?/\01\006\07\003\00\00p\1D\05\17\10\80\00\17\048\00\04\18\00\13\E2\14\01\0C\84\01*\80\05\E8\06\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\80\07\12\03\C0\01:\06\80\00\01\00\13\06\D8\01\04(\0B\0D\C8\01\1A\00\08\00\04\F8\00\13\018\00\04\A8\00\0D\01\00)\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__6_1_0___block_tile_h64(%arg0: i32, %arg1: !llvm.ptr<1>) attributes {disc.elimargs = [1 : index, 3 : index, 4 : index, 5 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(1 : index) : i32
      %1 = llvm.mlir.constant(0.000000e+00 : f32) : f32
      %2 = nvvm.read.ptx.sreg.ctaid.x : i32
      %3 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %4 = llvm.mul %2, %arg0  : i32
      %5 = llvm.add %3, %4  : i32
      %6 = llvm.icmp "ult" %5, %0 : i32
      llvm.cond_br %6, ^bb2, ^bb3
    ^bb2:  // pred: ^bb1
      %7 = llvm.getelementptr %arg1[%5] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      llvm.store %1, %7 : f32, !llvm.ptr<1>
      llvm.br ^bb3
    ^bb3:  // 2 preds: ^bb1, ^bb2
      llvm.return
    }
  }
  gpu.module @main_kernel_6 attributes {gpu.binary = "P\EDU\BA\01\00\10\00\D8\07\00\00\00\00\00\00\02\00\01\01@\00\00\00\98\07\00\00\00\00\00\00\93\07\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\A8\12\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00g\02\00\BE\00r\00\01\00\22\12\00\01\00\11\0F\06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\0Ce__6_1_0___block_tile_h64_1C\00\0F=\00&oshared?\00&\9Fconstant0B\00#\FA\01debug_frame\00.rel\11\00!nv\14\00\11aN\00\0FW\01 \0F\93\00 \0F\8B\01\A4\8F$____wg_<\00 \00\15\00/28\CD\010o_param\D4\01\1C\0F\01\00\09\8Cf\00\00\00\03\00\0A\00\01\00\11\DD\18\00,\0B\00\01\00 ^\01\18\00,\09\00\01\00\11\A7\18\00,\04\00\01\00\11\C5\18\00,\07\00\01\00\102\E3\03\17\10x\00\03#\00f\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0\06\07\00 \00\04\9B\00R\04\14\00\00\00E\002\04p\01\18\000/\08\00#\00\10\19\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\A8\04\F3\08\015\00\00\04\0A\08\00\03\00\00\00`\010\00\03\190\00\04\17\0C\CB\00U(\00\00\F0!\10\009\06\00 \10\009\05\00\18\10\00u\04\00\10\00\00\F0\11\10\009\03\00\0C\10\009\02\00\08\10\009\01\00\04\10\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\00\D0\05\00\00 \06\00\00\04\1E\94\01#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\11\02h\01\0F\01\00\FF\98@$v\01\FFw\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%s\02a\0E\00\19y\03\00\01\00\10!-\00\F0\04\0E\00$z\00\00\00Z\00\00\03\02\8E\07\00\CA\1F\00\0C\10\00\C5[\00\00p`\F0\03\00\DA\0F\00M\93\04\B0\80\03\00\EA\0F\00\19x\03\FF\1F\0F\00\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\00\EB\04Q\E2\0F\00Eyy\03\020\00a\E4\0F\00\11r\04q\00\C1\FF@\8F\07\00\C8\0F\00\12x\03\04Y\04\10\C0p\00@\0F\00$x\BC\02`\00\00\03\0A\8E\070\00c$r\03\FF\FF\00\C0\00\10\C60\00 \02\00p\00\22\FF\C0 \00P\19x\05\FF\05Q\000\16\01\00\10\00@\0Cr\00\020\001pR\F2p\00P\0Cx\00\00\7F\10\000@\F0\03\90\00@$x\02\02\D9\02\10\05\E0\00\12\C8\10\00\14\04`\00\96\CC\0F\00G\19\00\00@\03\E0\00c$v\03\FF\00X \00\10\E2p\001\04\FF\08\E3\04A\01\00\00\C8`\00$\03\01p\00u\E2\0F\04$x\04\04`\00\01\C0\00\16\05\C0\00\91\E2\0F\00\07z\03\03\00Y`\00\02@\00Uz\00\03\00X\B0\00\11\04\10\00\10Y\10\00\12\F4\B0\00\08\00\01\10\C4\B0\004\08\04@`\00\11\CA@\00\81\08\00\\\00\00pb\FA@\00@\10x\0A\08\90\00B\FF\E0\FF\07\10\006\10\08\02\10\00\000\00\12\0A0\00#\FC\03\10\00\12\10\10\00\11\F8\10\00T\10x\13\08\030\00\91\C6\0F\00\07\D2\06\08\FF\00\B0\00\11\04\E0\00H\D4\09\FF\04\D0\00#\D2\08 \00#\00\05P\00\12\13P\00\11\F6\C0\00\A3%\D6\06\06\00`\00\00\09\020\00S\E8\0C\0A\01\000\00\10\C6 \00H\08\08\00^ \006\0A\0A\01p\00P\00\81\D9\06\06p\00\BA\00\19\1E\0C\00\A4\00\00$\E4\0B\80\00S\C8\0E\10\02\00P\00u\E2\0F\04\81\D9\12\080\00\87\A2\02\00\07\C8\10\10\02P\00@%\E6\0C\0Cp\00\14\0Bp\001\B8\11\13\EF\01\03\90\00:$\C4\15`\00E\B8\07\13\03@\00\86\1F\00%\E6\0A\0A\00`@\00E\81\E9\14\0Cp\00p\E6\00\00%\C6\0E\0E`\001\15\02\8Ep\01E\81\E9\0A\0A \00\87\E4\0E\00$\B4\16\FF\04\D0\01c%\C6\08\10\00`0\00u\E2/\00\81\C9\0E\0E0\00p&\0F\00%\B6\10\11P\00\12\16P\00F\08\81\C9\08\E0\00\10$ \00D\0C\07\00` \00e\1F\00\81\B9\10\10 \00fh\0F\00\81\B9\0C\A0\00\10b\E0\014\05\05\04\E0\01\81\E2\0F\00#\D2\03\12\06\0C\07P\00\00\00\C6O\D0\02\10\05`\021pR\FA\C0\01T#\E2\03\14\0A \00\85\C8\8F\00#\C2\03\0E\08\10\00v\0F\01#\B2\03\10\0C\10\00p\02GY\00\00P\FD\D1\03\11\83@\03\14Ap\04\03P\03A\88s\00\02,\00\00\96\06f\E8\0F\00\1D{\00\01\00\84\EC\0F\00\84\89\04\02\00 \00\12\E2\C0\03\10?\90\000@\F2\03\A0\01c\84\89\05\02\00\10 \00\93$\0E\00!\82\05\04\05\00\01\00\8F\CA\1F\00\88\83\00\02\05`\00\09\1E\99`\00\14\1F \04\00`\00!\99\07\07\08\05`\00H\92\07\04\07`\00O\93\00\02\07\C0\00\0A\1A\03`\00\10r\EC\01\03\E0\03\13\C6\E0\00\18\04\C0\00J\03\03\04\00\C0\00\0F \01\099M\19\00P\01'\84yp\00\96\22\0E\00$v\04\FF\00b`\02c$v\05\FF\00c\10\00a\CA\0F\00\8Ey\00\81\05@\84\E7\10\0C\D0\02\1BM\A0\01cGy\00\00\F0\FF\C0\01f\C0\0F\00\18y\00\01\00\0F\10\00\A0\0F\01\00-\14\01\DC\02\0B\01\00\22@\00\01\00=W\01\000\00\08\01\00\1F\0B@\00\04\13\97)\00\1F\D4@\00\0C\13\13t\09\0C\01\00\13pU\00*\A8\00\98\09\22\08\00\01\00\22\18\00\01\00.*\01T\00\00\01\00\13\18u\02/p\00\80\00\0B\1F)'\00\03#\00\88@\00\04\10\0C\04\E4\00*\04\00\01\00\1Fl@\00\04\13\B8)\00&\B8\00@\00\1F\0A@\00\00!H\01D\01\0D@\00\13p\F5\03*\D8\00\01\00\1B\08\08\00?7\01\00F\0D\00Q\00\00H\06\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\E8\14\01\0C\84\01\13X@\00\17\901\01\0F\C0\00\01\132@\00\15\06R\00\0B\A1\00\16\07\80\00j\06\00\00\19\80\00\01\00\13\A9+\00+\03\00\01\00\04\DB\02\1F\04\80\00\0B\11\06\D9\06\07\E8\11\0B\01\00\1B\A8\08\00\04\97\00\13\018\00\04\E8\00\0D8\00\1A\08\08\00\088\00\18\06\A0\00\0F\01\00\05\03\A9\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00"} {
    llvm.mlir.global internal @__wg_main_kColReduction_reduce__6_1_0___block_tile_h64_1_0() {addr_space = 3 : i32} : !llvm.array<256 x f32>
    llvm.func @main_kColReduction_reduce__6_1_0___block_tile_h64_1(%arg0: i32, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: !llvm.ptr<1>, %arg6: !llvm.ptr<1>, %arg7: !llvm.ptr<1>) attributes {disc.elimargs = [5 : index, 7 : index, 8 : index, 9 : index, 10 : index, 11 : index, 12 : index, 14 : index, 15 : index, 16 : index, 17 : index, 18 : index, 19 : index, 21 : index, 22 : index, 23 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(32 : index) : i32
      %1 = llvm.mlir.constant(8 : index) : i32
      %2 = llvm.mlir.constant(64 : index) : i32
      %3 = llvm.mlir.constant(4 : index) : i32
      %4 = llvm.mlir.constant(0.000000e+00 : f32) : f32
      %5 = llvm.mlir.constant(2 : index) : i32
      %6 = llvm.mlir.constant(1 : index) : i32
      %7 = llvm.mlir.constant(256 : index) : i32
      %8 = llvm.mlir.constant(0 : index) : i32
      %9 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__6_1_0___block_tile_h64_1_0 : !llvm.ptr<3>
      %10 = llvm.getelementptr %9[0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<256 x f32>
      %11 = nvvm.read.ptx.sreg.ctaid.x : i32
      %12 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %13 = llvm.icmp "eq" %arg0, %6 : i32
      %14 = llvm.select %13, %arg1, %arg0 : i1, i32
      %15 = llvm.icmp "eq" %arg1, %14 : i32
      %16 = llvm.icmp "eq" %arg0, %14 : i32
      %17 = llvm.mul %11, %arg2  : i32
      %18 = llvm.add %12, %17  : i32
      %19 = llvm.icmp "ult" %18, %arg3 : i32
      llvm.cond_br %19, ^bb2, ^bb21
    ^bb2:  // pred: ^bb1
      %20 = llvm.srem %18, %7  : i32
      %21 = llvm.sdiv %18, %7  : i32
      %22 = llvm.udiv %20, %0  : i32
      %23 = llvm.urem %20, %0  : i32
      %24 = llvm.mul %23, %1  : i32
      %25 = llvm.add %22, %24  : i32
      %26 = llvm.icmp "ult" %23, %6 : i32
      llvm.cond_br %26, ^bb3, ^bb10(%4 : f32)
    ^bb3:  // pred: ^bb2
      %27 = llvm.mul %21, %1  : i32
      %28 = llvm.add %22, %27  : i32
      %29 = llvm.mul %28, %2  : i32
      llvm.br ^bb4(%8, %4 : i32, f32)
    ^bb4(%30: i32, %31: f32):  // 2 preds: ^bb3, ^bb9
      %32 = llvm.icmp "slt" %30, %2 : i32
      llvm.cond_br %32, ^bb5, ^bb10(%31 : f32)
    ^bb5:  // pred: ^bb4
      %33 = llvm.add %30, %29  : i32
      %34 = llvm.icmp "slt" %33, %arg4 : i32
      llvm.cond_br %34, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %35 = llvm.urem %33, %3  : i32
      %36 = llvm.udiv %33, %3  : i32
      %37 = llvm.select %15, %36, %8 : i1, i32
      %38 = llvm.mul %37, %3  : i32
      %39 = llvm.add %38, %35  : i32
      %40 = llvm.getelementptr %arg5[%39] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %41 = llvm.load %40 : !llvm.ptr<1> -> f32
      %42 = llvm.select %16, %36, %8 : i1, i32
      %43 = llvm.mul %42, %3  : i32
      %44 = llvm.add %43, %35  : i32
      %45 = llvm.getelementptr %arg6[%44] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %46 = llvm.load %45 : !llvm.ptr<1> -> f32
      %47 = llvm.fmul %41, %46  : f32
      %48 = llvm.fadd %31, %47  : f32
      llvm.br ^bb8(%48 : f32)
    ^bb7:  // pred: ^bb5
      llvm.br ^bb8(%31 : f32)
    ^bb8(%49: f32):  // 2 preds: ^bb6, ^bb7
      llvm.br ^bb9
    ^bb9:  // pred: ^bb8
      %50 = llvm.add %30, %6  : i32
      llvm.br ^bb4(%50, %49 : i32, f32)
    ^bb10(%51: f32):  // 2 preds: ^bb2, ^bb4
      llvm.br ^bb11(%51 : f32)
    ^bb11(%52: f32):  // pred: ^bb10
      llvm.br ^bb12
    ^bb12:  // pred: ^bb11
      %53 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      llvm.store %52, %53 : f32, !llvm.ptr<3>
      nvvm.barrier0
      %54 = llvm.icmp "slt" %22, %3 : i32
      llvm.cond_br %54, ^bb13, ^bb14
    ^bb13:  // pred: ^bb12
      %55 = llvm.add %25, %3  : i32
      %56 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %57 = llvm.load %56 : !llvm.ptr<3> -> f32
      %58 = llvm.getelementptr %10[%55] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %59 = llvm.load %58 : !llvm.ptr<3> -> f32
      %60 = llvm.fadd %57, %59  : f32
      %61 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      llvm.store %60, %61 : f32, !llvm.ptr<3>
      llvm.br ^bb14
    ^bb14:  // 2 preds: ^bb12, ^bb13
      nvvm.barrier0
      %62 = llvm.icmp "slt" %22, %5 : i32
      llvm.cond_br %62, ^bb15, ^bb16
    ^bb15:  // pred: ^bb14
      %63 = llvm.add %25, %5  : i32
      %64 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %65 = llvm.load %64 : !llvm.ptr<3> -> f32
      %66 = llvm.getelementptr %10[%63] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %67 = llvm.load %66 : !llvm.ptr<3> -> f32
      %68 = llvm.fadd %65, %67  : f32
      %69 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      llvm.store %68, %69 : f32, !llvm.ptr<3>
      llvm.br ^bb16
    ^bb16:  // 2 preds: ^bb14, ^bb15
      nvvm.barrier0
      %70 = llvm.icmp "slt" %22, %6 : i32
      llvm.cond_br %70, ^bb17, ^bb18
    ^bb17:  // pred: ^bb16
      %71 = llvm.add %25, %6  : i32
      %72 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %73 = llvm.load %72 : !llvm.ptr<3> -> f32
      %74 = llvm.getelementptr %10[%71] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %75 = llvm.load %74 : !llvm.ptr<3> -> f32
      %76 = llvm.fadd %73, %75  : f32
      %77 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      llvm.store %76, %77 : f32, !llvm.ptr<3>
      llvm.br ^bb18
    ^bb18:  // 2 preds: ^bb16, ^bb17
      nvvm.barrier0
      %78 = llvm.icmp "eq" %22, %8 : i32
      %79 = llvm.and %78, %26  : i1
      llvm.cond_br %79, ^bb19, ^bb20
    ^bb19:  // pred: ^bb18
      %80 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %81 = llvm.load %80 : !llvm.ptr<3> -> f32
      %82 = llvm.getelementptr %arg7[%23] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %83 = llvm.atomicrmw fadd %82, %81 acq_rel : !llvm.ptr<1>, f32
      llvm.br ^bb20
    ^bb20:  // 2 preds: ^bb18, ^bb19
      llvm.br ^bb21
    ^bb21:  // 2 preds: ^bb1, ^bb20
      llvm.return
    }
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = true, sym_name = "C4", value = 4 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = true, sym_name = "C1", value = 1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S4", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = false, sym_name = "S5", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %c4 = arith.constant 4 : index
    %0 = "disc_shape.dim"() {name = @S2} : () -> index
    %1 = "disc_shape.dim"() {name = @S3} : () -> index
    "disc_shape.tie_product_equal"(%c4, %0, %1) {operand_segment_sizes = array<i32: 2, 1>} : (index, index, index) -> ()
    %2 = "disc_shape.dim"() {name = @S4} : () -> index
    %3 = "disc_shape.dim"() {name = @S5} : () -> index
    "disc_shape.tie_product_equal"(%c4, %2, %3) {operand_segment_sizes = array<i32: 2, 1>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After ExpandStridedMetadata (expand-strided-metadata) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
  %c256 = arith.constant 256 : index
  %c32 = arith.constant 32 : index
  %c512 = arith.constant 512 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c4_i32 = arith.constant 4 : i32
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
  %2 = "disc_ral.dispatch"(%arg0, %c1) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
  %dim = memref.dim %2, %c0 : memref<?x4xf32>
  %dim_0 = memref.dim %1, %c0 : memref<?x4xf32>
  %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %reinterpret_cast_1 = memref.reinterpret_cast %2 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %3 = arith.cmpi eq, %dim, %c1 : index
  %4 = arith.select %3, %dim_0, %dim : index
  %alloc = memref.alloc(%dim_0) {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %5 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast, %alloc) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  %alloc_2 = memref.alloc(%dim) {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_1, %alloc_2) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  %6 = arith.index_cast %4 : index to i32
  %7 = arith.muli %6, %c4_i32 : i32
  %8 = arith.index_cast %7 : i32 to index
  %9 = arith.cmpi eq, %dim_0, %4 : index
  %10 = arith.cmpi eq, %dim, %4 : index
  %11 = arith.andi %10, %9 : i1
  %alloc_3 = memref.alloc() : memref<1xf32, #gpu.address_space<global>>
  cf.cond_br %11, ^bb1, ^bb5
^bb1:  // pred: ^bb0
  %12 = arith.cmpi slt, %8, %c1 : index
  cf.cond_br %12, ^bb2, ^bb3
^bb2:  // pred: ^bb1
  gpu.launch_func  @main_kernel::@main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32 blocks in (%c1, %c1, %c1) threads in (%c512, %c1, %c1) args(%c512 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
  %13 = arith.cmpi eq, %8, %c0 : index
  %14 = arith.subi %8, %c1 : index
  %15 = arith.divui %14, %c32 : index
  %16 = arith.addi %15, %c1 : index
  %17 = arith.select %13, %c0, %16 : index
  %18 = arith.muli %17, %c512 : index
  %19 = affine.apply affine_map<(d0) -> (d0 ceildiv 512)>(%18)
  gpu.launch_func  @main_kernel_0::@main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_1 blocks in (%19, %c1, %c1) threads in (%c512, %c1, %c1) args(%c512 : index, %18 : index, %8 : index, %4 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
  cf.br ^bb4
^bb3:  // pred: ^bb1
  gpu.launch_func  @main_kernel_1::@main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64 blocks in (%c1, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
  %20 = arith.cmpi eq, %8, %c0 : index
  %21 = arith.subi %8, %c1 : index
  %22 = arith.divui %21, %c512 : index
  %23 = arith.addi %22, %c1 : index
  %24 = arith.select %20, %c0, %23 : index
  %25 = arith.muli %24, %c256 : index
  %26 = affine.apply affine_map<(d0) -> (d0 ceildiv 256)>(%25)
  gpu.launch_func  @main_kernel_2::@main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1 blocks in (%26, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %25 : index, %8 : index, %4 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
  cf.br ^bb4
^bb4:  // 4 preds: ^bb2, ^bb3, ^bb6, ^bb7
  cf.br ^bb8
^bb5:  // pred: ^bb0
  %27 = arith.cmpi slt, %8, %c1 : index
  cf.cond_br %27, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  gpu.launch_func  @main_kernel_3::@main_kColReduction_reduce__6_1_0___thread_tile_h32 blocks in (%c1, %c1, %c1) threads in (%c512, %c1, %c1) args(%c512 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
  %28 = arith.cmpi eq, %8, %c0 : index
  %29 = arith.subi %8, %c1 : index
  %30 = arith.divui %29, %c32 : index
  %31 = arith.addi %30, %c1 : index
  %32 = arith.select %28, %c0, %31 : index
  %33 = arith.muli %32, %c512 : index
  %34 = affine.apply affine_map<(d0) -> (d0 ceildiv 512)>(%33)
  gpu.launch_func  @main_kernel_4::@main_kColReduction_reduce__6_1_0___thread_tile_h32_1 blocks in (%34, %c1, %c1) threads in (%c512, %c1, %c1) args(%dim : index, %dim_0 : index, %c512 : index, %33 : index, %8 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
  cf.br ^bb4
^bb7:  // pred: ^bb5
  gpu.launch_func  @main_kernel_5::@main_kColReduction_reduce__6_1_0___block_tile_h64 blocks in (%c1, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
  %35 = arith.cmpi eq, %8, %c0 : index
  %36 = arith.subi %8, %c1 : index
  %37 = arith.divui %36, %c512 : index
  %38 = arith.addi %37, %c1 : index
  %39 = arith.select %35, %c0, %38 : index
  %40 = arith.muli %39, %c256 : index
  %41 = affine.apply affine_map<(d0) -> (d0 ceildiv 256)>(%40)
  gpu.launch_func  @main_kernel_6::@main_kColReduction_reduce__6_1_0___block_tile_h64_1 blocks in (%41, %c1, %c1) threads in (%c256, %c1, %c1) args(%dim : index, %dim_0 : index, %c256 : index, %40 : index, %8 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
  cf.br ^bb4
^bb8:  // pred: ^bb4
  memref.dealloc %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc : memref<?x4xf32, #gpu.address_space<global>>
  %alloca = memref.alloca() : memref<0xindex>
  %42 = "disc_ral.dispatch"(%arg0, %5, %alloc_3, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1xf32, #gpu.address_space<global>>, memref<0xindex>) -> memref<f32, #gpu.address_space<global>>
  %reinterpret_cast_4 = memref.reinterpret_cast %42 to offset: [0], sizes: [], strides: [] : memref<f32, #gpu.address_space<global>> to memref<f32, #gpu.address_space<global>>
  memref.dealloc %alloc_3 : memref<1xf32, #gpu.address_space<global>>
  %alloc_5 = memref.alloc() : memref<f32>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_4, %alloc_5) {backend_config = "", call_target_name = "d2h", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<f32, #gpu.address_space<global>>, memref<f32>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  memref.dealloc %reinterpret_cast_4 : memref<f32, #gpu.address_space<global>>
  "disc_ral.dispatch"(%arg0, %c0, %alloc_5) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<f32>) -> ()
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
  %c256 = arith.constant 256 : index
  %c32 = arith.constant 32 : index
  %c512 = arith.constant 512 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c4_i32 = arith.constant 4 : i32
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
  %2 = "disc_ral.dispatch"(%arg0, %c1) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
  %dim = memref.dim %2, %c0 : memref<?x4xf32>
  %dim_0 = memref.dim %1, %c0 : memref<?x4xf32>
  %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %reinterpret_cast_1 = memref.reinterpret_cast %2 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
  %3 = arith.cmpi eq, %dim, %c1 : index
  %4 = arith.select %3, %dim_0, %dim : index
  %alloc = memref.alloc(%dim_0) {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  %5 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast, %alloc) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  %alloc_2 = memref.alloc(%dim) {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_1, %alloc_2) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  %6 = arith.index_cast %4 : index to i32
  %7 = arith.muli %6, %c4_i32 : i32
  %8 = arith.index_cast %7 : i32 to index
  %9 = arith.cmpi eq, %dim_0, %4 : index
  %10 = arith.cmpi eq, %dim, %4 : index
  %11 = arith.andi %10, %9 : i1
  %alloc_3 = memref.alloc() : memref<1xf32, #gpu.address_space<global>>
  cf.cond_br %11, ^bb1, ^bb4
^bb1:  // pred: ^bb0
  %12 = arith.cmpi slt, %8, %c1 : index
  cf.cond_br %12, ^bb2, ^bb3
^bb2:  // pred: ^bb1
  gpu.launch_func  @main_kernel::@main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32 blocks in (%c1, %c1, %c1) threads in (%c512, %c1, %c1) args(%c512 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
  %13 = arith.cmpi eq, %8, %c0 : index
  %14 = arith.subi %8, %c1 : index
  %15 = arith.divui %14, %c32 : index
  %16 = arith.addi %15, %c1 : index
  %17 = arith.select %13, %c0, %16 : index
  %18 = arith.muli %17, %c512 : index
  %19 = affine.apply affine_map<()[s0] -> (s0 ceildiv 512)>()[%18]
  gpu.launch_func  @main_kernel_0::@main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_1 blocks in (%19, %c1, %c1) threads in (%c512, %c1, %c1) args(%c512 : index, %18 : index, %8 : index, %4 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
  cf.br ^bb7
^bb3:  // pred: ^bb1
  gpu.launch_func  @main_kernel_1::@main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64 blocks in (%c1, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
  %20 = arith.cmpi eq, %8, %c0 : index
  %21 = arith.subi %8, %c1 : index
  %22 = arith.divui %21, %c512 : index
  %23 = arith.addi %22, %c1 : index
  %24 = arith.select %20, %c0, %23 : index
  %25 = arith.muli %24, %c256 : index
  %26 = affine.apply affine_map<()[s0] -> (s0 ceildiv 256)>()[%25]
  gpu.launch_func  @main_kernel_2::@main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1 blocks in (%26, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %25 : index, %8 : index, %4 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
  cf.br ^bb7
^bb4:  // pred: ^bb0
  %27 = arith.cmpi slt, %8, %c1 : index
  cf.cond_br %27, ^bb5, ^bb6
^bb5:  // pred: ^bb4
  gpu.launch_func  @main_kernel_3::@main_kColReduction_reduce__6_1_0___thread_tile_h32 blocks in (%c1, %c1, %c1) threads in (%c512, %c1, %c1) args(%c512 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
  %28 = arith.cmpi eq, %8, %c0 : index
  %29 = arith.subi %8, %c1 : index
  %30 = arith.divui %29, %c32 : index
  %31 = arith.addi %30, %c1 : index
  %32 = arith.select %28, %c0, %31 : index
  %33 = arith.muli %32, %c512 : index
  %34 = affine.apply affine_map<()[s0] -> (s0 ceildiv 512)>()[%33]
  gpu.launch_func  @main_kernel_4::@main_kColReduction_reduce__6_1_0___thread_tile_h32_1 blocks in (%34, %c1, %c1) threads in (%c512, %c1, %c1) args(%dim : index, %dim_0 : index, %c512 : index, %33 : index, %8 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
  cf.br ^bb7
^bb6:  // pred: ^bb4
  gpu.launch_func  @main_kernel_5::@main_kColReduction_reduce__6_1_0___block_tile_h64 blocks in (%c1, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
  %35 = arith.cmpi eq, %8, %c0 : index
  %36 = arith.subi %8, %c1 : index
  %37 = arith.divui %36, %c512 : index
  %38 = arith.addi %37, %c1 : index
  %39 = arith.select %35, %c0, %38 : index
  %40 = arith.muli %39, %c256 : index
  %41 = affine.apply affine_map<()[s0] -> (s0 ceildiv 256)>()[%40]
  gpu.launch_func  @main_kernel_6::@main_kColReduction_reduce__6_1_0___block_tile_h64_1 blocks in (%41, %c1, %c1) threads in (%c256, %c1, %c1) args(%dim : index, %dim_0 : index, %c256 : index, %40 : index, %8 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
  cf.br ^bb7
^bb7:  // 4 preds: ^bb2, ^bb3, ^bb5, ^bb6
  memref.dealloc %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>
  memref.dealloc %alloc : memref<?x4xf32, #gpu.address_space<global>>
  %alloca = memref.alloca() : memref<0xindex>
  %42 = "disc_ral.dispatch"(%arg0, %5, %alloc_3, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1xf32, #gpu.address_space<global>>, memref<0xindex>) -> memref<f32, #gpu.address_space<global>>
  %reinterpret_cast_4 = memref.reinterpret_cast %42 to offset: [0], sizes: [], strides: [] : memref<f32, #gpu.address_space<global>> to memref<f32, #gpu.address_space<global>>
  memref.dealloc %alloc_3 : memref<1xf32, #gpu.address_space<global>>
  %alloc_5 = memref.alloc() : memref<f32>
  "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_4, %alloc_5) {backend_config = "", call_target_name = "d2h", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<f32, #gpu.address_space<global>>, memref<f32>) -> ()
  "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
  memref.dealloc %reinterpret_cast_4 : memref<f32, #gpu.address_space<global>>
  "disc_ral.dispatch"(%arg0, %c0, %alloc_5) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<f32>) -> ()
  return
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
module attributes {gpu.container_module} {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
    %c256 = arith.constant 256 : index
    %c32 = arith.constant 32 : index
    %c512 = arith.constant 512 : index
    %0 = llvm.mlir.constant(0 : i32) : i32
    %c4_i32 = arith.constant 4 : i32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
    %2 = "disc_ral.dispatch"(%arg0, %c1) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
    %dim = memref.dim %2, %c0 : memref<?x4xf32>
    %dim_0 = memref.dim %1, %c0 : memref<?x4xf32>
    %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
    %reinterpret_cast_1 = memref.reinterpret_cast %2 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
    %3 = arith.cmpi eq, %dim, %c1 : index
    %4 = arith.select %3, %dim_0, %dim : index
    %alloc = memref.alloc(%dim_0) {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
    %5 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
    "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast, %alloc) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
    "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
    %alloc_2 = memref.alloc(%dim) {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
    "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_1, %alloc_2) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
    "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
    %6 = arith.index_cast %4 : index to i32
    %7 = arith.muli %6, %c4_i32 : i32
    %8 = arith.index_cast %7 : i32 to index
    %9 = arith.cmpi eq, %dim_0, %4 : index
    %10 = arith.cmpi eq, %dim, %4 : index
    %11 = arith.andi %10, %9 : i1
    %alloc_3 = memref.alloc() : memref<1xf32, #gpu.address_space<global>>
    cf.cond_br %11, ^bb1, ^bb4
  ^bb1:  // pred: ^bb0
    %12 = arith.cmpi slt, %8, %c1 : index
    cf.cond_br %12, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    gpu.launch_func  @main_kernel::@main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32 blocks in (%c1, %c1, %c1) threads in (%c512, %c1, %c1) args(%c512 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    %13 = arith.cmpi eq, %8, %c0 : index
    %14 = arith.subi %8, %c1 : index
    %15 = arith.divui %14, %c32 : index
    %16 = arith.addi %15, %c1 : index
    %17 = arith.select %13, %c0, %16 : index
    %18 = arith.muli %17, %c512 : index
    %c512_4 = arith.constant 512 : index
    %c0_5 = arith.constant 0 : index
    %c1_6 = arith.constant 1 : index
    %19 = arith.cmpi sle, %18, %c0_5 : index
    %20 = arith.subi %c0_5, %18 : index
    %21 = arith.subi %18, %c1_6 : index
    %22 = arith.select %19, %20, %21 : index
    %23 = arith.divsi %22, %c512_4 : index
    %24 = arith.subi %c0_5, %23 : index
    %25 = arith.addi %23, %c1_6 : index
    %26 = arith.select %19, %24, %25 : index
    gpu.launch_func  @main_kernel_0::@main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_1 blocks in (%26, %c1, %c1) threads in (%c512, %c1, %c1) args(%c512 : index, %18 : index, %8 : index, %4 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    cf.br ^bb7
  ^bb3:  // pred: ^bb1
    gpu.launch_func  @main_kernel_1::@main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64 blocks in (%c1, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    %27 = arith.cmpi eq, %8, %c0 : index
    %28 = arith.subi %8, %c1 : index
    %29 = arith.divui %28, %c512 : index
    %30 = arith.addi %29, %c1 : index
    %31 = arith.select %27, %c0, %30 : index
    %32 = arith.muli %31, %c256 : index
    %c256_7 = arith.constant 256 : index
    %c0_8 = arith.constant 0 : index
    %c1_9 = arith.constant 1 : index
    %33 = arith.cmpi sle, %32, %c0_8 : index
    %34 = arith.subi %c0_8, %32 : index
    %35 = arith.subi %32, %c1_9 : index
    %36 = arith.select %33, %34, %35 : index
    %37 = arith.divsi %36, %c256_7 : index
    %38 = arith.subi %c0_8, %37 : index
    %39 = arith.addi %37, %c1_9 : index
    %40 = arith.select %33, %38, %39 : index
    gpu.launch_func  @main_kernel_2::@main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1 blocks in (%40, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %32 : index, %8 : index, %4 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    cf.br ^bb7
  ^bb4:  // pred: ^bb0
    %41 = arith.cmpi slt, %8, %c1 : index
    cf.cond_br %41, ^bb5, ^bb6
  ^bb5:  // pred: ^bb4
    gpu.launch_func  @main_kernel_3::@main_kColReduction_reduce__6_1_0___thread_tile_h32 blocks in (%c1, %c1, %c1) threads in (%c512, %c1, %c1) args(%c512 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    %42 = arith.cmpi eq, %8, %c0 : index
    %43 = arith.subi %8, %c1 : index
    %44 = arith.divui %43, %c32 : index
    %45 = arith.addi %44, %c1 : index
    %46 = arith.select %42, %c0, %45 : index
    %47 = arith.muli %46, %c512 : index
    %c512_10 = arith.constant 512 : index
    %c0_11 = arith.constant 0 : index
    %c1_12 = arith.constant 1 : index
    %48 = arith.cmpi sle, %47, %c0_11 : index
    %49 = arith.subi %c0_11, %47 : index
    %50 = arith.subi %47, %c1_12 : index
    %51 = arith.select %48, %49, %50 : index
    %52 = arith.divsi %51, %c512_10 : index
    %53 = arith.subi %c0_11, %52 : index
    %54 = arith.addi %52, %c1_12 : index
    %55 = arith.select %48, %53, %54 : index
    gpu.launch_func  @main_kernel_4::@main_kColReduction_reduce__6_1_0___thread_tile_h32_1 blocks in (%55, %c1, %c1) threads in (%c512, %c1, %c1) args(%dim : index, %dim_0 : index, %c512 : index, %47 : index, %8 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    cf.br ^bb7
  ^bb6:  // pred: ^bb4
    gpu.launch_func  @main_kernel_5::@main_kColReduction_reduce__6_1_0___block_tile_h64 blocks in (%c1, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    %56 = arith.cmpi eq, %8, %c0 : index
    %57 = arith.subi %8, %c1 : index
    %58 = arith.divui %57, %c512 : index
    %59 = arith.addi %58, %c1 : index
    %60 = arith.select %56, %c0, %59 : index
    %61 = arith.muli %60, %c256 : index
    %c256_13 = arith.constant 256 : index
    %c0_14 = arith.constant 0 : index
    %c1_15 = arith.constant 1 : index
    %62 = arith.cmpi sle, %61, %c0_14 : index
    %63 = arith.subi %c0_14, %61 : index
    %64 = arith.subi %61, %c1_15 : index
    %65 = arith.select %62, %63, %64 : index
    %66 = arith.divsi %65, %c256_13 : index
    %67 = arith.subi %c0_14, %66 : index
    %68 = arith.addi %66, %c1_15 : index
    %69 = arith.select %62, %67, %68 : index
    gpu.launch_func  @main_kernel_6::@main_kColReduction_reduce__6_1_0___block_tile_h64_1 blocks in (%69, %c1, %c1) threads in (%c256, %c1, %c1) args(%dim : index, %dim_0 : index, %c256 : index, %61 : index, %8 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    cf.br ^bb7
  ^bb7:  // 4 preds: ^bb2, ^bb3, ^bb5, ^bb6
    memref.dealloc %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>
    memref.dealloc %alloc : memref<?x4xf32, #gpu.address_space<global>>
    %alloca = memref.alloca() : memref<0xindex>
    %70 = "disc_ral.dispatch"(%arg0, %5, %alloc_3, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1xf32, #gpu.address_space<global>>, memref<0xindex>) -> memref<f32, #gpu.address_space<global>>
    %reinterpret_cast_16 = memref.reinterpret_cast %70 to offset: [0], sizes: [], strides: [] : memref<f32, #gpu.address_space<global>> to memref<f32, #gpu.address_space<global>>
    memref.dealloc %alloc_3 : memref<1xf32, #gpu.address_space<global>>
    %alloc_17 = memref.alloc() : memref<f32>
    "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_16, %alloc_17) {backend_config = "", call_target_name = "d2h", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<f32, #gpu.address_space<global>>, memref<f32>) -> ()
    "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
    memref.dealloc %reinterpret_cast_16 : memref<f32, #gpu.address_space<global>>
    "disc_ral.dispatch"(%arg0, %c0, %alloc_17) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<f32>) -> ()
    return
  }
  gpu.module @main_kernel attributes {gpu.binary = "P\EDU\BA\01\00\10\00H\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\08\04\00\00\00\00\00\00\08\04\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00h\0C\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\002\C0\0B\00\01\00\11\09\06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\11e__6_1_0___no_ibXthread_tile_h32H\00\0FB\00+osharedD\00+\9Fconstant0G\00(\FA\01debug_frame\00.rel\11\00!nv\14\00\11aS\00\0Fk\01 \0F\98\00%\0F\A4\01\FAo_param\AB\01\1C\0F\01\00\06\8Ck\00\00\00\03\00\0A\00\01\00 0\01\18\00,\09\00\01\00\11~\18\00,\04\00\01\00\11\9C\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\1C\00\00\00E\00#\04\10\18\00\80/\08\00\05\00\00\00\06\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04x\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C$\00u\01\00\08\00\00\F0!\10\00\01\01\00\C0\F0\11\00\03\1B\FF\00\04\1C\08\00p\B0\04\12\00\01\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00 \01/\05\00\01\00\FF\B0@$v\01\FF\0F\04\A3\FF\00\8E\07\00\C4\0F\00\C3y\EC\03\A2%\00\00\00\22\0E\00\19y\00\01\00\F3\0E!\00\00\00b\0E\00\90r\05?\04\00\00\80?\E0\FF\0F\00\C6\1F\00\B9z\04\00\00X\DE\02p\E4\0F\00\A4r\04\05B\00\B1?\02\8E\0F\00\CC\0F\00\0C|\00>\04\95pR\F0\0B\00\DA/\00Mc\04\F3\0D\80\03\00\EA\0F\00\02z\02\00\00Z\00\00\00\0F\00\00\00\E2\0F\00$v\03\FF\00[\90\00\22\E2\0F`\001F\00\00\B3\04\80\C8\0F\00\86y\00\02\FF\B0\030\19\10\0C \00*MyP\00PGy\00\00\F09\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\80\0F\01\00-\00\07\05.\03\00\01\00\22@\00\01\00=k\01\000\00\08\01\00\1F\0B@\00\04*\AB\01\08\00\0F@\00\05\13\13\0C\04\0C\01\00\13XU\00*\90\000\04\04M\04\22\18\00\01\00.>\01T\00\00\01\00\13\E8@\00/p\00\80\00\0B\1F)'\00\03A\00X\04\00\01\00\04@\06\04\E4\00*\04\00\01\00\1Fq@\00\04\13\88)\00&L\00@\00\1F\0A@\00\00!\\\01D\01\0D@\00\13\D8)\00*\D8\00\01\00\1B\08\08\00%K\01\DB\0A\09\01\00\13\B0]\05\17\10\80\00\17\048\00\04\18\00\13\F7\14\01\0C\84\01*\C0\05(\07\1F\00\C0\00\04\132@\00*\06\00\01\00*\80\07\C0\07\12\03\C0\01:\06\80\00\01\00\13\06\D8\01\05\A8\0B\0B\01\00*\A8\00\08\00\04\F8\00\14\018\00/\05\00\01\00\029@\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32(%arg0: i32, %arg1: !llvm.ptr<1>) attributes {disc.elimargs = [1 : index, 3 : index, 4 : index, 5 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(1 : index) : i32
      %1 = llvm.mlir.constant(0.000000e+00 : f32) : f32
      %2 = nvvm.read.ptx.sreg.ctaid.x : i32
      %3 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %4 = llvm.mul %2, %arg0  : i32
      %5 = llvm.add %3, %4  : i32
      %6 = llvm.icmp "ult" %5, %0 : i32
      llvm.cond_br %6, ^bb2, ^bb3
    ^bb2:  // pred: ^bb1
      %7 = llvm.getelementptr %arg1[%5] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      llvm.store %1, %7 : f32, !llvm.ptr<1>
      llvm.br ^bb3
    ^bb3:  // 2 preds: ^bb1, ^bb2
      llvm.return
    }
  }
  gpu.module @main_kernel_0 attributes {gpu.binary = "P\EDU\BA\01\00\10\00\00\09\00\00\00\00\00\00\02\00\01\01@\00\00\00\C0\08\00\00\00\00\00\00\BF\08\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00h\17\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\002\C0\16\00\01\00\11\14\06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\13e__6_1_0___no_ibXthread_tile_h32_1J\00\0FD\00-osharedF\00-\9Fconstant0I\00*\FA\01debug_frame\00.rel\11\00!nv\14\00\11aU\00\0Fs\01 \0F\9A\00'\0F\AE\01\FF\03o_param\B5\01\1C\0F\01\00\04\8Cm\00\00\00\03\00\0A\00\01\00 8\01\18\00,\09\00\01\00\11\88\18\00,\04\00\01\00\11\A6\18\00,\07\00\01\00g2\00\00\00\12\10`\00\11\0C\06\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0\0B\07\00 \00\04\9B\00 \04\18i\00\01:\002\04\B8\02\18\00p/\08\00\05\00\00\00\1A\00\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\88\04\80\015\00\00\04\0A\08\00F\00\A2`\01(\00\03\19(\00\04\17\C5\00u\05\00 \00\00\F0!\10\009\04\00\18\10\009\03\00\10\10\00u\02\00\08\00\00\F0\11\10\009\01\00\04\10\00\01\01\00\F3\01\F0\11\00\03\1B\FF\00\04\1C\08\00`\00\00\00P\D8\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\00\D2\05b,\00\00\00H\00\01\00\00`\01/\05\00\01\00\FF\E0@$v\01\FF\7F\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\BB\02Q\0E\00\19y\03\0F\00 \00!-\00\F0\14\0E\00$z\00\00\00X\00\00\03\02\8E\07\00\CA\1F\00\12x\FF\00\FF\01\00\00\FF\C0\80\07\00\C8\0F\00\0C \00\C5Y\00\00pdp\00\00\DA\0F\00M\C3\04\B0\80\03\00\EA\0F\00\19x\03\FF\1F\0F\00\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\00\03\05a\C6\0F\00\11r\03q\001\FFH\8FP\00\000\00\00C\00\10\030\00\93\CA\0F\00$x\07\03 \00\B0\00p\C8\0F\00%x\04\07s\04\10\FF\90\00\F0\09\E2\0F\04\0Cz\00\07\00Z\00\00pb\F4\03\00\E4\0F\04\10x\00\07\01 \00\B0\E0\FF\07\00\E4\0F\00\12x\04\04\01\031\FF\FC\8E\10\00\01\B0\00\010\00\10\F60\00p\00\10z\02\04\00\\0\00!\F1\07@\00Pz\04\04\00^\10\00\11\F3 \01\00P\00\00,\03\04P\00\C2\10z\03\05\00]\00\00\FF\E4\7F\000\00@\05\05\00_\10\00*\FF\00`\00 \F8\03\F0\00\F4\06\81\B9\0B\02\04\FC\FF\FF\00\19\1E\0C\00\A2\0E\00\10x\06\07\03P\00\93\C6\0F\00\81\A9\00\02\04\F8 \00u\E8\0E\00\81\A9\09\04\10\00 \E2\0EP\00\12\06P\00!\FA\030\005\B9\08\04P\00p\A8\0E\00\81\C9\0D\02\10\01\01\10\00y(\0F\00\81\C9\0A\04\10\00S\D9\0F\02\04\04\10\00\10h\10\00%\0C\04\10\00!b\0F\90\00\14\04\90\00\01\F0\006\0E\07\05\F0\00\16\04\80\00\12\FC0\01Jx\10\07\06 \00\12\0E \00\13\F0 \00:\0E\07\07 \00\12\10 \00\11\F2\10\01T$r\06\FF\FF\D0\01\12\E2P\00\14\0A0\00a\E2\0F\04#\A2\06\06\07\00$\00F\00\E2\8F\00`\00\12\F4P\01S\E9\09\02\04\08\C0\00!\E2\0Ep\00\18\08P\018\E9\00\04 \00@#\B2\06\0B\0F\00\11\06P\00\17OP\00\02\00\02P\81\89\0B\02\04\0F\06\05\A0\018\0E\07\09P\008\89\08\04 \00T#\C2\06\0D\0AP\00\93\C6\0F\01\81\99\0D\02\04\10 \00'\22\0F`\00\12\F8\B0\01W\99\0A\04\04\10\90\01X\A9\11\02\04\14\10\00F\0E\04\04\14@\00U#\D2\06\0F\0C\B0\00&\0F\02@\01 \FA\03\A0\01g\81\B9\0F\02\04\18\D0\019\B9\0C\04\10\00X\C9\13\02\04\1C\10\00)\10\04\10\00X\D9\15\02\04 \10\00H\12\04\04 \10\026\14\07\0B\90\01e\00#\E2\06\09\00\90\00\11\8F\10\03\17\0C0\02E\0Cz\00\14 \02\000\00\1D\82p\01\1A\00 \027\00\07\0D \02X\10x\08\07\0Ep\00\17\92`\016\E2\0F\01@\00C\F2\03\00\CA\00\025$\00\00\00\03\00\F0\01\17$\A0\01F\A2\06\11\0E@\00\00\80\00\17\08 \04\02\F0\01\18(@\028\08\07\0F\F0\01\00\D0\01\17,\90\01\1D\B2\90\01\19\08@\02\00\E0\01\070\00\\\10x\0C\07\10@\02\07p\00Z#\C2\06\13\10\A0\00\15\0C0\02\00P\00X\A9\0F\02\040\90\018\10\07\11P\008\A9\0C\04 \00Z#\D2\06\15\12P\00\060\02\00P\00W\B9\11\02\044\F0\01[\B9\0E\04\0440\02\1B80\02\1B80\02\1B<0\02\1A<0\02\1F\120\02\06\11O\F0\01\1F\130\02\16\1A\8F0\02\13\D6\F0\01\18@\A0\03?\00\07\140\02\08\00\D0\01\16\15`\02\19\00@\02\000\00\1F\A2\D0\01\05\11\F4 \00\01P\02\09p\009\08\07\16\B0\027\0A\07\17\80\00\01P\02\19Dp\04\0F\80\02\04\12\F6\C0\03\00\10\02\040\00\00\F0\05\00p\02\17Hp\02\0E \02\19\0A \02\00p\02\070\00\00\B0\04\19\18 \02O\0F\02\04L \02\0A\19\0E \02\00P\02\17L\E0\01\000\02\1BP0\02\1BP0\02\1BT0\02\1BT0\02\1BX0\02\1AX0\02\1F\190\02\05*\C6O \02\030\04\18\1A0\00\0B0\02\00\A0\01\14\1B \00\13\D20\02\16\\\C0\01\0FP\04\0A\03`\00\1A\1C\B0\06\1A\08\B0\048\08\07\1D\80\00\0E@\02\15\00@\02\13\C4@\02\18\\@\028\0A\07\1E\A0\01_\99\0B\02\04`0\02\14\01\F0\01(\08\040\00\00`\04\18\1FP\00_\89\07\02\04d@\02\18X\89\0A\04\04d\80\06O\0D\02\04h0\02\0A\15\0C0\02\13\C40\02\17h\E0\01\00\90\06\1Bl0\02\1Bl \02\18p\10\00K\11\02\04p \02\18t\10\00%\13\02\10\00\1Bb \02 \C8O\D0\01\06\00\02 \C8\8F\10\02%\07\0A\10\00v\0F\01#\A2\06\0D\0C\10\00\10\02\C0\05\08`\01c$v\08\FF\00`\80\08\10\E4\10\00C\09\FF\00a\10\00\11\E2@\01&\11\10@\00\00\10\01\15\13\10\01p\CA\0F\00\8Ey\00\08\0C\00@\84\E7\10\0C0\00*My\F0\0APGy\00\00\F09\0F\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00p\0F\01\00-\11\01h\0F\0E\01\00\22@\00\01\00=s\01\000\00\08\01\00\1F\0B@\00\04\13\B3)\00\1F\B5@\00\0C\13\13\BC\0E\0C\01\00\13hU\00*\90\00\E0\0E\22\08\00\01\00\22\18\00\01\00.F\01T\00\00\01\00\13\F8@\00/p\00\80\00\0B\1F)'\00\03A\00h\04\00\01\00\00\9B\07\17\00\E4\00*\04\00\01\00\1Fs@\00\04\13\98)\00&\8C\00@\00\1F\0A@\00\00!d\01D\01\0D@\001(\05\00\01\00*\D8\00\01\00\1B\08\08\00?S\01\00f\12\00\04\E1\02\10\00B\11\05\80\00\17\048\00\04\18\00\13\FD\14\01\0C\84\01&\10\06\B0\12\04\01\00\0F\C0\00\01\132@\00+\06\00\01\00\1A\08\B0\12\12\03\C0\01>\18\80\00\A7\00\18\05\A8\16\0B\01\00*\A8\00\08\00\04W\00\13\018\00\04\A8\00\0D\D0\12)\0D\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_1(%arg0: i32, %arg1: i32, %arg2: i32, %arg3: !llvm.ptr<1>, %arg4: !llvm.ptr<1>, %arg5: !llvm.ptr<1>) attributes {disc.elimargs = [3 : index, 4 : index, 6 : index, 7 : index, 8 : index, 9 : index, 10 : index, 11 : index, 13 : index, 14 : index, 15 : index, 16 : index, 17 : index, 18 : index, 20 : index, 21 : index, 22 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(0 : index) : i32
      %1 = llvm.mlir.constant(1 : index) : i32
      %2 = llvm.mlir.constant(512 : index) : i32
      %3 = llvm.mlir.constant(32 : index) : i32
      %4 = llvm.mlir.constant(0.000000e+00 : f32) : f32
      %5 = nvvm.read.ptx.sreg.ctaid.x : i32
      %6 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %7 = llvm.mul %5, %arg0  : i32
      %8 = llvm.add %6, %7  : i32
      %9 = llvm.icmp "ult" %8, %arg1 : i32
      llvm.cond_br %9, ^bb2, ^bb12
    ^bb2:  // pred: ^bb1
      %10 = llvm.srem %8, %2  : i32
      %11 = llvm.sdiv %8, %2  : i32
      %12 = llvm.icmp "ult" %10, %1 : i32
      llvm.cond_br %12, ^bb3, ^bb11
    ^bb3:  // pred: ^bb2
      %13 = llvm.mul %11, %3  : i32
      llvm.br ^bb4(%0, %4 : i32, f32)
    ^bb4(%14: i32, %15: f32):  // 2 preds: ^bb3, ^bb9
      %16 = llvm.icmp "slt" %14, %3 : i32
      llvm.cond_br %16, ^bb5, ^bb10
    ^bb5:  // pred: ^bb4
      %17 = llvm.add %13, %14  : i32
      %18 = llvm.icmp "slt" %17, %arg2 : i32
      llvm.cond_br %18, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %19 = llvm.getelementptr %arg3[%17] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %20 = llvm.load %19 : !llvm.ptr<1> -> f32
      %21 = llvm.getelementptr %arg4[%17] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %22 = llvm.load %21 : !llvm.ptr<1> -> f32
      %23 = llvm.fmul %20, %22  : f32
      %24 = llvm.fadd %15, %23  : f32
      llvm.br ^bb8(%24 : f32)
    ^bb7:  // pred: ^bb5
      llvm.br ^bb8(%15 : f32)
    ^bb8(%25: f32):  // 2 preds: ^bb6, ^bb7
      llvm.br ^bb9
    ^bb9:  // pred: ^bb8
      %26 = llvm.add %14, %1  : i32
      llvm.br ^bb4(%26, %25 : i32, f32)
    ^bb10:  // pred: ^bb4
      %27 = llvm.getelementptr %arg5[%10] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %28 = llvm.atomicrmw fadd %27, %15 acq_rel : !llvm.ptr<1>, f32
      llvm.br ^bb11
    ^bb11:  // 2 preds: ^bb2, ^bb10
      llvm.br ^bb12
    ^bb12:  // 2 preds: ^bb1, ^bb11
      llvm.return
    }
  }
  gpu.module @main_kernel_1 attributes {gpu.binary = "P\EDU\BA\01\00\10\00H\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\08\04\00\00\00\00\00\00\06\04\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00h\0C\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\002\C0\0B\00\01\00\11\09\06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\10e__6_1_0___no_ibXblock_tile_h64G\00\0FA\00*osharedC\00*\9Fconstant0F\00'\FA\01debug_frame\00.rel\11\00!nv\14\00\11aR\00\0Fg\01 \0F\97\00$\0F\9F\01\F6o_param\A6\01\1C\0F\01\00\07\8Cj\00\00\00\03\00\0A\00\01\00 ,\01\18\00,\09\00\01\00\11y\18\00,\04\00\01\00\11\97\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\1C\00\00\00E\00#\04\10\18\00\80/\08\00\05\00\00\00\06\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04p\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C$\00u\01\00\08\00\00\F0!\10\00\01\01\00\C0\F0\11\00\03\1B\FF\00\04\1C\08\00p\A8\04\12\00\01\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00 \01/\05\00\01\00\FF\B8@$v\01\FF\17\04\A3\FF\00\8E\07\00\C4\0F\00\C3y\F4\03\A2%\00\00\00\22\0E\00\19y\00\01\00\F3\0E!\00\00\00b\0E\00\90r\05?\04\00\00\80?\E0\FF\0F\00\C6\1F\00\B9z\04\00\00X\E6\02p\E4\0F\00\A4r\04\05B\00\B1?\02\8E\0F\00\CC\0F\00\0C|\00F\04\95pR\F0\0B\00\DA/\00Mk\04\F3\0D\80\03\00\EA\0F\00\02z\02\00\00Z\00\00\00\0F\00\00\00\E2\0F\00$v\03\FF\00[\90\00\22\E2\0F`\001F\00\00\BB\04\80\C8\0F\00\86y\00\02\FF\B8\030\19\10\0C \00*MyP\00PGy\00\00\F0A\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\80\0F\01\00-\00\0F\05.\03\00\01\00\22@\00\01\00=g\01\000\00\08\01\00\1F\0B@\00\04\13\A7)\00\1F\A6@\00\0C\13\13\14\04\0C\01\00\13PU\00*\90\008\04\04U\04\22\18\00\01\00.:\01T\00\00\01\00\13\E0@\00/p\00\80\00\0B\1F)'\00\03A\00P\04\00\01\00\04H\06\04\E4\00*\04\00\01\00\1Fp@\00\04\13\80)\00&L\00@\00\1F\0A@\00\00!X\01D\01\0D@\00\13\D0)\00*\D8\00\01\00\1B\08\08\00%G\01\DB\0A\09\01\00\13\A8e\05\17\10\80\00\17\048\00\04\18\00\13\F4\14\01\0C\84\01*\B8\050\07\1F\00\C0\00\04\132@\00*\06\00\01\00*\80\07\C8\07\12\03\C0\01:\06\80\00\01\00\13\06\D8\01\05\A8\0B\0C\C8\00\1A\00\08\00\04\F8\00\13\018\00\04\A8\00\0C\01\009H\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64(%arg0: i32, %arg1: !llvm.ptr<1>) attributes {disc.elimargs = [1 : index, 3 : index, 4 : index, 5 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(1 : index) : i32
      %1 = llvm.mlir.constant(0.000000e+00 : f32) : f32
      %2 = nvvm.read.ptx.sreg.ctaid.x : i32
      %3 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %4 = llvm.mul %2, %arg0  : i32
      %5 = llvm.add %3, %4  : i32
      %6 = llvm.icmp "ult" %5, %0 : i32
      llvm.cond_br %6, ^bb2, ^bb3
    ^bb2:  // pred: ^bb1
      %7 = llvm.getelementptr %arg1[%5] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      llvm.store %1, %7 : f32, !llvm.ptr<1>
      llvm.br ^bb3
    ^bb3:  // 2 preds: ^bb1, ^bb2
      llvm.return
    }
  }
  gpu.module @main_kernel_2 attributes {gpu.binary = "P\EDU\BA\01\00\10\00\A8\0B\00\00\00\00\00\00\02\00\01\01@\00\00\00h\0B\00\00\00\00\00\00c\0B\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\A8#\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00g\02\00\BE\00r\00\01\00\22#\00\01\00\11 \06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\12e__6_1_0___no_ibXblock_tile_h64_1I\00\0FC\00,osharedE\00,\9Fconstant0H\00)\FA\01debug_frame\00.rel\11\00!nv\14\00\11aT\00\0Fo\01 \0F\99\00&\0F\A9\01\B6\8F$____wg_B\00&\00\1B\00/26\F1\016o_param\F8\01\1C\0F\01\00\05\8Cl\00\00\00\03\00\0A\00\01\00\11\EF\18\00,\0B\00\01\00 |\01\18\00,\09\00\01\00\11\CB\18\00,\04\00\01\00\11\E9\18\00,\07\00\01\00g2\00\00\00\12\10x\00\11\18\06\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0\17\07\00 \00\04\9B\00R\04\14\00\00\00E\002\04\C0\05\18\00C/\08\00\06\7F\00\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E0\04\F1\08\015\00\00\04\0A\08\00\03\00\00\00`\01(\00\03\19(\00\04\17\0C$\00\10\05\ED\04%\F0!\10\009\04\00\18\10\009\03\00\10\10\00u\02\00\08\00\00\F0\11\10\009\01\00\04\10\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\00\10\17\00\00`\17\00\00\04\1Et\01#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\005\02\00\00\84\01\0F\01\00\FFz@$v\01\FF?\04\A3\FF\00\8E\07\00\C4\0F\00\19y\A6\01\10%[\02a\0E\00\19y\03\00\01\00\10!-\00\F5\14\0E\00$z\06\06\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\06\00Y\00\00p`\F0\03\00\DA\0F\00M[\04\B0\80\03\00\EA\0F\00\19x\03\FF\1F\FC\01\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\00\B3\04\93\E2\0F\00Ey\00\00@\150\00\93\E2\0F\00$r\08\FF\FF\00\90\00p\E2\0F\00\11r\03\03\92\00\C1\FF@\8F\07\00\C8\0F\00\12x\05\031\04\10\C0\80\00p\0F\00$x\06\06\01\B4\03\12\0A\10\00@\12x\05\06p\00\01 \00p\E4\0F\04\19x\00\FF*\04\C0\06\16\01\00\00\E4\0F\00\0Cr\00\05`\00@pR\F2\03 \00P\0Cx\00\06\7F\10\00\22@\F0\80\002x\07\05\C9\02\11\8Ep\00T$x\07\07\04\90\00\9A\CC\0F\00G\19\00\00\80\14\E0\00\000\00\10\03\E0\00\01\90\00*\03\03@\004\09\03@@\00q\C8\0F\00%x\04\09P\00\11\02\E0\00P\04\10x\00\09\C0\001\FF\E0\FF\B0\00\B0\0Cz\00\09\00Z\00\00pb\F4\A0\00P\00\12x\04\04P\00!\FF\FC\D0\00\00p\01\12\00 \00\11\F6 \00`\10z\02\04\00\\@\00\11\F3@\00`\10z\04\04\00^\10\00\11\F9\D0\01\00`\00\00|\03\03`\00\D2\00\10z\03\05\00]\00\00\FF\E4\FF\000\00@\05\05\00_\10\00*\7F\02`\00\11\F8\10\01\F4\06\81\B9\0D\02\04\FC\FF\FF\00\19\1E\0C\00\A2\0E\00\10x\0A\09\03P\00\93\C6\0F\00\81\A9\00\02\04\F8 \00 \E2\0E@\00\12\0A@\00\22\FA\03 \00%\0B\04 \00u\E8\0E\00\81\B9\0A\04P\00p\A8\0E\00\81\C9\0F\02\10\01\01\10\00y(\0F\00\81\C9\0C\04\10\00S\D9\11\02\04\04\10\00\10h\10\00%\0E\04\10\00\10b\E0\004\10\09\04\90\00\01\F0\00:\12\09\05P\01\12\10\90\00#\FC\03\E0\00\12\12\10\00\12\F2@\01Dx\10\09\060\00a\E2\0F\04#\A2\086\07\00$\021\00\E2\8F@\01\14\07 \00q\CE\0F\00\81\E9\0B\02\91\01\06\F0\00\15\10\B0\01\93\E2\0F\00#\B2\08\0D\0A\00=\05!\E2O0\01\16\08`\00`\00\81\99\0D\02\04\17\06\03P\01\09\D0\01\000\018\E9\00\04`\00U#\C2\08\0F\0CP\00&\0F\01`\01\11\F8\C0\00H\81\99\0A\04P\00\00\D0\00\19\09\A0\01C\0F\02\04\10 \00\86\22\0F\00#\D2\08\11\0EP\00\16\02\C0\00\12\FAP\00W\A9\0C\04\04\10\80\01W\B9\11\02\04\14\80\019\B9\0E\04\10\00X\C9\13\02\04\18\10\00)\10\04\10\00X\D9\15\02\04\1C\10\00H\12\04\04\1C\C0\017\14\09\0A \01U#\E2\08\0B\00\A0\00\02\80\01\17\0B\E0\01E\0Cz\00\14\D0\01\000\00\1A\92p\01\06P\01\04\E0\017\00\09\0C\10\02\00\90\01\18\0Dp\00\1D\A2`\01\15\00\D0\01\13\CA\F0\01\18 `\018\0C\09\0F`\01\00\B0\01\07 \00-#\B2`\01\19\0A\B0\03Lx\0A\09\0E\10\02\19$\F0\01$\13\10@\00\02\B0\02\06\F0\01\13\E2\D0\01\17(\A0\01\00\00\02\08@\00F\D2\08\15\12\80\00\00@\00\15\0C\E0\01\13\C4\D0\01\17,\90\01\00\F0\01\08P\00\00\E0\01\1B,\E0\01\1B0\E0\01\1B0\E0\01\1B4\E0\01\1A4\E0\01\1F\10\E0\01\06\11O\A0\01\1F\11\E0\01\16\1A\8F\E0\01\13\D6\A0\01\188\A0\01;\00\09\12\F0\01\1F\13\F0\01\15\13\E4\D0\01\1F8\D0\01\1B\16\14`\00\11\04\D0\01\16<\90\01X\10x\0E\09\15\80\00\08\E0\01\1B\E2\E0\01\04\B0\03\1B@\E0\01\1F<\E0\01\0A\1D\0E\C0\03\1B@\C0\03\1BD\C0\03\1BD\E0\01\1BH\E0\01\1BH\E0\01\1BL\E0\01\1AL\E0\01\1F\16\E0\01\0C\1F\17\E0\01-\1AP\E0\01\1F\18\D0\01\08\00\90\01\16\19\00\02\1F\00\E0\01\02\1FP\E0\01\17\01\D0\01\18T\D0\017\0A\09\1A\D0\00\00\10\04\1F\1B\E0\01\15\13\E4\E0\01\1BX\E0\01\1FT\C0\03\1C\1B\\\C0\03\1BX\C0\03\1B\\\E0\01\1B`\E0\01\1B`\E0\01\1Bd\E0\01\1Ad\E0\01\1F\1C\E0\01\0C\1F\1D\E0\01\05\13\DA\C0\01\17hp\01\0F\F0\01\09\01P\03;\00\09\1E\C0\03\1F\1F\C0\03\1D\1Fh\E0\01\14\01\C0\01<\0A\09 \C0\03\1Al\C0\03\1F!\C0\03\1D\1Bp\E0\01\1Fl\C0\03\1C\1Bp\C0\03\1Bt\C0\03\1Bt\E0\01\1Bx\E0\01\1Bx\E0\01\1B|\E0\01\1A|\E0\01\1F\22\E0\01\0C\1F#\C0\03-\16\80\90\01\00P\00\1F$\C0\03\0C\1F%\C0\03\0D\1F\80\C0\03\1C\18\84\D0\017\0A\09&\D0\00\00\C0\03\1F'\C0\03\1D\1B\88\E0\01\1F\84\C0\03\1C\1B\8C\C0\03\1B\88\C0\03\1B\8C\E0\01\1B\90\E0\01\1B\90\E0\01\1B\94\E0\01\1A\94\E0\01\1F(\E0\01\0C\1F)\C0\03\0D\1F\98\C0\03\1B\1B*\C0\03\1F+\C0\03\1D\1F\98\C0\03\1B\1C,\C0\03\1A\9C\C0\03\1F-\C0\03\1D\1B\A0\E0\01\1F\9C\C0\03\1C\1B\A0\C0\03\1B\A4\C0\03\1B\A4\E0\01\1B\A8\E0\01\1B\A8\E0\01\1B\AC\E0\01\1A\AC\E0\01\1F.\E0\01\0C\1F/\C0\03-\16\B0\90\01\00P\00\1F0\C0\03\0C\1F1\C0\03\0D\1F\B0\C0\03\1C\18\B4\D0\017\0A\092\D0\00\00\C0\03\1F3\C0\03\1D\1B\B8\E0\01\1F\B4\C0\03\1C\1B\BC\C0\03\1B\B8\C0\03\1B\BC\E0\01\1B\C0\E0\01\1B\C0\E0\01\1B\C4\E0\01\1A\C4\E0\01\1F4\E0\01\0C\1F5\C0\03\0D\1F\C8\C0\03\1B\1B6\C0\03\1F7\C0\03\1D\1F\C8\C0\03\1B\1C8\C0\03\1A\CC\C0\03\1F9\C0\03\1D\1B\D0\E0\01\1F\CC\C0\03\1C\1B\D0\C0\03\1B\D4\C0\03\1B\D4\E0\01\1B\D8\E0\01\1B\D8\E0\01\1B\DC\E0\01\1A\DC\E0\01\1F:\E0\01\0C\1F;\C0\03-\16\E0\90\01\00P\00\1F<\C0\03\0C\1F=\C0\03\0D\1F\E0\C0\03\17\00P\00\17>\C0\00\00\B0\03\17?`\00g\81\99\09\02\04\E4\A0\01\0F\C0\03\0D\00\D0\01\040\00\00P\12Y\A9\0D\02\04\E8\E0\10\0F\C0\03\0B\00\E0\01\18\E8\E0\01K\0F\02\04\EC\E0\01\1B\EC\D0\01\18\F0\10\00K\11\02\04\F0\D0\01\18\F4\10\00%\13\02\10\00\1Bb\D0\01 \C8O\B0\01\15\09\B0\01 \C8\8F\80\01\15\0D\80\01\86\C8\0F\01#\B2\08\0F\0E\10\00f\02#\C2\08\11\10\10\00\00\E0\00\15\13\E0\00P\C4\0F\00Ay\09\00\06\90\14c\88s\00\07\08\00!\00f\E8\0F\00\1D{\00\01\00S\EC\0F\00\84\89\CD\19\13\08 \01Ax\00\06?\00\15\11\F2@\03c\84\89\03\07\00\10 \00s$\0E\00!\82\00\00\02\16\10\00\F0\15'\88\83@\00\0F`\00\01-\99\02`\00\14\1F`\15\00`\00W\99\03\07\00\08`\009\92\02\02`\00O\93\00\07\02\C0\00\0A\1A\03`\005r\00\06\D0\15\01\C0\00H\04\07\00\04\C0\00J\04\03\04\00\C0\00\1F\04`\00\089M\19\00P\016\84y\07p\00\93\22\0E\00$v\02\FF\00`\D0\15\93\C4\0F\00$v\03\FF\00a\10\00a\CA\0F\00\8Ey\00\01\01\9B\84\E7\10\0C\00\E2\1F\00M\A0\01PGy\00\00\F0\C0\16\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00`\0F\01\00-'\01\00 \02\07\01\00\22@\00\01\00=o\01\000\00\08\01\00\1F\0B@\00\04\13\AF)\00\1F\F8@\00\0C\13\13\\\1A\0C\01\00\13\A8U\00*\A8\00\80\1A\22\08\00\01\00\22\18\00\01\00.B\01T\00\00\01\00\13P5\02/p\00\80\00\0B/)\00'\00\02#\00\C0@\00\00,\09\17\00\E4\00*\04\00\01\00\1Fr@\00\04\13\F0)\00&\98\00@\00\1F\0A@\00\00!`\01D\01\0D@\001\88\05\00\01\00*\D8\00\01\00\1B\08\08\00?O\01\00\0E\1E\00Q\00\00`\06\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\FA\14\01\0C\84\01\13p@\00\17\881\01\0F\C0\00\01\132@\00\15\06R\00\0B\A1\00\16\18\80\00j\06\00\00\18\80\00\01\00\13\B5+\00+\03\00\01\00\00\D5\0F\1A\00q\00\0F\80\00\01\11\06\F0\1D\07\E8\22\0CH\02\1A\00\08\00\04\97\00\13\018\00\04\E8\00\0C\01\00*\90\19\08\00\088\00\18\06\A0\00\0F\01\00\05\03\A9\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00"} {
    llvm.mlir.global internal @__wg_main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1_0() {addr_space = 3 : i32} : !llvm.array<256 x f32>
    llvm.func @main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1(%arg0: i32, %arg1: i32, %arg2: i32, %arg3: !llvm.ptr<1>, %arg4: !llvm.ptr<1>, %arg5: !llvm.ptr<1>) attributes {disc.elimargs = [3 : index, 4 : index, 6 : index, 7 : index, 8 : index, 9 : index, 10 : index, 11 : index, 13 : index, 14 : index, 15 : index, 16 : index, 17 : index, 18 : index, 20 : index, 21 : index, 22 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(32 : index) : i32
      %1 = llvm.mlir.constant(8 : index) : i32
      %2 = llvm.mlir.constant(64 : index) : i32
      %3 = llvm.mlir.constant(4 : index) : i32
      %4 = llvm.mlir.constant(0.000000e+00 : f32) : f32
      %5 = llvm.mlir.constant(2 : index) : i32
      %6 = llvm.mlir.constant(1 : index) : i32
      %7 = llvm.mlir.constant(256 : index) : i32
      %8 = llvm.mlir.constant(0 : index) : i32
      %9 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1_0 : !llvm.ptr<3>
      %10 = llvm.getelementptr %9[0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<256 x f32>
      %11 = nvvm.read.ptx.sreg.ctaid.x : i32
      %12 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %13 = llvm.mul %11, %arg0  : i32
      %14 = llvm.add %12, %13  : i32
      %15 = llvm.icmp "ult" %14, %arg1 : i32
      llvm.cond_br %15, ^bb2, ^bb21
    ^bb2:  // pred: ^bb1
      %16 = llvm.srem %14, %7  : i32
      %17 = llvm.sdiv %14, %7  : i32
      %18 = llvm.udiv %16, %0  : i32
      %19 = llvm.urem %16, %0  : i32
      %20 = llvm.mul %19, %1  : i32
      %21 = llvm.add %18, %20  : i32
      %22 = llvm.icmp "ult" %19, %6 : i32
      llvm.cond_br %22, ^bb3, ^bb10(%4 : f32)
    ^bb3:  // pred: ^bb2
      %23 = llvm.mul %17, %1  : i32
      %24 = llvm.add %18, %23  : i32
      %25 = llvm.mul %24, %2  : i32
      llvm.br ^bb4(%8, %4 : i32, f32)
    ^bb4(%26: i32, %27: f32):  // 2 preds: ^bb3, ^bb9
      %28 = llvm.icmp "slt" %26, %2 : i32
      llvm.cond_br %28, ^bb5, ^bb10(%27 : f32)
    ^bb5:  // pred: ^bb4
      %29 = llvm.add %26, %25  : i32
      %30 = llvm.icmp "slt" %29, %arg2 : i32
      llvm.cond_br %30, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %31 = llvm.getelementptr %arg3[%29] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %32 = llvm.load %31 : !llvm.ptr<1> -> f32
      %33 = llvm.getelementptr %arg4[%29] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %34 = llvm.load %33 : !llvm.ptr<1> -> f32
      %35 = llvm.fmul %32, %34  : f32
      %36 = llvm.fadd %27, %35  : f32
      llvm.br ^bb8(%36 : f32)
    ^bb7:  // pred: ^bb5
      llvm.br ^bb8(%27 : f32)
    ^bb8(%37: f32):  // 2 preds: ^bb6, ^bb7
      llvm.br ^bb9
    ^bb9:  // pred: ^bb8
      %38 = llvm.add %26, %6  : i32
      llvm.br ^bb4(%38, %37 : i32, f32)
    ^bb10(%39: f32):  // 2 preds: ^bb2, ^bb4
      llvm.br ^bb11(%39 : f32)
    ^bb11(%40: f32):  // pred: ^bb10
      llvm.br ^bb12
    ^bb12:  // pred: ^bb11
      %41 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      llvm.store %40, %41 : f32, !llvm.ptr<3>
      nvvm.barrier0
      %42 = llvm.icmp "slt" %18, %3 : i32
      llvm.cond_br %42, ^bb13, ^bb14
    ^bb13:  // pred: ^bb12
      %43 = llvm.add %21, %3  : i32
      %44 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %45 = llvm.load %44 : !llvm.ptr<3> -> f32
      %46 = llvm.getelementptr %10[%43] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %47 = llvm.load %46 : !llvm.ptr<3> -> f32
      %48 = llvm.fadd %45, %47  : f32
      %49 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      llvm.store %48, %49 : f32, !llvm.ptr<3>
      llvm.br ^bb14
    ^bb14:  // 2 preds: ^bb12, ^bb13
      nvvm.barrier0
      %50 = llvm.icmp "slt" %18, %5 : i32
      llvm.cond_br %50, ^bb15, ^bb16
    ^bb15:  // pred: ^bb14
      %51 = llvm.add %21, %5  : i32
      %52 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %53 = llvm.load %52 : !llvm.ptr<3> -> f32
      %54 = llvm.getelementptr %10[%51] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %55 = llvm.load %54 : !llvm.ptr<3> -> f32
      %56 = llvm.fadd %53, %55  : f32
      %57 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      llvm.store %56, %57 : f32, !llvm.ptr<3>
      llvm.br ^bb16
    ^bb16:  // 2 preds: ^bb14, ^bb15
      nvvm.barrier0
      %58 = llvm.icmp "slt" %18, %6 : i32
      llvm.cond_br %58, ^bb17, ^bb18
    ^bb17:  // pred: ^bb16
      %59 = llvm.add %21, %6  : i32
      %60 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %61 = llvm.load %60 : !llvm.ptr<3> -> f32
      %62 = llvm.getelementptr %10[%59] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %63 = llvm.load %62 : !llvm.ptr<3> -> f32
      %64 = llvm.fadd %61, %63  : f32
      %65 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      llvm.store %64, %65 : f32, !llvm.ptr<3>
      llvm.br ^bb18
    ^bb18:  // 2 preds: ^bb16, ^bb17
      nvvm.barrier0
      %66 = llvm.icmp "eq" %18, %8 : i32
      %67 = llvm.and %66, %22  : i1
      llvm.cond_br %67, ^bb19, ^bb20
    ^bb19:  // pred: ^bb18
      %68 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %69 = llvm.load %68 : !llvm.ptr<3> -> f32
      %70 = llvm.getelementptr %arg5[%19] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %71 = llvm.atomicrmw fadd %70, %69 acq_rel : !llvm.ptr<1>, f32
      llvm.br ^bb20
    ^bb20:  // 2 preds: ^bb18, ^bb19
      llvm.br ^bb21
    ^bb21:  // 2 preds: ^bb1, ^bb20
      llvm.return
    }
  }
  gpu.module @main_kernel_3 attributes {gpu.binary = "P\EDU\BA\01\00\10\00H\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\08\04\00\00\00\00\00\00\01\04\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\0Be__6_1_0___thread_tile_h32B\00\0F<\00%oshared>\00%\9Fconstant0A\00\22\FA\01debug_frame\00.rel\11\00!nv\14\00\11aM\00\0FS\01 \0F\92\00\1F\0F\86\01\E2o_param\8D\01\1C\0F\01\00\04\8Ce\00\00\00\03\00\0A\00\01\00 \18\01\18\00,\09\00\01\00\11`\18\00,\04\00\01\00\11~\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\1C\00\00\00E\00#\04\10\18\00\80/\08\00\05\00\00\00\06\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04@\04\91\015\00\00\04\0A\08\00\02\FC\00\91\10\00\03\19\10\00\04\17\0C$\00u\01\00\08\00\00\F0!\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00p\00\00\00\C0\00\01\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00 \01/\05\00\01\00\FFh@$v\01\FF\C7\03\A3\FF\00\8E\07\00\C4\0F\00\C3y\A4\03\A2%\00\00\00\22\0E\00\19y\00\01\00\F3\0E!\00\00\00b\0E\00\90r\05?\04\00\00\80?\E0\FF\0F\00\C6\1F\00\B9z\04\00\00X\96\02p\E4\0F\00\A4r\04\05B\00\B1?\02\8E\0F\00\CC\0F\00\0C|\00\F6\03\95pR\F0\0B\00\DA/\00M\1B\04\F3\0D\80\03\00\EA\0F\00\02z\02\00\00Z\00\00\00\0F\00\00\00\E2\0F\00$v\03\FF\00[\90\00\22\E2\0F`\001F\00\00k\04\80\C8\0F\00\86y\00\02\FFh\030\19\10\0C \00*MyP\00PGy\00\00\F0\F1\03\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\80\0F\01\00-\00\BF\04.\03\00\01\00\22@\00\01\00=S\01\000\00\08\01\00\1F\0B@\00\04\13\93)\00\1F\8D@\00\0C\13\13\C4\03\0C\01\00\13 U\00*\90\00\E8\03\04\05\04\22\18\00\01\00.&\01T\00\00\01\00\13\B0@\00/p\00\80\00\0B\1F)'\00\03A\00 \04\00\01\00\04\F8\05\04\E4\00*\04\00\01\00\1Fk@\00\04\13P)\00&L\00@\00\1F\0A@\00\00!D\01D\01\0D@\00\13\A0)\00*\D8\00\01\00\1B\08\08\00?3\01\00.\07\003\00\00x\15\05\17\10\80\00\17\048\00\04\18\00\13\E5\14\01\0C\84\01*\88\05\E0\06\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07x\07\12\03\C0\01:\06\80\00\01\00\13\06\D8\01\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\13\018\00\04\A8\00\0C\01\00*\F8\02\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__6_1_0___thread_tile_h32(%arg0: i32, %arg1: !llvm.ptr<1>) attributes {disc.elimargs = [1 : index, 3 : index, 4 : index, 5 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(1 : index) : i32
      %1 = llvm.mlir.constant(0.000000e+00 : f32) : f32
      %2 = nvvm.read.ptx.sreg.ctaid.x : i32
      %3 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %4 = llvm.mul %2, %arg0  : i32
      %5 = llvm.add %3, %4  : i32
      %6 = llvm.icmp "ult" %5, %0 : i32
      llvm.cond_br %6, ^bb2, ^bb3
    ^bb2:  // pred: ^bb1
      %7 = llvm.getelementptr %arg1[%5] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      llvm.store %1, %7 : f32, !llvm.ptr<1>
      llvm.br ^bb3
    ^bb3:  // 2 preds: ^bb1, ^bb2
      llvm.return
    }
  }
  gpu.module @main_kernel_4 attributes {gpu.binary = "P\EDU\BA\01\00\10\00\18\0D\00\00\00\00\00\00\02\00\01\01@\00\00\00\D8\0C\00\00\00\00\00\00\D8\0C\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8!\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\00!@!\07\001\00\80\1E\07\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\0De__6_1_0___thread_tile_h32_1D\00\0F>\00'oshared@\00'\9Fconstant0C\00$\FA\01debug_frame\00.rel\11\00!nv\14\00\11aO\00\0F[\01 \0F\94\00!\0F\90\01\EAo_param\97\01\1C\0F\01\00\0A\8Cg\00\00\00\03\00\0A\00\01\00  \01\18\00,\09\00\01\00\11j\18\00,\04\00\01\00\11\88\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\16\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\18\00\00\00E\002\04H\05\18\000/\08\00\0A\00\10\22\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04X\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\010\00\03\190\00\04\17\0C$\00u\07\00(\00\00\F0!\10\009\06\00 \10\009\05\00\18\10\00u\04\00\10\00\00\F0\11\10\009\03\00\0C\10\009\02\00\08\10\009\01\00\04\10\00\01\01\00\F1\02\F0\11\00\03\1B\FF\00\04\1C\08\00`\00\00\00\90\15\16\003\00K\00\01\00`\02\02\08\10\0A/\E7\00\11\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00\80\01/\05\00\01\00\FF\F0@$v\01\FF\AF\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\CB\02Q\0E\00\19y\03\0F\00\10\00\08\08\F0\15$\0E\00$z\00\00\00Z\00\00\03\02\8E\07\00\CA\1F\00\12x\FF\00\FF\01\00\00\FF\C0\80\07\00\C8\0F\00\0C \00\C5[\00\00pdp\00\00\DA\0F\00M\F3\04\B0\80\03\00\EA\0F\00\19x\03\FF\1F\0F\00\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\003\05a\C6\0F\00\11r\03q\00@\FFH\8F\07 \00c$v\00\FF\00X\A0\00\12\C6@\00\00S\00\10\03@\00P\E4\0F\00\0Cx$\03`\00\00pR\F0\03 \00c$x\03\03 \000\00@\E2\0F\00\07\90\00\22Y\00\01\00p\C8\0F\00\10x\0A\030\00\F0\05\FF\E0\FF\07\00\E4\0F\04\0Cz\00\03\00\\\00\00pb\F4\03\10\00Z\10x\0C\03\02 \00\12\0A \00\10\FC \00f\00\10x\12\03\03 \00\00\F0\00\12\0C \00C\FA\03\00\C4\00\01\13X\90\00\02@\00\01\80\00ApR\F2\03\D0\00G\A4\09\FF\04\A0\00B\0Cz\00\12@\00\13\F6`\004\02\03\04`\00i\E2\0F\04$\E4\0B0\00c\07\A2\04\03\FF\00\FF\04c\E4\0F\04\07\A2\08\10\00 \80\04 \00<$\D4\0D`\00\12\02`\00\11\F8\80\00@%\A6\04\04\E9\04#\09\02P\00D\E8\06\0A\01@\00\10\C4\10\004\0A\0A\01`\00\010\00G\08\08\00^0\00F\D8\0E\0C\02p\00A\04\81\A9\05#\06\D8\00\19\1E\0C\00\A2\00\00\07\D8\0C\0C\02@\000\E6\06\06@\00\12\0B@\00f\08\07\B8\10\12\03@\00U\00\81\A9\00\08@\00\95\A4\02\00%\E6\0A\0A\00`0\00g\00\07\B8\12\12\03P\00E\81\E9\07\060\00p\E4\0E\00%\D6\0E\0E`\00\14\0D0\008\C2\13\02\10\01F\81\E9\14\0A0\00V\08\00$\B4\15 \01\96\C4\0F\00%\D6\0C\0C\00`@\00E\81\D9\0F\0E0\00pf\0F\00%\B6\10\10`\00\14\15\C0\00F\C2\0A\02\FF\90\00U\01\81\D9\0C\0C0\00\10d0\00C\08\12\00`0\00u\E4/\00\81\B9\11\10 \00i$\03\00$\C4\0B\80\006\81\B9\08\00\01u$\0F\00%\C6\12\130\01\10\C8\10\00\07\10\01u\E4\0F\00\81\C9\13\120\00\10(\10\00\16\0A\E0\00\84\22\0F\00$r\02\FF\FF`\00\10\E2P\025\04\03\05\B0\02u\1F\04\10x\10\03\07`\02Q/\04#\A2\02\08\06\01\D4\00\84\E2O\00\10x\05\03\06 \00\02\D0\02\15\04 \03\83\E2\0F\00#\E2\02\07\14@\060\00\E2\8F \00\15\05 \03\84\C6\0F\00#\D2\02\0F\0C \00\85\C8\0F\02#\B2\02\11\080\00j\0F\01\0Cz\00\10\F0\028\11\03\08\F0\02\0B \03W\07\A8\08\04\01\F0\02V\10x\00\03\090\00f\00#\C2\02\13\0A`\00\00\90\00\15\11\F0\02\10\E4@\007\0A\04\01\10\02*$\E4 \03Y\07\E8\06\05\02\00\039\0C\05\02\00\03'\0A\0A\00\03\11\08\D0\03\04\F0\03\00`\008\B8\0E\10\D0\02\010\03\08`\03G\B8\10\10\03P\006\81\A9\05\B0\01%\A4\00 \03\07\C0\02)\11\FF \03\19\04 \039$\B4\17@\02*%\E6\C0\02_\07\C2\12\11\FF0\03\09\12\B60\03\12\17P\02Y\08\81\E9\14\0C \03&\C4\16`\00\10\E4\D0\02C\0A\10\00`0\00W\E2\1F\00\81\B9 \03\10b\10\04\11\15\80\05\04@\011%\C6\10\C0\02\14\16`\009\B9\0A\0A \03\12\C6 \03\01 \00 \E2/0\04(\00\01P\03\18\C90\03\1A\01\D0\04\01\00\03\0B0\035\D6\12\15@\01\1B\C8\D0\03\000\00\1B\D90\03\08\B0\03\10\22\80\029\10\03\0C \034\00\03\0E\10\00\00`\02E\A2\02\05\04`\02\11OP\03\17\0A \03\000\03\14\0B\10\00\1F\E40\03\0Cw\C8\8F\00#\B2\02\0F\B0\02\16\02@\03\12\F6@\03\1F\C20\03\05\02\D0\02\000\03\17\0D\A0\00\0CP\06\00\F0\02\14\02\90\01\02\80\06\18\11\C0\06\00P\03\17\02 \02Z#\D2\02\13\0C@\03\06\E0\02\00`\00\1B\B40\037\B8\06\05\F0\02\1B\04 \03d\00\07\B8\0C\05\03`\00\00P\03O\C2\0E\10\FF \03\09O\C2\10\10\FF \03\09\19\B6 \03i\08\07\E8\13\11\01\80\06\0B \03\17\C4 \03\01\C0\02\0B \03H\E8\12\11\01p\00\1B\B9 \03\17\C6 \03\12\E2\E0\02\17\02\C0\00+\81\B90\03*\E4\16\90\03\1B\C60\03\1B\C9P\06\17\E6 \03\00\D0\00I\D8\0C\00\02\00\03\0A0\03\09\D0\07\00\D0\00\1A\E6@\03'\81\E90\03*&\01\10\03\00`\06\18\E90\03\1F\03 \03\22\1B\11 \03\14\13\10\00\0F \03\04\14\0F \00\03 \03\14\10\10\00\0F \03\01\1F\B2P\06\05\05\10\03\0C0\03\06\10\03\1C\CE\00\03\11/\00\03\08\A0\02*#\E2P\03\00@\03\17\12\90\00\00\10\03\16\03\80\01\0F\10\03\01\1F\11\10\03\0A:\B2\06\05\E0\02\090\06\07P\03\11\FC \0A3\07\B2\0C0\00\1F\00\10\03\03:\C8\0E\10\E0\02)\05\0A`\09H\C8\10\10\01@\00\08 \03\00p\05\0F\10\03\1DH\D8\13\11\02p\00\08\10\03\88\E2\0E\00\07\D8\12\11\02p\00\08 \03\1F\C4\10\03\00\1B\D4@\06\0B\10\03H\07\E8\15\00\A0\01\09 \03\10d\C0\05\0B \03O\E8\0C\00\03 \03\08'%\D6\10\03[\C4/\00\81\D9@\06)\E4\0D\90\00+\81\D9@\06\1B\E6@\06\08P\07\000\00\1B\E9 \03\1E\E9 \03\1B\16 \03\14\18\10\00\0F \03\04\1B\14 \03\14\15\10\00\0F \03=\11\C6\D0\02\0D\00\03\14\17p\00\1F\C6@\06\00O\A2\0A\04\FF@\06\02\03\F0\08G\A2\08\04\FF\10\02/#\E2@\06\05\1F\FC@\06\06\1F\01@\06\0C\18\01@\06F\C8\0E\10\020\00\0F0\03\00?\10\10\02@\06\19O\D8\13\11\03@\06)O\D8\12\11\03@\06\05.$\07@\06G\E2\15\00\FF\C0\00\090\03*$\0B0\03\0F@\06\00H\07\E2\00\00p\01\090\03*$\010\03\00\00\04\08 \03\10$\F0\0A\17\06\D0\00\1A\8F0\03\1B\E20\03%\E6\0E\10\03\22\06\02\90\00\090\03\00 \07S\E6\0C\00\00` \009\E4\0F\02 \03\1Bh \03\10b\E0\026\00\03\1A\E0\02d\04\10x\06\03\1B\10\00\01\00\034\0E\03\1C\10\00/\E2\1F0\03\007\05\03\19\B0\02\09\10\03;\C8\0F\01\00\03\00\80\02\15\06@\03\0D\00\03\17\8F\A0\02\12\FA \00\0A\C0\02\16\02`\03\04\F0\0F9\0A\03\1D\C0\009\08\03\1E\10\007\07\03\1F\A0\002\07\D8\16L\00\00\B0\01\02\D0\03\15\0E\B0\03\02\10\00\15\0A\A0\03\00P\03G\E8\10\05\010\00W\07\A8\0D\06\03\10\001\03x\0C\C0\01\01\01\00\02\90\0D\18\08\90\03H\0Cz\00\07\B0\00\06\C0\00\12\F4`\00H\B2\0B\0E\FF`\00&\C8\09\E0\0F\10\CA\B0\00(\04\08\B0\00H\07\E8\03\07\80\00\060\01\01\D0\10d\00\07\A8\14\05\01\D0\02\00 \000r\00\0C`\00$pRp\00%\0E\0E\F0\02\01\C0\03(\0A\0A0\00T\07\D8\08\08\02\10\00\000\01H\88\12\00\02 \00H\E8\00\07\03\10\008\A8\18\06\10\00\06\E0\00\03P\11\1A\84\B0\039$\A4\1A\10\00E%\86\12\12\90\03f\D0\0F\00$\94\11 \00\00P\03&\89\07\C0\02p\A6\00\00%\96\14\140\00\14\11\10\06!\96\10\90\03#\11\020\005\99\05\140\00v\E6\02\00%\86\16\16\B0\03\00 \00\17\06P\03`\08\00%\A6\18\18P\00\14\1A \005\89\0C\16 \00f\A6\0A\00$\B4\1C\90\00\00\00\04S\A6\1A\0D\00`0\00\00\80\06:\A9\0D\18\D0\04\02`\04\15\1C\F0\07&\1A\1A \00V\0E\00$\C4\1DP\00\01\C0\04 \14\0BP\00\14\1C\F0\03\08\C0\0Dl\A6\0E\00$\D4\1F\80\04\01\90\05\13\1D0\05H\81\B9\14\140\00c%\C6\10\09\00` \00\10\E4\90\07\19\0Bp\041%\D6\120\12\11\1F \00\00\00\0B*\10\10\90\04\090\0BA\02%\D6\08\90\12\14\1F \01\08\90\0A\010\0B)\18\00\E0\04\08\90\04\00\00\08U\E6\16\03\00` \00F\00\81\E9\190\01\01\90\04(\16\16\90\04c$v\04\FF\00b\80\00\00\C0\06H\92\02\05\06\A0\0D5\82\02\07\10\04 \C8O\90\04%\0D\1A\10\00\01\80\04\16\0F\90\07\00P\00C\05\FF\00cP\00\02\90\07(\0B\10\A0\04H\D2\02\13\08\10\11D\E2\02\19\16\10\00a\CA\0F\00\8Ey\00\90\0D@\84\E7\10\0CP\00\14M\80\15\030\15PGy\00\00\F0\A9\19\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\B0\0F\01\00-\00\\\04.\03\00\01\00\22@\00\01\00=[\01\000\00\08\01\00\1F\0B@\00\04\13\9B)\00\1F\97@\00\0C\14\13\CC\01\0B\01\00\138U\00\11\90\06\00\06p\19\22\08\00\01\00\22\18\00\01\00..\01T\00\00\01\00\13\C8@\00/p\00\80\00\0B\1F)'\00\03!\008\F5\02$\00\00\E0\1B\04\E4\00*\04\00\01\00\1Fm@\00\04\13h)\00&\AC\00@\00\1F\0A@\00\00!L\01D\01\0D@\001\18\05\00\01\00*\D8\00\01\00\1B\08\08\00?;\01\00\16\1D\003\00\00\F0@\00&\10\00\80\00\17\048\00\04\18\00\13\EB\14\01\0D\84\01!\06\00\01\00\17\901\01\0F\C0\00\01\132@\00+\06\00\01\00\1A\08`\1D\12\03\C0\01>\22\80\00g\00\17\05(!\0C\01\00*\A8\00\08\00\04\C0\00\13\018\00\0Ey\00\03x\00\1A\18\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__6_1_0___thread_tile_h32_1(%arg0: i32, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: !llvm.ptr<1>, %arg6: !llvm.ptr<1>, %arg7: !llvm.ptr<1>) attributes {disc.elimargs = [5 : index, 7 : index, 8 : index, 9 : index, 10 : index, 11 : index, 12 : index, 14 : index, 15 : index, 16 : index, 17 : index, 18 : index, 19 : index, 21 : index, 22 : index, 23 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(0 : index) : i32
      %1 = llvm.mlir.constant(1 : index) : i32
      %2 = llvm.mlir.constant(512 : index) : i32
      %3 = llvm.mlir.constant(32 : index) : i32
      %4 = llvm.mlir.constant(4 : index) : i32
      %5 = llvm.mlir.constant(0.000000e+00 : f32) : f32
      %6 = nvvm.read.ptx.sreg.ctaid.x : i32
      %7 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %8 = llvm.icmp "eq" %arg0, %1 : i32
      %9 = llvm.select %8, %arg1, %arg0 : i1, i32
      %10 = llvm.icmp "eq" %arg1, %9 : i32
      %11 = llvm.icmp "eq" %arg0, %9 : i32
      %12 = llvm.mul %6, %arg2  : i32
      %13 = llvm.add %7, %12  : i32
      %14 = llvm.icmp "ult" %13, %arg3 : i32
      llvm.cond_br %14, ^bb2, ^bb12
    ^bb2:  // pred: ^bb1
      %15 = llvm.srem %13, %2  : i32
      %16 = llvm.sdiv %13, %2  : i32
      %17 = llvm.icmp "ult" %15, %1 : i32
      llvm.cond_br %17, ^bb3, ^bb11
    ^bb3:  // pred: ^bb2
      %18 = llvm.mul %16, %3  : i32
      llvm.br ^bb4(%0, %5 : i32, f32)
    ^bb4(%19: i32, %20: f32):  // 2 preds: ^bb3, ^bb9
      %21 = llvm.icmp "slt" %19, %3 : i32
      llvm.cond_br %21, ^bb5, ^bb10
    ^bb5:  // pred: ^bb4
      %22 = llvm.add %18, %19  : i32
      %23 = llvm.icmp "slt" %22, %arg4 : i32
      llvm.cond_br %23, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %24 = llvm.urem %22, %4  : i32
      %25 = llvm.udiv %22, %4  : i32
      %26 = llvm.select %10, %25, %0 : i1, i32
      %27 = llvm.mul %26, %4  : i32
      %28 = llvm.add %27, %24  : i32
      %29 = llvm.getelementptr %arg5[%28] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %30 = llvm.load %29 : !llvm.ptr<1> -> f32
      %31 = llvm.select %11, %25, %0 : i1, i32
      %32 = llvm.mul %31, %4  : i32
      %33 = llvm.add %32, %24  : i32
      %34 = llvm.getelementptr %arg6[%33] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %35 = llvm.load %34 : !llvm.ptr<1> -> f32
      %36 = llvm.fmul %30, %35  : f32
      %37 = llvm.fadd %20, %36  : f32
      llvm.br ^bb8(%37 : f32)
    ^bb7:  // pred: ^bb5
      llvm.br ^bb8(%20 : f32)
    ^bb8(%38: f32):  // 2 preds: ^bb6, ^bb7
      llvm.br ^bb9
    ^bb9:  // pred: ^bb8
      %39 = llvm.add %19, %1  : i32
      llvm.br ^bb4(%39, %38 : i32, f32)
    ^bb10:  // pred: ^bb4
      %40 = llvm.getelementptr %arg7[%15] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %41 = llvm.atomicrmw fadd %40, %20 acq_rel : !llvm.ptr<1>, f32
      llvm.br ^bb11
    ^bb11:  // 2 preds: ^bb2, ^bb10
      llvm.br ^bb12
    ^bb12:  // 2 preds: ^bb1, ^bb11
      llvm.return
    }
  }
  gpu.module @main_kernel_5 attributes {gpu.binary = "P\EDU\BA\01\00\10\00H\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\08\04\00\00\00\00\00\00\01\04\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\0Ae__6_1_0___block_tile_h64A\00\0F;\00$oshared=\00$\9Fconstant0@\00!\FA\01debug_frame\00.rel\11\00!nv\14\00\11aL\00\0FO\01 \0F\91\00\1E\0F\81\01\DEo_param\88\01\1C\0F\01\00\05\8Cd\00\00\00\03\00\0A\00\01\00 \14\01\18\00,\09\00\01\00\11[\18\00,\04\00\01\00\11y\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\1C\00\00\00E\00#\04\10\18\00\80/\08\00\05\00\00\00\06\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\048\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C$\00u\01\00\08\00\00\F0!\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00p\00\00\00\C0\00\01\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\00#\02b,\00\00\00H\00\01\00\00 \01/\05\00\01\00\FFp@$v\01\FF\CF\03\A3\FF\00\8E\07\00\C4\0F\00\C3y\AC\03\A2%\00\00\00\22\0E\00\19y\00\01\00\F3\0E!\00\00\00b\0E\00\90r\05?\04\00\00\80?\E0\FF\0F\00\C6\1F\00\B9z\04\00\00X\9E\02p\E4\0F\00\A4r\04\05B\00\B1?\02\8E\0F\00\CC\0F\00\0C|\00\FE\03\95pR\F0\0B\00\DA/\00M#\04\F3\0D\80\03\00\EA\0F\00\02z\02\00\00Z\00\00\00\0F\00\00\00\E2\0F\00$v\03\FF\00[\90\00\22\E2\0F`\001F\00\00s\04\80\C8\0F\00\86y\00\02\FFp\030\19\10\0C \00*MyP\00PGy\00\00\F0\F9\03\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\80\0F\01\00-\00\C7\04.\03\00\01\00\22@\00\01\00=O\01\000\00\08\01\00\1F\0B@\00\04\13\8F)\00\1F\88@\00\0C\13\13\CC\03\0C\01\00\13\18U\00*\90\00\F0\03\04\0D\04\22\18\00\01\00.\22\01T\00\00\01\00\13\A8@\00/p\00\80\00\0B\1F)'\00\03A\00\18\04\00\01\00\04\00\06\04\E4\00*\04\00\01\00\1Fj@\00\04\13H)\00&L\00@\00\1F\0A@\00\00!@\01D\01\0D@\00\13\98)\00*\D8\00\01\00\1B\08\08\00?/\01\006\07\003\00\00p\1D\05\17\10\80\00\17\048\00\04\18\00\13\E2\14\01\0C\84\01*\80\05\E8\06\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\80\07\12\03\C0\01:\06\80\00\01\00\13\06\D8\01\04(\0B\0D\C8\01\1A\00\08\00\04\F8\00\13\018\00\04\A8\00\0D\01\00)\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__6_1_0___block_tile_h64(%arg0: i32, %arg1: !llvm.ptr<1>) attributes {disc.elimargs = [1 : index, 3 : index, 4 : index, 5 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(1 : index) : i32
      %1 = llvm.mlir.constant(0.000000e+00 : f32) : f32
      %2 = nvvm.read.ptx.sreg.ctaid.x : i32
      %3 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %4 = llvm.mul %2, %arg0  : i32
      %5 = llvm.add %3, %4  : i32
      %6 = llvm.icmp "ult" %5, %0 : i32
      llvm.cond_br %6, ^bb2, ^bb3
    ^bb2:  // pred: ^bb1
      %7 = llvm.getelementptr %arg1[%5] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      llvm.store %1, %7 : f32, !llvm.ptr<1>
      llvm.br ^bb3
    ^bb3:  // 2 preds: ^bb1, ^bb2
      llvm.return
    }
  }
  gpu.module @main_kernel_6 attributes {gpu.binary = "P\EDU\BA\01\00\10\00\D8\07\00\00\00\00\00\00\02\00\01\01@\00\00\00\98\07\00\00\00\00\00\00\93\07\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\A8\12\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00g\02\00\BE\00r\00\01\00\22\12\00\01\00\11\0F\06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\0Ce__6_1_0___block_tile_h64_1C\00\0F=\00&oshared?\00&\9Fconstant0B\00#\FA\01debug_frame\00.rel\11\00!nv\14\00\11aN\00\0FW\01 \0F\93\00 \0F\8B\01\A4\8F$____wg_<\00 \00\15\00/28\CD\010o_param\D4\01\1C\0F\01\00\09\8Cf\00\00\00\03\00\0A\00\01\00\11\DD\18\00,\0B\00\01\00 ^\01\18\00,\09\00\01\00\11\A7\18\00,\04\00\01\00\11\C5\18\00,\07\00\01\00\102\E3\03\17\10x\00\03#\00f\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0\06\07\00 \00\04\9B\00R\04\14\00\00\00E\002\04p\01\18\000/\08\00#\00\10\19\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\A8\04\F3\08\015\00\00\04\0A\08\00\03\00\00\00`\010\00\03\190\00\04\17\0C\CB\00U(\00\00\F0!\10\009\06\00 \10\009\05\00\18\10\00u\04\00\10\00\00\F0\11\10\009\03\00\0C\10\009\02\00\08\10\009\01\00\04\10\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\00\D0\05\00\00 \06\00\00\04\1E\94\01#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\11\02h\01\0F\01\00\FF\98@$v\01\FFw\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%s\02a\0E\00\19y\03\00\01\00\10!-\00\F0\04\0E\00$z\00\00\00Z\00\00\03\02\8E\07\00\CA\1F\00\0C\10\00\C5[\00\00p`\F0\03\00\DA\0F\00M\93\04\B0\80\03\00\EA\0F\00\19x\03\FF\1F\0F\00\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\00\EB\04Q\E2\0F\00Eyy\03\020\00a\E4\0F\00\11r\04q\00\C1\FF@\8F\07\00\C8\0F\00\12x\03\04Y\04\10\C0p\00@\0F\00$x\BC\02`\00\00\03\0A\8E\070\00c$r\03\FF\FF\00\C0\00\10\C60\00 \02\00p\00\22\FF\C0 \00P\19x\05\FF\05Q\000\16\01\00\10\00@\0Cr\00\020\001pR\F2p\00P\0Cx\00\00\7F\10\000@\F0\03\90\00@$x\02\02\D9\02\10\05\E0\00\12\C8\10\00\14\04`\00\96\CC\0F\00G\19\00\00@\03\E0\00c$v\03\FF\00X \00\10\E2p\001\04\FF\08\E3\04A\01\00\00\C8`\00$\03\01p\00u\E2\0F\04$x\04\04`\00\01\C0\00\16\05\C0\00\91\E2\0F\00\07z\03\03\00Y`\00\02@\00Uz\00\03\00X\B0\00\11\04\10\00\10Y\10\00\12\F4\B0\00\08\00\01\10\C4\B0\004\08\04@`\00\11\CA@\00\81\08\00\\\00\00pb\FA@\00@\10x\0A\08\90\00B\FF\E0\FF\07\10\006\10\08\02\10\00\000\00\12\0A0\00#\FC\03\10\00\12\10\10\00\11\F8\10\00T\10x\13\08\030\00\91\C6\0F\00\07\D2\06\08\FF\00\B0\00\11\04\E0\00H\D4\09\FF\04\D0\00#\D2\08 \00#\00\05P\00\12\13P\00\11\F6\C0\00\A3%\D6\06\06\00`\00\00\09\020\00S\E8\0C\0A\01\000\00\10\C6 \00H\08\08\00^ \006\0A\0A\01p\00P\00\81\D9\06\06p\00\BA\00\19\1E\0C\00\A4\00\00$\E4\0B\80\00S\C8\0E\10\02\00P\00u\E2\0F\04\81\D9\12\080\00\87\A2\02\00\07\C8\10\10\02P\00@%\E6\0C\0Cp\00\14\0Bp\001\B8\11\13\EF\01\03\90\00:$\C4\15`\00E\B8\07\13\03@\00\86\1F\00%\E6\0A\0A\00`@\00E\81\E9\14\0Cp\00p\E6\00\00%\C6\0E\0E`\001\15\02\8Ep\01E\81\E9\0A\0A \00\87\E4\0E\00$\B4\16\FF\04\D0\01c%\C6\08\10\00`0\00u\E2/\00\81\C9\0E\0E0\00p&\0F\00%\B6\10\11P\00\12\16P\00F\08\81\C9\08\E0\00\10$ \00D\0C\07\00` \00e\1F\00\81\B9\10\10 \00fh\0F\00\81\B9\0C\A0\00\10b\E0\014\05\05\04\E0\01\81\E2\0F\00#\D2\03\12\06\0C\07P\00\00\00\C6O\D0\02\10\05`\021pR\FA\C0\01T#\E2\03\14\0A \00\85\C8\8F\00#\C2\03\0E\08\10\00v\0F\01#\B2\03\10\0C\10\00p\02GY\00\00P\FD\D1\03\11\83@\03\14Ap\04\03P\03A\88s\00\02,\00\00\96\06f\E8\0F\00\1D{\00\01\00\84\EC\0F\00\84\89\04\02\00 \00\12\E2\C0\03\10?\90\000@\F2\03\A0\01c\84\89\05\02\00\10 \00\93$\0E\00!\82\05\04\05\00\01\00\8F\CA\1F\00\88\83\00\02\05`\00\09\1E\99`\00\14\1F \04\00`\00!\99\07\07\08\05`\00H\92\07\04\07`\00O\93\00\02\07\C0\00\0A\1A\03`\00\10r\EC\01\03\E0\03\13\C6\E0\00\18\04\C0\00J\03\03\04\00\C0\00\0F \01\099M\19\00P\01'\84yp\00\96\22\0E\00$v\04\FF\00b`\02c$v\05\FF\00c\10\00a\CA\0F\00\8Ey\00\81\05@\84\E7\10\0C\D0\02\1BM\A0\01cGy\00\00\F0\FF\C0\01f\C0\0F\00\18y\00\01\00\0F\10\00\A0\0F\01\00-\14\01\DC\02\0B\01\00\22@\00\01\00=W\01\000\00\08\01\00\1F\0B@\00\04\13\97)\00\1F\D4@\00\0C\13\13t\09\0C\01\00\13pU\00*\A8\00\98\09\22\08\00\01\00\22\18\00\01\00.*\01T\00\00\01\00\13\18u\02/p\00\80\00\0B\1F)'\00\03#\00\88@\00\04\10\0C\04\E4\00*\04\00\01\00\1Fl@\00\04\13\B8)\00&\B8\00@\00\1F\0A@\00\00!H\01D\01\0D@\00\13p\F5\03*\D8\00\01\00\1B\08\08\00?7\01\00F\0D\00Q\00\00H\06\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\E8\14\01\0C\84\01\13X@\00\17\901\01\0F\C0\00\01\132@\00\15\06R\00\0B\A1\00\16\07\80\00j\06\00\00\19\80\00\01\00\13\A9+\00+\03\00\01\00\04\DB\02\1F\04\80\00\0B\11\06\D9\06\07\E8\11\0B\01\00\1B\A8\08\00\04\97\00\13\018\00\04\E8\00\0D8\00\1A\08\08\00\088\00\18\06\A0\00\0F\01\00\05\03\A9\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00"} {
    llvm.mlir.global internal @__wg_main_kColReduction_reduce__6_1_0___block_tile_h64_1_0() {addr_space = 3 : i32} : !llvm.array<256 x f32>
    llvm.func @main_kColReduction_reduce__6_1_0___block_tile_h64_1(%arg0: i32, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: !llvm.ptr<1>, %arg6: !llvm.ptr<1>, %arg7: !llvm.ptr<1>) attributes {disc.elimargs = [5 : index, 7 : index, 8 : index, 9 : index, 10 : index, 11 : index, 12 : index, 14 : index, 15 : index, 16 : index, 17 : index, 18 : index, 19 : index, 21 : index, 22 : index, 23 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(32 : index) : i32
      %1 = llvm.mlir.constant(8 : index) : i32
      %2 = llvm.mlir.constant(64 : index) : i32
      %3 = llvm.mlir.constant(4 : index) : i32
      %4 = llvm.mlir.constant(0.000000e+00 : f32) : f32
      %5 = llvm.mlir.constant(2 : index) : i32
      %6 = llvm.mlir.constant(1 : index) : i32
      %7 = llvm.mlir.constant(256 : index) : i32
      %8 = llvm.mlir.constant(0 : index) : i32
      %9 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__6_1_0___block_tile_h64_1_0 : !llvm.ptr<3>
      %10 = llvm.getelementptr %9[0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<256 x f32>
      %11 = nvvm.read.ptx.sreg.ctaid.x : i32
      %12 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %13 = llvm.icmp "eq" %arg0, %6 : i32
      %14 = llvm.select %13, %arg1, %arg0 : i1, i32
      %15 = llvm.icmp "eq" %arg1, %14 : i32
      %16 = llvm.icmp "eq" %arg0, %14 : i32
      %17 = llvm.mul %11, %arg2  : i32
      %18 = llvm.add %12, %17  : i32
      %19 = llvm.icmp "ult" %18, %arg3 : i32
      llvm.cond_br %19, ^bb2, ^bb21
    ^bb2:  // pred: ^bb1
      %20 = llvm.srem %18, %7  : i32
      %21 = llvm.sdiv %18, %7  : i32
      %22 = llvm.udiv %20, %0  : i32
      %23 = llvm.urem %20, %0  : i32
      %24 = llvm.mul %23, %1  : i32
      %25 = llvm.add %22, %24  : i32
      %26 = llvm.icmp "ult" %23, %6 : i32
      llvm.cond_br %26, ^bb3, ^bb10(%4 : f32)
    ^bb3:  // pred: ^bb2
      %27 = llvm.mul %21, %1  : i32
      %28 = llvm.add %22, %27  : i32
      %29 = llvm.mul %28, %2  : i32
      llvm.br ^bb4(%8, %4 : i32, f32)
    ^bb4(%30: i32, %31: f32):  // 2 preds: ^bb3, ^bb9
      %32 = llvm.icmp "slt" %30, %2 : i32
      llvm.cond_br %32, ^bb5, ^bb10(%31 : f32)
    ^bb5:  // pred: ^bb4
      %33 = llvm.add %30, %29  : i32
      %34 = llvm.icmp "slt" %33, %arg4 : i32
      llvm.cond_br %34, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %35 = llvm.urem %33, %3  : i32
      %36 = llvm.udiv %33, %3  : i32
      %37 = llvm.select %15, %36, %8 : i1, i32
      %38 = llvm.mul %37, %3  : i32
      %39 = llvm.add %38, %35  : i32
      %40 = llvm.getelementptr %arg5[%39] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %41 = llvm.load %40 : !llvm.ptr<1> -> f32
      %42 = llvm.select %16, %36, %8 : i1, i32
      %43 = llvm.mul %42, %3  : i32
      %44 = llvm.add %43, %35  : i32
      %45 = llvm.getelementptr %arg6[%44] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %46 = llvm.load %45 : !llvm.ptr<1> -> f32
      %47 = llvm.fmul %41, %46  : f32
      %48 = llvm.fadd %31, %47  : f32
      llvm.br ^bb8(%48 : f32)
    ^bb7:  // pred: ^bb5
      llvm.br ^bb8(%31 : f32)
    ^bb8(%49: f32):  // 2 preds: ^bb6, ^bb7
      llvm.br ^bb9
    ^bb9:  // pred: ^bb8
      %50 = llvm.add %30, %6  : i32
      llvm.br ^bb4(%50, %49 : i32, f32)
    ^bb10(%51: f32):  // 2 preds: ^bb2, ^bb4
      llvm.br ^bb11(%51 : f32)
    ^bb11(%52: f32):  // pred: ^bb10
      llvm.br ^bb12
    ^bb12:  // pred: ^bb11
      %53 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      llvm.store %52, %53 : f32, !llvm.ptr<3>
      nvvm.barrier0
      %54 = llvm.icmp "slt" %22, %3 : i32
      llvm.cond_br %54, ^bb13, ^bb14
    ^bb13:  // pred: ^bb12
      %55 = llvm.add %25, %3  : i32
      %56 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %57 = llvm.load %56 : !llvm.ptr<3> -> f32
      %58 = llvm.getelementptr %10[%55] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %59 = llvm.load %58 : !llvm.ptr<3> -> f32
      %60 = llvm.fadd %57, %59  : f32
      %61 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      llvm.store %60, %61 : f32, !llvm.ptr<3>
      llvm.br ^bb14
    ^bb14:  // 2 preds: ^bb12, ^bb13
      nvvm.barrier0
      %62 = llvm.icmp "slt" %22, %5 : i32
      llvm.cond_br %62, ^bb15, ^bb16
    ^bb15:  // pred: ^bb14
      %63 = llvm.add %25, %5  : i32
      %64 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %65 = llvm.load %64 : !llvm.ptr<3> -> f32
      %66 = llvm.getelementptr %10[%63] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %67 = llvm.load %66 : !llvm.ptr<3> -> f32
      %68 = llvm.fadd %65, %67  : f32
      %69 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      llvm.store %68, %69 : f32, !llvm.ptr<3>
      llvm.br ^bb16
    ^bb16:  // 2 preds: ^bb14, ^bb15
      nvvm.barrier0
      %70 = llvm.icmp "slt" %22, %6 : i32
      llvm.cond_br %70, ^bb17, ^bb18
    ^bb17:  // pred: ^bb16
      %71 = llvm.add %25, %6  : i32
      %72 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %73 = llvm.load %72 : !llvm.ptr<3> -> f32
      %74 = llvm.getelementptr %10[%71] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %75 = llvm.load %74 : !llvm.ptr<3> -> f32
      %76 = llvm.fadd %73, %75  : f32
      %77 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      llvm.store %76, %77 : f32, !llvm.ptr<3>
      llvm.br ^bb18
    ^bb18:  // 2 preds: ^bb16, ^bb17
      nvvm.barrier0
      %78 = llvm.icmp "eq" %22, %8 : i32
      %79 = llvm.and %78, %26  : i1
      llvm.cond_br %79, ^bb19, ^bb20
    ^bb19:  // pred: ^bb18
      %80 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %81 = llvm.load %80 : !llvm.ptr<3> -> f32
      %82 = llvm.getelementptr %arg7[%23] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %83 = llvm.atomicrmw fadd %82, %81 acq_rel : !llvm.ptr<1>, f32
      llvm.br ^bb20
    ^bb20:  // 2 preds: ^bb18, ^bb19
      llvm.br ^bb21
    ^bb21:  // 2 preds: ^bb1, ^bb20
      llvm.return
    }
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = true, sym_name = "C4", value = 4 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = true, sym_name = "C1", value = 1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S4", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = false, sym_name = "S5", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %c4 = arith.constant 4 : index
    %0 = "disc_shape.dim"() {name = @S2} : () -> index
    %1 = "disc_shape.dim"() {name = @S3} : () -> index
    "disc_shape.tie_product_equal"(%c4, %0, %1) {operand_segment_sizes = array<i32: 2, 1>} : (index, index, index) -> ()
    %2 = "disc_shape.dim"() {name = @S4} : () -> index
    %3 = "disc_shape.dim"() {name = @S5} : () -> index
    "disc_shape.tie_product_equal"(%c4, %2, %3) {operand_segment_sizes = array<i32: 2, 1>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After StripDebugInfo (strip-debuginfo) //----- //
module attributes {gpu.container_module} {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
    %c256 = arith.constant 256 : index
    %c32 = arith.constant 32 : index
    %c512 = arith.constant 512 : index
    %0 = llvm.mlir.constant(0 : i32) : i32
    %c4_i32 = arith.constant 4 : i32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
    %2 = "disc_ral.dispatch"(%arg0, %c1) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
    %dim = memref.dim %2, %c0 : memref<?x4xf32>
    %dim_0 = memref.dim %1, %c0 : memref<?x4xf32>
    %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
    %reinterpret_cast_1 = memref.reinterpret_cast %2 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
    %3 = arith.cmpi eq, %dim, %c1 : index
    %4 = arith.select %3, %dim_0, %dim : index
    %alloc = memref.alloc(%dim_0) {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
    %5 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
    "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast, %alloc) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
    "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
    %alloc_2 = memref.alloc(%dim) {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
    "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_1, %alloc_2) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
    "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
    %6 = arith.index_cast %4 : index to i32
    %7 = arith.muli %6, %c4_i32 : i32
    %8 = arith.index_cast %7 : i32 to index
    %9 = arith.cmpi eq, %dim_0, %4 : index
    %10 = arith.cmpi eq, %dim, %4 : index
    %11 = arith.andi %10, %9 : i1
    %alloc_3 = memref.alloc() : memref<1xf32, #gpu.address_space<global>>
    cf.cond_br %11, ^bb1, ^bb4
  ^bb1:  // pred: ^bb0
    %12 = arith.cmpi slt, %8, %c1 : index
    cf.cond_br %12, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    gpu.launch_func  @main_kernel::@main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32 blocks in (%c1, %c1, %c1) threads in (%c512, %c1, %c1) args(%c512 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    %13 = arith.cmpi eq, %8, %c0 : index
    %14 = arith.subi %8, %c1 : index
    %15 = arith.divui %14, %c32 : index
    %16 = arith.addi %15, %c1 : index
    %17 = arith.select %13, %c0, %16 : index
    %18 = arith.muli %17, %c512 : index
    %c512_4 = arith.constant 512 : index
    %c0_5 = arith.constant 0 : index
    %c1_6 = arith.constant 1 : index
    %19 = arith.cmpi sle, %18, %c0_5 : index
    %20 = arith.subi %c0_5, %18 : index
    %21 = arith.subi %18, %c1_6 : index
    %22 = arith.select %19, %20, %21 : index
    %23 = arith.divsi %22, %c512_4 : index
    %24 = arith.subi %c0_5, %23 : index
    %25 = arith.addi %23, %c1_6 : index
    %26 = arith.select %19, %24, %25 : index
    gpu.launch_func  @main_kernel_0::@main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_1 blocks in (%26, %c1, %c1) threads in (%c512, %c1, %c1) args(%c512 : index, %18 : index, %8 : index, %4 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    cf.br ^bb7
  ^bb3:  // pred: ^bb1
    gpu.launch_func  @main_kernel_1::@main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64 blocks in (%c1, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    %27 = arith.cmpi eq, %8, %c0 : index
    %28 = arith.subi %8, %c1 : index
    %29 = arith.divui %28, %c512 : index
    %30 = arith.addi %29, %c1 : index
    %31 = arith.select %27, %c0, %30 : index
    %32 = arith.muli %31, %c256 : index
    %c256_7 = arith.constant 256 : index
    %c0_8 = arith.constant 0 : index
    %c1_9 = arith.constant 1 : index
    %33 = arith.cmpi sle, %32, %c0_8 : index
    %34 = arith.subi %c0_8, %32 : index
    %35 = arith.subi %32, %c1_9 : index
    %36 = arith.select %33, %34, %35 : index
    %37 = arith.divsi %36, %c256_7 : index
    %38 = arith.subi %c0_8, %37 : index
    %39 = arith.addi %37, %c1_9 : index
    %40 = arith.select %33, %38, %39 : index
    gpu.launch_func  @main_kernel_2::@main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1 blocks in (%40, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %32 : index, %8 : index, %4 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    cf.br ^bb7
  ^bb4:  // pred: ^bb0
    %41 = arith.cmpi slt, %8, %c1 : index
    cf.cond_br %41, ^bb5, ^bb6
  ^bb5:  // pred: ^bb4
    gpu.launch_func  @main_kernel_3::@main_kColReduction_reduce__6_1_0___thread_tile_h32 blocks in (%c1, %c1, %c1) threads in (%c512, %c1, %c1) args(%c512 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    %42 = arith.cmpi eq, %8, %c0 : index
    %43 = arith.subi %8, %c1 : index
    %44 = arith.divui %43, %c32 : index
    %45 = arith.addi %44, %c1 : index
    %46 = arith.select %42, %c0, %45 : index
    %47 = arith.muli %46, %c512 : index
    %c512_10 = arith.constant 512 : index
    %c0_11 = arith.constant 0 : index
    %c1_12 = arith.constant 1 : index
    %48 = arith.cmpi sle, %47, %c0_11 : index
    %49 = arith.subi %c0_11, %47 : index
    %50 = arith.subi %47, %c1_12 : index
    %51 = arith.select %48, %49, %50 : index
    %52 = arith.divsi %51, %c512_10 : index
    %53 = arith.subi %c0_11, %52 : index
    %54 = arith.addi %52, %c1_12 : index
    %55 = arith.select %48, %53, %54 : index
    gpu.launch_func  @main_kernel_4::@main_kColReduction_reduce__6_1_0___thread_tile_h32_1 blocks in (%55, %c1, %c1) threads in (%c512, %c1, %c1) args(%dim : index, %dim_0 : index, %c512 : index, %47 : index, %8 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    cf.br ^bb7
  ^bb6:  // pred: ^bb4
    gpu.launch_func  @main_kernel_5::@main_kColReduction_reduce__6_1_0___block_tile_h64 blocks in (%c1, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    %56 = arith.cmpi eq, %8, %c0 : index
    %57 = arith.subi %8, %c1 : index
    %58 = arith.divui %57, %c512 : index
    %59 = arith.addi %58, %c1 : index
    %60 = arith.select %56, %c0, %59 : index
    %61 = arith.muli %60, %c256 : index
    %c256_13 = arith.constant 256 : index
    %c0_14 = arith.constant 0 : index
    %c1_15 = arith.constant 1 : index
    %62 = arith.cmpi sle, %61, %c0_14 : index
    %63 = arith.subi %c0_14, %61 : index
    %64 = arith.subi %61, %c1_15 : index
    %65 = arith.select %62, %63, %64 : index
    %66 = arith.divsi %65, %c256_13 : index
    %67 = arith.subi %c0_14, %66 : index
    %68 = arith.addi %66, %c1_15 : index
    %69 = arith.select %62, %67, %68 : index
    gpu.launch_func  @main_kernel_6::@main_kColReduction_reduce__6_1_0___block_tile_h64_1 blocks in (%69, %c1, %c1) threads in (%c256, %c1, %c1) args(%dim : index, %dim_0 : index, %c256 : index, %61 : index, %8 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    cf.br ^bb7
  ^bb7:  // 4 preds: ^bb2, ^bb3, ^bb5, ^bb6
    memref.dealloc %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>
    memref.dealloc %alloc : memref<?x4xf32, #gpu.address_space<global>>
    %alloca = memref.alloca() : memref<0xindex>
    %70 = "disc_ral.dispatch"(%arg0, %5, %alloc_3, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1xf32, #gpu.address_space<global>>, memref<0xindex>) -> memref<f32, #gpu.address_space<global>>
    %reinterpret_cast_16 = memref.reinterpret_cast %70 to offset: [0], sizes: [], strides: [] : memref<f32, #gpu.address_space<global>> to memref<f32, #gpu.address_space<global>>
    memref.dealloc %alloc_3 : memref<1xf32, #gpu.address_space<global>>
    %alloc_17 = memref.alloc() : memref<f32>
    "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_16, %alloc_17) {backend_config = "", call_target_name = "d2h", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<f32, #gpu.address_space<global>>, memref<f32>) -> ()
    "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
    memref.dealloc %reinterpret_cast_16 : memref<f32, #gpu.address_space<global>>
    "disc_ral.dispatch"(%arg0, %c0, %alloc_17) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<f32>) -> ()
    return
  }
  gpu.module @main_kernel attributes {gpu.binary = "P\EDU\BA\01\00\10\00H\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\08\04\00\00\00\00\00\00\08\04\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00h\0C\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\002\C0\0B\00\01\00\11\09\06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\11e__6_1_0___no_ibXthread_tile_h32H\00\0FB\00+osharedD\00+\9Fconstant0G\00(\FA\01debug_frame\00.rel\11\00!nv\14\00\11aS\00\0Fk\01 \0F\98\00%\0F\A4\01\FAo_param\AB\01\1C\0F\01\00\06\8Ck\00\00\00\03\00\0A\00\01\00 0\01\18\00,\09\00\01\00\11~\18\00,\04\00\01\00\11\9C\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\1C\00\00\00E\00#\04\10\18\00\80/\08\00\05\00\00\00\06\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04x\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C$\00u\01\00\08\00\00\F0!\10\00\01\01\00\C0\F0\11\00\03\1B\FF\00\04\1C\08\00p\B0\04\12\00\01\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00 \01/\05\00\01\00\FF\B0@$v\01\FF\0F\04\A3\FF\00\8E\07\00\C4\0F\00\C3y\EC\03\A2%\00\00\00\22\0E\00\19y\00\01\00\F3\0E!\00\00\00b\0E\00\90r\05?\04\00\00\80?\E0\FF\0F\00\C6\1F\00\B9z\04\00\00X\DE\02p\E4\0F\00\A4r\04\05B\00\B1?\02\8E\0F\00\CC\0F\00\0C|\00>\04\95pR\F0\0B\00\DA/\00Mc\04\F3\0D\80\03\00\EA\0F\00\02z\02\00\00Z\00\00\00\0F\00\00\00\E2\0F\00$v\03\FF\00[\90\00\22\E2\0F`\001F\00\00\B3\04\80\C8\0F\00\86y\00\02\FF\B0\030\19\10\0C \00*MyP\00PGy\00\00\F09\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\80\0F\01\00-\00\07\05.\03\00\01\00\22@\00\01\00=k\01\000\00\08\01\00\1F\0B@\00\04*\AB\01\08\00\0F@\00\05\13\13\0C\04\0C\01\00\13XU\00*\90\000\04\04M\04\22\18\00\01\00.>\01T\00\00\01\00\13\E8@\00/p\00\80\00\0B\1F)'\00\03A\00X\04\00\01\00\04@\06\04\E4\00*\04\00\01\00\1Fq@\00\04\13\88)\00&L\00@\00\1F\0A@\00\00!\\\01D\01\0D@\00\13\D8)\00*\D8\00\01\00\1B\08\08\00%K\01\DB\0A\09\01\00\13\B0]\05\17\10\80\00\17\048\00\04\18\00\13\F7\14\01\0C\84\01*\C0\05(\07\1F\00\C0\00\04\132@\00*\06\00\01\00*\80\07\C0\07\12\03\C0\01:\06\80\00\01\00\13\06\D8\01\05\A8\0B\0B\01\00*\A8\00\08\00\04\F8\00\14\018\00/\05\00\01\00\029@\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32(%arg0: i32, %arg1: !llvm.ptr<1>) attributes {disc.elimargs = [1 : index, 3 : index, 4 : index, 5 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(1 : index) : i32
      %1 = llvm.mlir.constant(0.000000e+00 : f32) : f32
      %2 = nvvm.read.ptx.sreg.ctaid.x : i32
      %3 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %4 = llvm.mul %2, %arg0  : i32
      %5 = llvm.add %3, %4  : i32
      %6 = llvm.icmp "ult" %5, %0 : i32
      llvm.cond_br %6, ^bb2, ^bb3
    ^bb2:  // pred: ^bb1
      %7 = llvm.getelementptr %arg1[%5] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      llvm.store %1, %7 : f32, !llvm.ptr<1>
      llvm.br ^bb3
    ^bb3:  // 2 preds: ^bb1, ^bb2
      llvm.return
    }
  }
  gpu.module @main_kernel_0 attributes {gpu.binary = "P\EDU\BA\01\00\10\00\00\09\00\00\00\00\00\00\02\00\01\01@\00\00\00\C0\08\00\00\00\00\00\00\BF\08\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00h\17\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\002\C0\16\00\01\00\11\14\06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\13e__6_1_0___no_ibXthread_tile_h32_1J\00\0FD\00-osharedF\00-\9Fconstant0I\00*\FA\01debug_frame\00.rel\11\00!nv\14\00\11aU\00\0Fs\01 \0F\9A\00'\0F\AE\01\FF\03o_param\B5\01\1C\0F\01\00\04\8Cm\00\00\00\03\00\0A\00\01\00 8\01\18\00,\09\00\01\00\11\88\18\00,\04\00\01\00\11\A6\18\00,\07\00\01\00g2\00\00\00\12\10`\00\11\0C\06\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0\0B\07\00 \00\04\9B\00 \04\18i\00\01:\002\04\B8\02\18\00p/\08\00\05\00\00\00\1A\00\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\88\04\80\015\00\00\04\0A\08\00F\00\A2`\01(\00\03\19(\00\04\17\C5\00u\05\00 \00\00\F0!\10\009\04\00\18\10\009\03\00\10\10\00u\02\00\08\00\00\F0\11\10\009\01\00\04\10\00\01\01\00\F3\01\F0\11\00\03\1B\FF\00\04\1C\08\00`\00\00\00P\D8\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\00\D2\05b,\00\00\00H\00\01\00\00`\01/\05\00\01\00\FF\E0@$v\01\FF\7F\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\BB\02Q\0E\00\19y\03\0F\00 \00!-\00\F0\14\0E\00$z\00\00\00X\00\00\03\02\8E\07\00\CA\1F\00\12x\FF\00\FF\01\00\00\FF\C0\80\07\00\C8\0F\00\0C \00\C5Y\00\00pdp\00\00\DA\0F\00M\C3\04\B0\80\03\00\EA\0F\00\19x\03\FF\1F\0F\00\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\00\03\05a\C6\0F\00\11r\03q\001\FFH\8FP\00\000\00\00C\00\10\030\00\93\CA\0F\00$x\07\03 \00\B0\00p\C8\0F\00%x\04\07s\04\10\FF\90\00\F0\09\E2\0F\04\0Cz\00\07\00Z\00\00pb\F4\03\00\E4\0F\04\10x\00\07\01 \00\B0\E0\FF\07\00\E4\0F\00\12x\04\04\01\031\FF\FC\8E\10\00\01\B0\00\010\00\10\F60\00p\00\10z\02\04\00\\0\00!\F1\07@\00Pz\04\04\00^\10\00\11\F3 \01\00P\00\00,\03\04P\00\C2\10z\03\05\00]\00\00\FF\E4\7F\000\00@\05\05\00_\10\00*\FF\00`\00 \F8\03\F0\00\F4\06\81\B9\0B\02\04\FC\FF\FF\00\19\1E\0C\00\A2\0E\00\10x\06\07\03P\00\93\C6\0F\00\81\A9\00\02\04\F8 \00u\E8\0E\00\81\A9\09\04\10\00 \E2\0EP\00\12\06P\00!\FA\030\005\B9\08\04P\00p\A8\0E\00\81\C9\0D\02\10\01\01\10\00y(\0F\00\81\C9\0A\04\10\00S\D9\0F\02\04\04\10\00\10h\10\00%\0C\04\10\00!b\0F\90\00\14\04\90\00\01\F0\006\0E\07\05\F0\00\16\04\80\00\12\FC0\01Jx\10\07\06 \00\12\0E \00\13\F0 \00:\0E\07\07 \00\12\10 \00\11\F2\10\01T$r\06\FF\FF\D0\01\12\E2P\00\14\0A0\00a\E2\0F\04#\A2\06\06\07\00$\00F\00\E2\8F\00`\00\12\F4P\01S\E9\09\02\04\08\C0\00!\E2\0Ep\00\18\08P\018\E9\00\04 \00@#\B2\06\0B\0F\00\11\06P\00\17OP\00\02\00\02P\81\89\0B\02\04\0F\06\05\A0\018\0E\07\09P\008\89\08\04 \00T#\C2\06\0D\0AP\00\93\C6\0F\01\81\99\0D\02\04\10 \00'\22\0F`\00\12\F8\B0\01W\99\0A\04\04\10\90\01X\A9\11\02\04\14\10\00F\0E\04\04\14@\00U#\D2\06\0F\0C\B0\00&\0F\02@\01 \FA\03\A0\01g\81\B9\0F\02\04\18\D0\019\B9\0C\04\10\00X\C9\13\02\04\1C\10\00)\10\04\10\00X\D9\15\02\04 \10\00H\12\04\04 \10\026\14\07\0B\90\01e\00#\E2\06\09\00\90\00\11\8F\10\03\17\0C0\02E\0Cz\00\14 \02\000\00\1D\82p\01\1A\00 \027\00\07\0D \02X\10x\08\07\0Ep\00\17\92`\016\E2\0F\01@\00C\F2\03\00\CA\00\025$\00\00\00\03\00\F0\01\17$\A0\01F\A2\06\11\0E@\00\00\80\00\17\08 \04\02\F0\01\18(@\028\08\07\0F\F0\01\00\D0\01\17,\90\01\1D\B2\90\01\19\08@\02\00\E0\01\070\00\\\10x\0C\07\10@\02\07p\00Z#\C2\06\13\10\A0\00\15\0C0\02\00P\00X\A9\0F\02\040\90\018\10\07\11P\008\A9\0C\04 \00Z#\D2\06\15\12P\00\060\02\00P\00W\B9\11\02\044\F0\01[\B9\0E\04\0440\02\1B80\02\1B80\02\1B<0\02\1A<0\02\1F\120\02\06\11O\F0\01\1F\130\02\16\1A\8F0\02\13\D6\F0\01\18@\A0\03?\00\07\140\02\08\00\D0\01\16\15`\02\19\00@\02\000\00\1F\A2\D0\01\05\11\F4 \00\01P\02\09p\009\08\07\16\B0\027\0A\07\17\80\00\01P\02\19Dp\04\0F\80\02\04\12\F6\C0\03\00\10\02\040\00\00\F0\05\00p\02\17Hp\02\0E \02\19\0A \02\00p\02\070\00\00\B0\04\19\18 \02O\0F\02\04L \02\0A\19\0E \02\00P\02\17L\E0\01\000\02\1BP0\02\1BP0\02\1BT0\02\1BT0\02\1BX0\02\1AX0\02\1F\190\02\05*\C6O \02\030\04\18\1A0\00\0B0\02\00\A0\01\14\1B \00\13\D20\02\16\\\C0\01\0FP\04\0A\03`\00\1A\1C\B0\06\1A\08\B0\048\08\07\1D\80\00\0E@\02\15\00@\02\13\C4@\02\18\\@\028\0A\07\1E\A0\01_\99\0B\02\04`0\02\14\01\F0\01(\08\040\00\00`\04\18\1FP\00_\89\07\02\04d@\02\18X\89\0A\04\04d\80\06O\0D\02\04h0\02\0A\15\0C0\02\13\C40\02\17h\E0\01\00\90\06\1Bl0\02\1Bl \02\18p\10\00K\11\02\04p \02\18t\10\00%\13\02\10\00\1Bb \02 \C8O\D0\01\06\00\02 \C8\8F\10\02%\07\0A\10\00v\0F\01#\A2\06\0D\0C\10\00\10\02\C0\05\08`\01c$v\08\FF\00`\80\08\10\E4\10\00C\09\FF\00a\10\00\11\E2@\01&\11\10@\00\00\10\01\15\13\10\01p\CA\0F\00\8Ey\00\08\0C\00@\84\E7\10\0C0\00*My\F0\0APGy\00\00\F09\0F\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00p\0F\01\00-\11\01h\0F\0E\01\00\22@\00\01\00=s\01\000\00\08\01\00\1F\0B@\00\04\13\B3)\00\1F\B5@\00\0C\13\13\BC\0E\0C\01\00\13hU\00*\90\00\E0\0E\22\08\00\01\00\22\18\00\01\00.F\01T\00\00\01\00\13\F8@\00/p\00\80\00\0B\1F)'\00\03A\00h\04\00\01\00\00\9B\07\17\00\E4\00*\04\00\01\00\1Fs@\00\04\13\98)\00&\8C\00@\00\1F\0A@\00\00!d\01D\01\0D@\001(\05\00\01\00*\D8\00\01\00\1B\08\08\00?S\01\00f\12\00\04\E1\02\10\00B\11\05\80\00\17\048\00\04\18\00\13\FD\14\01\0C\84\01&\10\06\B0\12\04\01\00\0F\C0\00\01\132@\00+\06\00\01\00\1A\08\B0\12\12\03\C0\01>\18\80\00\A7\00\18\05\A8\16\0B\01\00*\A8\00\08\00\04W\00\13\018\00\04\A8\00\0D\D0\12)\0D\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_1(%arg0: i32, %arg1: i32, %arg2: i32, %arg3: !llvm.ptr<1>, %arg4: !llvm.ptr<1>, %arg5: !llvm.ptr<1>) attributes {disc.elimargs = [3 : index, 4 : index, 6 : index, 7 : index, 8 : index, 9 : index, 10 : index, 11 : index, 13 : index, 14 : index, 15 : index, 16 : index, 17 : index, 18 : index, 20 : index, 21 : index, 22 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(0 : index) : i32
      %1 = llvm.mlir.constant(1 : index) : i32
      %2 = llvm.mlir.constant(512 : index) : i32
      %3 = llvm.mlir.constant(32 : index) : i32
      %4 = llvm.mlir.constant(0.000000e+00 : f32) : f32
      %5 = nvvm.read.ptx.sreg.ctaid.x : i32
      %6 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %7 = llvm.mul %5, %arg0  : i32
      %8 = llvm.add %6, %7  : i32
      %9 = llvm.icmp "ult" %8, %arg1 : i32
      llvm.cond_br %9, ^bb2, ^bb12
    ^bb2:  // pred: ^bb1
      %10 = llvm.srem %8, %2  : i32
      %11 = llvm.sdiv %8, %2  : i32
      %12 = llvm.icmp "ult" %10, %1 : i32
      llvm.cond_br %12, ^bb3, ^bb11
    ^bb3:  // pred: ^bb2
      %13 = llvm.mul %11, %3  : i32
      llvm.br ^bb4(%0, %4 : i32, f32)
    ^bb4(%14: i32, %15: f32):  // 2 preds: ^bb3, ^bb9
      %16 = llvm.icmp "slt" %14, %3 : i32
      llvm.cond_br %16, ^bb5, ^bb10
    ^bb5:  // pred: ^bb4
      %17 = llvm.add %13, %14  : i32
      %18 = llvm.icmp "slt" %17, %arg2 : i32
      llvm.cond_br %18, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %19 = llvm.getelementptr %arg3[%17] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %20 = llvm.load %19 : !llvm.ptr<1> -> f32
      %21 = llvm.getelementptr %arg4[%17] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %22 = llvm.load %21 : !llvm.ptr<1> -> f32
      %23 = llvm.fmul %20, %22  : f32
      %24 = llvm.fadd %15, %23  : f32
      llvm.br ^bb8(%24 : f32)
    ^bb7:  // pred: ^bb5
      llvm.br ^bb8(%15 : f32)
    ^bb8(%25: f32):  // 2 preds: ^bb6, ^bb7
      llvm.br ^bb9
    ^bb9:  // pred: ^bb8
      %26 = llvm.add %14, %1  : i32
      llvm.br ^bb4(%26, %25 : i32, f32)
    ^bb10:  // pred: ^bb4
      %27 = llvm.getelementptr %arg5[%10] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %28 = llvm.atomicrmw fadd %27, %15 acq_rel : !llvm.ptr<1>, f32
      llvm.br ^bb11
    ^bb11:  // 2 preds: ^bb2, ^bb10
      llvm.br ^bb12
    ^bb12:  // 2 preds: ^bb1, ^bb11
      llvm.return
    }
  }
  gpu.module @main_kernel_1 attributes {gpu.binary = "P\EDU\BA\01\00\10\00H\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\08\04\00\00\00\00\00\00\06\04\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00h\0C\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\002\C0\0B\00\01\00\11\09\06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\10e__6_1_0___no_ibXblock_tile_h64G\00\0FA\00*osharedC\00*\9Fconstant0F\00'\FA\01debug_frame\00.rel\11\00!nv\14\00\11aR\00\0Fg\01 \0F\97\00$\0F\9F\01\F6o_param\A6\01\1C\0F\01\00\07\8Cj\00\00\00\03\00\0A\00\01\00 ,\01\18\00,\09\00\01\00\11y\18\00,\04\00\01\00\11\97\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\1C\00\00\00E\00#\04\10\18\00\80/\08\00\05\00\00\00\06\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04p\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C$\00u\01\00\08\00\00\F0!\10\00\01\01\00\C0\F0\11\00\03\1B\FF\00\04\1C\08\00p\A8\04\12\00\01\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00 \01/\05\00\01\00\FF\B8@$v\01\FF\17\04\A3\FF\00\8E\07\00\C4\0F\00\C3y\F4\03\A2%\00\00\00\22\0E\00\19y\00\01\00\F3\0E!\00\00\00b\0E\00\90r\05?\04\00\00\80?\E0\FF\0F\00\C6\1F\00\B9z\04\00\00X\E6\02p\E4\0F\00\A4r\04\05B\00\B1?\02\8E\0F\00\CC\0F\00\0C|\00F\04\95pR\F0\0B\00\DA/\00Mk\04\F3\0D\80\03\00\EA\0F\00\02z\02\00\00Z\00\00\00\0F\00\00\00\E2\0F\00$v\03\FF\00[\90\00\22\E2\0F`\001F\00\00\BB\04\80\C8\0F\00\86y\00\02\FF\B8\030\19\10\0C \00*MyP\00PGy\00\00\F0A\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\80\0F\01\00-\00\0F\05.\03\00\01\00\22@\00\01\00=g\01\000\00\08\01\00\1F\0B@\00\04\13\A7)\00\1F\A6@\00\0C\13\13\14\04\0C\01\00\13PU\00*\90\008\04\04U\04\22\18\00\01\00.:\01T\00\00\01\00\13\E0@\00/p\00\80\00\0B\1F)'\00\03A\00P\04\00\01\00\04H\06\04\E4\00*\04\00\01\00\1Fp@\00\04\13\80)\00&L\00@\00\1F\0A@\00\00!X\01D\01\0D@\00\13\D0)\00*\D8\00\01\00\1B\08\08\00%G\01\DB\0A\09\01\00\13\A8e\05\17\10\80\00\17\048\00\04\18\00\13\F4\14\01\0C\84\01*\B8\050\07\1F\00\C0\00\04\132@\00*\06\00\01\00*\80\07\C8\07\12\03\C0\01:\06\80\00\01\00\13\06\D8\01\05\A8\0B\0C\C8\00\1A\00\08\00\04\F8\00\13\018\00\04\A8\00\0C\01\009H\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64(%arg0: i32, %arg1: !llvm.ptr<1>) attributes {disc.elimargs = [1 : index, 3 : index, 4 : index, 5 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(1 : index) : i32
      %1 = llvm.mlir.constant(0.000000e+00 : f32) : f32
      %2 = nvvm.read.ptx.sreg.ctaid.x : i32
      %3 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %4 = llvm.mul %2, %arg0  : i32
      %5 = llvm.add %3, %4  : i32
      %6 = llvm.icmp "ult" %5, %0 : i32
      llvm.cond_br %6, ^bb2, ^bb3
    ^bb2:  // pred: ^bb1
      %7 = llvm.getelementptr %arg1[%5] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      llvm.store %1, %7 : f32, !llvm.ptr<1>
      llvm.br ^bb3
    ^bb3:  // 2 preds: ^bb1, ^bb2
      llvm.return
    }
  }
  gpu.module @main_kernel_2 attributes {gpu.binary = "P\EDU\BA\01\00\10\00\A8\0B\00\00\00\00\00\00\02\00\01\01@\00\00\00h\0B\00\00\00\00\00\00c\0B\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\A8#\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00g\02\00\BE\00r\00\01\00\22#\00\01\00\11 \06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\12e__6_1_0___no_ibXblock_tile_h64_1I\00\0FC\00,osharedE\00,\9Fconstant0H\00)\FA\01debug_frame\00.rel\11\00!nv\14\00\11aT\00\0Fo\01 \0F\99\00&\0F\A9\01\B6\8F$____wg_B\00&\00\1B\00/26\F1\016o_param\F8\01\1C\0F\01\00\05\8Cl\00\00\00\03\00\0A\00\01\00\11\EF\18\00,\0B\00\01\00 |\01\18\00,\09\00\01\00\11\CB\18\00,\04\00\01\00\11\E9\18\00,\07\00\01\00g2\00\00\00\12\10x\00\11\18\06\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0\17\07\00 \00\04\9B\00R\04\14\00\00\00E\002\04\C0\05\18\00C/\08\00\06\7F\00\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E0\04\F1\08\015\00\00\04\0A\08\00\03\00\00\00`\01(\00\03\19(\00\04\17\0C$\00\10\05\ED\04%\F0!\10\009\04\00\18\10\009\03\00\10\10\00u\02\00\08\00\00\F0\11\10\009\01\00\04\10\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\00\10\17\00\00`\17\00\00\04\1Et\01#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\005\02\00\00\84\01\0F\01\00\FFz@$v\01\FF?\04\A3\FF\00\8E\07\00\C4\0F\00\19y\A6\01\10%[\02a\0E\00\19y\03\00\01\00\10!-\00\F5\14\0E\00$z\06\06\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\06\00Y\00\00p`\F0\03\00\DA\0F\00M[\04\B0\80\03\00\EA\0F\00\19x\03\FF\1F\FC\01\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\00\B3\04\93\E2\0F\00Ey\00\00@\150\00\93\E2\0F\00$r\08\FF\FF\00\90\00p\E2\0F\00\11r\03\03\92\00\C1\FF@\8F\07\00\C8\0F\00\12x\05\031\04\10\C0\80\00p\0F\00$x\06\06\01\B4\03\12\0A\10\00@\12x\05\06p\00\01 \00p\E4\0F\04\19x\00\FF*\04\C0\06\16\01\00\00\E4\0F\00\0Cr\00\05`\00@pR\F2\03 \00P\0Cx\00\06\7F\10\00\22@\F0\80\002x\07\05\C9\02\11\8Ep\00T$x\07\07\04\90\00\9A\CC\0F\00G\19\00\00\80\14\E0\00\000\00\10\03\E0\00\01\90\00*\03\03@\004\09\03@@\00q\C8\0F\00%x\04\09P\00\11\02\E0\00P\04\10x\00\09\C0\001\FF\E0\FF\B0\00\B0\0Cz\00\09\00Z\00\00pb\F4\A0\00P\00\12x\04\04P\00!\FF\FC\D0\00\00p\01\12\00 \00\11\F6 \00`\10z\02\04\00\\@\00\11\F3@\00`\10z\04\04\00^\10\00\11\F9\D0\01\00`\00\00|\03\03`\00\D2\00\10z\03\05\00]\00\00\FF\E4\FF\000\00@\05\05\00_\10\00*\7F\02`\00\11\F8\10\01\F4\06\81\B9\0D\02\04\FC\FF\FF\00\19\1E\0C\00\A2\0E\00\10x\0A\09\03P\00\93\C6\0F\00\81\A9\00\02\04\F8 \00 \E2\0E@\00\12\0A@\00\22\FA\03 \00%\0B\04 \00u\E8\0E\00\81\B9\0A\04P\00p\A8\0E\00\81\C9\0F\02\10\01\01\10\00y(\0F\00\81\C9\0C\04\10\00S\D9\11\02\04\04\10\00\10h\10\00%\0E\04\10\00\10b\E0\004\10\09\04\90\00\01\F0\00:\12\09\05P\01\12\10\90\00#\FC\03\E0\00\12\12\10\00\12\F2@\01Dx\10\09\060\00a\E2\0F\04#\A2\086\07\00$\021\00\E2\8F@\01\14\07 \00q\CE\0F\00\81\E9\0B\02\91\01\06\F0\00\15\10\B0\01\93\E2\0F\00#\B2\08\0D\0A\00=\05!\E2O0\01\16\08`\00`\00\81\99\0D\02\04\17\06\03P\01\09\D0\01\000\018\E9\00\04`\00U#\C2\08\0F\0CP\00&\0F\01`\01\11\F8\C0\00H\81\99\0A\04P\00\00\D0\00\19\09\A0\01C\0F\02\04\10 \00\86\22\0F\00#\D2\08\11\0EP\00\16\02\C0\00\12\FAP\00W\A9\0C\04\04\10\80\01W\B9\11\02\04\14\80\019\B9\0E\04\10\00X\C9\13\02\04\18\10\00)\10\04\10\00X\D9\15\02\04\1C\10\00H\12\04\04\1C\C0\017\14\09\0A \01U#\E2\08\0B\00\A0\00\02\80\01\17\0B\E0\01E\0Cz\00\14\D0\01\000\00\1A\92p\01\06P\01\04\E0\017\00\09\0C\10\02\00\90\01\18\0Dp\00\1D\A2`\01\15\00\D0\01\13\CA\F0\01\18 `\018\0C\09\0F`\01\00\B0\01\07 \00-#\B2`\01\19\0A\B0\03Lx\0A\09\0E\10\02\19$\F0\01$\13\10@\00\02\B0\02\06\F0\01\13\E2\D0\01\17(\A0\01\00\00\02\08@\00F\D2\08\15\12\80\00\00@\00\15\0C\E0\01\13\C4\D0\01\17,\90\01\00\F0\01\08P\00\00\E0\01\1B,\E0\01\1B0\E0\01\1B0\E0\01\1B4\E0\01\1A4\E0\01\1F\10\E0\01\06\11O\A0\01\1F\11\E0\01\16\1A\8F\E0\01\13\D6\A0\01\188\A0\01;\00\09\12\F0\01\1F\13\F0\01\15\13\E4\D0\01\1F8\D0\01\1B\16\14`\00\11\04\D0\01\16<\90\01X\10x\0E\09\15\80\00\08\E0\01\1B\E2\E0\01\04\B0\03\1B@\E0\01\1F<\E0\01\0A\1D\0E\C0\03\1B@\C0\03\1BD\C0\03\1BD\E0\01\1BH\E0\01\1BH\E0\01\1BL\E0\01\1AL\E0\01\1F\16\E0\01\0C\1F\17\E0\01-\1AP\E0\01\1F\18\D0\01\08\00\90\01\16\19\00\02\1F\00\E0\01\02\1FP\E0\01\17\01\D0\01\18T\D0\017\0A\09\1A\D0\00\00\10\04\1F\1B\E0\01\15\13\E4\E0\01\1BX\E0\01\1FT\C0\03\1C\1B\\\C0\03\1BX\C0\03\1B\\\E0\01\1B`\E0\01\1B`\E0\01\1Bd\E0\01\1Ad\E0\01\1F\1C\E0\01\0C\1F\1D\E0\01\05\13\DA\C0\01\17hp\01\0F\F0\01\09\01P\03;\00\09\1E\C0\03\1F\1F\C0\03\1D\1Fh\E0\01\14\01\C0\01<\0A\09 \C0\03\1Al\C0\03\1F!\C0\03\1D\1Bp\E0\01\1Fl\C0\03\1C\1Bp\C0\03\1Bt\C0\03\1Bt\E0\01\1Bx\E0\01\1Bx\E0\01\1B|\E0\01\1A|\E0\01\1F\22\E0\01\0C\1F#\C0\03-\16\80\90\01\00P\00\1F$\C0\03\0C\1F%\C0\03\0D\1F\80\C0\03\1C\18\84\D0\017\0A\09&\D0\00\00\C0\03\1F'\C0\03\1D\1B\88\E0\01\1F\84\C0\03\1C\1B\8C\C0\03\1B\88\C0\03\1B\8C\E0\01\1B\90\E0\01\1B\90\E0\01\1B\94\E0\01\1A\94\E0\01\1F(\E0\01\0C\1F)\C0\03\0D\1F\98\C0\03\1B\1B*\C0\03\1F+\C0\03\1D\1F\98\C0\03\1B\1C,\C0\03\1A\9C\C0\03\1F-\C0\03\1D\1B\A0\E0\01\1F\9C\C0\03\1C\1B\A0\C0\03\1B\A4\C0\03\1B\A4\E0\01\1B\A8\E0\01\1B\A8\E0\01\1B\AC\E0\01\1A\AC\E0\01\1F.\E0\01\0C\1F/\C0\03-\16\B0\90\01\00P\00\1F0\C0\03\0C\1F1\C0\03\0D\1F\B0\C0\03\1C\18\B4\D0\017\0A\092\D0\00\00\C0\03\1F3\C0\03\1D\1B\B8\E0\01\1F\B4\C0\03\1C\1B\BC\C0\03\1B\B8\C0\03\1B\BC\E0\01\1B\C0\E0\01\1B\C0\E0\01\1B\C4\E0\01\1A\C4\E0\01\1F4\E0\01\0C\1F5\C0\03\0D\1F\C8\C0\03\1B\1B6\C0\03\1F7\C0\03\1D\1F\C8\C0\03\1B\1C8\C0\03\1A\CC\C0\03\1F9\C0\03\1D\1B\D0\E0\01\1F\CC\C0\03\1C\1B\D0\C0\03\1B\D4\C0\03\1B\D4\E0\01\1B\D8\E0\01\1B\D8\E0\01\1B\DC\E0\01\1A\DC\E0\01\1F:\E0\01\0C\1F;\C0\03-\16\E0\90\01\00P\00\1F<\C0\03\0C\1F=\C0\03\0D\1F\E0\C0\03\17\00P\00\17>\C0\00\00\B0\03\17?`\00g\81\99\09\02\04\E4\A0\01\0F\C0\03\0D\00\D0\01\040\00\00P\12Y\A9\0D\02\04\E8\E0\10\0F\C0\03\0B\00\E0\01\18\E8\E0\01K\0F\02\04\EC\E0\01\1B\EC\D0\01\18\F0\10\00K\11\02\04\F0\D0\01\18\F4\10\00%\13\02\10\00\1Bb\D0\01 \C8O\B0\01\15\09\B0\01 \C8\8F\80\01\15\0D\80\01\86\C8\0F\01#\B2\08\0F\0E\10\00f\02#\C2\08\11\10\10\00\00\E0\00\15\13\E0\00P\C4\0F\00Ay\09\00\06\90\14c\88s\00\07\08\00!\00f\E8\0F\00\1D{\00\01\00S\EC\0F\00\84\89\CD\19\13\08 \01Ax\00\06?\00\15\11\F2@\03c\84\89\03\07\00\10 \00s$\0E\00!\82\00\00\02\16\10\00\F0\15'\88\83@\00\0F`\00\01-\99\02`\00\14\1F`\15\00`\00W\99\03\07\00\08`\009\92\02\02`\00O\93\00\07\02\C0\00\0A\1A\03`\005r\00\06\D0\15\01\C0\00H\04\07\00\04\C0\00J\04\03\04\00\C0\00\1F\04`\00\089M\19\00P\016\84y\07p\00\93\22\0E\00$v\02\FF\00`\D0\15\93\C4\0F\00$v\03\FF\00a\10\00a\CA\0F\00\8Ey\00\01\01\9B\84\E7\10\0C\00\E2\1F\00M\A0\01PGy\00\00\F0\C0\16\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00`\0F\01\00-'\01\00 \02\07\01\00\22@\00\01\00=o\01\000\00\08\01\00\1F\0B@\00\04\13\AF)\00\1F\F8@\00\0C\13\13\\\1A\0C\01\00\13\A8U\00*\A8\00\80\1A\22\08\00\01\00\22\18\00\01\00.B\01T\00\00\01\00\13P5\02/p\00\80\00\0B/)\00'\00\02#\00\C0@\00\00,\09\17\00\E4\00*\04\00\01\00\1Fr@\00\04\13\F0)\00&\98\00@\00\1F\0A@\00\00!`\01D\01\0D@\001\88\05\00\01\00*\D8\00\01\00\1B\08\08\00?O\01\00\0E\1E\00Q\00\00`\06\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\FA\14\01\0C\84\01\13p@\00\17\881\01\0F\C0\00\01\132@\00\15\06R\00\0B\A1\00\16\18\80\00j\06\00\00\18\80\00\01\00\13\B5+\00+\03\00\01\00\00\D5\0F\1A\00q\00\0F\80\00\01\11\06\F0\1D\07\E8\22\0CH\02\1A\00\08\00\04\97\00\13\018\00\04\E8\00\0C\01\00*\90\19\08\00\088\00\18\06\A0\00\0F\01\00\05\03\A9\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00"} {
    llvm.mlir.global internal @__wg_main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1_0() {addr_space = 3 : i32} : !llvm.array<256 x f32>
    llvm.func @main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1(%arg0: i32, %arg1: i32, %arg2: i32, %arg3: !llvm.ptr<1>, %arg4: !llvm.ptr<1>, %arg5: !llvm.ptr<1>) attributes {disc.elimargs = [3 : index, 4 : index, 6 : index, 7 : index, 8 : index, 9 : index, 10 : index, 11 : index, 13 : index, 14 : index, 15 : index, 16 : index, 17 : index, 18 : index, 20 : index, 21 : index, 22 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(32 : index) : i32
      %1 = llvm.mlir.constant(8 : index) : i32
      %2 = llvm.mlir.constant(64 : index) : i32
      %3 = llvm.mlir.constant(4 : index) : i32
      %4 = llvm.mlir.constant(0.000000e+00 : f32) : f32
      %5 = llvm.mlir.constant(2 : index) : i32
      %6 = llvm.mlir.constant(1 : index) : i32
      %7 = llvm.mlir.constant(256 : index) : i32
      %8 = llvm.mlir.constant(0 : index) : i32
      %9 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1_0 : !llvm.ptr<3>
      %10 = llvm.getelementptr %9[0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<256 x f32>
      %11 = nvvm.read.ptx.sreg.ctaid.x : i32
      %12 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %13 = llvm.mul %11, %arg0  : i32
      %14 = llvm.add %12, %13  : i32
      %15 = llvm.icmp "ult" %14, %arg1 : i32
      llvm.cond_br %15, ^bb2, ^bb21
    ^bb2:  // pred: ^bb1
      %16 = llvm.srem %14, %7  : i32
      %17 = llvm.sdiv %14, %7  : i32
      %18 = llvm.udiv %16, %0  : i32
      %19 = llvm.urem %16, %0  : i32
      %20 = llvm.mul %19, %1  : i32
      %21 = llvm.add %18, %20  : i32
      %22 = llvm.icmp "ult" %19, %6 : i32
      llvm.cond_br %22, ^bb3, ^bb10(%4 : f32)
    ^bb3:  // pred: ^bb2
      %23 = llvm.mul %17, %1  : i32
      %24 = llvm.add %18, %23  : i32
      %25 = llvm.mul %24, %2  : i32
      llvm.br ^bb4(%8, %4 : i32, f32)
    ^bb4(%26: i32, %27: f32):  // 2 preds: ^bb3, ^bb9
      %28 = llvm.icmp "slt" %26, %2 : i32
      llvm.cond_br %28, ^bb5, ^bb10(%27 : f32)
    ^bb5:  // pred: ^bb4
      %29 = llvm.add %26, %25  : i32
      %30 = llvm.icmp "slt" %29, %arg2 : i32
      llvm.cond_br %30, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %31 = llvm.getelementptr %arg3[%29] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %32 = llvm.load %31 : !llvm.ptr<1> -> f32
      %33 = llvm.getelementptr %arg4[%29] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %34 = llvm.load %33 : !llvm.ptr<1> -> f32
      %35 = llvm.fmul %32, %34  : f32
      %36 = llvm.fadd %27, %35  : f32
      llvm.br ^bb8(%36 : f32)
    ^bb7:  // pred: ^bb5
      llvm.br ^bb8(%27 : f32)
    ^bb8(%37: f32):  // 2 preds: ^bb6, ^bb7
      llvm.br ^bb9
    ^bb9:  // pred: ^bb8
      %38 = llvm.add %26, %6  : i32
      llvm.br ^bb4(%38, %37 : i32, f32)
    ^bb10(%39: f32):  // 2 preds: ^bb2, ^bb4
      llvm.br ^bb11(%39 : f32)
    ^bb11(%40: f32):  // pred: ^bb10
      llvm.br ^bb12
    ^bb12:  // pred: ^bb11
      %41 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      llvm.store %40, %41 : f32, !llvm.ptr<3>
      nvvm.barrier0
      %42 = llvm.icmp "slt" %18, %3 : i32
      llvm.cond_br %42, ^bb13, ^bb14
    ^bb13:  // pred: ^bb12
      %43 = llvm.add %21, %3  : i32
      %44 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %45 = llvm.load %44 : !llvm.ptr<3> -> f32
      %46 = llvm.getelementptr %10[%43] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %47 = llvm.load %46 : !llvm.ptr<3> -> f32
      %48 = llvm.fadd %45, %47  : f32
      %49 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      llvm.store %48, %49 : f32, !llvm.ptr<3>
      llvm.br ^bb14
    ^bb14:  // 2 preds: ^bb12, ^bb13
      nvvm.barrier0
      %50 = llvm.icmp "slt" %18, %5 : i32
      llvm.cond_br %50, ^bb15, ^bb16
    ^bb15:  // pred: ^bb14
      %51 = llvm.add %21, %5  : i32
      %52 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %53 = llvm.load %52 : !llvm.ptr<3> -> f32
      %54 = llvm.getelementptr %10[%51] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %55 = llvm.load %54 : !llvm.ptr<3> -> f32
      %56 = llvm.fadd %53, %55  : f32
      %57 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      llvm.store %56, %57 : f32, !llvm.ptr<3>
      llvm.br ^bb16
    ^bb16:  // 2 preds: ^bb14, ^bb15
      nvvm.barrier0
      %58 = llvm.icmp "slt" %18, %6 : i32
      llvm.cond_br %58, ^bb17, ^bb18
    ^bb17:  // pred: ^bb16
      %59 = llvm.add %21, %6  : i32
      %60 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %61 = llvm.load %60 : !llvm.ptr<3> -> f32
      %62 = llvm.getelementptr %10[%59] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %63 = llvm.load %62 : !llvm.ptr<3> -> f32
      %64 = llvm.fadd %61, %63  : f32
      %65 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      llvm.store %64, %65 : f32, !llvm.ptr<3>
      llvm.br ^bb18
    ^bb18:  // 2 preds: ^bb16, ^bb17
      nvvm.barrier0
      %66 = llvm.icmp "eq" %18, %8 : i32
      %67 = llvm.and %66, %22  : i1
      llvm.cond_br %67, ^bb19, ^bb20
    ^bb19:  // pred: ^bb18
      %68 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %69 = llvm.load %68 : !llvm.ptr<3> -> f32
      %70 = llvm.getelementptr %arg5[%19] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %71 = llvm.atomicrmw fadd %70, %69 acq_rel : !llvm.ptr<1>, f32
      llvm.br ^bb20
    ^bb20:  // 2 preds: ^bb18, ^bb19
      llvm.br ^bb21
    ^bb21:  // 2 preds: ^bb1, ^bb20
      llvm.return
    }
  }
  gpu.module @main_kernel_3 attributes {gpu.binary = "P\EDU\BA\01\00\10\00H\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\08\04\00\00\00\00\00\00\01\04\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\0Be__6_1_0___thread_tile_h32B\00\0F<\00%oshared>\00%\9Fconstant0A\00\22\FA\01debug_frame\00.rel\11\00!nv\14\00\11aM\00\0FS\01 \0F\92\00\1F\0F\86\01\E2o_param\8D\01\1C\0F\01\00\04\8Ce\00\00\00\03\00\0A\00\01\00 \18\01\18\00,\09\00\01\00\11`\18\00,\04\00\01\00\11~\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\1C\00\00\00E\00#\04\10\18\00\80/\08\00\05\00\00\00\06\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04@\04\91\015\00\00\04\0A\08\00\02\FC\00\91\10\00\03\19\10\00\04\17\0C$\00u\01\00\08\00\00\F0!\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00p\00\00\00\C0\00\01\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00 \01/\05\00\01\00\FFh@$v\01\FF\C7\03\A3\FF\00\8E\07\00\C4\0F\00\C3y\A4\03\A2%\00\00\00\22\0E\00\19y\00\01\00\F3\0E!\00\00\00b\0E\00\90r\05?\04\00\00\80?\E0\FF\0F\00\C6\1F\00\B9z\04\00\00X\96\02p\E4\0F\00\A4r\04\05B\00\B1?\02\8E\0F\00\CC\0F\00\0C|\00\F6\03\95pR\F0\0B\00\DA/\00M\1B\04\F3\0D\80\03\00\EA\0F\00\02z\02\00\00Z\00\00\00\0F\00\00\00\E2\0F\00$v\03\FF\00[\90\00\22\E2\0F`\001F\00\00k\04\80\C8\0F\00\86y\00\02\FFh\030\19\10\0C \00*MyP\00PGy\00\00\F0\F1\03\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\80\0F\01\00-\00\BF\04.\03\00\01\00\22@\00\01\00=S\01\000\00\08\01\00\1F\0B@\00\04\13\93)\00\1F\8D@\00\0C\13\13\C4\03\0C\01\00\13 U\00*\90\00\E8\03\04\05\04\22\18\00\01\00.&\01T\00\00\01\00\13\B0@\00/p\00\80\00\0B\1F)'\00\03A\00 \04\00\01\00\04\F8\05\04\E4\00*\04\00\01\00\1Fk@\00\04\13P)\00&L\00@\00\1F\0A@\00\00!D\01D\01\0D@\00\13\A0)\00*\D8\00\01\00\1B\08\08\00?3\01\00.\07\003\00\00x\15\05\17\10\80\00\17\048\00\04\18\00\13\E5\14\01\0C\84\01*\88\05\E0\06\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07x\07\12\03\C0\01:\06\80\00\01\00\13\06\D8\01\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\13\018\00\04\A8\00\0C\01\00*\F8\02\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__6_1_0___thread_tile_h32(%arg0: i32, %arg1: !llvm.ptr<1>) attributes {disc.elimargs = [1 : index, 3 : index, 4 : index, 5 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(1 : index) : i32
      %1 = llvm.mlir.constant(0.000000e+00 : f32) : f32
      %2 = nvvm.read.ptx.sreg.ctaid.x : i32
      %3 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %4 = llvm.mul %2, %arg0  : i32
      %5 = llvm.add %3, %4  : i32
      %6 = llvm.icmp "ult" %5, %0 : i32
      llvm.cond_br %6, ^bb2, ^bb3
    ^bb2:  // pred: ^bb1
      %7 = llvm.getelementptr %arg1[%5] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      llvm.store %1, %7 : f32, !llvm.ptr<1>
      llvm.br ^bb3
    ^bb3:  // 2 preds: ^bb1, ^bb2
      llvm.return
    }
  }
  gpu.module @main_kernel_4 attributes {gpu.binary = "P\EDU\BA\01\00\10\00\18\0D\00\00\00\00\00\00\02\00\01\01@\00\00\00\D8\0C\00\00\00\00\00\00\D8\0C\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8!\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\00!@!\07\001\00\80\1E\07\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\0De__6_1_0___thread_tile_h32_1D\00\0F>\00'oshared@\00'\9Fconstant0C\00$\FA\01debug_frame\00.rel\11\00!nv\14\00\11aO\00\0F[\01 \0F\94\00!\0F\90\01\EAo_param\97\01\1C\0F\01\00\0A\8Cg\00\00\00\03\00\0A\00\01\00  \01\18\00,\09\00\01\00\11j\18\00,\04\00\01\00\11\88\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\16\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\18\00\00\00E\002\04H\05\18\000/\08\00\0A\00\10\22\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04X\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\010\00\03\190\00\04\17\0C$\00u\07\00(\00\00\F0!\10\009\06\00 \10\009\05\00\18\10\00u\04\00\10\00\00\F0\11\10\009\03\00\0C\10\009\02\00\08\10\009\01\00\04\10\00\01\01\00\F1\02\F0\11\00\03\1B\FF\00\04\1C\08\00`\00\00\00\90\15\16\003\00K\00\01\00`\02\02\08\10\0A/\E7\00\11\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00\80\01/\05\00\01\00\FF\F0@$v\01\FF\AF\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\CB\02Q\0E\00\19y\03\0F\00\10\00\08\08\F0\15$\0E\00$z\00\00\00Z\00\00\03\02\8E\07\00\CA\1F\00\12x\FF\00\FF\01\00\00\FF\C0\80\07\00\C8\0F\00\0C \00\C5[\00\00pdp\00\00\DA\0F\00M\F3\04\B0\80\03\00\EA\0F\00\19x\03\FF\1F\0F\00\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\003\05a\C6\0F\00\11r\03q\00@\FFH\8F\07 \00c$v\00\FF\00X\A0\00\12\C6@\00\00S\00\10\03@\00P\E4\0F\00\0Cx$\03`\00\00pR\F0\03 \00c$x\03\03 \000\00@\E2\0F\00\07\90\00\22Y\00\01\00p\C8\0F\00\10x\0A\030\00\F0\05\FF\E0\FF\07\00\E4\0F\04\0Cz\00\03\00\\\00\00pb\F4\03\10\00Z\10x\0C\03\02 \00\12\0A \00\10\FC \00f\00\10x\12\03\03 \00\00\F0\00\12\0C \00C\FA\03\00\C4\00\01\13X\90\00\02@\00\01\80\00ApR\F2\03\D0\00G\A4\09\FF\04\A0\00B\0Cz\00\12@\00\13\F6`\004\02\03\04`\00i\E2\0F\04$\E4\0B0\00c\07\A2\04\03\FF\00\FF\04c\E4\0F\04\07\A2\08\10\00 \80\04 \00<$\D4\0D`\00\12\02`\00\11\F8\80\00@%\A6\04\04\E9\04#\09\02P\00D\E8\06\0A\01@\00\10\C4\10\004\0A\0A\01`\00\010\00G\08\08\00^0\00F\D8\0E\0C\02p\00A\04\81\A9\05#\06\D8\00\19\1E\0C\00\A2\00\00\07\D8\0C\0C\02@\000\E6\06\06@\00\12\0B@\00f\08\07\B8\10\12\03@\00U\00\81\A9\00\08@\00\95\A4\02\00%\E6\0A\0A\00`0\00g\00\07\B8\12\12\03P\00E\81\E9\07\060\00p\E4\0E\00%\D6\0E\0E`\00\14\0D0\008\C2\13\02\10\01F\81\E9\14\0A0\00V\08\00$\B4\15 \01\96\C4\0F\00%\D6\0C\0C\00`@\00E\81\D9\0F\0E0\00pf\0F\00%\B6\10\10`\00\14\15\C0\00F\C2\0A\02\FF\90\00U\01\81\D9\0C\0C0\00\10d0\00C\08\12\00`0\00u\E4/\00\81\B9\11\10 \00i$\03\00$\C4\0B\80\006\81\B9\08\00\01u$\0F\00%\C6\12\130\01\10\C8\10\00\07\10\01u\E4\0F\00\81\C9\13\120\00\10(\10\00\16\0A\E0\00\84\22\0F\00$r\02\FF\FF`\00\10\E2P\025\04\03\05\B0\02u\1F\04\10x\10\03\07`\02Q/\04#\A2\02\08\06\01\D4\00\84\E2O\00\10x\05\03\06 \00\02\D0\02\15\04 \03\83\E2\0F\00#\E2\02\07\14@\060\00\E2\8F \00\15\05 \03\84\C6\0F\00#\D2\02\0F\0C \00\85\C8\0F\02#\B2\02\11\080\00j\0F\01\0Cz\00\10\F0\028\11\03\08\F0\02\0B \03W\07\A8\08\04\01\F0\02V\10x\00\03\090\00f\00#\C2\02\13\0A`\00\00\90\00\15\11\F0\02\10\E4@\007\0A\04\01\10\02*$\E4 \03Y\07\E8\06\05\02\00\039\0C\05\02\00\03'\0A\0A\00\03\11\08\D0\03\04\F0\03\00`\008\B8\0E\10\D0\02\010\03\08`\03G\B8\10\10\03P\006\81\A9\05\B0\01%\A4\00 \03\07\C0\02)\11\FF \03\19\04 \039$\B4\17@\02*%\E6\C0\02_\07\C2\12\11\FF0\03\09\12\B60\03\12\17P\02Y\08\81\E9\14\0C \03&\C4\16`\00\10\E4\D0\02C\0A\10\00`0\00W\E2\1F\00\81\B9 \03\10b\10\04\11\15\80\05\04@\011%\C6\10\C0\02\14\16`\009\B9\0A\0A \03\12\C6 \03\01 \00 \E2/0\04(\00\01P\03\18\C90\03\1A\01\D0\04\01\00\03\0B0\035\D6\12\15@\01\1B\C8\D0\03\000\00\1B\D90\03\08\B0\03\10\22\80\029\10\03\0C \034\00\03\0E\10\00\00`\02E\A2\02\05\04`\02\11OP\03\17\0A \03\000\03\14\0B\10\00\1F\E40\03\0Cw\C8\8F\00#\B2\02\0F\B0\02\16\02@\03\12\F6@\03\1F\C20\03\05\02\D0\02\000\03\17\0D\A0\00\0CP\06\00\F0\02\14\02\90\01\02\80\06\18\11\C0\06\00P\03\17\02 \02Z#\D2\02\13\0C@\03\06\E0\02\00`\00\1B\B40\037\B8\06\05\F0\02\1B\04 \03d\00\07\B8\0C\05\03`\00\00P\03O\C2\0E\10\FF \03\09O\C2\10\10\FF \03\09\19\B6 \03i\08\07\E8\13\11\01\80\06\0B \03\17\C4 \03\01\C0\02\0B \03H\E8\12\11\01p\00\1B\B9 \03\17\C6 \03\12\E2\E0\02\17\02\C0\00+\81\B90\03*\E4\16\90\03\1B\C60\03\1B\C9P\06\17\E6 \03\00\D0\00I\D8\0C\00\02\00\03\0A0\03\09\D0\07\00\D0\00\1A\E6@\03'\81\E90\03*&\01\10\03\00`\06\18\E90\03\1F\03 \03\22\1B\11 \03\14\13\10\00\0F \03\04\14\0F \00\03 \03\14\10\10\00\0F \03\01\1F\B2P\06\05\05\10\03\0C0\03\06\10\03\1C\CE\00\03\11/\00\03\08\A0\02*#\E2P\03\00@\03\17\12\90\00\00\10\03\16\03\80\01\0F\10\03\01\1F\11\10\03\0A:\B2\06\05\E0\02\090\06\07P\03\11\FC \0A3\07\B2\0C0\00\1F\00\10\03\03:\C8\0E\10\E0\02)\05\0A`\09H\C8\10\10\01@\00\08 \03\00p\05\0F\10\03\1DH\D8\13\11\02p\00\08\10\03\88\E2\0E\00\07\D8\12\11\02p\00\08 \03\1F\C4\10\03\00\1B\D4@\06\0B\10\03H\07\E8\15\00\A0\01\09 \03\10d\C0\05\0B \03O\E8\0C\00\03 \03\08'%\D6\10\03[\C4/\00\81\D9@\06)\E4\0D\90\00+\81\D9@\06\1B\E6@\06\08P\07\000\00\1B\E9 \03\1E\E9 \03\1B\16 \03\14\18\10\00\0F \03\04\1B\14 \03\14\15\10\00\0F \03=\11\C6\D0\02\0D\00\03\14\17p\00\1F\C6@\06\00O\A2\0A\04\FF@\06\02\03\F0\08G\A2\08\04\FF\10\02/#\E2@\06\05\1F\FC@\06\06\1F\01@\06\0C\18\01@\06F\C8\0E\10\020\00\0F0\03\00?\10\10\02@\06\19O\D8\13\11\03@\06)O\D8\12\11\03@\06\05.$\07@\06G\E2\15\00\FF\C0\00\090\03*$\0B0\03\0F@\06\00H\07\E2\00\00p\01\090\03*$\010\03\00\00\04\08 \03\10$\F0\0A\17\06\D0\00\1A\8F0\03\1B\E20\03%\E6\0E\10\03\22\06\02\90\00\090\03\00 \07S\E6\0C\00\00` \009\E4\0F\02 \03\1Bh \03\10b\E0\026\00\03\1A\E0\02d\04\10x\06\03\1B\10\00\01\00\034\0E\03\1C\10\00/\E2\1F0\03\007\05\03\19\B0\02\09\10\03;\C8\0F\01\00\03\00\80\02\15\06@\03\0D\00\03\17\8F\A0\02\12\FA \00\0A\C0\02\16\02`\03\04\F0\0F9\0A\03\1D\C0\009\08\03\1E\10\007\07\03\1F\A0\002\07\D8\16L\00\00\B0\01\02\D0\03\15\0E\B0\03\02\10\00\15\0A\A0\03\00P\03G\E8\10\05\010\00W\07\A8\0D\06\03\10\001\03x\0C\C0\01\01\01\00\02\90\0D\18\08\90\03H\0Cz\00\07\B0\00\06\C0\00\12\F4`\00H\B2\0B\0E\FF`\00&\C8\09\E0\0F\10\CA\B0\00(\04\08\B0\00H\07\E8\03\07\80\00\060\01\01\D0\10d\00\07\A8\14\05\01\D0\02\00 \000r\00\0C`\00$pRp\00%\0E\0E\F0\02\01\C0\03(\0A\0A0\00T\07\D8\08\08\02\10\00\000\01H\88\12\00\02 \00H\E8\00\07\03\10\008\A8\18\06\10\00\06\E0\00\03P\11\1A\84\B0\039$\A4\1A\10\00E%\86\12\12\90\03f\D0\0F\00$\94\11 \00\00P\03&\89\07\C0\02p\A6\00\00%\96\14\140\00\14\11\10\06!\96\10\90\03#\11\020\005\99\05\140\00v\E6\02\00%\86\16\16\B0\03\00 \00\17\06P\03`\08\00%\A6\18\18P\00\14\1A \005\89\0C\16 \00f\A6\0A\00$\B4\1C\90\00\00\00\04S\A6\1A\0D\00`0\00\00\80\06:\A9\0D\18\D0\04\02`\04\15\1C\F0\07&\1A\1A \00V\0E\00$\C4\1DP\00\01\C0\04 \14\0BP\00\14\1C\F0\03\08\C0\0Dl\A6\0E\00$\D4\1F\80\04\01\90\05\13\1D0\05H\81\B9\14\140\00c%\C6\10\09\00` \00\10\E4\90\07\19\0Bp\041%\D6\120\12\11\1F \00\00\00\0B*\10\10\90\04\090\0BA\02%\D6\08\90\12\14\1F \01\08\90\0A\010\0B)\18\00\E0\04\08\90\04\00\00\08U\E6\16\03\00` \00F\00\81\E9\190\01\01\90\04(\16\16\90\04c$v\04\FF\00b\80\00\00\C0\06H\92\02\05\06\A0\0D5\82\02\07\10\04 \C8O\90\04%\0D\1A\10\00\01\80\04\16\0F\90\07\00P\00C\05\FF\00cP\00\02\90\07(\0B\10\A0\04H\D2\02\13\08\10\11D\E2\02\19\16\10\00a\CA\0F\00\8Ey\00\90\0D@\84\E7\10\0CP\00\14M\80\15\030\15PGy\00\00\F0\A9\19\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\B0\0F\01\00-\00\\\04.\03\00\01\00\22@\00\01\00=[\01\000\00\08\01\00\1F\0B@\00\04\13\9B)\00\1F\97@\00\0C\14\13\CC\01\0B\01\00\138U\00\11\90\06\00\06p\19\22\08\00\01\00\22\18\00\01\00..\01T\00\00\01\00\13\C8@\00/p\00\80\00\0B\1F)'\00\03!\008\F5\02$\00\00\E0\1B\04\E4\00*\04\00\01\00\1Fm@\00\04\13h)\00&\AC\00@\00\1F\0A@\00\00!L\01D\01\0D@\001\18\05\00\01\00*\D8\00\01\00\1B\08\08\00?;\01\00\16\1D\003\00\00\F0@\00&\10\00\80\00\17\048\00\04\18\00\13\EB\14\01\0D\84\01!\06\00\01\00\17\901\01\0F\C0\00\01\132@\00+\06\00\01\00\1A\08`\1D\12\03\C0\01>\22\80\00g\00\17\05(!\0C\01\00*\A8\00\08\00\04\C0\00\13\018\00\0Ey\00\03x\00\1A\18\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__6_1_0___thread_tile_h32_1(%arg0: i32, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: !llvm.ptr<1>, %arg6: !llvm.ptr<1>, %arg7: !llvm.ptr<1>) attributes {disc.elimargs = [5 : index, 7 : index, 8 : index, 9 : index, 10 : index, 11 : index, 12 : index, 14 : index, 15 : index, 16 : index, 17 : index, 18 : index, 19 : index, 21 : index, 22 : index, 23 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(0 : index) : i32
      %1 = llvm.mlir.constant(1 : index) : i32
      %2 = llvm.mlir.constant(512 : index) : i32
      %3 = llvm.mlir.constant(32 : index) : i32
      %4 = llvm.mlir.constant(4 : index) : i32
      %5 = llvm.mlir.constant(0.000000e+00 : f32) : f32
      %6 = nvvm.read.ptx.sreg.ctaid.x : i32
      %7 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %8 = llvm.icmp "eq" %arg0, %1 : i32
      %9 = llvm.select %8, %arg1, %arg0 : i1, i32
      %10 = llvm.icmp "eq" %arg1, %9 : i32
      %11 = llvm.icmp "eq" %arg0, %9 : i32
      %12 = llvm.mul %6, %arg2  : i32
      %13 = llvm.add %7, %12  : i32
      %14 = llvm.icmp "ult" %13, %arg3 : i32
      llvm.cond_br %14, ^bb2, ^bb12
    ^bb2:  // pred: ^bb1
      %15 = llvm.srem %13, %2  : i32
      %16 = llvm.sdiv %13, %2  : i32
      %17 = llvm.icmp "ult" %15, %1 : i32
      llvm.cond_br %17, ^bb3, ^bb11
    ^bb3:  // pred: ^bb2
      %18 = llvm.mul %16, %3  : i32
      llvm.br ^bb4(%0, %5 : i32, f32)
    ^bb4(%19: i32, %20: f32):  // 2 preds: ^bb3, ^bb9
      %21 = llvm.icmp "slt" %19, %3 : i32
      llvm.cond_br %21, ^bb5, ^bb10
    ^bb5:  // pred: ^bb4
      %22 = llvm.add %18, %19  : i32
      %23 = llvm.icmp "slt" %22, %arg4 : i32
      llvm.cond_br %23, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %24 = llvm.urem %22, %4  : i32
      %25 = llvm.udiv %22, %4  : i32
      %26 = llvm.select %10, %25, %0 : i1, i32
      %27 = llvm.mul %26, %4  : i32
      %28 = llvm.add %27, %24  : i32
      %29 = llvm.getelementptr %arg5[%28] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %30 = llvm.load %29 : !llvm.ptr<1> -> f32
      %31 = llvm.select %11, %25, %0 : i1, i32
      %32 = llvm.mul %31, %4  : i32
      %33 = llvm.add %32, %24  : i32
      %34 = llvm.getelementptr %arg6[%33] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %35 = llvm.load %34 : !llvm.ptr<1> -> f32
      %36 = llvm.fmul %30, %35  : f32
      %37 = llvm.fadd %20, %36  : f32
      llvm.br ^bb8(%37 : f32)
    ^bb7:  // pred: ^bb5
      llvm.br ^bb8(%20 : f32)
    ^bb8(%38: f32):  // 2 preds: ^bb6, ^bb7
      llvm.br ^bb9
    ^bb9:  // pred: ^bb8
      %39 = llvm.add %19, %1  : i32
      llvm.br ^bb4(%39, %38 : i32, f32)
    ^bb10:  // pred: ^bb4
      %40 = llvm.getelementptr %arg7[%15] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %41 = llvm.atomicrmw fadd %40, %20 acq_rel : !llvm.ptr<1>, f32
      llvm.br ^bb11
    ^bb11:  // 2 preds: ^bb2, ^bb10
      llvm.br ^bb12
    ^bb12:  // 2 preds: ^bb1, ^bb11
      llvm.return
    }
  }
  gpu.module @main_kernel_5 attributes {gpu.binary = "P\EDU\BA\01\00\10\00H\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\08\04\00\00\00\00\00\00\01\04\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\0Ae__6_1_0___block_tile_h64A\00\0F;\00$oshared=\00$\9Fconstant0@\00!\FA\01debug_frame\00.rel\11\00!nv\14\00\11aL\00\0FO\01 \0F\91\00\1E\0F\81\01\DEo_param\88\01\1C\0F\01\00\05\8Cd\00\00\00\03\00\0A\00\01\00 \14\01\18\00,\09\00\01\00\11[\18\00,\04\00\01\00\11y\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\1C\00\00\00E\00#\04\10\18\00\80/\08\00\05\00\00\00\06\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\048\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C$\00u\01\00\08\00\00\F0!\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00p\00\00\00\C0\00\01\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\00#\02b,\00\00\00H\00\01\00\00 \01/\05\00\01\00\FFp@$v\01\FF\CF\03\A3\FF\00\8E\07\00\C4\0F\00\C3y\AC\03\A2%\00\00\00\22\0E\00\19y\00\01\00\F3\0E!\00\00\00b\0E\00\90r\05?\04\00\00\80?\E0\FF\0F\00\C6\1F\00\B9z\04\00\00X\9E\02p\E4\0F\00\A4r\04\05B\00\B1?\02\8E\0F\00\CC\0F\00\0C|\00\FE\03\95pR\F0\0B\00\DA/\00M#\04\F3\0D\80\03\00\EA\0F\00\02z\02\00\00Z\00\00\00\0F\00\00\00\E2\0F\00$v\03\FF\00[\90\00\22\E2\0F`\001F\00\00s\04\80\C8\0F\00\86y\00\02\FFp\030\19\10\0C \00*MyP\00PGy\00\00\F0\F9\03\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\80\0F\01\00-\00\C7\04.\03\00\01\00\22@\00\01\00=O\01\000\00\08\01\00\1F\0B@\00\04\13\8F)\00\1F\88@\00\0C\13\13\CC\03\0C\01\00\13\18U\00*\90\00\F0\03\04\0D\04\22\18\00\01\00.\22\01T\00\00\01\00\13\A8@\00/p\00\80\00\0B\1F)'\00\03A\00\18\04\00\01\00\04\00\06\04\E4\00*\04\00\01\00\1Fj@\00\04\13H)\00&L\00@\00\1F\0A@\00\00!@\01D\01\0D@\00\13\98)\00*\D8\00\01\00\1B\08\08\00?/\01\006\07\003\00\00p\1D\05\17\10\80\00\17\048\00\04\18\00\13\E2\14\01\0C\84\01*\80\05\E8\06\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\80\07\12\03\C0\01:\06\80\00\01\00\13\06\D8\01\04(\0B\0D\C8\01\1A\00\08\00\04\F8\00\13\018\00\04\A8\00\0D\01\00)\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__6_1_0___block_tile_h64(%arg0: i32, %arg1: !llvm.ptr<1>) attributes {disc.elimargs = [1 : index, 3 : index, 4 : index, 5 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(1 : index) : i32
      %1 = llvm.mlir.constant(0.000000e+00 : f32) : f32
      %2 = nvvm.read.ptx.sreg.ctaid.x : i32
      %3 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %4 = llvm.mul %2, %arg0  : i32
      %5 = llvm.add %3, %4  : i32
      %6 = llvm.icmp "ult" %5, %0 : i32
      llvm.cond_br %6, ^bb2, ^bb3
    ^bb2:  // pred: ^bb1
      %7 = llvm.getelementptr %arg1[%5] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      llvm.store %1, %7 : f32, !llvm.ptr<1>
      llvm.br ^bb3
    ^bb3:  // 2 preds: ^bb1, ^bb2
      llvm.return
    }
  }
  gpu.module @main_kernel_6 attributes {gpu.binary = "P\EDU\BA\01\00\10\00\D8\07\00\00\00\00\00\00\02\00\01\01@\00\00\00\98\07\00\00\00\00\00\00\93\07\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\A8\12\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00g\02\00\BE\00r\00\01\00\22\12\00\01\00\11\0F\06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\0Ce__6_1_0___block_tile_h64_1C\00\0F=\00&oshared?\00&\9Fconstant0B\00#\FA\01debug_frame\00.rel\11\00!nv\14\00\11aN\00\0FW\01 \0F\93\00 \0F\8B\01\A4\8F$____wg_<\00 \00\15\00/28\CD\010o_param\D4\01\1C\0F\01\00\09\8Cf\00\00\00\03\00\0A\00\01\00\11\DD\18\00,\0B\00\01\00 ^\01\18\00,\09\00\01\00\11\A7\18\00,\04\00\01\00\11\C5\18\00,\07\00\01\00\102\E3\03\17\10x\00\03#\00f\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0\06\07\00 \00\04\9B\00R\04\14\00\00\00E\002\04p\01\18\000/\08\00#\00\10\19\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\A8\04\F3\08\015\00\00\04\0A\08\00\03\00\00\00`\010\00\03\190\00\04\17\0C\CB\00U(\00\00\F0!\10\009\06\00 \10\009\05\00\18\10\00u\04\00\10\00\00\F0\11\10\009\03\00\0C\10\009\02\00\08\10\009\01\00\04\10\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\00\D0\05\00\00 \06\00\00\04\1E\94\01#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\11\02h\01\0F\01\00\FF\98@$v\01\FFw\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%s\02a\0E\00\19y\03\00\01\00\10!-\00\F0\04\0E\00$z\00\00\00Z\00\00\03\02\8E\07\00\CA\1F\00\0C\10\00\C5[\00\00p`\F0\03\00\DA\0F\00M\93\04\B0\80\03\00\EA\0F\00\19x\03\FF\1F\0F\00\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\00\EB\04Q\E2\0F\00Eyy\03\020\00a\E4\0F\00\11r\04q\00\C1\FF@\8F\07\00\C8\0F\00\12x\03\04Y\04\10\C0p\00@\0F\00$x\BC\02`\00\00\03\0A\8E\070\00c$r\03\FF\FF\00\C0\00\10\C60\00 \02\00p\00\22\FF\C0 \00P\19x\05\FF\05Q\000\16\01\00\10\00@\0Cr\00\020\001pR\F2p\00P\0Cx\00\00\7F\10\000@\F0\03\90\00@$x\02\02\D9\02\10\05\E0\00\12\C8\10\00\14\04`\00\96\CC\0F\00G\19\00\00@\03\E0\00c$v\03\FF\00X \00\10\E2p\001\04\FF\08\E3\04A\01\00\00\C8`\00$\03\01p\00u\E2\0F\04$x\04\04`\00\01\C0\00\16\05\C0\00\91\E2\0F\00\07z\03\03\00Y`\00\02@\00Uz\00\03\00X\B0\00\11\04\10\00\10Y\10\00\12\F4\B0\00\08\00\01\10\C4\B0\004\08\04@`\00\11\CA@\00\81\08\00\\\00\00pb\FA@\00@\10x\0A\08\90\00B\FF\E0\FF\07\10\006\10\08\02\10\00\000\00\12\0A0\00#\FC\03\10\00\12\10\10\00\11\F8\10\00T\10x\13\08\030\00\91\C6\0F\00\07\D2\06\08\FF\00\B0\00\11\04\E0\00H\D4\09\FF\04\D0\00#\D2\08 \00#\00\05P\00\12\13P\00\11\F6\C0\00\A3%\D6\06\06\00`\00\00\09\020\00S\E8\0C\0A\01\000\00\10\C6 \00H\08\08\00^ \006\0A\0A\01p\00P\00\81\D9\06\06p\00\BA\00\19\1E\0C\00\A4\00\00$\E4\0B\80\00S\C8\0E\10\02\00P\00u\E2\0F\04\81\D9\12\080\00\87\A2\02\00\07\C8\10\10\02P\00@%\E6\0C\0Cp\00\14\0Bp\001\B8\11\13\EF\01\03\90\00:$\C4\15`\00E\B8\07\13\03@\00\86\1F\00%\E6\0A\0A\00`@\00E\81\E9\14\0Cp\00p\E6\00\00%\C6\0E\0E`\001\15\02\8Ep\01E\81\E9\0A\0A \00\87\E4\0E\00$\B4\16\FF\04\D0\01c%\C6\08\10\00`0\00u\E2/\00\81\C9\0E\0E0\00p&\0F\00%\B6\10\11P\00\12\16P\00F\08\81\C9\08\E0\00\10$ \00D\0C\07\00` \00e\1F\00\81\B9\10\10 \00fh\0F\00\81\B9\0C\A0\00\10b\E0\014\05\05\04\E0\01\81\E2\0F\00#\D2\03\12\06\0C\07P\00\00\00\C6O\D0\02\10\05`\021pR\FA\C0\01T#\E2\03\14\0A \00\85\C8\8F\00#\C2\03\0E\08\10\00v\0F\01#\B2\03\10\0C\10\00p\02GY\00\00P\FD\D1\03\11\83@\03\14Ap\04\03P\03A\88s\00\02,\00\00\96\06f\E8\0F\00\1D{\00\01\00\84\EC\0F\00\84\89\04\02\00 \00\12\E2\C0\03\10?\90\000@\F2\03\A0\01c\84\89\05\02\00\10 \00\93$\0E\00!\82\05\04\05\00\01\00\8F\CA\1F\00\88\83\00\02\05`\00\09\1E\99`\00\14\1F \04\00`\00!\99\07\07\08\05`\00H\92\07\04\07`\00O\93\00\02\07\C0\00\0A\1A\03`\00\10r\EC\01\03\E0\03\13\C6\E0\00\18\04\C0\00J\03\03\04\00\C0\00\0F \01\099M\19\00P\01'\84yp\00\96\22\0E\00$v\04\FF\00b`\02c$v\05\FF\00c\10\00a\CA\0F\00\8Ey\00\81\05@\84\E7\10\0C\D0\02\1BM\A0\01cGy\00\00\F0\FF\C0\01f\C0\0F\00\18y\00\01\00\0F\10\00\A0\0F\01\00-\14\01\DC\02\0B\01\00\22@\00\01\00=W\01\000\00\08\01\00\1F\0B@\00\04\13\97)\00\1F\D4@\00\0C\13\13t\09\0C\01\00\13pU\00*\A8\00\98\09\22\08\00\01\00\22\18\00\01\00.*\01T\00\00\01\00\13\18u\02/p\00\80\00\0B\1F)'\00\03#\00\88@\00\04\10\0C\04\E4\00*\04\00\01\00\1Fl@\00\04\13\B8)\00&\B8\00@\00\1F\0A@\00\00!H\01D\01\0D@\00\13p\F5\03*\D8\00\01\00\1B\08\08\00?7\01\00F\0D\00Q\00\00H\06\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\E8\14\01\0C\84\01\13X@\00\17\901\01\0F\C0\00\01\132@\00\15\06R\00\0B\A1\00\16\07\80\00j\06\00\00\19\80\00\01\00\13\A9+\00+\03\00\01\00\04\DB\02\1F\04\80\00\0B\11\06\D9\06\07\E8\11\0B\01\00\1B\A8\08\00\04\97\00\13\018\00\04\E8\00\0D8\00\1A\08\08\00\088\00\18\06\A0\00\0F\01\00\05\03\A9\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00"} {
    llvm.mlir.global internal @__wg_main_kColReduction_reduce__6_1_0___block_tile_h64_1_0() {addr_space = 3 : i32} : !llvm.array<256 x f32>
    llvm.func @main_kColReduction_reduce__6_1_0___block_tile_h64_1(%arg0: i32, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: !llvm.ptr<1>, %arg6: !llvm.ptr<1>, %arg7: !llvm.ptr<1>) attributes {disc.elimargs = [5 : index, 7 : index, 8 : index, 9 : index, 10 : index, 11 : index, 12 : index, 14 : index, 15 : index, 16 : index, 17 : index, 18 : index, 19 : index, 21 : index, 22 : index, 23 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(32 : index) : i32
      %1 = llvm.mlir.constant(8 : index) : i32
      %2 = llvm.mlir.constant(64 : index) : i32
      %3 = llvm.mlir.constant(4 : index) : i32
      %4 = llvm.mlir.constant(0.000000e+00 : f32) : f32
      %5 = llvm.mlir.constant(2 : index) : i32
      %6 = llvm.mlir.constant(1 : index) : i32
      %7 = llvm.mlir.constant(256 : index) : i32
      %8 = llvm.mlir.constant(0 : index) : i32
      %9 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__6_1_0___block_tile_h64_1_0 : !llvm.ptr<3>
      %10 = llvm.getelementptr %9[0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<256 x f32>
      %11 = nvvm.read.ptx.sreg.ctaid.x : i32
      %12 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %13 = llvm.icmp "eq" %arg0, %6 : i32
      %14 = llvm.select %13, %arg1, %arg0 : i1, i32
      %15 = llvm.icmp "eq" %arg1, %14 : i32
      %16 = llvm.icmp "eq" %arg0, %14 : i32
      %17 = llvm.mul %11, %arg2  : i32
      %18 = llvm.add %12, %17  : i32
      %19 = llvm.icmp "ult" %18, %arg3 : i32
      llvm.cond_br %19, ^bb2, ^bb21
    ^bb2:  // pred: ^bb1
      %20 = llvm.srem %18, %7  : i32
      %21 = llvm.sdiv %18, %7  : i32
      %22 = llvm.udiv %20, %0  : i32
      %23 = llvm.urem %20, %0  : i32
      %24 = llvm.mul %23, %1  : i32
      %25 = llvm.add %22, %24  : i32
      %26 = llvm.icmp "ult" %23, %6 : i32
      llvm.cond_br %26, ^bb3, ^bb10(%4 : f32)
    ^bb3:  // pred: ^bb2
      %27 = llvm.mul %21, %1  : i32
      %28 = llvm.add %22, %27  : i32
      %29 = llvm.mul %28, %2  : i32
      llvm.br ^bb4(%8, %4 : i32, f32)
    ^bb4(%30: i32, %31: f32):  // 2 preds: ^bb3, ^bb9
      %32 = llvm.icmp "slt" %30, %2 : i32
      llvm.cond_br %32, ^bb5, ^bb10(%31 : f32)
    ^bb5:  // pred: ^bb4
      %33 = llvm.add %30, %29  : i32
      %34 = llvm.icmp "slt" %33, %arg4 : i32
      llvm.cond_br %34, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %35 = llvm.urem %33, %3  : i32
      %36 = llvm.udiv %33, %3  : i32
      %37 = llvm.select %15, %36, %8 : i1, i32
      %38 = llvm.mul %37, %3  : i32
      %39 = llvm.add %38, %35  : i32
      %40 = llvm.getelementptr %arg5[%39] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %41 = llvm.load %40 : !llvm.ptr<1> -> f32
      %42 = llvm.select %16, %36, %8 : i1, i32
      %43 = llvm.mul %42, %3  : i32
      %44 = llvm.add %43, %35  : i32
      %45 = llvm.getelementptr %arg6[%44] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %46 = llvm.load %45 : !llvm.ptr<1> -> f32
      %47 = llvm.fmul %41, %46  : f32
      %48 = llvm.fadd %31, %47  : f32
      llvm.br ^bb8(%48 : f32)
    ^bb7:  // pred: ^bb5
      llvm.br ^bb8(%31 : f32)
    ^bb8(%49: f32):  // 2 preds: ^bb6, ^bb7
      llvm.br ^bb9
    ^bb9:  // pred: ^bb8
      %50 = llvm.add %30, %6  : i32
      llvm.br ^bb4(%50, %49 : i32, f32)
    ^bb10(%51: f32):  // 2 preds: ^bb2, ^bb4
      llvm.br ^bb11(%51 : f32)
    ^bb11(%52: f32):  // pred: ^bb10
      llvm.br ^bb12
    ^bb12:  // pred: ^bb11
      %53 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      llvm.store %52, %53 : f32, !llvm.ptr<3>
      nvvm.barrier0
      %54 = llvm.icmp "slt" %22, %3 : i32
      llvm.cond_br %54, ^bb13, ^bb14
    ^bb13:  // pred: ^bb12
      %55 = llvm.add %25, %3  : i32
      %56 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %57 = llvm.load %56 : !llvm.ptr<3> -> f32
      %58 = llvm.getelementptr %10[%55] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %59 = llvm.load %58 : !llvm.ptr<3> -> f32
      %60 = llvm.fadd %57, %59  : f32
      %61 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      llvm.store %60, %61 : f32, !llvm.ptr<3>
      llvm.br ^bb14
    ^bb14:  // 2 preds: ^bb12, ^bb13
      nvvm.barrier0
      %62 = llvm.icmp "slt" %22, %5 : i32
      llvm.cond_br %62, ^bb15, ^bb16
    ^bb15:  // pred: ^bb14
      %63 = llvm.add %25, %5  : i32
      %64 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %65 = llvm.load %64 : !llvm.ptr<3> -> f32
      %66 = llvm.getelementptr %10[%63] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %67 = llvm.load %66 : !llvm.ptr<3> -> f32
      %68 = llvm.fadd %65, %67  : f32
      %69 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      llvm.store %68, %69 : f32, !llvm.ptr<3>
      llvm.br ^bb16
    ^bb16:  // 2 preds: ^bb14, ^bb15
      nvvm.barrier0
      %70 = llvm.icmp "slt" %22, %6 : i32
      llvm.cond_br %70, ^bb17, ^bb18
    ^bb17:  // pred: ^bb16
      %71 = llvm.add %25, %6  : i32
      %72 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %73 = llvm.load %72 : !llvm.ptr<3> -> f32
      %74 = llvm.getelementptr %10[%71] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %75 = llvm.load %74 : !llvm.ptr<3> -> f32
      %76 = llvm.fadd %73, %75  : f32
      %77 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      llvm.store %76, %77 : f32, !llvm.ptr<3>
      llvm.br ^bb18
    ^bb18:  // 2 preds: ^bb16, ^bb17
      nvvm.barrier0
      %78 = llvm.icmp "eq" %22, %8 : i32
      %79 = llvm.and %78, %26  : i1
      llvm.cond_br %79, ^bb19, ^bb20
    ^bb19:  // pred: ^bb18
      %80 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %81 = llvm.load %80 : !llvm.ptr<3> -> f32
      %82 = llvm.getelementptr %arg7[%23] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %83 = llvm.atomicrmw fadd %82, %81 acq_rel : !llvm.ptr<1>, f32
      llvm.br ^bb20
    ^bb20:  // 2 preds: ^bb18, ^bb19
      llvm.br ^bb21
    ^bb21:  // 2 preds: ^bb1, ^bb20
      llvm.return
    }
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = true, sym_name = "C4", value = 4 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = true, sym_name = "C1", value = 1 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S4", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = true, knownNonSizeZero = false, sym_name = "S5", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %c4 = arith.constant 4 : index
    %0 = "disc_shape.dim"() {name = @S2} : () -> index
    %1 = "disc_shape.dim"() {name = @S3} : () -> index
    "disc_shape.tie_product_equal"(%c4, %0, %1) {operand_segment_sizes = array<i32: 2, 1>} : (index, index, index) -> ()
    %2 = "disc_shape.dim"() {name = @S4} : () -> index
    %3 = "disc_shape.dim"() {name = @S5} : () -> index
    "disc_shape.tie_product_equal"(%c4, %2, %3) {operand_segment_sizes = array<i32: 2, 1>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After DiscStripShapeConstraintOpsPass (disc-strip-shape-constraint-ops) //----- //
module attributes {gpu.container_module} {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
    %c256 = arith.constant 256 : index
    %c32 = arith.constant 32 : index
    %c512 = arith.constant 512 : index
    %0 = llvm.mlir.constant(0 : i32) : i32
    %c4_i32 = arith.constant 4 : i32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
    %2 = "disc_ral.dispatch"(%arg0, %c1) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x4xf32>
    %dim = memref.dim %2, %c0 : memref<?x4xf32>
    %dim_0 = memref.dim %1, %c0 : memref<?x4xf32>
    %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_0, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32> to memref<?x4xf32>
    %reinterpret_cast_1 = memref.reinterpret_cast %2 to offset: [0], sizes: [%dim, 4], strides: [4, 1] {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32> to memref<?x4xf32>
    %3 = arith.cmpi eq, %dim, %c1 : index
    %4 = arith.select %3, %dim_0, %dim : index
    %alloc = memref.alloc(%dim_0) {kDiscSymbolicDimAttr = [@S0, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
    %5 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
    "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast, %alloc) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
    "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
    %alloc_2 = memref.alloc(%dim) {kDiscSymbolicDimAttr = [@S1, @C4]} : memref<?x4xf32, #gpu.address_space<global>>
    "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_1, %alloc_2) {backend_config = "", call_target_name = "h2d", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?x4xf32>, memref<?x4xf32, #gpu.address_space<global>>) -> ()
    "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
    %6 = arith.index_cast %4 : index to i32
    %7 = arith.muli %6, %c4_i32 : i32
    %8 = arith.index_cast %7 : i32 to index
    %9 = arith.cmpi eq, %dim_0, %4 : index
    %10 = arith.cmpi eq, %dim, %4 : index
    %11 = arith.andi %10, %9 : i1
    %alloc_3 = memref.alloc() : memref<1xf32, #gpu.address_space<global>>
    cf.cond_br %11, ^bb1, ^bb4
  ^bb1:  // pred: ^bb0
    %12 = arith.cmpi slt, %8, %c1 : index
    cf.cond_br %12, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    gpu.launch_func  @main_kernel::@main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32 blocks in (%c1, %c1, %c1) threads in (%c512, %c1, %c1) args(%c512 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    %13 = arith.cmpi eq, %8, %c0 : index
    %14 = arith.subi %8, %c1 : index
    %15 = arith.divui %14, %c32 : index
    %16 = arith.addi %15, %c1 : index
    %17 = arith.select %13, %c0, %16 : index
    %18 = arith.muli %17, %c512 : index
    %c512_4 = arith.constant 512 : index
    %c0_5 = arith.constant 0 : index
    %c1_6 = arith.constant 1 : index
    %19 = arith.cmpi sle, %18, %c0_5 : index
    %20 = arith.subi %c0_5, %18 : index
    %21 = arith.subi %18, %c1_6 : index
    %22 = arith.select %19, %20, %21 : index
    %23 = arith.divsi %22, %c512_4 : index
    %24 = arith.subi %c0_5, %23 : index
    %25 = arith.addi %23, %c1_6 : index
    %26 = arith.select %19, %24, %25 : index
    gpu.launch_func  @main_kernel_0::@main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_1 blocks in (%26, %c1, %c1) threads in (%c512, %c1, %c1) args(%c512 : index, %18 : index, %8 : index, %4 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    cf.br ^bb7
  ^bb3:  // pred: ^bb1
    gpu.launch_func  @main_kernel_1::@main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64 blocks in (%c1, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    %27 = arith.cmpi eq, %8, %c0 : index
    %28 = arith.subi %8, %c1 : index
    %29 = arith.divui %28, %c512 : index
    %30 = arith.addi %29, %c1 : index
    %31 = arith.select %27, %c0, %30 : index
    %32 = arith.muli %31, %c256 : index
    %c256_7 = arith.constant 256 : index
    %c0_8 = arith.constant 0 : index
    %c1_9 = arith.constant 1 : index
    %33 = arith.cmpi sle, %32, %c0_8 : index
    %34 = arith.subi %c0_8, %32 : index
    %35 = arith.subi %32, %c1_9 : index
    %36 = arith.select %33, %34, %35 : index
    %37 = arith.divsi %36, %c256_7 : index
    %38 = arith.subi %c0_8, %37 : index
    %39 = arith.addi %37, %c1_9 : index
    %40 = arith.select %33, %38, %39 : index
    gpu.launch_func  @main_kernel_2::@main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1 blocks in (%40, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %32 : index, %8 : index, %4 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    cf.br ^bb7
  ^bb4:  // pred: ^bb0
    %41 = arith.cmpi slt, %8, %c1 : index
    cf.cond_br %41, ^bb5, ^bb6
  ^bb5:  // pred: ^bb4
    gpu.launch_func  @main_kernel_3::@main_kColReduction_reduce__6_1_0___thread_tile_h32 blocks in (%c1, %c1, %c1) threads in (%c512, %c1, %c1) args(%c512 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    %42 = arith.cmpi eq, %8, %c0 : index
    %43 = arith.subi %8, %c1 : index
    %44 = arith.divui %43, %c32 : index
    %45 = arith.addi %44, %c1 : index
    %46 = arith.select %42, %c0, %45 : index
    %47 = arith.muli %46, %c512 : index
    %c512_10 = arith.constant 512 : index
    %c0_11 = arith.constant 0 : index
    %c1_12 = arith.constant 1 : index
    %48 = arith.cmpi sle, %47, %c0_11 : index
    %49 = arith.subi %c0_11, %47 : index
    %50 = arith.subi %47, %c1_12 : index
    %51 = arith.select %48, %49, %50 : index
    %52 = arith.divsi %51, %c512_10 : index
    %53 = arith.subi %c0_11, %52 : index
    %54 = arith.addi %52, %c1_12 : index
    %55 = arith.select %48, %53, %54 : index
    gpu.launch_func  @main_kernel_4::@main_kColReduction_reduce__6_1_0___thread_tile_h32_1 blocks in (%55, %c1, %c1) threads in (%c512, %c1, %c1) args(%dim : index, %dim_0 : index, %c512 : index, %47 : index, %8 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    cf.br ^bb7
  ^bb6:  // pred: ^bb4
    gpu.launch_func  @main_kernel_5::@main_kColReduction_reduce__6_1_0___block_tile_h64 blocks in (%c1, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    %56 = arith.cmpi eq, %8, %c0 : index
    %57 = arith.subi %8, %c1 : index
    %58 = arith.divui %57, %c512 : index
    %59 = arith.addi %58, %c1 : index
    %60 = arith.select %56, %c0, %59 : index
    %61 = arith.muli %60, %c256 : index
    %c256_13 = arith.constant 256 : index
    %c0_14 = arith.constant 0 : index
    %c1_15 = arith.constant 1 : index
    %62 = arith.cmpi sle, %61, %c0_14 : index
    %63 = arith.subi %c0_14, %61 : index
    %64 = arith.subi %61, %c1_15 : index
    %65 = arith.select %62, %63, %64 : index
    %66 = arith.divsi %65, %c256_13 : index
    %67 = arith.subi %c0_14, %66 : index
    %68 = arith.addi %66, %c1_15 : index
    %69 = arith.select %62, %67, %68 : index
    gpu.launch_func  @main_kernel_6::@main_kColReduction_reduce__6_1_0___block_tile_h64_1 blocks in (%69, %c1, %c1) threads in (%c256, %c1, %c1) args(%dim : index, %dim_0 : index, %c256 : index, %61 : index, %8 : index, %alloc : memref<?x4xf32, #gpu.address_space<global>>, %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>, %alloc_3 : memref<1xf32, #gpu.address_space<global>>)
    cf.br ^bb7
  ^bb7:  // 4 preds: ^bb2, ^bb3, ^bb5, ^bb6
    memref.dealloc %alloc_2 : memref<?x4xf32, #gpu.address_space<global>>
    memref.dealloc %alloc : memref<?x4xf32, #gpu.address_space<global>>
    %alloca = memref.alloca() : memref<0xindex>
    %70 = "disc_ral.dispatch"(%arg0, %5, %alloc_3, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1xf32, #gpu.address_space<global>>, memref<0xindex>) -> memref<f32, #gpu.address_space<global>>
    %reinterpret_cast_16 = memref.reinterpret_cast %70 to offset: [0], sizes: [], strides: [] : memref<f32, #gpu.address_space<global>> to memref<f32, #gpu.address_space<global>>
    memref.dealloc %alloc_3 : memref<1xf32, #gpu.address_space<global>>
    %alloc_17 = memref.alloc() : memref<f32>
    "disc_ral.dispatch"(%arg0, %5, %reinterpret_cast_16, %alloc_17) {backend_config = "", call_target_name = "d2h", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<f32, #gpu.address_space<global>>, memref<f32>) -> ()
    "disc_ral.dispatch"(%arg0, %5) {backend_config = "", call_target_name = "sync_on_stream", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>) -> ()
    memref.dealloc %reinterpret_cast_16 : memref<f32, #gpu.address_space<global>>
    "disc_ral.dispatch"(%arg0, %c0, %alloc_17) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<f32>) -> ()
    return
  }
  gpu.module @main_kernel attributes {gpu.binary = "P\EDU\BA\01\00\10\00H\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\08\04\00\00\00\00\00\00\08\04\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00h\0C\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\002\C0\0B\00\01\00\11\09\06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\11e__6_1_0___no_ibXthread_tile_h32H\00\0FB\00+osharedD\00+\9Fconstant0G\00(\FA\01debug_frame\00.rel\11\00!nv\14\00\11aS\00\0Fk\01 \0F\98\00%\0F\A4\01\FAo_param\AB\01\1C\0F\01\00\06\8Ck\00\00\00\03\00\0A\00\01\00 0\01\18\00,\09\00\01\00\11~\18\00,\04\00\01\00\11\9C\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\1C\00\00\00E\00#\04\10\18\00\80/\08\00\05\00\00\00\06\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04x\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C$\00u\01\00\08\00\00\F0!\10\00\01\01\00\C0\F0\11\00\03\1B\FF\00\04\1C\08\00p\B0\04\12\00\01\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00 \01/\05\00\01\00\FF\B0@$v\01\FF\0F\04\A3\FF\00\8E\07\00\C4\0F\00\C3y\EC\03\A2%\00\00\00\22\0E\00\19y\00\01\00\F3\0E!\00\00\00b\0E\00\90r\05?\04\00\00\80?\E0\FF\0F\00\C6\1F\00\B9z\04\00\00X\DE\02p\E4\0F\00\A4r\04\05B\00\B1?\02\8E\0F\00\CC\0F\00\0C|\00>\04\95pR\F0\0B\00\DA/\00Mc\04\F3\0D\80\03\00\EA\0F\00\02z\02\00\00Z\00\00\00\0F\00\00\00\E2\0F\00$v\03\FF\00[\90\00\22\E2\0F`\001F\00\00\B3\04\80\C8\0F\00\86y\00\02\FF\B0\030\19\10\0C \00*MyP\00PGy\00\00\F09\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\80\0F\01\00-\00\07\05.\03\00\01\00\22@\00\01\00=k\01\000\00\08\01\00\1F\0B@\00\04*\AB\01\08\00\0F@\00\05\13\13\0C\04\0C\01\00\13XU\00*\90\000\04\04M\04\22\18\00\01\00.>\01T\00\00\01\00\13\E8@\00/p\00\80\00\0B\1F)'\00\03A\00X\04\00\01\00\04@\06\04\E4\00*\04\00\01\00\1Fq@\00\04\13\88)\00&L\00@\00\1F\0A@\00\00!\\\01D\01\0D@\00\13\D8)\00*\D8\00\01\00\1B\08\08\00%K\01\DB\0A\09\01\00\13\B0]\05\17\10\80\00\17\048\00\04\18\00\13\F7\14\01\0C\84\01*\C0\05(\07\1F\00\C0\00\04\132@\00*\06\00\01\00*\80\07\C0\07\12\03\C0\01:\06\80\00\01\00\13\06\D8\01\05\A8\0B\0B\01\00*\A8\00\08\00\04\F8\00\14\018\00/\05\00\01\00\029@\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32(%arg0: i32, %arg1: !llvm.ptr<1>) attributes {disc.elimargs = [1 : index, 3 : index, 4 : index, 5 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(1 : index) : i32
      %1 = llvm.mlir.constant(0.000000e+00 : f32) : f32
      %2 = nvvm.read.ptx.sreg.ctaid.x : i32
      %3 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %4 = llvm.mul %2, %arg0  : i32
      %5 = llvm.add %3, %4  : i32
      %6 = llvm.icmp "ult" %5, %0 : i32
      llvm.cond_br %6, ^bb2, ^bb3
    ^bb2:  // pred: ^bb1
      %7 = llvm.getelementptr %arg1[%5] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      llvm.store %1, %7 : f32, !llvm.ptr<1>
      llvm.br ^bb3
    ^bb3:  // 2 preds: ^bb1, ^bb2
      llvm.return
    }
  }
  gpu.module @main_kernel_0 attributes {gpu.binary = "P\EDU\BA\01\00\10\00\00\09\00\00\00\00\00\00\02\00\01\01@\00\00\00\C0\08\00\00\00\00\00\00\BF\08\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00h\17\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\002\C0\16\00\01\00\11\14\06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\13e__6_1_0___no_ibXthread_tile_h32_1J\00\0FD\00-osharedF\00-\9Fconstant0I\00*\FA\01debug_frame\00.rel\11\00!nv\14\00\11aU\00\0Fs\01 \0F\9A\00'\0F\AE\01\FF\03o_param\B5\01\1C\0F\01\00\04\8Cm\00\00\00\03\00\0A\00\01\00 8\01\18\00,\09\00\01\00\11\88\18\00,\04\00\01\00\11\A6\18\00,\07\00\01\00g2\00\00\00\12\10`\00\11\0C\06\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0\0B\07\00 \00\04\9B\00 \04\18i\00\01:\002\04\B8\02\18\00p/\08\00\05\00\00\00\1A\00\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\88\04\80\015\00\00\04\0A\08\00F\00\A2`\01(\00\03\19(\00\04\17\C5\00u\05\00 \00\00\F0!\10\009\04\00\18\10\009\03\00\10\10\00u\02\00\08\00\00\F0\11\10\009\01\00\04\10\00\01\01\00\F3\01\F0\11\00\03\1B\FF\00\04\1C\08\00`\00\00\00P\D8\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\00\D2\05b,\00\00\00H\00\01\00\00`\01/\05\00\01\00\FF\E0@$v\01\FF\7F\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\BB\02Q\0E\00\19y\03\0F\00 \00!-\00\F0\14\0E\00$z\00\00\00X\00\00\03\02\8E\07\00\CA\1F\00\12x\FF\00\FF\01\00\00\FF\C0\80\07\00\C8\0F\00\0C \00\C5Y\00\00pdp\00\00\DA\0F\00M\C3\04\B0\80\03\00\EA\0F\00\19x\03\FF\1F\0F\00\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\00\03\05a\C6\0F\00\11r\03q\001\FFH\8FP\00\000\00\00C\00\10\030\00\93\CA\0F\00$x\07\03 \00\B0\00p\C8\0F\00%x\04\07s\04\10\FF\90\00\F0\09\E2\0F\04\0Cz\00\07\00Z\00\00pb\F4\03\00\E4\0F\04\10x\00\07\01 \00\B0\E0\FF\07\00\E4\0F\00\12x\04\04\01\031\FF\FC\8E\10\00\01\B0\00\010\00\10\F60\00p\00\10z\02\04\00\\0\00!\F1\07@\00Pz\04\04\00^\10\00\11\F3 \01\00P\00\00,\03\04P\00\C2\10z\03\05\00]\00\00\FF\E4\7F\000\00@\05\05\00_\10\00*\FF\00`\00 \F8\03\F0\00\F4\06\81\B9\0B\02\04\FC\FF\FF\00\19\1E\0C\00\A2\0E\00\10x\06\07\03P\00\93\C6\0F\00\81\A9\00\02\04\F8 \00u\E8\0E\00\81\A9\09\04\10\00 \E2\0EP\00\12\06P\00!\FA\030\005\B9\08\04P\00p\A8\0E\00\81\C9\0D\02\10\01\01\10\00y(\0F\00\81\C9\0A\04\10\00S\D9\0F\02\04\04\10\00\10h\10\00%\0C\04\10\00!b\0F\90\00\14\04\90\00\01\F0\006\0E\07\05\F0\00\16\04\80\00\12\FC0\01Jx\10\07\06 \00\12\0E \00\13\F0 \00:\0E\07\07 \00\12\10 \00\11\F2\10\01T$r\06\FF\FF\D0\01\12\E2P\00\14\0A0\00a\E2\0F\04#\A2\06\06\07\00$\00F\00\E2\8F\00`\00\12\F4P\01S\E9\09\02\04\08\C0\00!\E2\0Ep\00\18\08P\018\E9\00\04 \00@#\B2\06\0B\0F\00\11\06P\00\17OP\00\02\00\02P\81\89\0B\02\04\0F\06\05\A0\018\0E\07\09P\008\89\08\04 \00T#\C2\06\0D\0AP\00\93\C6\0F\01\81\99\0D\02\04\10 \00'\22\0F`\00\12\F8\B0\01W\99\0A\04\04\10\90\01X\A9\11\02\04\14\10\00F\0E\04\04\14@\00U#\D2\06\0F\0C\B0\00&\0F\02@\01 \FA\03\A0\01g\81\B9\0F\02\04\18\D0\019\B9\0C\04\10\00X\C9\13\02\04\1C\10\00)\10\04\10\00X\D9\15\02\04 \10\00H\12\04\04 \10\026\14\07\0B\90\01e\00#\E2\06\09\00\90\00\11\8F\10\03\17\0C0\02E\0Cz\00\14 \02\000\00\1D\82p\01\1A\00 \027\00\07\0D \02X\10x\08\07\0Ep\00\17\92`\016\E2\0F\01@\00C\F2\03\00\CA\00\025$\00\00\00\03\00\F0\01\17$\A0\01F\A2\06\11\0E@\00\00\80\00\17\08 \04\02\F0\01\18(@\028\08\07\0F\F0\01\00\D0\01\17,\90\01\1D\B2\90\01\19\08@\02\00\E0\01\070\00\\\10x\0C\07\10@\02\07p\00Z#\C2\06\13\10\A0\00\15\0C0\02\00P\00X\A9\0F\02\040\90\018\10\07\11P\008\A9\0C\04 \00Z#\D2\06\15\12P\00\060\02\00P\00W\B9\11\02\044\F0\01[\B9\0E\04\0440\02\1B80\02\1B80\02\1B<0\02\1A<0\02\1F\120\02\06\11O\F0\01\1F\130\02\16\1A\8F0\02\13\D6\F0\01\18@\A0\03?\00\07\140\02\08\00\D0\01\16\15`\02\19\00@\02\000\00\1F\A2\D0\01\05\11\F4 \00\01P\02\09p\009\08\07\16\B0\027\0A\07\17\80\00\01P\02\19Dp\04\0F\80\02\04\12\F6\C0\03\00\10\02\040\00\00\F0\05\00p\02\17Hp\02\0E \02\19\0A \02\00p\02\070\00\00\B0\04\19\18 \02O\0F\02\04L \02\0A\19\0E \02\00P\02\17L\E0\01\000\02\1BP0\02\1BP0\02\1BT0\02\1BT0\02\1BX0\02\1AX0\02\1F\190\02\05*\C6O \02\030\04\18\1A0\00\0B0\02\00\A0\01\14\1B \00\13\D20\02\16\\\C0\01\0FP\04\0A\03`\00\1A\1C\B0\06\1A\08\B0\048\08\07\1D\80\00\0E@\02\15\00@\02\13\C4@\02\18\\@\028\0A\07\1E\A0\01_\99\0B\02\04`0\02\14\01\F0\01(\08\040\00\00`\04\18\1FP\00_\89\07\02\04d@\02\18X\89\0A\04\04d\80\06O\0D\02\04h0\02\0A\15\0C0\02\13\C40\02\17h\E0\01\00\90\06\1Bl0\02\1Bl \02\18p\10\00K\11\02\04p \02\18t\10\00%\13\02\10\00\1Bb \02 \C8O\D0\01\06\00\02 \C8\8F\10\02%\07\0A\10\00v\0F\01#\A2\06\0D\0C\10\00\10\02\C0\05\08`\01c$v\08\FF\00`\80\08\10\E4\10\00C\09\FF\00a\10\00\11\E2@\01&\11\10@\00\00\10\01\15\13\10\01p\CA\0F\00\8Ey\00\08\0C\00@\84\E7\10\0C0\00*My\F0\0APGy\00\00\F09\0F\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00p\0F\01\00-\11\01h\0F\0E\01\00\22@\00\01\00=s\01\000\00\08\01\00\1F\0B@\00\04\13\B3)\00\1F\B5@\00\0C\13\13\BC\0E\0C\01\00\13hU\00*\90\00\E0\0E\22\08\00\01\00\22\18\00\01\00.F\01T\00\00\01\00\13\F8@\00/p\00\80\00\0B\1F)'\00\03A\00h\04\00\01\00\00\9B\07\17\00\E4\00*\04\00\01\00\1Fs@\00\04\13\98)\00&\8C\00@\00\1F\0A@\00\00!d\01D\01\0D@\001(\05\00\01\00*\D8\00\01\00\1B\08\08\00?S\01\00f\12\00\04\E1\02\10\00B\11\05\80\00\17\048\00\04\18\00\13\FD\14\01\0C\84\01&\10\06\B0\12\04\01\00\0F\C0\00\01\132@\00+\06\00\01\00\1A\08\B0\12\12\03\C0\01>\18\80\00\A7\00\18\05\A8\16\0B\01\00*\A8\00\08\00\04W\00\13\018\00\04\A8\00\0D\D0\12)\0D\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_1(%arg0: i32, %arg1: i32, %arg2: i32, %arg3: !llvm.ptr<1>, %arg4: !llvm.ptr<1>, %arg5: !llvm.ptr<1>) attributes {disc.elimargs = [3 : index, 4 : index, 6 : index, 7 : index, 8 : index, 9 : index, 10 : index, 11 : index, 13 : index, 14 : index, 15 : index, 16 : index, 17 : index, 18 : index, 20 : index, 21 : index, 22 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(0 : index) : i32
      %1 = llvm.mlir.constant(1 : index) : i32
      %2 = llvm.mlir.constant(512 : index) : i32
      %3 = llvm.mlir.constant(32 : index) : i32
      %4 = llvm.mlir.constant(0.000000e+00 : f32) : f32
      %5 = nvvm.read.ptx.sreg.ctaid.x : i32
      %6 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %7 = llvm.mul %5, %arg0  : i32
      %8 = llvm.add %6, %7  : i32
      %9 = llvm.icmp "ult" %8, %arg1 : i32
      llvm.cond_br %9, ^bb2, ^bb12
    ^bb2:  // pred: ^bb1
      %10 = llvm.srem %8, %2  : i32
      %11 = llvm.sdiv %8, %2  : i32
      %12 = llvm.icmp "ult" %10, %1 : i32
      llvm.cond_br %12, ^bb3, ^bb11
    ^bb3:  // pred: ^bb2
      %13 = llvm.mul %11, %3  : i32
      llvm.br ^bb4(%0, %4 : i32, f32)
    ^bb4(%14: i32, %15: f32):  // 2 preds: ^bb3, ^bb9
      %16 = llvm.icmp "slt" %14, %3 : i32
      llvm.cond_br %16, ^bb5, ^bb10
    ^bb5:  // pred: ^bb4
      %17 = llvm.add %13, %14  : i32
      %18 = llvm.icmp "slt" %17, %arg2 : i32
      llvm.cond_br %18, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %19 = llvm.getelementptr %arg3[%17] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %20 = llvm.load %19 : !llvm.ptr<1> -> f32
      %21 = llvm.getelementptr %arg4[%17] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %22 = llvm.load %21 : !llvm.ptr<1> -> f32
      %23 = llvm.fmul %20, %22  : f32
      %24 = llvm.fadd %15, %23  : f32
      llvm.br ^bb8(%24 : f32)
    ^bb7:  // pred: ^bb5
      llvm.br ^bb8(%15 : f32)
    ^bb8(%25: f32):  // 2 preds: ^bb6, ^bb7
      llvm.br ^bb9
    ^bb9:  // pred: ^bb8
      %26 = llvm.add %14, %1  : i32
      llvm.br ^bb4(%26, %25 : i32, f32)
    ^bb10:  // pred: ^bb4
      %27 = llvm.getelementptr %arg5[%10] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %28 = llvm.atomicrmw fadd %27, %15 acq_rel : !llvm.ptr<1>, f32
      llvm.br ^bb11
    ^bb11:  // 2 preds: ^bb2, ^bb10
      llvm.br ^bb12
    ^bb12:  // 2 preds: ^bb1, ^bb11
      llvm.return
    }
  }
  gpu.module @main_kernel_1 attributes {gpu.binary = "P\EDU\BA\01\00\10\00H\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\08\04\00\00\00\00\00\00\06\04\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00h\0C\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\002\C0\0B\00\01\00\11\09\06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\10e__6_1_0___no_ibXblock_tile_h64G\00\0FA\00*osharedC\00*\9Fconstant0F\00'\FA\01debug_frame\00.rel\11\00!nv\14\00\11aR\00\0Fg\01 \0F\97\00$\0F\9F\01\F6o_param\A6\01\1C\0F\01\00\07\8Cj\00\00\00\03\00\0A\00\01\00 ,\01\18\00,\09\00\01\00\11y\18\00,\04\00\01\00\11\97\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\1C\00\00\00E\00#\04\10\18\00\80/\08\00\05\00\00\00\06\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04p\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C$\00u\01\00\08\00\00\F0!\10\00\01\01\00\C0\F0\11\00\03\1B\FF\00\04\1C\08\00p\A8\04\12\00\01\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00 \01/\05\00\01\00\FF\B8@$v\01\FF\17\04\A3\FF\00\8E\07\00\C4\0F\00\C3y\F4\03\A2%\00\00\00\22\0E\00\19y\00\01\00\F3\0E!\00\00\00b\0E\00\90r\05?\04\00\00\80?\E0\FF\0F\00\C6\1F\00\B9z\04\00\00X\E6\02p\E4\0F\00\A4r\04\05B\00\B1?\02\8E\0F\00\CC\0F\00\0C|\00F\04\95pR\F0\0B\00\DA/\00Mk\04\F3\0D\80\03\00\EA\0F\00\02z\02\00\00Z\00\00\00\0F\00\00\00\E2\0F\00$v\03\FF\00[\90\00\22\E2\0F`\001F\00\00\BB\04\80\C8\0F\00\86y\00\02\FF\B8\030\19\10\0C \00*MyP\00PGy\00\00\F0A\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\80\0F\01\00-\00\0F\05.\03\00\01\00\22@\00\01\00=g\01\000\00\08\01\00\1F\0B@\00\04\13\A7)\00\1F\A6@\00\0C\13\13\14\04\0C\01\00\13PU\00*\90\008\04\04U\04\22\18\00\01\00.:\01T\00\00\01\00\13\E0@\00/p\00\80\00\0B\1F)'\00\03A\00P\04\00\01\00\04H\06\04\E4\00*\04\00\01\00\1Fp@\00\04\13\80)\00&L\00@\00\1F\0A@\00\00!X\01D\01\0D@\00\13\D0)\00*\D8\00\01\00\1B\08\08\00%G\01\DB\0A\09\01\00\13\A8e\05\17\10\80\00\17\048\00\04\18\00\13\F4\14\01\0C\84\01*\B8\050\07\1F\00\C0\00\04\132@\00*\06\00\01\00*\80\07\C8\07\12\03\C0\01:\06\80\00\01\00\13\06\D8\01\05\A8\0B\0C\C8\00\1A\00\08\00\04\F8\00\13\018\00\04\A8\00\0C\01\009H\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64(%arg0: i32, %arg1: !llvm.ptr<1>) attributes {disc.elimargs = [1 : index, 3 : index, 4 : index, 5 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(1 : index) : i32
      %1 = llvm.mlir.constant(0.000000e+00 : f32) : f32
      %2 = nvvm.read.ptx.sreg.ctaid.x : i32
      %3 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %4 = llvm.mul %2, %arg0  : i32
      %5 = llvm.add %3, %4  : i32
      %6 = llvm.icmp "ult" %5, %0 : i32
      llvm.cond_br %6, ^bb2, ^bb3
    ^bb2:  // pred: ^bb1
      %7 = llvm.getelementptr %arg1[%5] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      llvm.store %1, %7 : f32, !llvm.ptr<1>
      llvm.br ^bb3
    ^bb3:  // 2 preds: ^bb1, ^bb2
      llvm.return
    }
  }
  gpu.module @main_kernel_2 attributes {gpu.binary = "P\EDU\BA\01\00\10\00\A8\0B\00\00\00\00\00\00\02\00\01\01@\00\00\00h\0B\00\00\00\00\00\00c\0B\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\A8#\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00g\02\00\BE\00r\00\01\00\22#\00\01\00\11 \06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\12e__6_1_0___no_ibXblock_tile_h64_1I\00\0FC\00,osharedE\00,\9Fconstant0H\00)\FA\01debug_frame\00.rel\11\00!nv\14\00\11aT\00\0Fo\01 \0F\99\00&\0F\A9\01\B6\8F$____wg_B\00&\00\1B\00/26\F1\016o_param\F8\01\1C\0F\01\00\05\8Cl\00\00\00\03\00\0A\00\01\00\11\EF\18\00,\0B\00\01\00 |\01\18\00,\09\00\01\00\11\CB\18\00,\04\00\01\00\11\E9\18\00,\07\00\01\00g2\00\00\00\12\10x\00\11\18\06\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0\17\07\00 \00\04\9B\00R\04\14\00\00\00E\002\04\C0\05\18\00C/\08\00\06\7F\00\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E0\04\F1\08\015\00\00\04\0A\08\00\03\00\00\00`\01(\00\03\19(\00\04\17\0C$\00\10\05\ED\04%\F0!\10\009\04\00\18\10\009\03\00\10\10\00u\02\00\08\00\00\F0\11\10\009\01\00\04\10\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\00\10\17\00\00`\17\00\00\04\1Et\01#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\005\02\00\00\84\01\0F\01\00\FFz@$v\01\FF?\04\A3\FF\00\8E\07\00\C4\0F\00\19y\A6\01\10%[\02a\0E\00\19y\03\00\01\00\10!-\00\F5\14\0E\00$z\06\06\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\06\00Y\00\00p`\F0\03\00\DA\0F\00M[\04\B0\80\03\00\EA\0F\00\19x\03\FF\1F\FC\01\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\00\B3\04\93\E2\0F\00Ey\00\00@\150\00\93\E2\0F\00$r\08\FF\FF\00\90\00p\E2\0F\00\11r\03\03\92\00\C1\FF@\8F\07\00\C8\0F\00\12x\05\031\04\10\C0\80\00p\0F\00$x\06\06\01\B4\03\12\0A\10\00@\12x\05\06p\00\01 \00p\E4\0F\04\19x\00\FF*\04\C0\06\16\01\00\00\E4\0F\00\0Cr\00\05`\00@pR\F2\03 \00P\0Cx\00\06\7F\10\00\22@\F0\80\002x\07\05\C9\02\11\8Ep\00T$x\07\07\04\90\00\9A\CC\0F\00G\19\00\00\80\14\E0\00\000\00\10\03\E0\00\01\90\00*\03\03@\004\09\03@@\00q\C8\0F\00%x\04\09P\00\11\02\E0\00P\04\10x\00\09\C0\001\FF\E0\FF\B0\00\B0\0Cz\00\09\00Z\00\00pb\F4\A0\00P\00\12x\04\04P\00!\FF\FC\D0\00\00p\01\12\00 \00\11\F6 \00`\10z\02\04\00\\@\00\11\F3@\00`\10z\04\04\00^\10\00\11\F9\D0\01\00`\00\00|\03\03`\00\D2\00\10z\03\05\00]\00\00\FF\E4\FF\000\00@\05\05\00_\10\00*\7F\02`\00\11\F8\10\01\F4\06\81\B9\0D\02\04\FC\FF\FF\00\19\1E\0C\00\A2\0E\00\10x\0A\09\03P\00\93\C6\0F\00\81\A9\00\02\04\F8 \00 \E2\0E@\00\12\0A@\00\22\FA\03 \00%\0B\04 \00u\E8\0E\00\81\B9\0A\04P\00p\A8\0E\00\81\C9\0F\02\10\01\01\10\00y(\0F\00\81\C9\0C\04\10\00S\D9\11\02\04\04\10\00\10h\10\00%\0E\04\10\00\10b\E0\004\10\09\04\90\00\01\F0\00:\12\09\05P\01\12\10\90\00#\FC\03\E0\00\12\12\10\00\12\F2@\01Dx\10\09\060\00a\E2\0F\04#\A2\086\07\00$\021\00\E2\8F@\01\14\07 \00q\CE\0F\00\81\E9\0B\02\91\01\06\F0\00\15\10\B0\01\93\E2\0F\00#\B2\08\0D\0A\00=\05!\E2O0\01\16\08`\00`\00\81\99\0D\02\04\17\06\03P\01\09\D0\01\000\018\E9\00\04`\00U#\C2\08\0F\0CP\00&\0F\01`\01\11\F8\C0\00H\81\99\0A\04P\00\00\D0\00\19\09\A0\01C\0F\02\04\10 \00\86\22\0F\00#\D2\08\11\0EP\00\16\02\C0\00\12\FAP\00W\A9\0C\04\04\10\80\01W\B9\11\02\04\14\80\019\B9\0E\04\10\00X\C9\13\02\04\18\10\00)\10\04\10\00X\D9\15\02\04\1C\10\00H\12\04\04\1C\C0\017\14\09\0A \01U#\E2\08\0B\00\A0\00\02\80\01\17\0B\E0\01E\0Cz\00\14\D0\01\000\00\1A\92p\01\06P\01\04\E0\017\00\09\0C\10\02\00\90\01\18\0Dp\00\1D\A2`\01\15\00\D0\01\13\CA\F0\01\18 `\018\0C\09\0F`\01\00\B0\01\07 \00-#\B2`\01\19\0A\B0\03Lx\0A\09\0E\10\02\19$\F0\01$\13\10@\00\02\B0\02\06\F0\01\13\E2\D0\01\17(\A0\01\00\00\02\08@\00F\D2\08\15\12\80\00\00@\00\15\0C\E0\01\13\C4\D0\01\17,\90\01\00\F0\01\08P\00\00\E0\01\1B,\E0\01\1B0\E0\01\1B0\E0\01\1B4\E0\01\1A4\E0\01\1F\10\E0\01\06\11O\A0\01\1F\11\E0\01\16\1A\8F\E0\01\13\D6\A0\01\188\A0\01;\00\09\12\F0\01\1F\13\F0\01\15\13\E4\D0\01\1F8\D0\01\1B\16\14`\00\11\04\D0\01\16<\90\01X\10x\0E\09\15\80\00\08\E0\01\1B\E2\E0\01\04\B0\03\1B@\E0\01\1F<\E0\01\0A\1D\0E\C0\03\1B@\C0\03\1BD\C0\03\1BD\E0\01\1BH\E0\01\1BH\E0\01\1BL\E0\01\1AL\E0\01\1F\16\E0\01\0C\1F\17\E0\01-\1AP\E0\01\1F\18\D0\01\08\00\90\01\16\19\00\02\1F\00\E0\01\02\1FP\E0\01\17\01\D0\01\18T\D0\017\0A\09\1A\D0\00\00\10\04\1F\1B\E0\01\15\13\E4\E0\01\1BX\E0\01\1FT\C0\03\1C\1B\\\C0\03\1BX\C0\03\1B\\\E0\01\1B`\E0\01\1B`\E0\01\1Bd\E0\01\1Ad\E0\01\1F\1C\E0\01\0C\1F\1D\E0\01\05\13\DA\C0\01\17hp\01\0F\F0\01\09\01P\03;\00\09\1E\C0\03\1F\1F\C0\03\1D\1Fh\E0\01\14\01\C0\01<\0A\09 \C0\03\1Al\C0\03\1F!\C0\03\1D\1Bp\E0\01\1Fl\C0\03\1C\1Bp\C0\03\1Bt\C0\03\1Bt\E0\01\1Bx\E0\01\1Bx\E0\01\1B|\E0\01\1A|\E0\01\1F\22\E0\01\0C\1F#\C0\03-\16\80\90\01\00P\00\1F$\C0\03\0C\1F%\C0\03\0D\1F\80\C0\03\1C\18\84\D0\017\0A\09&\D0\00\00\C0\03\1F'\C0\03\1D\1B\88\E0\01\1F\84\C0\03\1C\1B\8C\C0\03\1B\88\C0\03\1B\8C\E0\01\1B\90\E0\01\1B\90\E0\01\1B\94\E0\01\1A\94\E0\01\1F(\E0\01\0C\1F)\C0\03\0D\1F\98\C0\03\1B\1B*\C0\03\1F+\C0\03\1D\1F\98\C0\03\1B\1C,\C0\03\1A\9C\C0\03\1F-\C0\03\1D\1B\A0\E0\01\1F\9C\C0\03\1C\1B\A0\C0\03\1B\A4\C0\03\1B\A4\E0\01\1B\A8\E0\01\1B\A8\E0\01\1B\AC\E0\01\1A\AC\E0\01\1F.\E0\01\0C\1F/\C0\03-\16\B0\90\01\00P\00\1F0\C0\03\0C\1F1\C0\03\0D\1F\B0\C0\03\1C\18\B4\D0\017\0A\092\D0\00\00\C0\03\1F3\C0\03\1D\1B\B8\E0\01\1F\B4\C0\03\1C\1B\BC\C0\03\1B\B8\C0\03\1B\BC\E0\01\1B\C0\E0\01\1B\C0\E0\01\1B\C4\E0\01\1A\C4\E0\01\1F4\E0\01\0C\1F5\C0\03\0D\1F\C8\C0\03\1B\1B6\C0\03\1F7\C0\03\1D\1F\C8\C0\03\1B\1C8\C0\03\1A\CC\C0\03\1F9\C0\03\1D\1B\D0\E0\01\1F\CC\C0\03\1C\1B\D0\C0\03\1B\D4\C0\03\1B\D4\E0\01\1B\D8\E0\01\1B\D8\E0\01\1B\DC\E0\01\1A\DC\E0\01\1F:\E0\01\0C\1F;\C0\03-\16\E0\90\01\00P\00\1F<\C0\03\0C\1F=\C0\03\0D\1F\E0\C0\03\17\00P\00\17>\C0\00\00\B0\03\17?`\00g\81\99\09\02\04\E4\A0\01\0F\C0\03\0D\00\D0\01\040\00\00P\12Y\A9\0D\02\04\E8\E0\10\0F\C0\03\0B\00\E0\01\18\E8\E0\01K\0F\02\04\EC\E0\01\1B\EC\D0\01\18\F0\10\00K\11\02\04\F0\D0\01\18\F4\10\00%\13\02\10\00\1Bb\D0\01 \C8O\B0\01\15\09\B0\01 \C8\8F\80\01\15\0D\80\01\86\C8\0F\01#\B2\08\0F\0E\10\00f\02#\C2\08\11\10\10\00\00\E0\00\15\13\E0\00P\C4\0F\00Ay\09\00\06\90\14c\88s\00\07\08\00!\00f\E8\0F\00\1D{\00\01\00S\EC\0F\00\84\89\CD\19\13\08 \01Ax\00\06?\00\15\11\F2@\03c\84\89\03\07\00\10 \00s$\0E\00!\82\00\00\02\16\10\00\F0\15'\88\83@\00\0F`\00\01-\99\02`\00\14\1F`\15\00`\00W\99\03\07\00\08`\009\92\02\02`\00O\93\00\07\02\C0\00\0A\1A\03`\005r\00\06\D0\15\01\C0\00H\04\07\00\04\C0\00J\04\03\04\00\C0\00\1F\04`\00\089M\19\00P\016\84y\07p\00\93\22\0E\00$v\02\FF\00`\D0\15\93\C4\0F\00$v\03\FF\00a\10\00a\CA\0F\00\8Ey\00\01\01\9B\84\E7\10\0C\00\E2\1F\00M\A0\01PGy\00\00\F0\C0\16\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00`\0F\01\00-'\01\00 \02\07\01\00\22@\00\01\00=o\01\000\00\08\01\00\1F\0B@\00\04\13\AF)\00\1F\F8@\00\0C\13\13\\\1A\0C\01\00\13\A8U\00*\A8\00\80\1A\22\08\00\01\00\22\18\00\01\00.B\01T\00\00\01\00\13P5\02/p\00\80\00\0B/)\00'\00\02#\00\C0@\00\00,\09\17\00\E4\00*\04\00\01\00\1Fr@\00\04\13\F0)\00&\98\00@\00\1F\0A@\00\00!`\01D\01\0D@\001\88\05\00\01\00*\D8\00\01\00\1B\08\08\00?O\01\00\0E\1E\00Q\00\00`\06\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\FA\14\01\0C\84\01\13p@\00\17\881\01\0F\C0\00\01\132@\00\15\06R\00\0B\A1\00\16\18\80\00j\06\00\00\18\80\00\01\00\13\B5+\00+\03\00\01\00\00\D5\0F\1A\00q\00\0F\80\00\01\11\06\F0\1D\07\E8\22\0CH\02\1A\00\08\00\04\97\00\13\018\00\04\E8\00\0C\01\00*\90\19\08\00\088\00\18\06\A0\00\0F\01\00\05\03\A9\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00"} {
    llvm.mlir.global internal @__wg_main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1_0() {addr_space = 3 : i32} : !llvm.array<256 x f32>
    llvm.func @main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1(%arg0: i32, %arg1: i32, %arg2: i32, %arg3: !llvm.ptr<1>, %arg4: !llvm.ptr<1>, %arg5: !llvm.ptr<1>) attributes {disc.elimargs = [3 : index, 4 : index, 6 : index, 7 : index, 8 : index, 9 : index, 10 : index, 11 : index, 13 : index, 14 : index, 15 : index, 16 : index, 17 : index, 18 : index, 20 : index, 21 : index, 22 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(32 : index) : i32
      %1 = llvm.mlir.constant(8 : index) : i32
      %2 = llvm.mlir.constant(64 : index) : i32
      %3 = llvm.mlir.constant(4 : index) : i32
      %4 = llvm.mlir.constant(0.000000e+00 : f32) : f32
      %5 = llvm.mlir.constant(2 : index) : i32
      %6 = llvm.mlir.constant(1 : index) : i32
      %7 = llvm.mlir.constant(256 : index) : i32
      %8 = llvm.mlir.constant(0 : index) : i32
      %9 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1_0 : !llvm.ptr<3>
      %10 = llvm.getelementptr %9[0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<256 x f32>
      %11 = nvvm.read.ptx.sreg.ctaid.x : i32
      %12 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %13 = llvm.mul %11, %arg0  : i32
      %14 = llvm.add %12, %13  : i32
      %15 = llvm.icmp "ult" %14, %arg1 : i32
      llvm.cond_br %15, ^bb2, ^bb21
    ^bb2:  // pred: ^bb1
      %16 = llvm.srem %14, %7  : i32
      %17 = llvm.sdiv %14, %7  : i32
      %18 = llvm.udiv %16, %0  : i32
      %19 = llvm.urem %16, %0  : i32
      %20 = llvm.mul %19, %1  : i32
      %21 = llvm.add %18, %20  : i32
      %22 = llvm.icmp "ult" %19, %6 : i32
      llvm.cond_br %22, ^bb3, ^bb10(%4 : f32)
    ^bb3:  // pred: ^bb2
      %23 = llvm.mul %17, %1  : i32
      %24 = llvm.add %18, %23  : i32
      %25 = llvm.mul %24, %2  : i32
      llvm.br ^bb4(%8, %4 : i32, f32)
    ^bb4(%26: i32, %27: f32):  // 2 preds: ^bb3, ^bb9
      %28 = llvm.icmp "slt" %26, %2 : i32
      llvm.cond_br %28, ^bb5, ^bb10(%27 : f32)
    ^bb5:  // pred: ^bb4
      %29 = llvm.add %26, %25  : i32
      %30 = llvm.icmp "slt" %29, %arg2 : i32
      llvm.cond_br %30, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %31 = llvm.getelementptr %arg3[%29] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %32 = llvm.load %31 : !llvm.ptr<1> -> f32
      %33 = llvm.getelementptr %arg4[%29] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %34 = llvm.load %33 : !llvm.ptr<1> -> f32
      %35 = llvm.fmul %32, %34  : f32
      %36 = llvm.fadd %27, %35  : f32
      llvm.br ^bb8(%36 : f32)
    ^bb7:  // pred: ^bb5
      llvm.br ^bb8(%27 : f32)
    ^bb8(%37: f32):  // 2 preds: ^bb6, ^bb7
      llvm.br ^bb9
    ^bb9:  // pred: ^bb8
      %38 = llvm.add %26, %6  : i32
      llvm.br ^bb4(%38, %37 : i32, f32)
    ^bb10(%39: f32):  // 2 preds: ^bb2, ^bb4
      llvm.br ^bb11(%39 : f32)
    ^bb11(%40: f32):  // pred: ^bb10
      llvm.br ^bb12
    ^bb12:  // pred: ^bb11
      %41 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      llvm.store %40, %41 : f32, !llvm.ptr<3>
      nvvm.barrier0
      %42 = llvm.icmp "slt" %18, %3 : i32
      llvm.cond_br %42, ^bb13, ^bb14
    ^bb13:  // pred: ^bb12
      %43 = llvm.add %21, %3  : i32
      %44 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %45 = llvm.load %44 : !llvm.ptr<3> -> f32
      %46 = llvm.getelementptr %10[%43] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %47 = llvm.load %46 : !llvm.ptr<3> -> f32
      %48 = llvm.fadd %45, %47  : f32
      %49 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      llvm.store %48, %49 : f32, !llvm.ptr<3>
      llvm.br ^bb14
    ^bb14:  // 2 preds: ^bb12, ^bb13
      nvvm.barrier0
      %50 = llvm.icmp "slt" %18, %5 : i32
      llvm.cond_br %50, ^bb15, ^bb16
    ^bb15:  // pred: ^bb14
      %51 = llvm.add %21, %5  : i32
      %52 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %53 = llvm.load %52 : !llvm.ptr<3> -> f32
      %54 = llvm.getelementptr %10[%51] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %55 = llvm.load %54 : !llvm.ptr<3> -> f32
      %56 = llvm.fadd %53, %55  : f32
      %57 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      llvm.store %56, %57 : f32, !llvm.ptr<3>
      llvm.br ^bb16
    ^bb16:  // 2 preds: ^bb14, ^bb15
      nvvm.barrier0
      %58 = llvm.icmp "slt" %18, %6 : i32
      llvm.cond_br %58, ^bb17, ^bb18
    ^bb17:  // pred: ^bb16
      %59 = llvm.add %21, %6  : i32
      %60 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %61 = llvm.load %60 : !llvm.ptr<3> -> f32
      %62 = llvm.getelementptr %10[%59] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %63 = llvm.load %62 : !llvm.ptr<3> -> f32
      %64 = llvm.fadd %61, %63  : f32
      %65 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      llvm.store %64, %65 : f32, !llvm.ptr<3>
      llvm.br ^bb18
    ^bb18:  // 2 preds: ^bb16, ^bb17
      nvvm.barrier0
      %66 = llvm.icmp "eq" %18, %8 : i32
      %67 = llvm.and %66, %22  : i1
      llvm.cond_br %67, ^bb19, ^bb20
    ^bb19:  // pred: ^bb18
      %68 = llvm.getelementptr %10[%21] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %69 = llvm.load %68 : !llvm.ptr<3> -> f32
      %70 = llvm.getelementptr %arg5[%19] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %71 = llvm.atomicrmw fadd %70, %69 acq_rel : !llvm.ptr<1>, f32
      llvm.br ^bb20
    ^bb20:  // 2 preds: ^bb18, ^bb19
      llvm.br ^bb21
    ^bb21:  // 2 preds: ^bb1, ^bb20
      llvm.return
    }
  }
  gpu.module @main_kernel_3 attributes {gpu.binary = "P\EDU\BA\01\00\10\00H\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\08\04\00\00\00\00\00\00\01\04\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\0Be__6_1_0___thread_tile_h32B\00\0F<\00%oshared>\00%\9Fconstant0A\00\22\FA\01debug_frame\00.rel\11\00!nv\14\00\11aM\00\0FS\01 \0F\92\00\1F\0F\86\01\E2o_param\8D\01\1C\0F\01\00\04\8Ce\00\00\00\03\00\0A\00\01\00 \18\01\18\00,\09\00\01\00\11`\18\00,\04\00\01\00\11~\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\1C\00\00\00E\00#\04\10\18\00\80/\08\00\05\00\00\00\06\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04@\04\91\015\00\00\04\0A\08\00\02\FC\00\91\10\00\03\19\10\00\04\17\0C$\00u\01\00\08\00\00\F0!\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00p\00\00\00\C0\00\01\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00 \01/\05\00\01\00\FFh@$v\01\FF\C7\03\A3\FF\00\8E\07\00\C4\0F\00\C3y\A4\03\A2%\00\00\00\22\0E\00\19y\00\01\00\F3\0E!\00\00\00b\0E\00\90r\05?\04\00\00\80?\E0\FF\0F\00\C6\1F\00\B9z\04\00\00X\96\02p\E4\0F\00\A4r\04\05B\00\B1?\02\8E\0F\00\CC\0F\00\0C|\00\F6\03\95pR\F0\0B\00\DA/\00M\1B\04\F3\0D\80\03\00\EA\0F\00\02z\02\00\00Z\00\00\00\0F\00\00\00\E2\0F\00$v\03\FF\00[\90\00\22\E2\0F`\001F\00\00k\04\80\C8\0F\00\86y\00\02\FFh\030\19\10\0C \00*MyP\00PGy\00\00\F0\F1\03\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\80\0F\01\00-\00\BF\04.\03\00\01\00\22@\00\01\00=S\01\000\00\08\01\00\1F\0B@\00\04\13\93)\00\1F\8D@\00\0C\13\13\C4\03\0C\01\00\13 U\00*\90\00\E8\03\04\05\04\22\18\00\01\00.&\01T\00\00\01\00\13\B0@\00/p\00\80\00\0B\1F)'\00\03A\00 \04\00\01\00\04\F8\05\04\E4\00*\04\00\01\00\1Fk@\00\04\13P)\00&L\00@\00\1F\0A@\00\00!D\01D\01\0D@\00\13\A0)\00*\D8\00\01\00\1B\08\08\00?3\01\00.\07\003\00\00x\15\05\17\10\80\00\17\048\00\04\18\00\13\E5\14\01\0C\84\01*\88\05\E0\06\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07x\07\12\03\C0\01:\06\80\00\01\00\13\06\D8\01\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\13\018\00\04\A8\00\0C\01\00*\F8\02\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__6_1_0___thread_tile_h32(%arg0: i32, %arg1: !llvm.ptr<1>) attributes {disc.elimargs = [1 : index, 3 : index, 4 : index, 5 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(1 : index) : i32
      %1 = llvm.mlir.constant(0.000000e+00 : f32) : f32
      %2 = nvvm.read.ptx.sreg.ctaid.x : i32
      %3 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %4 = llvm.mul %2, %arg0  : i32
      %5 = llvm.add %3, %4  : i32
      %6 = llvm.icmp "ult" %5, %0 : i32
      llvm.cond_br %6, ^bb2, ^bb3
    ^bb2:  // pred: ^bb1
      %7 = llvm.getelementptr %arg1[%5] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      llvm.store %1, %7 : f32, !llvm.ptr<1>
      llvm.br ^bb3
    ^bb3:  // 2 preds: ^bb1, ^bb2
      llvm.return
    }
  }
  gpu.module @main_kernel_4 attributes {gpu.binary = "P\EDU\BA\01\00\10\00\18\0D\00\00\00\00\00\00\02\00\01\01@\00\00\00\D8\0C\00\00\00\00\00\00\D8\0C\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8!\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\00!@!\07\001\00\80\1E\07\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\0De__6_1_0___thread_tile_h32_1D\00\0F>\00'oshared@\00'\9Fconstant0C\00$\FA\01debug_frame\00.rel\11\00!nv\14\00\11aO\00\0F[\01 \0F\94\00!\0F\90\01\EAo_param\97\01\1C\0F\01\00\0A\8Cg\00\00\00\03\00\0A\00\01\00  \01\18\00,\09\00\01\00\11j\18\00,\04\00\01\00\11\88\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\16\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\18\00\00\00E\002\04H\05\18\000/\08\00\0A\00\10\22\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04X\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\010\00\03\190\00\04\17\0C$\00u\07\00(\00\00\F0!\10\009\06\00 \10\009\05\00\18\10\00u\04\00\10\00\00\F0\11\10\009\03\00\0C\10\009\02\00\08\10\009\01\00\04\10\00\01\01\00\F1\02\F0\11\00\03\1B\FF\00\04\1C\08\00`\00\00\00\90\15\16\003\00K\00\01\00`\02\02\08\10\0A/\E7\00\11\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00\80\01/\05\00\01\00\FF\F0@$v\01\FF\AF\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\CB\02Q\0E\00\19y\03\0F\00\10\00\08\08\F0\15$\0E\00$z\00\00\00Z\00\00\03\02\8E\07\00\CA\1F\00\12x\FF\00\FF\01\00\00\FF\C0\80\07\00\C8\0F\00\0C \00\C5[\00\00pdp\00\00\DA\0F\00M\F3\04\B0\80\03\00\EA\0F\00\19x\03\FF\1F\0F\00\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\003\05a\C6\0F\00\11r\03q\00@\FFH\8F\07 \00c$v\00\FF\00X\A0\00\12\C6@\00\00S\00\10\03@\00P\E4\0F\00\0Cx$\03`\00\00pR\F0\03 \00c$x\03\03 \000\00@\E2\0F\00\07\90\00\22Y\00\01\00p\C8\0F\00\10x\0A\030\00\F0\05\FF\E0\FF\07\00\E4\0F\04\0Cz\00\03\00\\\00\00pb\F4\03\10\00Z\10x\0C\03\02 \00\12\0A \00\10\FC \00f\00\10x\12\03\03 \00\00\F0\00\12\0C \00C\FA\03\00\C4\00\01\13X\90\00\02@\00\01\80\00ApR\F2\03\D0\00G\A4\09\FF\04\A0\00B\0Cz\00\12@\00\13\F6`\004\02\03\04`\00i\E2\0F\04$\E4\0B0\00c\07\A2\04\03\FF\00\FF\04c\E4\0F\04\07\A2\08\10\00 \80\04 \00<$\D4\0D`\00\12\02`\00\11\F8\80\00@%\A6\04\04\E9\04#\09\02P\00D\E8\06\0A\01@\00\10\C4\10\004\0A\0A\01`\00\010\00G\08\08\00^0\00F\D8\0E\0C\02p\00A\04\81\A9\05#\06\D8\00\19\1E\0C\00\A2\00\00\07\D8\0C\0C\02@\000\E6\06\06@\00\12\0B@\00f\08\07\B8\10\12\03@\00U\00\81\A9\00\08@\00\95\A4\02\00%\E6\0A\0A\00`0\00g\00\07\B8\12\12\03P\00E\81\E9\07\060\00p\E4\0E\00%\D6\0E\0E`\00\14\0D0\008\C2\13\02\10\01F\81\E9\14\0A0\00V\08\00$\B4\15 \01\96\C4\0F\00%\D6\0C\0C\00`@\00E\81\D9\0F\0E0\00pf\0F\00%\B6\10\10`\00\14\15\C0\00F\C2\0A\02\FF\90\00U\01\81\D9\0C\0C0\00\10d0\00C\08\12\00`0\00u\E4/\00\81\B9\11\10 \00i$\03\00$\C4\0B\80\006\81\B9\08\00\01u$\0F\00%\C6\12\130\01\10\C8\10\00\07\10\01u\E4\0F\00\81\C9\13\120\00\10(\10\00\16\0A\E0\00\84\22\0F\00$r\02\FF\FF`\00\10\E2P\025\04\03\05\B0\02u\1F\04\10x\10\03\07`\02Q/\04#\A2\02\08\06\01\D4\00\84\E2O\00\10x\05\03\06 \00\02\D0\02\15\04 \03\83\E2\0F\00#\E2\02\07\14@\060\00\E2\8F \00\15\05 \03\84\C6\0F\00#\D2\02\0F\0C \00\85\C8\0F\02#\B2\02\11\080\00j\0F\01\0Cz\00\10\F0\028\11\03\08\F0\02\0B \03W\07\A8\08\04\01\F0\02V\10x\00\03\090\00f\00#\C2\02\13\0A`\00\00\90\00\15\11\F0\02\10\E4@\007\0A\04\01\10\02*$\E4 \03Y\07\E8\06\05\02\00\039\0C\05\02\00\03'\0A\0A\00\03\11\08\D0\03\04\F0\03\00`\008\B8\0E\10\D0\02\010\03\08`\03G\B8\10\10\03P\006\81\A9\05\B0\01%\A4\00 \03\07\C0\02)\11\FF \03\19\04 \039$\B4\17@\02*%\E6\C0\02_\07\C2\12\11\FF0\03\09\12\B60\03\12\17P\02Y\08\81\E9\14\0C \03&\C4\16`\00\10\E4\D0\02C\0A\10\00`0\00W\E2\1F\00\81\B9 \03\10b\10\04\11\15\80\05\04@\011%\C6\10\C0\02\14\16`\009\B9\0A\0A \03\12\C6 \03\01 \00 \E2/0\04(\00\01P\03\18\C90\03\1A\01\D0\04\01\00\03\0B0\035\D6\12\15@\01\1B\C8\D0\03\000\00\1B\D90\03\08\B0\03\10\22\80\029\10\03\0C \034\00\03\0E\10\00\00`\02E\A2\02\05\04`\02\11OP\03\17\0A \03\000\03\14\0B\10\00\1F\E40\03\0Cw\C8\8F\00#\B2\02\0F\B0\02\16\02@\03\12\F6@\03\1F\C20\03\05\02\D0\02\000\03\17\0D\A0\00\0CP\06\00\F0\02\14\02\90\01\02\80\06\18\11\C0\06\00P\03\17\02 \02Z#\D2\02\13\0C@\03\06\E0\02\00`\00\1B\B40\037\B8\06\05\F0\02\1B\04 \03d\00\07\B8\0C\05\03`\00\00P\03O\C2\0E\10\FF \03\09O\C2\10\10\FF \03\09\19\B6 \03i\08\07\E8\13\11\01\80\06\0B \03\17\C4 \03\01\C0\02\0B \03H\E8\12\11\01p\00\1B\B9 \03\17\C6 \03\12\E2\E0\02\17\02\C0\00+\81\B90\03*\E4\16\90\03\1B\C60\03\1B\C9P\06\17\E6 \03\00\D0\00I\D8\0C\00\02\00\03\0A0\03\09\D0\07\00\D0\00\1A\E6@\03'\81\E90\03*&\01\10\03\00`\06\18\E90\03\1F\03 \03\22\1B\11 \03\14\13\10\00\0F \03\04\14\0F \00\03 \03\14\10\10\00\0F \03\01\1F\B2P\06\05\05\10\03\0C0\03\06\10\03\1C\CE\00\03\11/\00\03\08\A0\02*#\E2P\03\00@\03\17\12\90\00\00\10\03\16\03\80\01\0F\10\03\01\1F\11\10\03\0A:\B2\06\05\E0\02\090\06\07P\03\11\FC \0A3\07\B2\0C0\00\1F\00\10\03\03:\C8\0E\10\E0\02)\05\0A`\09H\C8\10\10\01@\00\08 \03\00p\05\0F\10\03\1DH\D8\13\11\02p\00\08\10\03\88\E2\0E\00\07\D8\12\11\02p\00\08 \03\1F\C4\10\03\00\1B\D4@\06\0B\10\03H\07\E8\15\00\A0\01\09 \03\10d\C0\05\0B \03O\E8\0C\00\03 \03\08'%\D6\10\03[\C4/\00\81\D9@\06)\E4\0D\90\00+\81\D9@\06\1B\E6@\06\08P\07\000\00\1B\E9 \03\1E\E9 \03\1B\16 \03\14\18\10\00\0F \03\04\1B\14 \03\14\15\10\00\0F \03=\11\C6\D0\02\0D\00\03\14\17p\00\1F\C6@\06\00O\A2\0A\04\FF@\06\02\03\F0\08G\A2\08\04\FF\10\02/#\E2@\06\05\1F\FC@\06\06\1F\01@\06\0C\18\01@\06F\C8\0E\10\020\00\0F0\03\00?\10\10\02@\06\19O\D8\13\11\03@\06)O\D8\12\11\03@\06\05.$\07@\06G\E2\15\00\FF\C0\00\090\03*$\0B0\03\0F@\06\00H\07\E2\00\00p\01\090\03*$\010\03\00\00\04\08 \03\10$\F0\0A\17\06\D0\00\1A\8F0\03\1B\E20\03%\E6\0E\10\03\22\06\02\90\00\090\03\00 \07S\E6\0C\00\00` \009\E4\0F\02 \03\1Bh \03\10b\E0\026\00\03\1A\E0\02d\04\10x\06\03\1B\10\00\01\00\034\0E\03\1C\10\00/\E2\1F0\03\007\05\03\19\B0\02\09\10\03;\C8\0F\01\00\03\00\80\02\15\06@\03\0D\00\03\17\8F\A0\02\12\FA \00\0A\C0\02\16\02`\03\04\F0\0F9\0A\03\1D\C0\009\08\03\1E\10\007\07\03\1F\A0\002\07\D8\16L\00\00\B0\01\02\D0\03\15\0E\B0\03\02\10\00\15\0A\A0\03\00P\03G\E8\10\05\010\00W\07\A8\0D\06\03\10\001\03x\0C\C0\01\01\01\00\02\90\0D\18\08\90\03H\0Cz\00\07\B0\00\06\C0\00\12\F4`\00H\B2\0B\0E\FF`\00&\C8\09\E0\0F\10\CA\B0\00(\04\08\B0\00H\07\E8\03\07\80\00\060\01\01\D0\10d\00\07\A8\14\05\01\D0\02\00 \000r\00\0C`\00$pRp\00%\0E\0E\F0\02\01\C0\03(\0A\0A0\00T\07\D8\08\08\02\10\00\000\01H\88\12\00\02 \00H\E8\00\07\03\10\008\A8\18\06\10\00\06\E0\00\03P\11\1A\84\B0\039$\A4\1A\10\00E%\86\12\12\90\03f\D0\0F\00$\94\11 \00\00P\03&\89\07\C0\02p\A6\00\00%\96\14\140\00\14\11\10\06!\96\10\90\03#\11\020\005\99\05\140\00v\E6\02\00%\86\16\16\B0\03\00 \00\17\06P\03`\08\00%\A6\18\18P\00\14\1A \005\89\0C\16 \00f\A6\0A\00$\B4\1C\90\00\00\00\04S\A6\1A\0D\00`0\00\00\80\06:\A9\0D\18\D0\04\02`\04\15\1C\F0\07&\1A\1A \00V\0E\00$\C4\1DP\00\01\C0\04 \14\0BP\00\14\1C\F0\03\08\C0\0Dl\A6\0E\00$\D4\1F\80\04\01\90\05\13\1D0\05H\81\B9\14\140\00c%\C6\10\09\00` \00\10\E4\90\07\19\0Bp\041%\D6\120\12\11\1F \00\00\00\0B*\10\10\90\04\090\0BA\02%\D6\08\90\12\14\1F \01\08\90\0A\010\0B)\18\00\E0\04\08\90\04\00\00\08U\E6\16\03\00` \00F\00\81\E9\190\01\01\90\04(\16\16\90\04c$v\04\FF\00b\80\00\00\C0\06H\92\02\05\06\A0\0D5\82\02\07\10\04 \C8O\90\04%\0D\1A\10\00\01\80\04\16\0F\90\07\00P\00C\05\FF\00cP\00\02\90\07(\0B\10\A0\04H\D2\02\13\08\10\11D\E2\02\19\16\10\00a\CA\0F\00\8Ey\00\90\0D@\84\E7\10\0CP\00\14M\80\15\030\15PGy\00\00\F0\A9\19\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\B0\0F\01\00-\00\\\04.\03\00\01\00\22@\00\01\00=[\01\000\00\08\01\00\1F\0B@\00\04\13\9B)\00\1F\97@\00\0C\14\13\CC\01\0B\01\00\138U\00\11\90\06\00\06p\19\22\08\00\01\00\22\18\00\01\00..\01T\00\00\01\00\13\C8@\00/p\00\80\00\0B\1F)'\00\03!\008\F5\02$\00\00\E0\1B\04\E4\00*\04\00\01\00\1Fm@\00\04\13h)\00&\AC\00@\00\1F\0A@\00\00!L\01D\01\0D@\001\18\05\00\01\00*\D8\00\01\00\1B\08\08\00?;\01\00\16\1D\003\00\00\F0@\00&\10\00\80\00\17\048\00\04\18\00\13\EB\14\01\0D\84\01!\06\00\01\00\17\901\01\0F\C0\00\01\132@\00+\06\00\01\00\1A\08`\1D\12\03\C0\01>\22\80\00g\00\17\05(!\0C\01\00*\A8\00\08\00\04\C0\00\13\018\00\0Ey\00\03x\00\1A\18\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__6_1_0___thread_tile_h32_1(%arg0: i32, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: !llvm.ptr<1>, %arg6: !llvm.ptr<1>, %arg7: !llvm.ptr<1>) attributes {disc.elimargs = [5 : index, 7 : index, 8 : index, 9 : index, 10 : index, 11 : index, 12 : index, 14 : index, 15 : index, 16 : index, 17 : index, 18 : index, 19 : index, 21 : index, 22 : index, 23 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(0 : index) : i32
      %1 = llvm.mlir.constant(1 : index) : i32
      %2 = llvm.mlir.constant(512 : index) : i32
      %3 = llvm.mlir.constant(32 : index) : i32
      %4 = llvm.mlir.constant(4 : index) : i32
      %5 = llvm.mlir.constant(0.000000e+00 : f32) : f32
      %6 = nvvm.read.ptx.sreg.ctaid.x : i32
      %7 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %8 = llvm.icmp "eq" %arg0, %1 : i32
      %9 = llvm.select %8, %arg1, %arg0 : i1, i32
      %10 = llvm.icmp "eq" %arg1, %9 : i32
      %11 = llvm.icmp "eq" %arg0, %9 : i32
      %12 = llvm.mul %6, %arg2  : i32
      %13 = llvm.add %7, %12  : i32
      %14 = llvm.icmp "ult" %13, %arg3 : i32
      llvm.cond_br %14, ^bb2, ^bb12
    ^bb2:  // pred: ^bb1
      %15 = llvm.srem %13, %2  : i32
      %16 = llvm.sdiv %13, %2  : i32
      %17 = llvm.icmp "ult" %15, %1 : i32
      llvm.cond_br %17, ^bb3, ^bb11
    ^bb3:  // pred: ^bb2
      %18 = llvm.mul %16, %3  : i32
      llvm.br ^bb4(%0, %5 : i32, f32)
    ^bb4(%19: i32, %20: f32):  // 2 preds: ^bb3, ^bb9
      %21 = llvm.icmp "slt" %19, %3 : i32
      llvm.cond_br %21, ^bb5, ^bb10
    ^bb5:  // pred: ^bb4
      %22 = llvm.add %18, %19  : i32
      %23 = llvm.icmp "slt" %22, %arg4 : i32
      llvm.cond_br %23, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %24 = llvm.urem %22, %4  : i32
      %25 = llvm.udiv %22, %4  : i32
      %26 = llvm.select %10, %25, %0 : i1, i32
      %27 = llvm.mul %26, %4  : i32
      %28 = llvm.add %27, %24  : i32
      %29 = llvm.getelementptr %arg5[%28] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %30 = llvm.load %29 : !llvm.ptr<1> -> f32
      %31 = llvm.select %11, %25, %0 : i1, i32
      %32 = llvm.mul %31, %4  : i32
      %33 = llvm.add %32, %24  : i32
      %34 = llvm.getelementptr %arg6[%33] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %35 = llvm.load %34 : !llvm.ptr<1> -> f32
      %36 = llvm.fmul %30, %35  : f32
      %37 = llvm.fadd %20, %36  : f32
      llvm.br ^bb8(%37 : f32)
    ^bb7:  // pred: ^bb5
      llvm.br ^bb8(%20 : f32)
    ^bb8(%38: f32):  // 2 preds: ^bb6, ^bb7
      llvm.br ^bb9
    ^bb9:  // pred: ^bb8
      %39 = llvm.add %19, %1  : i32
      llvm.br ^bb4(%39, %38 : i32, f32)
    ^bb10:  // pred: ^bb4
      %40 = llvm.getelementptr %arg7[%15] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %41 = llvm.atomicrmw fadd %40, %20 acq_rel : !llvm.ptr<1>, f32
      llvm.br ^bb11
    ^bb11:  // 2 preds: ^bb2, ^bb10
      llvm.br ^bb12
    ^bb12:  // 2 preds: ^bb1, ^bb11
      llvm.return
    }
  }
  gpu.module @main_kernel_5 attributes {gpu.binary = "P\EDU\BA\01\00\10\00H\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\08\04\00\00\00\00\00\00\01\04\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\0Ae__6_1_0___block_tile_h64A\00\0F;\00$oshared=\00$\9Fconstant0@\00!\FA\01debug_frame\00.rel\11\00!nv\14\00\11aL\00\0FO\01 \0F\91\00\1E\0F\81\01\DEo_param\88\01\1C\0F\01\00\05\8Cd\00\00\00\03\00\0A\00\01\00 \14\01\18\00,\09\00\01\00\11[\18\00,\04\00\01\00\11y\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\1C\00\00\00E\00#\04\10\18\00\80/\08\00\05\00\00\00\06\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\048\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C$\00u\01\00\08\00\00\F0!\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00p\00\00\00\C0\00\01\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\00#\02b,\00\00\00H\00\01\00\00 \01/\05\00\01\00\FFp@$v\01\FF\CF\03\A3\FF\00\8E\07\00\C4\0F\00\C3y\AC\03\A2%\00\00\00\22\0E\00\19y\00\01\00\F3\0E!\00\00\00b\0E\00\90r\05?\04\00\00\80?\E0\FF\0F\00\C6\1F\00\B9z\04\00\00X\9E\02p\E4\0F\00\A4r\04\05B\00\B1?\02\8E\0F\00\CC\0F\00\0C|\00\FE\03\95pR\F0\0B\00\DA/\00M#\04\F3\0D\80\03\00\EA\0F\00\02z\02\00\00Z\00\00\00\0F\00\00\00\E2\0F\00$v\03\FF\00[\90\00\22\E2\0F`\001F\00\00s\04\80\C8\0F\00\86y\00\02\FFp\030\19\10\0C \00*MyP\00PGy\00\00\F0\F9\03\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\80\0F\01\00-\00\C7\04.\03\00\01\00\22@\00\01\00=O\01\000\00\08\01\00\1F\0B@\00\04\13\8F)\00\1F\88@\00\0C\13\13\CC\03\0C\01\00\13\18U\00*\90\00\F0\03\04\0D\04\22\18\00\01\00.\22\01T\00\00\01\00\13\A8@\00/p\00\80\00\0B\1F)'\00\03A\00\18\04\00\01\00\04\00\06\04\E4\00*\04\00\01\00\1Fj@\00\04\13H)\00&L\00@\00\1F\0A@\00\00!@\01D\01\0D@\00\13\98)\00*\D8\00\01\00\1B\08\08\00?/\01\006\07\003\00\00p\1D\05\17\10\80\00\17\048\00\04\18\00\13\E2\14\01\0C\84\01*\80\05\E8\06\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\80\07\12\03\C0\01:\06\80\00\01\00\13\06\D8\01\04(\0B\0D\C8\01\1A\00\08\00\04\F8\00\13\018\00\04\A8\00\0D\01\00)\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__6_1_0___block_tile_h64(%arg0: i32, %arg1: !llvm.ptr<1>) attributes {disc.elimargs = [1 : index, 3 : index, 4 : index, 5 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(1 : index) : i32
      %1 = llvm.mlir.constant(0.000000e+00 : f32) : f32
      %2 = nvvm.read.ptx.sreg.ctaid.x : i32
      %3 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %4 = llvm.mul %2, %arg0  : i32
      %5 = llvm.add %3, %4  : i32
      %6 = llvm.icmp "ult" %5, %0 : i32
      llvm.cond_br %6, ^bb2, ^bb3
    ^bb2:  // pred: ^bb1
      %7 = llvm.getelementptr %arg1[%5] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      llvm.store %1, %7 : f32, !llvm.ptr<1>
      llvm.br ^bb3
    ^bb3:  // 2 preds: ^bb1, ^bb2
      llvm.return
    }
  }
  gpu.module @main_kernel_6 attributes {gpu.binary = "P\EDU\BA\01\00\10\00\D8\07\00\00\00\00\00\00\02\00\01\01@\00\00\00\98\07\00\00\00\00\00\00\93\07\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\A8\12\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00g\02\00\BE\00r\00\01\00\22\12\00\01\00\11\0F\06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\0Ce__6_1_0___block_tile_h64_1C\00\0F=\00&oshared?\00&\9Fconstant0B\00#\FA\01debug_frame\00.rel\11\00!nv\14\00\11aN\00\0FW\01 \0F\93\00 \0F\8B\01\A4\8F$____wg_<\00 \00\15\00/28\CD\010o_param\D4\01\1C\0F\01\00\09\8Cf\00\00\00\03\00\0A\00\01\00\11\DD\18\00,\0B\00\01\00 ^\01\18\00,\09\00\01\00\11\A7\18\00,\04\00\01\00\11\C5\18\00,\07\00\01\00\102\E3\03\17\10x\00\03#\00f\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0\06\07\00 \00\04\9B\00R\04\14\00\00\00E\002\04p\01\18\000/\08\00#\00\10\19\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\A8\04\F3\08\015\00\00\04\0A\08\00\03\00\00\00`\010\00\03\190\00\04\17\0C\CB\00U(\00\00\F0!\10\009\06\00 \10\009\05\00\18\10\00u\04\00\10\00\00\F0\11\10\009\03\00\0C\10\009\02\00\08\10\009\01\00\04\10\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\00\D0\05\00\00 \06\00\00\04\1E\94\01#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\11\02h\01\0F\01\00\FF\98@$v\01\FFw\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%s\02a\0E\00\19y\03\00\01\00\10!-\00\F0\04\0E\00$z\00\00\00Z\00\00\03\02\8E\07\00\CA\1F\00\0C\10\00\C5[\00\00p`\F0\03\00\DA\0F\00M\93\04\B0\80\03\00\EA\0F\00\19x\03\FF\1F\0F\00\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\00\EB\04Q\E2\0F\00Eyy\03\020\00a\E4\0F\00\11r\04q\00\C1\FF@\8F\07\00\C8\0F\00\12x\03\04Y\04\10\C0p\00@\0F\00$x\BC\02`\00\00\03\0A\8E\070\00c$r\03\FF\FF\00\C0\00\10\C60\00 \02\00p\00\22\FF\C0 \00P\19x\05\FF\05Q\000\16\01\00\10\00@\0Cr\00\020\001pR\F2p\00P\0Cx\00\00\7F\10\000@\F0\03\90\00@$x\02\02\D9\02\10\05\E0\00\12\C8\10\00\14\04`\00\96\CC\0F\00G\19\00\00@\03\E0\00c$v\03\FF\00X \00\10\E2p\001\04\FF\08\E3\04A\01\00\00\C8`\00$\03\01p\00u\E2\0F\04$x\04\04`\00\01\C0\00\16\05\C0\00\91\E2\0F\00\07z\03\03\00Y`\00\02@\00Uz\00\03\00X\B0\00\11\04\10\00\10Y\10\00\12\F4\B0\00\08\00\01\10\C4\B0\004\08\04@`\00\11\CA@\00\81\08\00\\\00\00pb\FA@\00@\10x\0A\08\90\00B\FF\E0\FF\07\10\006\10\08\02\10\00\000\00\12\0A0\00#\FC\03\10\00\12\10\10\00\11\F8\10\00T\10x\13\08\030\00\91\C6\0F\00\07\D2\06\08\FF\00\B0\00\11\04\E0\00H\D4\09\FF\04\D0\00#\D2\08 \00#\00\05P\00\12\13P\00\11\F6\C0\00\A3%\D6\06\06\00`\00\00\09\020\00S\E8\0C\0A\01\000\00\10\C6 \00H\08\08\00^ \006\0A\0A\01p\00P\00\81\D9\06\06p\00\BA\00\19\1E\0C\00\A4\00\00$\E4\0B\80\00S\C8\0E\10\02\00P\00u\E2\0F\04\81\D9\12\080\00\87\A2\02\00\07\C8\10\10\02P\00@%\E6\0C\0Cp\00\14\0Bp\001\B8\11\13\EF\01\03\90\00:$\C4\15`\00E\B8\07\13\03@\00\86\1F\00%\E6\0A\0A\00`@\00E\81\E9\14\0Cp\00p\E6\00\00%\C6\0E\0E`\001\15\02\8Ep\01E\81\E9\0A\0A \00\87\E4\0E\00$\B4\16\FF\04\D0\01c%\C6\08\10\00`0\00u\E2/\00\81\C9\0E\0E0\00p&\0F\00%\B6\10\11P\00\12\16P\00F\08\81\C9\08\E0\00\10$ \00D\0C\07\00` \00e\1F\00\81\B9\10\10 \00fh\0F\00\81\B9\0C\A0\00\10b\E0\014\05\05\04\E0\01\81\E2\0F\00#\D2\03\12\06\0C\07P\00\00\00\C6O\D0\02\10\05`\021pR\FA\C0\01T#\E2\03\14\0A \00\85\C8\8F\00#\C2\03\0E\08\10\00v\0F\01#\B2\03\10\0C\10\00p\02GY\00\00P\FD\D1\03\11\83@\03\14Ap\04\03P\03A\88s\00\02,\00\00\96\06f\E8\0F\00\1D{\00\01\00\84\EC\0F\00\84\89\04\02\00 \00\12\E2\C0\03\10?\90\000@\F2\03\A0\01c\84\89\05\02\00\10 \00\93$\0E\00!\82\05\04\05\00\01\00\8F\CA\1F\00\88\83\00\02\05`\00\09\1E\99`\00\14\1F \04\00`\00!\99\07\07\08\05`\00H\92\07\04\07`\00O\93\00\02\07\C0\00\0A\1A\03`\00\10r\EC\01\03\E0\03\13\C6\E0\00\18\04\C0\00J\03\03\04\00\C0\00\0F \01\099M\19\00P\01'\84yp\00\96\22\0E\00$v\04\FF\00b`\02c$v\05\FF\00c\10\00a\CA\0F\00\8Ey\00\81\05@\84\E7\10\0C\D0\02\1BM\A0\01cGy\00\00\F0\FF\C0\01f\C0\0F\00\18y\00\01\00\0F\10\00\A0\0F\01\00-\14\01\DC\02\0B\01\00\22@\00\01\00=W\01\000\00\08\01\00\1F\0B@\00\04\13\97)\00\1F\D4@\00\0C\13\13t\09\0C\01\00\13pU\00*\A8\00\98\09\22\08\00\01\00\22\18\00\01\00.*\01T\00\00\01\00\13\18u\02/p\00\80\00\0B\1F)'\00\03#\00\88@\00\04\10\0C\04\E4\00*\04\00\01\00\1Fl@\00\04\13\B8)\00&\B8\00@\00\1F\0A@\00\00!H\01D\01\0D@\00\13p\F5\03*\D8\00\01\00\1B\08\08\00?7\01\00F\0D\00Q\00\00H\06\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\E8\14\01\0C\84\01\13X@\00\17\901\01\0F\C0\00\01\132@\00\15\06R\00\0B\A1\00\16\07\80\00j\06\00\00\19\80\00\01\00\13\A9+\00+\03\00\01\00\04\DB\02\1F\04\80\00\0B\11\06\D9\06\07\E8\11\0B\01\00\1B\A8\08\00\04\97\00\13\018\00\04\E8\00\0D8\00\1A\08\08\00\088\00\18\06\A0\00\0F\01\00\05\03\A9\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00"} {
    llvm.mlir.global internal @__wg_main_kColReduction_reduce__6_1_0___block_tile_h64_1_0() {addr_space = 3 : i32} : !llvm.array<256 x f32>
    llvm.func @main_kColReduction_reduce__6_1_0___block_tile_h64_1(%arg0: i32, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: !llvm.ptr<1>, %arg6: !llvm.ptr<1>, %arg7: !llvm.ptr<1>) attributes {disc.elimargs = [5 : index, 7 : index, 8 : index, 9 : index, 10 : index, 11 : index, 12 : index, 14 : index, 15 : index, 16 : index, 17 : index, 18 : index, 19 : index, 21 : index, 22 : index, 23 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(32 : index) : i32
      %1 = llvm.mlir.constant(8 : index) : i32
      %2 = llvm.mlir.constant(64 : index) : i32
      %3 = llvm.mlir.constant(4 : index) : i32
      %4 = llvm.mlir.constant(0.000000e+00 : f32) : f32
      %5 = llvm.mlir.constant(2 : index) : i32
      %6 = llvm.mlir.constant(1 : index) : i32
      %7 = llvm.mlir.constant(256 : index) : i32
      %8 = llvm.mlir.constant(0 : index) : i32
      %9 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__6_1_0___block_tile_h64_1_0 : !llvm.ptr<3>
      %10 = llvm.getelementptr %9[0, 0] : (!llvm.ptr<3>) -> !llvm.ptr<3>, !llvm.array<256 x f32>
      %11 = nvvm.read.ptx.sreg.ctaid.x : i32
      %12 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %13 = llvm.icmp "eq" %arg0, %6 : i32
      %14 = llvm.select %13, %arg1, %arg0 : i1, i32
      %15 = llvm.icmp "eq" %arg1, %14 : i32
      %16 = llvm.icmp "eq" %arg0, %14 : i32
      %17 = llvm.mul %11, %arg2  : i32
      %18 = llvm.add %12, %17  : i32
      %19 = llvm.icmp "ult" %18, %arg3 : i32
      llvm.cond_br %19, ^bb2, ^bb21
    ^bb2:  // pred: ^bb1
      %20 = llvm.srem %18, %7  : i32
      %21 = llvm.sdiv %18, %7  : i32
      %22 = llvm.udiv %20, %0  : i32
      %23 = llvm.urem %20, %0  : i32
      %24 = llvm.mul %23, %1  : i32
      %25 = llvm.add %22, %24  : i32
      %26 = llvm.icmp "ult" %23, %6 : i32
      llvm.cond_br %26, ^bb3, ^bb10(%4 : f32)
    ^bb3:  // pred: ^bb2
      %27 = llvm.mul %21, %1  : i32
      %28 = llvm.add %22, %27  : i32
      %29 = llvm.mul %28, %2  : i32
      llvm.br ^bb4(%8, %4 : i32, f32)
    ^bb4(%30: i32, %31: f32):  // 2 preds: ^bb3, ^bb9
      %32 = llvm.icmp "slt" %30, %2 : i32
      llvm.cond_br %32, ^bb5, ^bb10(%31 : f32)
    ^bb5:  // pred: ^bb4
      %33 = llvm.add %30, %29  : i32
      %34 = llvm.icmp "slt" %33, %arg4 : i32
      llvm.cond_br %34, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %35 = llvm.urem %33, %3  : i32
      %36 = llvm.udiv %33, %3  : i32
      %37 = llvm.select %15, %36, %8 : i1, i32
      %38 = llvm.mul %37, %3  : i32
      %39 = llvm.add %38, %35  : i32
      %40 = llvm.getelementptr %arg5[%39] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %41 = llvm.load %40 : !llvm.ptr<1> -> f32
      %42 = llvm.select %16, %36, %8 : i1, i32
      %43 = llvm.mul %42, %3  : i32
      %44 = llvm.add %43, %35  : i32
      %45 = llvm.getelementptr %arg6[%44] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %46 = llvm.load %45 : !llvm.ptr<1> -> f32
      %47 = llvm.fmul %41, %46  : f32
      %48 = llvm.fadd %31, %47  : f32
      llvm.br ^bb8(%48 : f32)
    ^bb7:  // pred: ^bb5
      llvm.br ^bb8(%31 : f32)
    ^bb8(%49: f32):  // 2 preds: ^bb6, ^bb7
      llvm.br ^bb9
    ^bb9:  // pred: ^bb8
      %50 = llvm.add %30, %6  : i32
      llvm.br ^bb4(%50, %49 : i32, f32)
    ^bb10(%51: f32):  // 2 preds: ^bb2, ^bb4
      llvm.br ^bb11(%51 : f32)
    ^bb11(%52: f32):  // pred: ^bb10
      llvm.br ^bb12
    ^bb12:  // pred: ^bb11
      %53 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      llvm.store %52, %53 : f32, !llvm.ptr<3>
      nvvm.barrier0
      %54 = llvm.icmp "slt" %22, %3 : i32
      llvm.cond_br %54, ^bb13, ^bb14
    ^bb13:  // pred: ^bb12
      %55 = llvm.add %25, %3  : i32
      %56 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %57 = llvm.load %56 : !llvm.ptr<3> -> f32
      %58 = llvm.getelementptr %10[%55] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %59 = llvm.load %58 : !llvm.ptr<3> -> f32
      %60 = llvm.fadd %57, %59  : f32
      %61 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      llvm.store %60, %61 : f32, !llvm.ptr<3>
      llvm.br ^bb14
    ^bb14:  // 2 preds: ^bb12, ^bb13
      nvvm.barrier0
      %62 = llvm.icmp "slt" %22, %5 : i32
      llvm.cond_br %62, ^bb15, ^bb16
    ^bb15:  // pred: ^bb14
      %63 = llvm.add %25, %5  : i32
      %64 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %65 = llvm.load %64 : !llvm.ptr<3> -> f32
      %66 = llvm.getelementptr %10[%63] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %67 = llvm.load %66 : !llvm.ptr<3> -> f32
      %68 = llvm.fadd %65, %67  : f32
      %69 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      llvm.store %68, %69 : f32, !llvm.ptr<3>
      llvm.br ^bb16
    ^bb16:  // 2 preds: ^bb14, ^bb15
      nvvm.barrier0
      %70 = llvm.icmp "slt" %22, %6 : i32
      llvm.cond_br %70, ^bb17, ^bb18
    ^bb17:  // pred: ^bb16
      %71 = llvm.add %25, %6  : i32
      %72 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %73 = llvm.load %72 : !llvm.ptr<3> -> f32
      %74 = llvm.getelementptr %10[%71] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %75 = llvm.load %74 : !llvm.ptr<3> -> f32
      %76 = llvm.fadd %73, %75  : f32
      %77 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      llvm.store %76, %77 : f32, !llvm.ptr<3>
      llvm.br ^bb18
    ^bb18:  // 2 preds: ^bb16, ^bb17
      nvvm.barrier0
      %78 = llvm.icmp "eq" %22, %8 : i32
      %79 = llvm.and %78, %26  : i1
      llvm.cond_br %79, ^bb19, ^bb20
    ^bb19:  // pred: ^bb18
      %80 = llvm.getelementptr %10[%25] : (!llvm.ptr<3>, i32) -> !llvm.ptr<3>, f32
      %81 = llvm.load %80 : !llvm.ptr<3> -> f32
      %82 = llvm.getelementptr %arg7[%23] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32
      %83 = llvm.atomicrmw fadd %82, %81 acq_rel : !llvm.ptr<1>, f32
      llvm.br ^bb20
    ^bb20:  // 2 preds: ^bb18, ^bb19
      llvm.br ^bb21
    ^bb21:  // 2 preds: ^bb1, ^bb20
      llvm.return
    }
  }
}


// -----// IR Dump After DiscToLLVMPass (disc-to-llvm) //----- //
module attributes {gpu.container_module} {
  llvm.mlir.global internal constant @main_kernel_6_main_kColReduction_reduce__6_1_0___block_tile_h64_1_kernel_name("main_kColReduction_reduce__6_1_0___block_tile_h64_1\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @main_kernel_6_blob_gpu.binary("P\EDU\BA\01\00\10\00\D8\07\00\00\00\00\00\00\02\00\01\01@\00\00\00\98\07\00\00\00\00\00\00\93\07\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\A8\12\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00g\02\00\BE\00r\00\01\00\22\12\00\01\00\11\0F\06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\0Ce__6_1_0___block_tile_h64_1C\00\0F=\00&oshared?\00&\9Fconstant0B\00#\FA\01debug_frame\00.rel\11\00!nv\14\00\11aN\00\0FW\01 \0F\93\00 \0F\8B\01\A4\8F$____wg_<\00 \00\15\00/28\CD\010o_param\D4\01\1C\0F\01\00\09\8Cf\00\00\00\03\00\0A\00\01\00\11\DD\18\00,\0B\00\01\00 ^\01\18\00,\09\00\01\00\11\A7\18\00,\04\00\01\00\11\C5\18\00,\07\00\01\00\102\E3\03\17\10x\00\03#\00f\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0\06\07\00 \00\04\9B\00R\04\14\00\00\00E\002\04p\01\18\000/\08\00#\00\10\19\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\A8\04\F3\08\015\00\00\04\0A\08\00\03\00\00\00`\010\00\03\190\00\04\17\0C\CB\00U(\00\00\F0!\10\009\06\00 \10\009\05\00\18\10\00u\04\00\10\00\00\F0\11\10\009\03\00\0C\10\009\02\00\08\10\009\01\00\04\10\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\00\D0\05\00\00 \06\00\00\04\1E\94\01#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\11\02h\01\0F\01\00\FF\98@$v\01\FFw\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%s\02a\0E\00\19y\03\00\01\00\10!-\00\F0\04\0E\00$z\00\00\00Z\00\00\03\02\8E\07\00\CA\1F\00\0C\10\00\C5[\00\00p`\F0\03\00\DA\0F\00M\93\04\B0\80\03\00\EA\0F\00\19x\03\FF\1F\0F\00\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\00\EB\04Q\E2\0F\00Eyy\03\020\00a\E4\0F\00\11r\04q\00\C1\FF@\8F\07\00\C8\0F\00\12x\03\04Y\04\10\C0p\00@\0F\00$x\BC\02`\00\00\03\0A\8E\070\00c$r\03\FF\FF\00\C0\00\10\C60\00 \02\00p\00\22\FF\C0 \00P\19x\05\FF\05Q\000\16\01\00\10\00@\0Cr\00\020\001pR\F2p\00P\0Cx\00\00\7F\10\000@\F0\03\90\00@$x\02\02\D9\02\10\05\E0\00\12\C8\10\00\14\04`\00\96\CC\0F\00G\19\00\00@\03\E0\00c$v\03\FF\00X \00\10\E2p\001\04\FF\08\E3\04A\01\00\00\C8`\00$\03\01p\00u\E2\0F\04$x\04\04`\00\01\C0\00\16\05\C0\00\91\E2\0F\00\07z\03\03\00Y`\00\02@\00Uz\00\03\00X\B0\00\11\04\10\00\10Y\10\00\12\F4\B0\00\08\00\01\10\C4\B0\004\08\04@`\00\11\CA@\00\81\08\00\\\00\00pb\FA@\00@\10x\0A\08\90\00B\FF\E0\FF\07\10\006\10\08\02\10\00\000\00\12\0A0\00#\FC\03\10\00\12\10\10\00\11\F8\10\00T\10x\13\08\030\00\91\C6\0F\00\07\D2\06\08\FF\00\B0\00\11\04\E0\00H\D4\09\FF\04\D0\00#\D2\08 \00#\00\05P\00\12\13P\00\11\F6\C0\00\A3%\D6\06\06\00`\00\00\09\020\00S\E8\0C\0A\01\000\00\10\C6 \00H\08\08\00^ \006\0A\0A\01p\00P\00\81\D9\06\06p\00\BA\00\19\1E\0C\00\A4\00\00$\E4\0B\80\00S\C8\0E\10\02\00P\00u\E2\0F\04\81\D9\12\080\00\87\A2\02\00\07\C8\10\10\02P\00@%\E6\0C\0Cp\00\14\0Bp\001\B8\11\13\EF\01\03\90\00:$\C4\15`\00E\B8\07\13\03@\00\86\1F\00%\E6\0A\0A\00`@\00E\81\E9\14\0Cp\00p\E6\00\00%\C6\0E\0E`\001\15\02\8Ep\01E\81\E9\0A\0A \00\87\E4\0E\00$\B4\16\FF\04\D0\01c%\C6\08\10\00`0\00u\E2/\00\81\C9\0E\0E0\00p&\0F\00%\B6\10\11P\00\12\16P\00F\08\81\C9\08\E0\00\10$ \00D\0C\07\00` \00e\1F\00\81\B9\10\10 \00fh\0F\00\81\B9\0C\A0\00\10b\E0\014\05\05\04\E0\01\81\E2\0F\00#\D2\03\12\06\0C\07P\00\00\00\C6O\D0\02\10\05`\021pR\FA\C0\01T#\E2\03\14\0A \00\85\C8\8F\00#\C2\03\0E\08\10\00v\0F\01#\B2\03\10\0C\10\00p\02GY\00\00P\FD\D1\03\11\83@\03\14Ap\04\03P\03A\88s\00\02,\00\00\96\06f\E8\0F\00\1D{\00\01\00\84\EC\0F\00\84\89\04\02\00 \00\12\E2\C0\03\10?\90\000@\F2\03\A0\01c\84\89\05\02\00\10 \00\93$\0E\00!\82\05\04\05\00\01\00\8F\CA\1F\00\88\83\00\02\05`\00\09\1E\99`\00\14\1F \04\00`\00!\99\07\07\08\05`\00H\92\07\04\07`\00O\93\00\02\07\C0\00\0A\1A\03`\00\10r\EC\01\03\E0\03\13\C6\E0\00\18\04\C0\00J\03\03\04\00\C0\00\0F \01\099M\19\00P\01'\84yp\00\96\22\0E\00$v\04\FF\00b`\02c$v\05\FF\00c\10\00a\CA\0F\00\8Ey\00\81\05@\84\E7\10\0C\D0\02\1BM\A0\01cGy\00\00\F0\FF\C0\01f\C0\0F\00\18y\00\01\00\0F\10\00\A0\0F\01\00-\14\01\DC\02\0B\01\00\22@\00\01\00=W\01\000\00\08\01\00\1F\0B@\00\04\13\97)\00\1F\D4@\00\0C\13\13t\09\0C\01\00\13pU\00*\A8\00\98\09\22\08\00\01\00\22\18\00\01\00.*\01T\00\00\01\00\13\18u\02/p\00\80\00\0B\1F)'\00\03#\00\88@\00\04\10\0C\04\E4\00*\04\00\01\00\1Fl@\00\04\13\B8)\00&\B8\00@\00\1F\0A@\00\00!H\01D\01\0D@\00\13p\F5\03*\D8\00\01\00\1B\08\08\00?7\01\00F\0D\00Q\00\00H\06\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\E8\14\01\0C\84\01\13X@\00\17\901\01\0F\C0\00\01\132@\00\15\06R\00\0B\A1\00\16\07\80\00j\06\00\00\19\80\00\01\00\13\A9+\00+\03\00\01\00\04\DB\02\1F\04\80\00\0B\11\06\D9\06\07\E8\11\0B\01\00\1B\A8\08\00\04\97\00\13\018\00\04\E8\00\0D8\00\1A\08\08\00\088\00\18\06\A0\00\0F\01\00\05\03\A9\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @main_kernel_5_main_kColReduction_reduce__6_1_0___block_tile_h64_kernel_name("main_kColReduction_reduce__6_1_0___block_tile_h64\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @main_kernel_5_blob_gpu.binary("P\EDU\BA\01\00\10\00H\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\08\04\00\00\00\00\00\00\01\04\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\0Ae__6_1_0___block_tile_h64A\00\0F;\00$oshared=\00$\9Fconstant0@\00!\FA\01debug_frame\00.rel\11\00!nv\14\00\11aL\00\0FO\01 \0F\91\00\1E\0F\81\01\DEo_param\88\01\1C\0F\01\00\05\8Cd\00\00\00\03\00\0A\00\01\00 \14\01\18\00,\09\00\01\00\11[\18\00,\04\00\01\00\11y\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\1C\00\00\00E\00#\04\10\18\00\80/\08\00\05\00\00\00\06\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\048\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C$\00u\01\00\08\00\00\F0!\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00p\00\00\00\C0\00\01\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\00#\02b,\00\00\00H\00\01\00\00 \01/\05\00\01\00\FFp@$v\01\FF\CF\03\A3\FF\00\8E\07\00\C4\0F\00\C3y\AC\03\A2%\00\00\00\22\0E\00\19y\00\01\00\F3\0E!\00\00\00b\0E\00\90r\05?\04\00\00\80?\E0\FF\0F\00\C6\1F\00\B9z\04\00\00X\9E\02p\E4\0F\00\A4r\04\05B\00\B1?\02\8E\0F\00\CC\0F\00\0C|\00\FE\03\95pR\F0\0B\00\DA/\00M#\04\F3\0D\80\03\00\EA\0F\00\02z\02\00\00Z\00\00\00\0F\00\00\00\E2\0F\00$v\03\FF\00[\90\00\22\E2\0F`\001F\00\00s\04\80\C8\0F\00\86y\00\02\FFp\030\19\10\0C \00*MyP\00PGy\00\00\F0\F9\03\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\80\0F\01\00-\00\C7\04.\03\00\01\00\22@\00\01\00=O\01\000\00\08\01\00\1F\0B@\00\04\13\8F)\00\1F\88@\00\0C\13\13\CC\03\0C\01\00\13\18U\00*\90\00\F0\03\04\0D\04\22\18\00\01\00.\22\01T\00\00\01\00\13\A8@\00/p\00\80\00\0B\1F)'\00\03A\00\18\04\00\01\00\04\00\06\04\E4\00*\04\00\01\00\1Fj@\00\04\13H)\00&L\00@\00\1F\0A@\00\00!@\01D\01\0D@\00\13\98)\00*\D8\00\01\00\1B\08\08\00?/\01\006\07\003\00\00p\1D\05\17\10\80\00\17\048\00\04\18\00\13\E2\14\01\0C\84\01*\80\05\E8\06\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\80\07\12\03\C0\01:\06\80\00\01\00\13\06\D8\01\04(\0B\0D\C8\01\1A\00\08\00\04\F8\00\13\018\00\04\A8\00\0D\01\00)\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00\00\00\00\00\00\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @main_kernel_4_main_kColReduction_reduce__6_1_0___thread_tile_h32_1_kernel_name("main_kColReduction_reduce__6_1_0___thread_tile_h32_1\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @main_kernel_4_blob_gpu.binary("P\EDU\BA\01\00\10\00\18\0D\00\00\00\00\00\00\02\00\01\01@\00\00\00\D8\0C\00\00\00\00\00\00\D8\0C\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8!\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\00!@!\07\001\00\80\1E\07\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\0De__6_1_0___thread_tile_h32_1D\00\0F>\00'oshared@\00'\9Fconstant0C\00$\FA\01debug_frame\00.rel\11\00!nv\14\00\11aO\00\0F[\01 \0F\94\00!\0F\90\01\EAo_param\97\01\1C\0F\01\00\0A\8Cg\00\00\00\03\00\0A\00\01\00  \01\18\00,\09\00\01\00\11j\18\00,\04\00\01\00\11\88\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\16\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\18\00\00\00E\002\04H\05\18\000/\08\00\0A\00\10\22\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04X\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\010\00\03\190\00\04\17\0C$\00u\07\00(\00\00\F0!\10\009\06\00 \10\009\05\00\18\10\00u\04\00\10\00\00\F0\11\10\009\03\00\0C\10\009\02\00\08\10\009\01\00\04\10\00\01\01\00\F1\02\F0\11\00\03\1B\FF\00\04\1C\08\00`\00\00\00\90\15\16\003\00K\00\01\00`\02\02\08\10\0A/\E7\00\11\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00\80\01/\05\00\01\00\FF\F0@$v\01\FF\AF\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\CB\02Q\0E\00\19y\03\0F\00\10\00\08\08\F0\15$\0E\00$z\00\00\00Z\00\00\03\02\8E\07\00\CA\1F\00\12x\FF\00\FF\01\00\00\FF\C0\80\07\00\C8\0F\00\0C \00\C5[\00\00pdp\00\00\DA\0F\00M\F3\04\B0\80\03\00\EA\0F\00\19x\03\FF\1F\0F\00\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\003\05a\C6\0F\00\11r\03q\00@\FFH\8F\07 \00c$v\00\FF\00X\A0\00\12\C6@\00\00S\00\10\03@\00P\E4\0F\00\0Cx$\03`\00\00pR\F0\03 \00c$x\03\03 \000\00@\E2\0F\00\07\90\00\22Y\00\01\00p\C8\0F\00\10x\0A\030\00\F0\05\FF\E0\FF\07\00\E4\0F\04\0Cz\00\03\00\\\00\00pb\F4\03\10\00Z\10x\0C\03\02 \00\12\0A \00\10\FC \00f\00\10x\12\03\03 \00\00\F0\00\12\0C \00C\FA\03\00\C4\00\01\13X\90\00\02@\00\01\80\00ApR\F2\03\D0\00G\A4\09\FF\04\A0\00B\0Cz\00\12@\00\13\F6`\004\02\03\04`\00i\E2\0F\04$\E4\0B0\00c\07\A2\04\03\FF\00\FF\04c\E4\0F\04\07\A2\08\10\00 \80\04 \00<$\D4\0D`\00\12\02`\00\11\F8\80\00@%\A6\04\04\E9\04#\09\02P\00D\E8\06\0A\01@\00\10\C4\10\004\0A\0A\01`\00\010\00G\08\08\00^0\00F\D8\0E\0C\02p\00A\04\81\A9\05#\06\D8\00\19\1E\0C\00\A2\00\00\07\D8\0C\0C\02@\000\E6\06\06@\00\12\0B@\00f\08\07\B8\10\12\03@\00U\00\81\A9\00\08@\00\95\A4\02\00%\E6\0A\0A\00`0\00g\00\07\B8\12\12\03P\00E\81\E9\07\060\00p\E4\0E\00%\D6\0E\0E`\00\14\0D0\008\C2\13\02\10\01F\81\E9\14\0A0\00V\08\00$\B4\15 \01\96\C4\0F\00%\D6\0C\0C\00`@\00E\81\D9\0F\0E0\00pf\0F\00%\B6\10\10`\00\14\15\C0\00F\C2\0A\02\FF\90\00U\01\81\D9\0C\0C0\00\10d0\00C\08\12\00`0\00u\E4/\00\81\B9\11\10 \00i$\03\00$\C4\0B\80\006\81\B9\08\00\01u$\0F\00%\C6\12\130\01\10\C8\10\00\07\10\01u\E4\0F\00\81\C9\13\120\00\10(\10\00\16\0A\E0\00\84\22\0F\00$r\02\FF\FF`\00\10\E2P\025\04\03\05\B0\02u\1F\04\10x\10\03\07`\02Q/\04#\A2\02\08\06\01\D4\00\84\E2O\00\10x\05\03\06 \00\02\D0\02\15\04 \03\83\E2\0F\00#\E2\02\07\14@\060\00\E2\8F \00\15\05 \03\84\C6\0F\00#\D2\02\0F\0C \00\85\C8\0F\02#\B2\02\11\080\00j\0F\01\0Cz\00\10\F0\028\11\03\08\F0\02\0B \03W\07\A8\08\04\01\F0\02V\10x\00\03\090\00f\00#\C2\02\13\0A`\00\00\90\00\15\11\F0\02\10\E4@\007\0A\04\01\10\02*$\E4 \03Y\07\E8\06\05\02\00\039\0C\05\02\00\03'\0A\0A\00\03\11\08\D0\03\04\F0\03\00`\008\B8\0E\10\D0\02\010\03\08`\03G\B8\10\10\03P\006\81\A9\05\B0\01%\A4\00 \03\07\C0\02)\11\FF \03\19\04 \039$\B4\17@\02*%\E6\C0\02_\07\C2\12\11\FF0\03\09\12\B60\03\12\17P\02Y\08\81\E9\14\0C \03&\C4\16`\00\10\E4\D0\02C\0A\10\00`0\00W\E2\1F\00\81\B9 \03\10b\10\04\11\15\80\05\04@\011%\C6\10\C0\02\14\16`\009\B9\0A\0A \03\12\C6 \03\01 \00 \E2/0\04(\00\01P\03\18\C90\03\1A\01\D0\04\01\00\03\0B0\035\D6\12\15@\01\1B\C8\D0\03\000\00\1B\D90\03\08\B0\03\10\22\80\029\10\03\0C \034\00\03\0E\10\00\00`\02E\A2\02\05\04`\02\11OP\03\17\0A \03\000\03\14\0B\10\00\1F\E40\03\0Cw\C8\8F\00#\B2\02\0F\B0\02\16\02@\03\12\F6@\03\1F\C20\03\05\02\D0\02\000\03\17\0D\A0\00\0CP\06\00\F0\02\14\02\90\01\02\80\06\18\11\C0\06\00P\03\17\02 \02Z#\D2\02\13\0C@\03\06\E0\02\00`\00\1B\B40\037\B8\06\05\F0\02\1B\04 \03d\00\07\B8\0C\05\03`\00\00P\03O\C2\0E\10\FF \03\09O\C2\10\10\FF \03\09\19\B6 \03i\08\07\E8\13\11\01\80\06\0B \03\17\C4 \03\01\C0\02\0B \03H\E8\12\11\01p\00\1B\B9 \03\17\C6 \03\12\E2\E0\02\17\02\C0\00+\81\B90\03*\E4\16\90\03\1B\C60\03\1B\C9P\06\17\E6 \03\00\D0\00I\D8\0C\00\02\00\03\0A0\03\09\D0\07\00\D0\00\1A\E6@\03'\81\E90\03*&\01\10\03\00`\06\18\E90\03\1F\03 \03\22\1B\11 \03\14\13\10\00\0F \03\04\14\0F \00\03 \03\14\10\10\00\0F \03\01\1F\B2P\06\05\05\10\03\0C0\03\06\10\03\1C\CE\00\03\11/\00\03\08\A0\02*#\E2P\03\00@\03\17\12\90\00\00\10\03\16\03\80\01\0F\10\03\01\1F\11\10\03\0A:\B2\06\05\E0\02\090\06\07P\03\11\FC \0A3\07\B2\0C0\00\1F\00\10\03\03:\C8\0E\10\E0\02)\05\0A`\09H\C8\10\10\01@\00\08 \03\00p\05\0F\10\03\1DH\D8\13\11\02p\00\08\10\03\88\E2\0E\00\07\D8\12\11\02p\00\08 \03\1F\C4\10\03\00\1B\D4@\06\0B\10\03H\07\E8\15\00\A0\01\09 \03\10d\C0\05\0B \03O\E8\0C\00\03 \03\08'%\D6\10\03[\C4/\00\81\D9@\06)\E4\0D\90\00+\81\D9@\06\1B\E6@\06\08P\07\000\00\1B\E9 \03\1E\E9 \03\1B\16 \03\14\18\10\00\0F \03\04\1B\14 \03\14\15\10\00\0F \03=\11\C6\D0\02\0D\00\03\14\17p\00\1F\C6@\06\00O\A2\0A\04\FF@\06\02\03\F0\08G\A2\08\04\FF\10\02/#\E2@\06\05\1F\FC@\06\06\1F\01@\06\0C\18\01@\06F\C8\0E\10\020\00\0F0\03\00?\10\10\02@\06\19O\D8\13\11\03@\06)O\D8\12\11\03@\06\05.$\07@\06G\E2\15\00\FF\C0\00\090\03*$\0B0\03\0F@\06\00H\07\E2\00\00p\01\090\03*$\010\03\00\00\04\08 \03\10$\F0\0A\17\06\D0\00\1A\8F0\03\1B\E20\03%\E6\0E\10\03\22\06\02\90\00\090\03\00 \07S\E6\0C\00\00` \009\E4\0F\02 \03\1Bh \03\10b\E0\026\00\03\1A\E0\02d\04\10x\06\03\1B\10\00\01\00\034\0E\03\1C\10\00/\E2\1F0\03\007\05\03\19\B0\02\09\10\03;\C8\0F\01\00\03\00\80\02\15\06@\03\0D\00\03\17\8F\A0\02\12\FA \00\0A\C0\02\16\02`\03\04\F0\0F9\0A\03\1D\C0\009\08\03\1E\10\007\07\03\1F\A0\002\07\D8\16L\00\00\B0\01\02\D0\03\15\0E\B0\03\02\10\00\15\0A\A0\03\00P\03G\E8\10\05\010\00W\07\A8\0D\06\03\10\001\03x\0C\C0\01\01\01\00\02\90\0D\18\08\90\03H\0Cz\00\07\B0\00\06\C0\00\12\F4`\00H\B2\0B\0E\FF`\00&\C8\09\E0\0F\10\CA\B0\00(\04\08\B0\00H\07\E8\03\07\80\00\060\01\01\D0\10d\00\07\A8\14\05\01\D0\02\00 \000r\00\0C`\00$pRp\00%\0E\0E\F0\02\01\C0\03(\0A\0A0\00T\07\D8\08\08\02\10\00\000\01H\88\12\00\02 \00H\E8\00\07\03\10\008\A8\18\06\10\00\06\E0\00\03P\11\1A\84\B0\039$\A4\1A\10\00E%\86\12\12\90\03f\D0\0F\00$\94\11 \00\00P\03&\89\07\C0\02p\A6\00\00%\96\14\140\00\14\11\10\06!\96\10\90\03#\11\020\005\99\05\140\00v\E6\02\00%\86\16\16\B0\03\00 \00\17\06P\03`\08\00%\A6\18\18P\00\14\1A \005\89\0C\16 \00f\A6\0A\00$\B4\1C\90\00\00\00\04S\A6\1A\0D\00`0\00\00\80\06:\A9\0D\18\D0\04\02`\04\15\1C\F0\07&\1A\1A \00V\0E\00$\C4\1DP\00\01\C0\04 \14\0BP\00\14\1C\F0\03\08\C0\0Dl\A6\0E\00$\D4\1F\80\04\01\90\05\13\1D0\05H\81\B9\14\140\00c%\C6\10\09\00` \00\10\E4\90\07\19\0Bp\041%\D6\120\12\11\1F \00\00\00\0B*\10\10\90\04\090\0BA\02%\D6\08\90\12\14\1F \01\08\90\0A\010\0B)\18\00\E0\04\08\90\04\00\00\08U\E6\16\03\00` \00F\00\81\E9\190\01\01\90\04(\16\16\90\04c$v\04\FF\00b\80\00\00\C0\06H\92\02\05\06\A0\0D5\82\02\07\10\04 \C8O\90\04%\0D\1A\10\00\01\80\04\16\0F\90\07\00P\00C\05\FF\00cP\00\02\90\07(\0B\10\A0\04H\D2\02\13\08\10\11D\E2\02\19\16\10\00a\CA\0F\00\8Ey\00\90\0D@\84\E7\10\0CP\00\14M\80\15\030\15PGy\00\00\F0\A9\19\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\B0\0F\01\00-\00\\\04.\03\00\01\00\22@\00\01\00=[\01\000\00\08\01\00\1F\0B@\00\04\13\9B)\00\1F\97@\00\0C\14\13\CC\01\0B\01\00\138U\00\11\90\06\00\06p\19\22\08\00\01\00\22\18\00\01\00..\01T\00\00\01\00\13\C8@\00/p\00\80\00\0B\1F)'\00\03!\008\F5\02$\00\00\E0\1B\04\E4\00*\04\00\01\00\1Fm@\00\04\13h)\00&\AC\00@\00\1F\0A@\00\00!L\01D\01\0D@\001\18\05\00\01\00*\D8\00\01\00\1B\08\08\00?;\01\00\16\1D\003\00\00\F0@\00&\10\00\80\00\17\048\00\04\18\00\13\EB\14\01\0D\84\01!\06\00\01\00\17\901\01\0F\C0\00\01\132@\00+\06\00\01\00\1A\08`\1D\12\03\C0\01>\22\80\00g\00\17\05(!\0C\01\00*\A8\00\08\00\04\C0\00\13\018\00\0Ey\00\03x\00\1A\18\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @main_kernel_3_main_kColReduction_reduce__6_1_0___thread_tile_h32_kernel_name("main_kColReduction_reduce__6_1_0___thread_tile_h32\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @main_kernel_3_blob_gpu.binary("P\EDU\BA\01\00\10\00H\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\08\04\00\00\00\00\00\00\01\04\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\0Be__6_1_0___thread_tile_h32B\00\0F<\00%oshared>\00%\9Fconstant0A\00\22\FA\01debug_frame\00.rel\11\00!nv\14\00\11aM\00\0FS\01 \0F\92\00\1F\0F\86\01\E2o_param\8D\01\1C\0F\01\00\04\8Ce\00\00\00\03\00\0A\00\01\00 \18\01\18\00,\09\00\01\00\11`\18\00,\04\00\01\00\11~\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\1C\00\00\00E\00#\04\10\18\00\80/\08\00\05\00\00\00\06\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04@\04\91\015\00\00\04\0A\08\00\02\FC\00\91\10\00\03\19\10\00\04\17\0C$\00u\01\00\08\00\00\F0!\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00p\00\00\00\C0\00\01\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00 \01/\05\00\01\00\FFh@$v\01\FF\C7\03\A3\FF\00\8E\07\00\C4\0F\00\C3y\A4\03\A2%\00\00\00\22\0E\00\19y\00\01\00\F3\0E!\00\00\00b\0E\00\90r\05?\04\00\00\80?\E0\FF\0F\00\C6\1F\00\B9z\04\00\00X\96\02p\E4\0F\00\A4r\04\05B\00\B1?\02\8E\0F\00\CC\0F\00\0C|\00\F6\03\95pR\F0\0B\00\DA/\00M\1B\04\F3\0D\80\03\00\EA\0F\00\02z\02\00\00Z\00\00\00\0F\00\00\00\E2\0F\00$v\03\FF\00[\90\00\22\E2\0F`\001F\00\00k\04\80\C8\0F\00\86y\00\02\FFh\030\19\10\0C \00*MyP\00PGy\00\00\F0\F1\03\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\80\0F\01\00-\00\BF\04.\03\00\01\00\22@\00\01\00=S\01\000\00\08\01\00\1F\0B@\00\04\13\93)\00\1F\8D@\00\0C\13\13\C4\03\0C\01\00\13 U\00*\90\00\E8\03\04\05\04\22\18\00\01\00.&\01T\00\00\01\00\13\B0@\00/p\00\80\00\0B\1F)'\00\03A\00 \04\00\01\00\04\F8\05\04\E4\00*\04\00\01\00\1Fk@\00\04\13P)\00&L\00@\00\1F\0A@\00\00!D\01D\01\0D@\00\13\A0)\00*\D8\00\01\00\1B\08\08\00?3\01\00.\07\003\00\00x\15\05\17\10\80\00\17\048\00\04\18\00\13\E5\14\01\0C\84\01*\88\05\E0\06\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07x\07\12\03\C0\01:\06\80\00\01\00\13\06\D8\01\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\13\018\00\04\A8\00\0C\01\00*\F8\02\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00\00\00\00\00\00\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @main_kernel_2_main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1_kernel_name("main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @main_kernel_2_blob_gpu.binary("P\EDU\BA\01\00\10\00\A8\0B\00\00\00\00\00\00\02\00\01\01@\00\00\00h\0B\00\00\00\00\00\00c\0B\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\A8#\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00g\02\00\BE\00r\00\01\00\22#\00\01\00\11 \06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\12e__6_1_0___no_ibXblock_tile_h64_1I\00\0FC\00,osharedE\00,\9Fconstant0H\00)\FA\01debug_frame\00.rel\11\00!nv\14\00\11aT\00\0Fo\01 \0F\99\00&\0F\A9\01\B6\8F$____wg_B\00&\00\1B\00/26\F1\016o_param\F8\01\1C\0F\01\00\05\8Cl\00\00\00\03\00\0A\00\01\00\11\EF\18\00,\0B\00\01\00 |\01\18\00,\09\00\01\00\11\CB\18\00,\04\00\01\00\11\E9\18\00,\07\00\01\00g2\00\00\00\12\10x\00\11\18\06\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0\17\07\00 \00\04\9B\00R\04\14\00\00\00E\002\04\C0\05\18\00C/\08\00\06\7F\00\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E0\04\F1\08\015\00\00\04\0A\08\00\03\00\00\00`\01(\00\03\19(\00\04\17\0C$\00\10\05\ED\04%\F0!\10\009\04\00\18\10\009\03\00\10\10\00u\02\00\08\00\00\F0\11\10\009\01\00\04\10\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\00\10\17\00\00`\17\00\00\04\1Et\01#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\005\02\00\00\84\01\0F\01\00\FFz@$v\01\FF?\04\A3\FF\00\8E\07\00\C4\0F\00\19y\A6\01\10%[\02a\0E\00\19y\03\00\01\00\10!-\00\F5\14\0E\00$z\06\06\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\06\00Y\00\00p`\F0\03\00\DA\0F\00M[\04\B0\80\03\00\EA\0F\00\19x\03\FF\1F\FC\01\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\00\B3\04\93\E2\0F\00Ey\00\00@\150\00\93\E2\0F\00$r\08\FF\FF\00\90\00p\E2\0F\00\11r\03\03\92\00\C1\FF@\8F\07\00\C8\0F\00\12x\05\031\04\10\C0\80\00p\0F\00$x\06\06\01\B4\03\12\0A\10\00@\12x\05\06p\00\01 \00p\E4\0F\04\19x\00\FF*\04\C0\06\16\01\00\00\E4\0F\00\0Cr\00\05`\00@pR\F2\03 \00P\0Cx\00\06\7F\10\00\22@\F0\80\002x\07\05\C9\02\11\8Ep\00T$x\07\07\04\90\00\9A\CC\0F\00G\19\00\00\80\14\E0\00\000\00\10\03\E0\00\01\90\00*\03\03@\004\09\03@@\00q\C8\0F\00%x\04\09P\00\11\02\E0\00P\04\10x\00\09\C0\001\FF\E0\FF\B0\00\B0\0Cz\00\09\00Z\00\00pb\F4\A0\00P\00\12x\04\04P\00!\FF\FC\D0\00\00p\01\12\00 \00\11\F6 \00`\10z\02\04\00\\@\00\11\F3@\00`\10z\04\04\00^\10\00\11\F9\D0\01\00`\00\00|\03\03`\00\D2\00\10z\03\05\00]\00\00\FF\E4\FF\000\00@\05\05\00_\10\00*\7F\02`\00\11\F8\10\01\F4\06\81\B9\0D\02\04\FC\FF\FF\00\19\1E\0C\00\A2\0E\00\10x\0A\09\03P\00\93\C6\0F\00\81\A9\00\02\04\F8 \00 \E2\0E@\00\12\0A@\00\22\FA\03 \00%\0B\04 \00u\E8\0E\00\81\B9\0A\04P\00p\A8\0E\00\81\C9\0F\02\10\01\01\10\00y(\0F\00\81\C9\0C\04\10\00S\D9\11\02\04\04\10\00\10h\10\00%\0E\04\10\00\10b\E0\004\10\09\04\90\00\01\F0\00:\12\09\05P\01\12\10\90\00#\FC\03\E0\00\12\12\10\00\12\F2@\01Dx\10\09\060\00a\E2\0F\04#\A2\086\07\00$\021\00\E2\8F@\01\14\07 \00q\CE\0F\00\81\E9\0B\02\91\01\06\F0\00\15\10\B0\01\93\E2\0F\00#\B2\08\0D\0A\00=\05!\E2O0\01\16\08`\00`\00\81\99\0D\02\04\17\06\03P\01\09\D0\01\000\018\E9\00\04`\00U#\C2\08\0F\0CP\00&\0F\01`\01\11\F8\C0\00H\81\99\0A\04P\00\00\D0\00\19\09\A0\01C\0F\02\04\10 \00\86\22\0F\00#\D2\08\11\0EP\00\16\02\C0\00\12\FAP\00W\A9\0C\04\04\10\80\01W\B9\11\02\04\14\80\019\B9\0E\04\10\00X\C9\13\02\04\18\10\00)\10\04\10\00X\D9\15\02\04\1C\10\00H\12\04\04\1C\C0\017\14\09\0A \01U#\E2\08\0B\00\A0\00\02\80\01\17\0B\E0\01E\0Cz\00\14\D0\01\000\00\1A\92p\01\06P\01\04\E0\017\00\09\0C\10\02\00\90\01\18\0Dp\00\1D\A2`\01\15\00\D0\01\13\CA\F0\01\18 `\018\0C\09\0F`\01\00\B0\01\07 \00-#\B2`\01\19\0A\B0\03Lx\0A\09\0E\10\02\19$\F0\01$\13\10@\00\02\B0\02\06\F0\01\13\E2\D0\01\17(\A0\01\00\00\02\08@\00F\D2\08\15\12\80\00\00@\00\15\0C\E0\01\13\C4\D0\01\17,\90\01\00\F0\01\08P\00\00\E0\01\1B,\E0\01\1B0\E0\01\1B0\E0\01\1B4\E0\01\1A4\E0\01\1F\10\E0\01\06\11O\A0\01\1F\11\E0\01\16\1A\8F\E0\01\13\D6\A0\01\188\A0\01;\00\09\12\F0\01\1F\13\F0\01\15\13\E4\D0\01\1F8\D0\01\1B\16\14`\00\11\04\D0\01\16<\90\01X\10x\0E\09\15\80\00\08\E0\01\1B\E2\E0\01\04\B0\03\1B@\E0\01\1F<\E0\01\0A\1D\0E\C0\03\1B@\C0\03\1BD\C0\03\1BD\E0\01\1BH\E0\01\1BH\E0\01\1BL\E0\01\1AL\E0\01\1F\16\E0\01\0C\1F\17\E0\01-\1AP\E0\01\1F\18\D0\01\08\00\90\01\16\19\00\02\1F\00\E0\01\02\1FP\E0\01\17\01\D0\01\18T\D0\017\0A\09\1A\D0\00\00\10\04\1F\1B\E0\01\15\13\E4\E0\01\1BX\E0\01\1FT\C0\03\1C\1B\\\C0\03\1BX\C0\03\1B\\\E0\01\1B`\E0\01\1B`\E0\01\1Bd\E0\01\1Ad\E0\01\1F\1C\E0\01\0C\1F\1D\E0\01\05\13\DA\C0\01\17hp\01\0F\F0\01\09\01P\03;\00\09\1E\C0\03\1F\1F\C0\03\1D\1Fh\E0\01\14\01\C0\01<\0A\09 \C0\03\1Al\C0\03\1F!\C0\03\1D\1Bp\E0\01\1Fl\C0\03\1C\1Bp\C0\03\1Bt\C0\03\1Bt\E0\01\1Bx\E0\01\1Bx\E0\01\1B|\E0\01\1A|\E0\01\1F\22\E0\01\0C\1F#\C0\03-\16\80\90\01\00P\00\1F$\C0\03\0C\1F%\C0\03\0D\1F\80\C0\03\1C\18\84\D0\017\0A\09&\D0\00\00\C0\03\1F'\C0\03\1D\1B\88\E0\01\1F\84\C0\03\1C\1B\8C\C0\03\1B\88\C0\03\1B\8C\E0\01\1B\90\E0\01\1B\90\E0\01\1B\94\E0\01\1A\94\E0\01\1F(\E0\01\0C\1F)\C0\03\0D\1F\98\C0\03\1B\1B*\C0\03\1F+\C0\03\1D\1F\98\C0\03\1B\1C,\C0\03\1A\9C\C0\03\1F-\C0\03\1D\1B\A0\E0\01\1F\9C\C0\03\1C\1B\A0\C0\03\1B\A4\C0\03\1B\A4\E0\01\1B\A8\E0\01\1B\A8\E0\01\1B\AC\E0\01\1A\AC\E0\01\1F.\E0\01\0C\1F/\C0\03-\16\B0\90\01\00P\00\1F0\C0\03\0C\1F1\C0\03\0D\1F\B0\C0\03\1C\18\B4\D0\017\0A\092\D0\00\00\C0\03\1F3\C0\03\1D\1B\B8\E0\01\1F\B4\C0\03\1C\1B\BC\C0\03\1B\B8\C0\03\1B\BC\E0\01\1B\C0\E0\01\1B\C0\E0\01\1B\C4\E0\01\1A\C4\E0\01\1F4\E0\01\0C\1F5\C0\03\0D\1F\C8\C0\03\1B\1B6\C0\03\1F7\C0\03\1D\1F\C8\C0\03\1B\1C8\C0\03\1A\CC\C0\03\1F9\C0\03\1D\1B\D0\E0\01\1F\CC\C0\03\1C\1B\D0\C0\03\1B\D4\C0\03\1B\D4\E0\01\1B\D8\E0\01\1B\D8\E0\01\1B\DC\E0\01\1A\DC\E0\01\1F:\E0\01\0C\1F;\C0\03-\16\E0\90\01\00P\00\1F<\C0\03\0C\1F=\C0\03\0D\1F\E0\C0\03\17\00P\00\17>\C0\00\00\B0\03\17?`\00g\81\99\09\02\04\E4\A0\01\0F\C0\03\0D\00\D0\01\040\00\00P\12Y\A9\0D\02\04\E8\E0\10\0F\C0\03\0B\00\E0\01\18\E8\E0\01K\0F\02\04\EC\E0\01\1B\EC\D0\01\18\F0\10\00K\11\02\04\F0\D0\01\18\F4\10\00%\13\02\10\00\1Bb\D0\01 \C8O\B0\01\15\09\B0\01 \C8\8F\80\01\15\0D\80\01\86\C8\0F\01#\B2\08\0F\0E\10\00f\02#\C2\08\11\10\10\00\00\E0\00\15\13\E0\00P\C4\0F\00Ay\09\00\06\90\14c\88s\00\07\08\00!\00f\E8\0F\00\1D{\00\01\00S\EC\0F\00\84\89\CD\19\13\08 \01Ax\00\06?\00\15\11\F2@\03c\84\89\03\07\00\10 \00s$\0E\00!\82\00\00\02\16\10\00\F0\15'\88\83@\00\0F`\00\01-\99\02`\00\14\1F`\15\00`\00W\99\03\07\00\08`\009\92\02\02`\00O\93\00\07\02\C0\00\0A\1A\03`\005r\00\06\D0\15\01\C0\00H\04\07\00\04\C0\00J\04\03\04\00\C0\00\1F\04`\00\089M\19\00P\016\84y\07p\00\93\22\0E\00$v\02\FF\00`\D0\15\93\C4\0F\00$v\03\FF\00a\10\00a\CA\0F\00\8Ey\00\01\01\9B\84\E7\10\0C\00\E2\1F\00M\A0\01PGy\00\00\F0\C0\16\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00`\0F\01\00-'\01\00 \02\07\01\00\22@\00\01\00=o\01\000\00\08\01\00\1F\0B@\00\04\13\AF)\00\1F\F8@\00\0C\13\13\\\1A\0C\01\00\13\A8U\00*\A8\00\80\1A\22\08\00\01\00\22\18\00\01\00.B\01T\00\00\01\00\13P5\02/p\00\80\00\0B/)\00'\00\02#\00\C0@\00\00,\09\17\00\E4\00*\04\00\01\00\1Fr@\00\04\13\F0)\00&\98\00@\00\1F\0A@\00\00!`\01D\01\0D@\001\88\05\00\01\00*\D8\00\01\00\1B\08\08\00?O\01\00\0E\1E\00Q\00\00`\06\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\FA\14\01\0C\84\01\13p@\00\17\881\01\0F\C0\00\01\132@\00\15\06R\00\0B\A1\00\16\18\80\00j\06\00\00\18\80\00\01\00\13\B5+\00+\03\00\01\00\00\D5\0F\1A\00q\00\0F\80\00\01\11\06\F0\1D\07\E8\22\0CH\02\1A\00\08\00\04\97\00\13\018\00\04\E8\00\0C\01\00*\90\19\08\00\088\00\18\06\A0\00\0F\01\00\05\03\A9\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @main_kernel_1_main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_kernel_name("main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @main_kernel_1_blob_gpu.binary("P\EDU\BA\01\00\10\00H\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\08\04\00\00\00\00\00\00\06\04\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00h\0C\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\002\C0\0B\00\01\00\11\09\06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\10e__6_1_0___no_ibXblock_tile_h64G\00\0FA\00*osharedC\00*\9Fconstant0F\00'\FA\01debug_frame\00.rel\11\00!nv\14\00\11aR\00\0Fg\01 \0F\97\00$\0F\9F\01\F6o_param\A6\01\1C\0F\01\00\07\8Cj\00\00\00\03\00\0A\00\01\00 ,\01\18\00,\09\00\01\00\11y\18\00,\04\00\01\00\11\97\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\1C\00\00\00E\00#\04\10\18\00\80/\08\00\05\00\00\00\06\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04p\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C$\00u\01\00\08\00\00\F0!\10\00\01\01\00\C0\F0\11\00\03\1B\FF\00\04\1C\08\00p\A8\04\12\00\01\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00 \01/\05\00\01\00\FF\B8@$v\01\FF\17\04\A3\FF\00\8E\07\00\C4\0F\00\C3y\F4\03\A2%\00\00\00\22\0E\00\19y\00\01\00\F3\0E!\00\00\00b\0E\00\90r\05?\04\00\00\80?\E0\FF\0F\00\C6\1F\00\B9z\04\00\00X\E6\02p\E4\0F\00\A4r\04\05B\00\B1?\02\8E\0F\00\CC\0F\00\0C|\00F\04\95pR\F0\0B\00\DA/\00Mk\04\F3\0D\80\03\00\EA\0F\00\02z\02\00\00Z\00\00\00\0F\00\00\00\E2\0F\00$v\03\FF\00[\90\00\22\E2\0F`\001F\00\00\BB\04\80\C8\0F\00\86y\00\02\FF\B8\030\19\10\0C \00*MyP\00PGy\00\00\F0A\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\80\0F\01\00-\00\0F\05.\03\00\01\00\22@\00\01\00=g\01\000\00\08\01\00\1F\0B@\00\04\13\A7)\00\1F\A6@\00\0C\13\13\14\04\0C\01\00\13PU\00*\90\008\04\04U\04\22\18\00\01\00.:\01T\00\00\01\00\13\E0@\00/p\00\80\00\0B\1F)'\00\03A\00P\04\00\01\00\04H\06\04\E4\00*\04\00\01\00\1Fp@\00\04\13\80)\00&L\00@\00\1F\0A@\00\00!X\01D\01\0D@\00\13\D0)\00*\D8\00\01\00\1B\08\08\00%G\01\DB\0A\09\01\00\13\A8e\05\17\10\80\00\17\048\00\04\18\00\13\F4\14\01\0C\84\01*\B8\050\07\1F\00\C0\00\04\132@\00*\06\00\01\00*\80\07\C8\07\12\03\C0\01:\06\80\00\01\00\13\06\D8\01\05\A8\0B\0C\C8\00\1A\00\08\00\04\F8\00\13\018\00\04\A8\00\0C\01\009H\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @ral_send_output___cpu___pvoid_i64_m0df32___void("ral_send_output___cpu___pvoid_i64_m0df32___void\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @d2h___gpu___pvoid_pvoid_m0df32_m0df32___void("d2h___gpu___pvoid_pvoid_m0df32_m0df32___void\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @alloc___cpu___pvoid_i64___pvoid("alloc___cpu___pvoid_i64___pvoid\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @inc_ref___gpu___pvoid_pvoid_m1df32_m1di64___m0df32("inc_ref___gpu___pvoid_pvoid_m1df32_m1di64___m0df32\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @dealloc___gpu___pvoid_pvoid___void("dealloc___gpu___pvoid_pvoid___void\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @main_kernel_0_main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_1_kernel_name("main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_1\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @main_kernel_0_blob_gpu.binary("P\EDU\BA\01\00\10\00\00\09\00\00\00\00\00\00\02\00\01\01@\00\00\00\C0\08\00\00\00\00\00\00\BF\08\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00h\17\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\002\C0\16\00\01\00\11\14\06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\13e__6_1_0___no_ibXthread_tile_h32_1J\00\0FD\00-osharedF\00-\9Fconstant0I\00*\FA\01debug_frame\00.rel\11\00!nv\14\00\11aU\00\0Fs\01 \0F\9A\00'\0F\AE\01\FF\03o_param\B5\01\1C\0F\01\00\04\8Cm\00\00\00\03\00\0A\00\01\00 8\01\18\00,\09\00\01\00\11\88\18\00,\04\00\01\00\11\A6\18\00,\07\00\01\00g2\00\00\00\12\10`\00\11\0C\06\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0\0B\07\00 \00\04\9B\00 \04\18i\00\01:\002\04\B8\02\18\00p/\08\00\05\00\00\00\1A\00\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\88\04\80\015\00\00\04\0A\08\00F\00\A2`\01(\00\03\19(\00\04\17\C5\00u\05\00 \00\00\F0!\10\009\04\00\18\10\009\03\00\10\10\00u\02\00\08\00\00\F0\11\10\009\01\00\04\10\00\01\01\00\F3\01\F0\11\00\03\1B\FF\00\04\1C\08\00`\00\00\00P\D8\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\00\D2\05b,\00\00\00H\00\01\00\00`\01/\05\00\01\00\FF\E0@$v\01\FF\7F\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\BB\02Q\0E\00\19y\03\0F\00 \00!-\00\F0\14\0E\00$z\00\00\00X\00\00\03\02\8E\07\00\CA\1F\00\12x\FF\00\FF\01\00\00\FF\C0\80\07\00\C8\0F\00\0C \00\C5Y\00\00pdp\00\00\DA\0F\00M\C3\04\B0\80\03\00\EA\0F\00\19x\03\FF\1F\0F\00\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\00\03\05a\C6\0F\00\11r\03q\001\FFH\8FP\00\000\00\00C\00\10\030\00\93\CA\0F\00$x\07\03 \00\B0\00p\C8\0F\00%x\04\07s\04\10\FF\90\00\F0\09\E2\0F\04\0Cz\00\07\00Z\00\00pb\F4\03\00\E4\0F\04\10x\00\07\01 \00\B0\E0\FF\07\00\E4\0F\00\12x\04\04\01\031\FF\FC\8E\10\00\01\B0\00\010\00\10\F60\00p\00\10z\02\04\00\\0\00!\F1\07@\00Pz\04\04\00^\10\00\11\F3 \01\00P\00\00,\03\04P\00\C2\10z\03\05\00]\00\00\FF\E4\7F\000\00@\05\05\00_\10\00*\FF\00`\00 \F8\03\F0\00\F4\06\81\B9\0B\02\04\FC\FF\FF\00\19\1E\0C\00\A2\0E\00\10x\06\07\03P\00\93\C6\0F\00\81\A9\00\02\04\F8 \00u\E8\0E\00\81\A9\09\04\10\00 \E2\0EP\00\12\06P\00!\FA\030\005\B9\08\04P\00p\A8\0E\00\81\C9\0D\02\10\01\01\10\00y(\0F\00\81\C9\0A\04\10\00S\D9\0F\02\04\04\10\00\10h\10\00%\0C\04\10\00!b\0F\90\00\14\04\90\00\01\F0\006\0E\07\05\F0\00\16\04\80\00\12\FC0\01Jx\10\07\06 \00\12\0E \00\13\F0 \00:\0E\07\07 \00\12\10 \00\11\F2\10\01T$r\06\FF\FF\D0\01\12\E2P\00\14\0A0\00a\E2\0F\04#\A2\06\06\07\00$\00F\00\E2\8F\00`\00\12\F4P\01S\E9\09\02\04\08\C0\00!\E2\0Ep\00\18\08P\018\E9\00\04 \00@#\B2\06\0B\0F\00\11\06P\00\17OP\00\02\00\02P\81\89\0B\02\04\0F\06\05\A0\018\0E\07\09P\008\89\08\04 \00T#\C2\06\0D\0AP\00\93\C6\0F\01\81\99\0D\02\04\10 \00'\22\0F`\00\12\F8\B0\01W\99\0A\04\04\10\90\01X\A9\11\02\04\14\10\00F\0E\04\04\14@\00U#\D2\06\0F\0C\B0\00&\0F\02@\01 \FA\03\A0\01g\81\B9\0F\02\04\18\D0\019\B9\0C\04\10\00X\C9\13\02\04\1C\10\00)\10\04\10\00X\D9\15\02\04 \10\00H\12\04\04 \10\026\14\07\0B\90\01e\00#\E2\06\09\00\90\00\11\8F\10\03\17\0C0\02E\0Cz\00\14 \02\000\00\1D\82p\01\1A\00 \027\00\07\0D \02X\10x\08\07\0Ep\00\17\92`\016\E2\0F\01@\00C\F2\03\00\CA\00\025$\00\00\00\03\00\F0\01\17$\A0\01F\A2\06\11\0E@\00\00\80\00\17\08 \04\02\F0\01\18(@\028\08\07\0F\F0\01\00\D0\01\17,\90\01\1D\B2\90\01\19\08@\02\00\E0\01\070\00\\\10x\0C\07\10@\02\07p\00Z#\C2\06\13\10\A0\00\15\0C0\02\00P\00X\A9\0F\02\040\90\018\10\07\11P\008\A9\0C\04 \00Z#\D2\06\15\12P\00\060\02\00P\00W\B9\11\02\044\F0\01[\B9\0E\04\0440\02\1B80\02\1B80\02\1B<0\02\1A<0\02\1F\120\02\06\11O\F0\01\1F\130\02\16\1A\8F0\02\13\D6\F0\01\18@\A0\03?\00\07\140\02\08\00\D0\01\16\15`\02\19\00@\02\000\00\1F\A2\D0\01\05\11\F4 \00\01P\02\09p\009\08\07\16\B0\027\0A\07\17\80\00\01P\02\19Dp\04\0F\80\02\04\12\F6\C0\03\00\10\02\040\00\00\F0\05\00p\02\17Hp\02\0E \02\19\0A \02\00p\02\070\00\00\B0\04\19\18 \02O\0F\02\04L \02\0A\19\0E \02\00P\02\17L\E0\01\000\02\1BP0\02\1BP0\02\1BT0\02\1BT0\02\1BX0\02\1AX0\02\1F\190\02\05*\C6O \02\030\04\18\1A0\00\0B0\02\00\A0\01\14\1B \00\13\D20\02\16\\\C0\01\0FP\04\0A\03`\00\1A\1C\B0\06\1A\08\B0\048\08\07\1D\80\00\0E@\02\15\00@\02\13\C4@\02\18\\@\028\0A\07\1E\A0\01_\99\0B\02\04`0\02\14\01\F0\01(\08\040\00\00`\04\18\1FP\00_\89\07\02\04d@\02\18X\89\0A\04\04d\80\06O\0D\02\04h0\02\0A\15\0C0\02\13\C40\02\17h\E0\01\00\90\06\1Bl0\02\1Bl \02\18p\10\00K\11\02\04p \02\18t\10\00%\13\02\10\00\1Bb \02 \C8O\D0\01\06\00\02 \C8\8F\10\02%\07\0A\10\00v\0F\01#\A2\06\0D\0C\10\00\10\02\C0\05\08`\01c$v\08\FF\00`\80\08\10\E4\10\00C\09\FF\00a\10\00\11\E2@\01&\11\10@\00\00\10\01\15\13\10\01p\CA\0F\00\8Ey\00\08\0C\00@\84\E7\10\0C0\00*My\F0\0APGy\00\00\F09\0F\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00p\0F\01\00-\11\01h\0F\0E\01\00\22@\00\01\00=s\01\000\00\08\01\00\1F\0B@\00\04\13\B3)\00\1F\B5@\00\0C\13\13\BC\0E\0C\01\00\13hU\00*\90\00\E0\0E\22\08\00\01\00\22\18\00\01\00.F\01T\00\00\01\00\13\F8@\00/p\00\80\00\0B\1F)'\00\03A\00h\04\00\01\00\00\9B\07\17\00\E4\00*\04\00\01\00\1Fs@\00\04\13\98)\00&\8C\00@\00\1F\0A@\00\00!d\01D\01\0D@\001(\05\00\01\00*\D8\00\01\00\1B\08\08\00?S\01\00f\12\00\04\E1\02\10\00B\11\05\80\00\17\048\00\04\18\00\13\FD\14\01\0C\84\01&\10\06\B0\12\04\01\00\0F\C0\00\01\132@\00+\06\00\01\00\1A\08\B0\12\12\03\C0\01>\18\80\00\A7\00\18\05\A8\16\0B\01\00*\A8\00\08\00\04W\00\13\018\00\04\A8\00\0D\D0\12)\0D\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void("ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @main_kernel_main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_kernel_name("main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @main_kernel_blob_gpu.binary("P\EDU\BA\01\00\10\00H\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\08\04\00\00\00\00\00\00\08\04\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00h\0C\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\002\C0\0B\00\01\00\11\09\06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\11e__6_1_0___no_ibXthread_tile_h32H\00\0FB\00+osharedD\00+\9Fconstant0G\00(\FA\01debug_frame\00.rel\11\00!nv\14\00\11aS\00\0Fk\01 \0F\98\00%\0F\A4\01\FAo_param\AB\01\1C\0F\01\00\06\8Ck\00\00\00\03\00\0A\00\01\00 0\01\18\00,\09\00\01\00\11~\18\00,\04\00\01\00\11\9C\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\1C\00\00\00E\00#\04\10\18\00\80/\08\00\05\00\00\00\06\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04x\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C$\00u\01\00\08\00\00\F0!\10\00\01\01\00\C0\F0\11\00\03\1B\FF\00\04\1C\08\00p\B0\04\12\00\01\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00 \01/\05\00\01\00\FF\B0@$v\01\FF\0F\04\A3\FF\00\8E\07\00\C4\0F\00\C3y\EC\03\A2%\00\00\00\22\0E\00\19y\00\01\00\F3\0E!\00\00\00b\0E\00\90r\05?\04\00\00\80?\E0\FF\0F\00\C6\1F\00\B9z\04\00\00X\DE\02p\E4\0F\00\A4r\04\05B\00\B1?\02\8E\0F\00\CC\0F\00\0C|\00>\04\95pR\F0\0B\00\DA/\00Mc\04\F3\0D\80\03\00\EA\0F\00\02z\02\00\00Z\00\00\00\0F\00\00\00\E2\0F\00$v\03\FF\00[\90\00\22\E2\0F`\001F\00\00\B3\04\80\C8\0F\00\86y\00\02\FF\B0\030\19\10\0C \00*MyP\00PGy\00\00\F09\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\80\0F\01\00-\00\07\05.\03\00\01\00\22@\00\01\00=k\01\000\00\08\01\00\1F\0B@\00\04*\AB\01\08\00\0F@\00\05\13\13\0C\04\0C\01\00\13XU\00*\90\000\04\04M\04\22\18\00\01\00.>\01T\00\00\01\00\13\E8@\00/p\00\80\00\0B\1F)'\00\03A\00X\04\00\01\00\04@\06\04\E4\00*\04\00\01\00\1Fq@\00\04\13\88)\00&L\00@\00\1F\0A@\00\00!\\\01D\01\0D@\00\13\D8)\00*\D8\00\01\00\1B\08\08\00%K\01\DB\0A\09\01\00\13\B0]\05\17\10\80\00\17\048\00\04\18\00\13\F7\14\01\0C\84\01*\C0\05(\07\1F\00\C0\00\04\132@\00*\06\00\01\00*\80\07\C0\07\12\03\C0\01:\06\80\00\01\00\13\06\D8\01\05\A8\0B\0B\01\00*\A8\00\08\00\04\F8\00\14\018\00/\05\00\01\00\029@\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @sync_on_stream___gpu___pvoid_pvoid___void("sync_on_stream___gpu___pvoid_pvoid___void\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @h2d___gpu___pvoid_pvoid_m2df32_m2df32___void("h2d___gpu___pvoid_pvoid_m2df32_m2df32___void\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @alloc___gpu___pvoid_i64___pvoid("alloc___gpu___pvoid_i64___pvoid\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @ral_recv_input___cpu___pvoid_i64___m2df32("ral_recv_input___cpu___pvoid_i64___m2df32\00") {addr_space = 0 : i32}
  llvm.func @disc_ral_call(!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>)
  llvm.func @main(%arg0: !llvm.ptr<i8>) attributes {tf.entry_function = {input_placements = "cpu,cpu", inputs = "arg0,arg1", output_placements = "cpu", outputs = "out0"}} {
    %0 = llvm.mlir.constant(256 : index) : i64
    %1 = llvm.mlir.constant(32 : index) : i64
    %2 = llvm.mlir.constant(512 : index) : i64
    %3 = llvm.mlir.constant(0 : i32) : i32
    %4 = llvm.mlir.constant(4 : i32) : i32
    %5 = llvm.mlir.constant(1 : index) : i64
    %6 = llvm.mlir.constant(0 : index) : i64
    %7 = llvm.mlir.constant(0 : i32) : i32
    %8 = llvm.mlir.constant(1 : i32) : i32
    %9 = llvm.alloca %8 x !llvm.struct<"", (ptr<i8>, i64, struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>)> : (i32) -> !llvm.ptr<struct<"", (ptr<i8>, i64, struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>)>>
    %10 = llvm.mlir.constant(3 : i32) : i32
    %11 = llvm.alloca %10 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %12 = llvm.mlir.constant(0 : i32) : i32
    %13 = llvm.getelementptr %9[%7, 0] : (!llvm.ptr<struct<"", (ptr<i8>, i64, struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %13 : !llvm.ptr<ptr<i8>>
    %14 = llvm.getelementptr %11[%12] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %15 = llvm.bitcast %13 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %15, %14 : !llvm.ptr<ptr<i8>>
    %16 = llvm.mlir.constant(1 : i32) : i32
    %17 = llvm.getelementptr %9[%7, 1] : (!llvm.ptr<struct<"", (ptr<i8>, i64, struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %6, %17 : !llvm.ptr<i64>
    %18 = llvm.getelementptr %11[%16] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %19 = llvm.bitcast %17 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %19, %18 : !llvm.ptr<ptr<i8>>
    %20 = llvm.mlir.constant(2 : i32) : i32
    %21 = llvm.getelementptr %9[%7, 2] : (!llvm.ptr<struct<"", (ptr<i8>, i64, struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>>
    %22 = llvm.getelementptr %11[%20] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %23 = llvm.bitcast %21 : !llvm.ptr<struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>> to !llvm.ptr<i8>
    llvm.store %23, %22 : !llvm.ptr<ptr<i8>>
    %24 = llvm.mlir.addressof @ral_recv_input___cpu___pvoid_i64___m2df32 : !llvm.ptr<array<42 x i8>>
    %25 = llvm.mlir.constant(0 : index) : i64
    %26 = llvm.getelementptr %24[%25, %25] : (!llvm.ptr<array<42 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %26, %11) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %27 = llvm.load %21 : !llvm.ptr<struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>>
    %28 = llvm.mlir.constant(0 : i32) : i32
    %29 = llvm.mlir.constant(1 : i32) : i32
    %30 = llvm.alloca %29 x !llvm.struct<".1", (ptr<i8>, i64, struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>)> : (i32) -> !llvm.ptr<struct<".1", (ptr<i8>, i64, struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>)>>
    %31 = llvm.mlir.constant(3 : i32) : i32
    %32 = llvm.alloca %31 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %33 = llvm.mlir.constant(0 : i32) : i32
    %34 = llvm.getelementptr %30[%28, 0] : (!llvm.ptr<struct<".1", (ptr<i8>, i64, struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %34 : !llvm.ptr<ptr<i8>>
    %35 = llvm.getelementptr %32[%33] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %36 = llvm.bitcast %34 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %36, %35 : !llvm.ptr<ptr<i8>>
    %37 = llvm.mlir.constant(1 : i32) : i32
    %38 = llvm.getelementptr %30[%28, 1] : (!llvm.ptr<struct<".1", (ptr<i8>, i64, struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %38 : !llvm.ptr<i64>
    %39 = llvm.getelementptr %32[%37] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %40 = llvm.bitcast %38 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %40, %39 : !llvm.ptr<ptr<i8>>
    %41 = llvm.mlir.constant(2 : i32) : i32
    %42 = llvm.getelementptr %30[%28, 2] : (!llvm.ptr<struct<".1", (ptr<i8>, i64, struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>>
    %43 = llvm.getelementptr %32[%41] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %44 = llvm.bitcast %42 : !llvm.ptr<struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>> to !llvm.ptr<i8>
    llvm.store %44, %43 : !llvm.ptr<ptr<i8>>
    %45 = llvm.mlir.addressof @ral_recv_input___cpu___pvoid_i64___m2df32 : !llvm.ptr<array<42 x i8>>
    %46 = llvm.mlir.constant(0 : index) : i64
    %47 = llvm.getelementptr %45[%46, %46] : (!llvm.ptr<array<42 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %47, %32) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %48 = llvm.load %42 : !llvm.ptr<struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>>
    %49 = llvm.extractvalue %48[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %50 = llvm.extractvalue %27[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %51 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %52 = llvm.extractvalue %27[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %53 = llvm.extractvalue %27[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %54 = llvm.insertvalue %52, %51[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %55 = llvm.insertvalue %53, %54[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %56 = llvm.mlir.constant(0 : index) : i64
    %57 = llvm.insertvalue %56, %55[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %58 = llvm.insertvalue %50, %57[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %59 = llvm.mlir.constant(4 : index) : i64
    %60 = llvm.insertvalue %59, %58[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %61 = llvm.mlir.constant(4 : index) : i64
    %62 = llvm.insertvalue %61, %60[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %63 = llvm.mlir.constant(1 : index) : i64
    %64 = llvm.insertvalue %63, %62[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %65 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %66 = llvm.extractvalue %48[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %67 = llvm.extractvalue %48[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %68 = llvm.insertvalue %66, %65[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %69 = llvm.insertvalue %67, %68[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %70 = llvm.mlir.constant(0 : index) : i64
    %71 = llvm.insertvalue %70, %69[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %72 = llvm.insertvalue %49, %71[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %73 = llvm.mlir.constant(4 : index) : i64
    %74 = llvm.insertvalue %73, %72[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %75 = llvm.mlir.constant(4 : index) : i64
    %76 = llvm.insertvalue %75, %74[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %77 = llvm.mlir.constant(1 : index) : i64
    %78 = llvm.insertvalue %77, %76[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %79 = llvm.icmp "eq" %49, %5 : i64
    %80 = llvm.select %79, %50, %49 : i1, i64
    %81 = llvm.mlir.constant(4 : index) : i64
    %82 = llvm.mlir.constant(1 : index) : i64
    %83 = llvm.mul %81, %50  : i64
    %84 = llvm.mlir.null : !llvm.ptr
    %85 = llvm.getelementptr %84[%83] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %86 = llvm.ptrtoint %85 : !llvm.ptr to i64
    %87 = llvm.mlir.constant(0 : i32) : i32
    %88 = llvm.mlir.constant(1 : i32) : i32
    %89 = llvm.alloca %88 x !llvm.struct<".2", (ptr<i8>, i64, ptr)> : (i32) -> !llvm.ptr<struct<".2", (ptr<i8>, i64, ptr)>>
    %90 = llvm.mlir.constant(3 : i32) : i32
    %91 = llvm.alloca %90 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %92 = llvm.mlir.constant(0 : i32) : i32
    %93 = llvm.getelementptr %89[%87, 0] : (!llvm.ptr<struct<".2", (ptr<i8>, i64, ptr)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %93 : !llvm.ptr<ptr<i8>>
    %94 = llvm.getelementptr %91[%92] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %95 = llvm.bitcast %93 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %95, %94 : !llvm.ptr<ptr<i8>>
    %96 = llvm.mlir.constant(1 : i32) : i32
    %97 = llvm.getelementptr %89[%87, 1] : (!llvm.ptr<struct<".2", (ptr<i8>, i64, ptr)>>, i32) -> !llvm.ptr<i64>
    llvm.store %86, %97 : !llvm.ptr<i64>
    %98 = llvm.getelementptr %91[%96] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %99 = llvm.bitcast %97 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %99, %98 : !llvm.ptr<ptr<i8>>
    %100 = llvm.mlir.constant(2 : i32) : i32
    %101 = llvm.getelementptr %89[%87, 2] : (!llvm.ptr<struct<".2", (ptr<i8>, i64, ptr)>>, i32) -> !llvm.ptr<ptr>
    %102 = llvm.getelementptr %91[%100] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %103 = llvm.bitcast %101 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %103, %102 : !llvm.ptr<ptr<i8>>
    %104 = llvm.mlir.addressof @alloc___gpu___pvoid_i64___pvoid : !llvm.ptr<array<32 x i8>>
    %105 = llvm.mlir.constant(0 : index) : i64
    %106 = llvm.getelementptr %104[%105, %105] : (!llvm.ptr<array<32 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %106, %91) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %107 = llvm.load %101 : !llvm.ptr<ptr>
    %108 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %109 = llvm.bitcast %107 : !llvm.ptr to !llvm.ptr
    %110 = llvm.insertvalue %109, %108[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %111 = llvm.insertvalue %109, %110[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %112 = llvm.mlir.constant(0 : index) : i64
    %113 = llvm.insertvalue %112, %111[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %114 = llvm.mlir.constant(1 : index) : i64
    %115 = llvm.insertvalue %81, %113[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %116 = llvm.insertvalue %114, %115[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %117 = llvm.mul %114, %81  : i64
    %118 = llvm.insertvalue %50, %116[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %119 = llvm.insertvalue %117, %118[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %120 = llvm.inttoptr %3 : i32 to !llvm.ptr<i8>
    %121 = llvm.mlir.constant(0 : i32) : i32
    %122 = llvm.mlir.constant(1 : i32) : i32
    %123 = llvm.extractvalue %64[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %124 = llvm.extractvalue %64[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %125 = llvm.extractvalue %64[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %126 = llvm.extractvalue %64[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %127 = llvm.extractvalue %64[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %128 = llvm.extractvalue %64[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %129 = llvm.extractvalue %64[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %130 = llvm.extractvalue %119[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %131 = llvm.extractvalue %119[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %132 = llvm.extractvalue %119[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %133 = llvm.extractvalue %119[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %134 = llvm.extractvalue %119[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %135 = llvm.extractvalue %119[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %136 = llvm.extractvalue %119[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %137 = llvm.alloca %122 x !llvm.struct<".3", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64)> : (i32) -> !llvm.ptr<struct<".3", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64)>>
    %138 = llvm.mlir.constant(16 : i32) : i32
    %139 = llvm.alloca %138 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %140 = llvm.mlir.constant(0 : i32) : i32
    %141 = llvm.getelementptr %137[%121, 0] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %141 : !llvm.ptr<ptr<i8>>
    %142 = llvm.getelementptr %139[%140] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %143 = llvm.bitcast %141 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %143, %142 : !llvm.ptr<ptr<i8>>
    %144 = llvm.mlir.constant(1 : i32) : i32
    %145 = llvm.getelementptr %137[%121, 1] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %120, %145 : !llvm.ptr<ptr<i8>>
    %146 = llvm.getelementptr %139[%144] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %147 = llvm.bitcast %145 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %147, %146 : !llvm.ptr<ptr<i8>>
    %148 = llvm.mlir.constant(2 : i32) : i32
    %149 = llvm.getelementptr %137[%121, 2] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<ptr>
    llvm.store %123, %149 : !llvm.ptr<ptr>
    %150 = llvm.getelementptr %139[%148] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %151 = llvm.bitcast %149 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %151, %150 : !llvm.ptr<ptr<i8>>
    %152 = llvm.mlir.constant(3 : i32) : i32
    %153 = llvm.getelementptr %137[%121, 3] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<ptr>
    llvm.store %124, %153 : !llvm.ptr<ptr>
    %154 = llvm.getelementptr %139[%152] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %155 = llvm.bitcast %153 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %155, %154 : !llvm.ptr<ptr<i8>>
    %156 = llvm.mlir.constant(4 : i32) : i32
    %157 = llvm.getelementptr %137[%121, 4] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %125, %157 : !llvm.ptr<i64>
    %158 = llvm.getelementptr %139[%156] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %159 = llvm.bitcast %157 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %159, %158 : !llvm.ptr<ptr<i8>>
    %160 = llvm.mlir.constant(5 : i32) : i32
    %161 = llvm.getelementptr %137[%121, 5] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %126, %161 : !llvm.ptr<i64>
    %162 = llvm.getelementptr %139[%160] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %163 = llvm.bitcast %161 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %163, %162 : !llvm.ptr<ptr<i8>>
    %164 = llvm.mlir.constant(6 : i32) : i32
    %165 = llvm.getelementptr %137[%121, 6] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %127, %165 : !llvm.ptr<i64>
    %166 = llvm.getelementptr %139[%164] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %167 = llvm.bitcast %165 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %167, %166 : !llvm.ptr<ptr<i8>>
    %168 = llvm.mlir.constant(7 : i32) : i32
    %169 = llvm.getelementptr %137[%121, 7] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %128, %169 : !llvm.ptr<i64>
    %170 = llvm.getelementptr %139[%168] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %171 = llvm.bitcast %169 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %171, %170 : !llvm.ptr<ptr<i8>>
    %172 = llvm.mlir.constant(8 : i32) : i32
    %173 = llvm.getelementptr %137[%121, 8] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %129, %173 : !llvm.ptr<i64>
    %174 = llvm.getelementptr %139[%172] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %175 = llvm.bitcast %173 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %175, %174 : !llvm.ptr<ptr<i8>>
    %176 = llvm.mlir.constant(9 : i32) : i32
    %177 = llvm.getelementptr %137[%121, 9] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<ptr>
    llvm.store %130, %177 : !llvm.ptr<ptr>
    %178 = llvm.getelementptr %139[%176] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %179 = llvm.bitcast %177 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %179, %178 : !llvm.ptr<ptr<i8>>
    %180 = llvm.mlir.constant(10 : i32) : i32
    %181 = llvm.getelementptr %137[%121, 10] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<ptr>
    llvm.store %131, %181 : !llvm.ptr<ptr>
    %182 = llvm.getelementptr %139[%180] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %183 = llvm.bitcast %181 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %183, %182 : !llvm.ptr<ptr<i8>>
    %184 = llvm.mlir.constant(11 : i32) : i32
    %185 = llvm.getelementptr %137[%121, 11] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %132, %185 : !llvm.ptr<i64>
    %186 = llvm.getelementptr %139[%184] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %187 = llvm.bitcast %185 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %187, %186 : !llvm.ptr<ptr<i8>>
    %188 = llvm.mlir.constant(12 : i32) : i32
    %189 = llvm.getelementptr %137[%121, 12] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %133, %189 : !llvm.ptr<i64>
    %190 = llvm.getelementptr %139[%188] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %191 = llvm.bitcast %189 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %191, %190 : !llvm.ptr<ptr<i8>>
    %192 = llvm.mlir.constant(13 : i32) : i32
    %193 = llvm.getelementptr %137[%121, 13] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %134, %193 : !llvm.ptr<i64>
    %194 = llvm.getelementptr %139[%192] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %195 = llvm.bitcast %193 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %195, %194 : !llvm.ptr<ptr<i8>>
    %196 = llvm.mlir.constant(14 : i32) : i32
    %197 = llvm.getelementptr %137[%121, 14] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %135, %197 : !llvm.ptr<i64>
    %198 = llvm.getelementptr %139[%196] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %199 = llvm.bitcast %197 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %199, %198 : !llvm.ptr<ptr<i8>>
    %200 = llvm.mlir.constant(15 : i32) : i32
    %201 = llvm.getelementptr %137[%121, 15] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %136, %201 : !llvm.ptr<i64>
    %202 = llvm.getelementptr %139[%200] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %203 = llvm.bitcast %201 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %203, %202 : !llvm.ptr<ptr<i8>>
    %204 = llvm.mlir.addressof @h2d___gpu___pvoid_pvoid_m2df32_m2df32___void : !llvm.ptr<array<45 x i8>>
    %205 = llvm.mlir.constant(0 : index) : i64
    %206 = llvm.getelementptr %204[%205, %205] : (!llvm.ptr<array<45 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %206, %139) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %207 = llvm.mlir.constant(0 : i32) : i32
    %208 = llvm.mlir.constant(1 : i32) : i32
    %209 = llvm.alloca %208 x !llvm.struct<".4", (ptr<i8>, ptr<i8>)> : (i32) -> !llvm.ptr<struct<".4", (ptr<i8>, ptr<i8>)>>
    %210 = llvm.mlir.constant(2 : i32) : i32
    %211 = llvm.alloca %210 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %212 = llvm.mlir.constant(0 : i32) : i32
    %213 = llvm.getelementptr %209[%207, 0] : (!llvm.ptr<struct<".4", (ptr<i8>, ptr<i8>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %213 : !llvm.ptr<ptr<i8>>
    %214 = llvm.getelementptr %211[%212] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %215 = llvm.bitcast %213 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %215, %214 : !llvm.ptr<ptr<i8>>
    %216 = llvm.mlir.constant(1 : i32) : i32
    %217 = llvm.getelementptr %209[%207, 1] : (!llvm.ptr<struct<".4", (ptr<i8>, ptr<i8>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %120, %217 : !llvm.ptr<ptr<i8>>
    %218 = llvm.getelementptr %211[%216] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %219 = llvm.bitcast %217 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %219, %218 : !llvm.ptr<ptr<i8>>
    %220 = llvm.mlir.addressof @sync_on_stream___gpu___pvoid_pvoid___void : !llvm.ptr<array<42 x i8>>
    %221 = llvm.mlir.constant(0 : index) : i64
    %222 = llvm.getelementptr %220[%221, %221] : (!llvm.ptr<array<42 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %222, %211) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %223 = llvm.mlir.constant(4 : index) : i64
    %224 = llvm.mlir.constant(1 : index) : i64
    %225 = llvm.mul %223, %49  : i64
    %226 = llvm.mlir.null : !llvm.ptr
    %227 = llvm.getelementptr %226[%225] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %228 = llvm.ptrtoint %227 : !llvm.ptr to i64
    %229 = llvm.mlir.constant(0 : i32) : i32
    %230 = llvm.mlir.constant(1 : i32) : i32
    %231 = llvm.alloca %230 x !llvm.struct<".5", (ptr<i8>, i64, ptr)> : (i32) -> !llvm.ptr<struct<".5", (ptr<i8>, i64, ptr)>>
    %232 = llvm.mlir.constant(3 : i32) : i32
    %233 = llvm.alloca %232 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %234 = llvm.mlir.constant(0 : i32) : i32
    %235 = llvm.getelementptr %231[%229, 0] : (!llvm.ptr<struct<".5", (ptr<i8>, i64, ptr)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %235 : !llvm.ptr<ptr<i8>>
    %236 = llvm.getelementptr %233[%234] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %237 = llvm.bitcast %235 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %237, %236 : !llvm.ptr<ptr<i8>>
    %238 = llvm.mlir.constant(1 : i32) : i32
    %239 = llvm.getelementptr %231[%229, 1] : (!llvm.ptr<struct<".5", (ptr<i8>, i64, ptr)>>, i32) -> !llvm.ptr<i64>
    llvm.store %228, %239 : !llvm.ptr<i64>
    %240 = llvm.getelementptr %233[%238] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %241 = llvm.bitcast %239 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %241, %240 : !llvm.ptr<ptr<i8>>
    %242 = llvm.mlir.constant(2 : i32) : i32
    %243 = llvm.getelementptr %231[%229, 2] : (!llvm.ptr<struct<".5", (ptr<i8>, i64, ptr)>>, i32) -> !llvm.ptr<ptr>
    %244 = llvm.getelementptr %233[%242] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %245 = llvm.bitcast %243 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %245, %244 : !llvm.ptr<ptr<i8>>
    %246 = llvm.mlir.addressof @alloc___gpu___pvoid_i64___pvoid : !llvm.ptr<array<32 x i8>>
    %247 = llvm.mlir.constant(0 : index) : i64
    %248 = llvm.getelementptr %246[%247, %247] : (!llvm.ptr<array<32 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %248, %233) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %249 = llvm.load %243 : !llvm.ptr<ptr>
    %250 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %251 = llvm.bitcast %249 : !llvm.ptr to !llvm.ptr
    %252 = llvm.insertvalue %251, %250[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %253 = llvm.insertvalue %251, %252[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %254 = llvm.mlir.constant(0 : index) : i64
    %255 = llvm.insertvalue %254, %253[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %256 = llvm.mlir.constant(1 : index) : i64
    %257 = llvm.insertvalue %223, %255[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %258 = llvm.insertvalue %256, %257[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %259 = llvm.mul %256, %223  : i64
    %260 = llvm.insertvalue %49, %258[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %261 = llvm.insertvalue %259, %260[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %262 = llvm.mlir.constant(0 : i32) : i32
    %263 = llvm.mlir.constant(1 : i32) : i32
    %264 = llvm.extractvalue %78[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %265 = llvm.extractvalue %78[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %266 = llvm.extractvalue %78[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %267 = llvm.extractvalue %78[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %268 = llvm.extractvalue %78[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %269 = llvm.extractvalue %78[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %270 = llvm.extractvalue %78[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %271 = llvm.extractvalue %261[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %272 = llvm.extractvalue %261[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %273 = llvm.extractvalue %261[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %274 = llvm.extractvalue %261[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %275 = llvm.extractvalue %261[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %276 = llvm.extractvalue %261[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %277 = llvm.extractvalue %261[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %278 = llvm.alloca %263 x !llvm.struct<".6", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64)> : (i32) -> !llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64)>>
    %279 = llvm.mlir.constant(16 : i32) : i32
    %280 = llvm.alloca %279 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %281 = llvm.mlir.constant(0 : i32) : i32
    %282 = llvm.getelementptr %278[%262, 0] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %282 : !llvm.ptr<ptr<i8>>
    %283 = llvm.getelementptr %280[%281] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %284 = llvm.bitcast %282 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %284, %283 : !llvm.ptr<ptr<i8>>
    %285 = llvm.mlir.constant(1 : i32) : i32
    %286 = llvm.getelementptr %278[%262, 1] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %120, %286 : !llvm.ptr<ptr<i8>>
    %287 = llvm.getelementptr %280[%285] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %288 = llvm.bitcast %286 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %288, %287 : !llvm.ptr<ptr<i8>>
    %289 = llvm.mlir.constant(2 : i32) : i32
    %290 = llvm.getelementptr %278[%262, 2] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<ptr>
    llvm.store %264, %290 : !llvm.ptr<ptr>
    %291 = llvm.getelementptr %280[%289] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %292 = llvm.bitcast %290 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %292, %291 : !llvm.ptr<ptr<i8>>
    %293 = llvm.mlir.constant(3 : i32) : i32
    %294 = llvm.getelementptr %278[%262, 3] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<ptr>
    llvm.store %265, %294 : !llvm.ptr<ptr>
    %295 = llvm.getelementptr %280[%293] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %296 = llvm.bitcast %294 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %296, %295 : !llvm.ptr<ptr<i8>>
    %297 = llvm.mlir.constant(4 : i32) : i32
    %298 = llvm.getelementptr %278[%262, 4] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %266, %298 : !llvm.ptr<i64>
    %299 = llvm.getelementptr %280[%297] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %300 = llvm.bitcast %298 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %300, %299 : !llvm.ptr<ptr<i8>>
    %301 = llvm.mlir.constant(5 : i32) : i32
    %302 = llvm.getelementptr %278[%262, 5] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %267, %302 : !llvm.ptr<i64>
    %303 = llvm.getelementptr %280[%301] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %304 = llvm.bitcast %302 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %304, %303 : !llvm.ptr<ptr<i8>>
    %305 = llvm.mlir.constant(6 : i32) : i32
    %306 = llvm.getelementptr %278[%262, 6] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %268, %306 : !llvm.ptr<i64>
    %307 = llvm.getelementptr %280[%305] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %308 = llvm.bitcast %306 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %308, %307 : !llvm.ptr<ptr<i8>>
    %309 = llvm.mlir.constant(7 : i32) : i32
    %310 = llvm.getelementptr %278[%262, 7] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %269, %310 : !llvm.ptr<i64>
    %311 = llvm.getelementptr %280[%309] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %312 = llvm.bitcast %310 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %312, %311 : !llvm.ptr<ptr<i8>>
    %313 = llvm.mlir.constant(8 : i32) : i32
    %314 = llvm.getelementptr %278[%262, 8] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %270, %314 : !llvm.ptr<i64>
    %315 = llvm.getelementptr %280[%313] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %316 = llvm.bitcast %314 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %316, %315 : !llvm.ptr<ptr<i8>>
    %317 = llvm.mlir.constant(9 : i32) : i32
    %318 = llvm.getelementptr %278[%262, 9] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<ptr>
    llvm.store %271, %318 : !llvm.ptr<ptr>
    %319 = llvm.getelementptr %280[%317] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %320 = llvm.bitcast %318 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %320, %319 : !llvm.ptr<ptr<i8>>
    %321 = llvm.mlir.constant(10 : i32) : i32
    %322 = llvm.getelementptr %278[%262, 10] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<ptr>
    llvm.store %272, %322 : !llvm.ptr<ptr>
    %323 = llvm.getelementptr %280[%321] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %324 = llvm.bitcast %322 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %324, %323 : !llvm.ptr<ptr<i8>>
    %325 = llvm.mlir.constant(11 : i32) : i32
    %326 = llvm.getelementptr %278[%262, 11] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %273, %326 : !llvm.ptr<i64>
    %327 = llvm.getelementptr %280[%325] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %328 = llvm.bitcast %326 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %328, %327 : !llvm.ptr<ptr<i8>>
    %329 = llvm.mlir.constant(12 : i32) : i32
    %330 = llvm.getelementptr %278[%262, 12] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %274, %330 : !llvm.ptr<i64>
    %331 = llvm.getelementptr %280[%329] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %332 = llvm.bitcast %330 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %332, %331 : !llvm.ptr<ptr<i8>>
    %333 = llvm.mlir.constant(13 : i32) : i32
    %334 = llvm.getelementptr %278[%262, 13] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %275, %334 : !llvm.ptr<i64>
    %335 = llvm.getelementptr %280[%333] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %336 = llvm.bitcast %334 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %336, %335 : !llvm.ptr<ptr<i8>>
    %337 = llvm.mlir.constant(14 : i32) : i32
    %338 = llvm.getelementptr %278[%262, 14] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %276, %338 : !llvm.ptr<i64>
    %339 = llvm.getelementptr %280[%337] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %340 = llvm.bitcast %338 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %340, %339 : !llvm.ptr<ptr<i8>>
    %341 = llvm.mlir.constant(15 : i32) : i32
    %342 = llvm.getelementptr %278[%262, 15] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %277, %342 : !llvm.ptr<i64>
    %343 = llvm.getelementptr %280[%341] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %344 = llvm.bitcast %342 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %344, %343 : !llvm.ptr<ptr<i8>>
    %345 = llvm.mlir.addressof @h2d___gpu___pvoid_pvoid_m2df32_m2df32___void : !llvm.ptr<array<45 x i8>>
    %346 = llvm.mlir.constant(0 : index) : i64
    %347 = llvm.getelementptr %345[%346, %346] : (!llvm.ptr<array<45 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %347, %280) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %348 = llvm.mlir.constant(0 : i32) : i32
    %349 = llvm.mlir.constant(1 : i32) : i32
    %350 = llvm.alloca %349 x !llvm.struct<".7", (ptr<i8>, ptr<i8>)> : (i32) -> !llvm.ptr<struct<".7", (ptr<i8>, ptr<i8>)>>
    %351 = llvm.mlir.constant(2 : i32) : i32
    %352 = llvm.alloca %351 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %353 = llvm.mlir.constant(0 : i32) : i32
    %354 = llvm.getelementptr %350[%348, 0] : (!llvm.ptr<struct<".7", (ptr<i8>, ptr<i8>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %354 : !llvm.ptr<ptr<i8>>
    %355 = llvm.getelementptr %352[%353] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %356 = llvm.bitcast %354 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %356, %355 : !llvm.ptr<ptr<i8>>
    %357 = llvm.mlir.constant(1 : i32) : i32
    %358 = llvm.getelementptr %350[%348, 1] : (!llvm.ptr<struct<".7", (ptr<i8>, ptr<i8>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %120, %358 : !llvm.ptr<ptr<i8>>
    %359 = llvm.getelementptr %352[%357] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %360 = llvm.bitcast %358 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %360, %359 : !llvm.ptr<ptr<i8>>
    %361 = llvm.mlir.addressof @sync_on_stream___gpu___pvoid_pvoid___void : !llvm.ptr<array<42 x i8>>
    %362 = llvm.mlir.constant(0 : index) : i64
    %363 = llvm.getelementptr %361[%362, %362] : (!llvm.ptr<array<42 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %363, %352) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %364 = llvm.trunc %80 : i64 to i32
    %365 = llvm.mul %364, %4  : i32
    %366 = llvm.sext %365 : i32 to i64
    %367 = llvm.icmp "eq" %50, %80 : i64
    %368 = llvm.icmp "eq" %49, %80 : i64
    %369 = llvm.and %368, %367  : i1
    %370 = llvm.mlir.constant(1 : index) : i64
    %371 = llvm.mlir.constant(1 : index) : i64
    %372 = llvm.mlir.null : !llvm.ptr
    %373 = llvm.getelementptr %372[%370] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %374 = llvm.ptrtoint %373 : !llvm.ptr to i64
    %375 = llvm.mlir.constant(0 : i32) : i32
    %376 = llvm.mlir.constant(1 : i32) : i32
    %377 = llvm.alloca %376 x !llvm.struct<".8", (ptr<i8>, i64, ptr)> : (i32) -> !llvm.ptr<struct<".8", (ptr<i8>, i64, ptr)>>
    %378 = llvm.mlir.constant(3 : i32) : i32
    %379 = llvm.alloca %378 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %380 = llvm.mlir.constant(0 : i32) : i32
    %381 = llvm.getelementptr %377[%375, 0] : (!llvm.ptr<struct<".8", (ptr<i8>, i64, ptr)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %381 : !llvm.ptr<ptr<i8>>
    %382 = llvm.getelementptr %379[%380] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %383 = llvm.bitcast %381 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %383, %382 : !llvm.ptr<ptr<i8>>
    %384 = llvm.mlir.constant(1 : i32) : i32
    %385 = llvm.getelementptr %377[%375, 1] : (!llvm.ptr<struct<".8", (ptr<i8>, i64, ptr)>>, i32) -> !llvm.ptr<i64>
    llvm.store %374, %385 : !llvm.ptr<i64>
    %386 = llvm.getelementptr %379[%384] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %387 = llvm.bitcast %385 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %387, %386 : !llvm.ptr<ptr<i8>>
    %388 = llvm.mlir.constant(2 : i32) : i32
    %389 = llvm.getelementptr %377[%375, 2] : (!llvm.ptr<struct<".8", (ptr<i8>, i64, ptr)>>, i32) -> !llvm.ptr<ptr>
    %390 = llvm.getelementptr %379[%388] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %391 = llvm.bitcast %389 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %391, %390 : !llvm.ptr<ptr<i8>>
    %392 = llvm.mlir.addressof @alloc___gpu___pvoid_i64___pvoid : !llvm.ptr<array<32 x i8>>
    %393 = llvm.mlir.constant(0 : index) : i64
    %394 = llvm.getelementptr %392[%393, %393] : (!llvm.ptr<array<32 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %394, %379) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %395 = llvm.load %389 : !llvm.ptr<ptr>
    %396 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %397 = llvm.bitcast %395 : !llvm.ptr to !llvm.ptr
    %398 = llvm.insertvalue %397, %396[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %399 = llvm.insertvalue %397, %398[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %400 = llvm.mlir.constant(0 : index) : i64
    %401 = llvm.insertvalue %400, %399[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %402 = llvm.mlir.constant(1 : index) : i64
    %403 = llvm.insertvalue %370, %401[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %404 = llvm.insertvalue %402, %403[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    llvm.cond_br %369, ^bb1, ^bb4
  ^bb1:  // pred: ^bb0
    %405 = llvm.icmp "slt" %366, %5 : i64
    llvm.cond_br %405, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %406 = llvm.mlir.addressof @main_kernel_blob_gpu.binary : !llvm.ptr<array<1112 x i8>>
    %407 = llvm.mlir.constant(0 : index) : i64
    %408 = llvm.getelementptr %406[%407, %407] : (!llvm.ptr<array<1112 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %409 = llvm.mlir.constant(1 : i32) : i32
    %410 = llvm.alloca %409 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %411 = llvm.mlir.constant(0 : i32) : i32
    %412 = llvm.getelementptr %410[%411] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %408, %412 : !llvm.ptr<ptr<i8>>
    %413 = llvm.mlir.constant(1 : i64) : i64
    %414 = llvm.mlir.addressof @main_kernel_main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_kernel_name : !llvm.ptr<array<57 x i8>>
    %415 = llvm.mlir.constant(0 : index) : i64
    %416 = llvm.getelementptr %414[%415, %415] : (!llvm.ptr<array<57 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %417 = llvm.extractvalue %404[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %418 = llvm.extractvalue %404[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %419 = llvm.extractvalue %404[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %420 = llvm.extractvalue %404[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %421 = llvm.extractvalue %404[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %422 = llvm.mlir.constant(1 : i32) : i32
    %423 = llvm.alloca %422 x !llvm.struct<".9", (i64, ptr)> : (i32) -> !llvm.ptr<struct<".9", (i64, ptr)>>
    %424 = llvm.mlir.constant(2 : i32) : i32
    %425 = llvm.alloca %424 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %426 = llvm.mlir.constant(0 : i32) : i32
    %427 = llvm.mlir.constant(0 : i32) : i32
    %428 = llvm.getelementptr %423[%426, 0] : (!llvm.ptr<struct<".9", (i64, ptr)>>, i32) -> !llvm.ptr<i64>
    llvm.store %2, %428 : !llvm.ptr<i64>
    %429 = llvm.getelementptr %425[%427] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %430 = llvm.bitcast %428 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %430, %429 : !llvm.ptr<ptr<i8>>
    %431 = llvm.mlir.constant(1 : i32) : i32
    %432 = llvm.getelementptr %423[%426, 1] : (!llvm.ptr<struct<".9", (i64, ptr)>>, i32) -> !llvm.ptr<ptr>
    llvm.store %418, %432 : !llvm.ptr<ptr>
    %433 = llvm.getelementptr %425[%431] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %434 = llvm.bitcast %432 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %434, %433 : !llvm.ptr<ptr<i8>>
    %435 = llvm.mlir.constant(0 : i32) : i32
    %436 = llvm.mlir.constant(2 : i32) : i32
    %437 = llvm.inttoptr %435 : i32 to !llvm.ptr<i8>
    %438 = llvm.mlir.constant(0 : i32) : i32
    %439 = llvm.mlir.constant(1 : i32) : i32
    %440 = llvm.alloca %439 x !llvm.struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)> : (i32) -> !llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>
    %441 = llvm.mlir.constant(14 : i32) : i32
    %442 = llvm.alloca %441 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %443 = llvm.mlir.constant(0 : i32) : i32
    %444 = llvm.getelementptr %440[%438, 0] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %444 : !llvm.ptr<ptr<i8>>
    %445 = llvm.getelementptr %442[%443] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %446 = llvm.bitcast %444 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %446, %445 : !llvm.ptr<ptr<i8>>
    %447 = llvm.mlir.constant(1 : i32) : i32
    %448 = llvm.getelementptr %440[%438, 1] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %410, %448 : !llvm.ptr<ptr<ptr<i8>>>
    %449 = llvm.getelementptr %442[%447] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %450 = llvm.bitcast %448 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %450, %449 : !llvm.ptr<ptr<i8>>
    %451 = llvm.mlir.constant(2 : i32) : i32
    %452 = llvm.getelementptr %440[%438, 2] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %413, %452 : !llvm.ptr<i64>
    %453 = llvm.getelementptr %442[%451] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %454 = llvm.bitcast %452 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %454, %453 : !llvm.ptr<ptr<i8>>
    %455 = llvm.mlir.constant(3 : i32) : i32
    %456 = llvm.getelementptr %440[%438, 3] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %416, %456 : !llvm.ptr<ptr<i8>>
    %457 = llvm.getelementptr %442[%455] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %458 = llvm.bitcast %456 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %458, %457 : !llvm.ptr<ptr<i8>>
    %459 = llvm.mlir.constant(4 : i32) : i32
    %460 = llvm.getelementptr %440[%438, 4] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %460 : !llvm.ptr<i64>
    %461 = llvm.getelementptr %442[%459] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %462 = llvm.bitcast %460 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %462, %461 : !llvm.ptr<ptr<i8>>
    %463 = llvm.mlir.constant(5 : i32) : i32
    %464 = llvm.getelementptr %440[%438, 5] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %464 : !llvm.ptr<i64>
    %465 = llvm.getelementptr %442[%463] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %466 = llvm.bitcast %464 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %466, %465 : !llvm.ptr<ptr<i8>>
    %467 = llvm.mlir.constant(6 : i32) : i32
    %468 = llvm.getelementptr %440[%438, 6] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %468 : !llvm.ptr<i64>
    %469 = llvm.getelementptr %442[%467] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %470 = llvm.bitcast %468 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %470, %469 : !llvm.ptr<ptr<i8>>
    %471 = llvm.mlir.constant(7 : i32) : i32
    %472 = llvm.getelementptr %440[%438, 7] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %2, %472 : !llvm.ptr<i64>
    %473 = llvm.getelementptr %442[%471] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %474 = llvm.bitcast %472 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %474, %473 : !llvm.ptr<ptr<i8>>
    %475 = llvm.mlir.constant(8 : i32) : i32
    %476 = llvm.getelementptr %440[%438, 8] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %476 : !llvm.ptr<i64>
    %477 = llvm.getelementptr %442[%475] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %478 = llvm.bitcast %476 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %478, %477 : !llvm.ptr<ptr<i8>>
    %479 = llvm.mlir.constant(9 : i32) : i32
    %480 = llvm.getelementptr %440[%438, 9] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %480 : !llvm.ptr<i64>
    %481 = llvm.getelementptr %442[%479] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %482 = llvm.bitcast %480 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %482, %481 : !llvm.ptr<ptr<i8>>
    %483 = llvm.mlir.constant(10 : i32) : i32
    %484 = llvm.getelementptr %440[%438, 10] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %435, %484 : !llvm.ptr<i32>
    %485 = llvm.getelementptr %442[%483] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %486 = llvm.bitcast %484 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %486, %485 : !llvm.ptr<ptr<i8>>
    %487 = llvm.mlir.constant(11 : i32) : i32
    %488 = llvm.getelementptr %440[%438, 11] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %437, %488 : !llvm.ptr<ptr<i8>>
    %489 = llvm.getelementptr %442[%487] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %490 = llvm.bitcast %488 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %490, %489 : !llvm.ptr<ptr<i8>>
    %491 = llvm.mlir.constant(12 : i32) : i32
    %492 = llvm.getelementptr %440[%438, 12] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %436, %492 : !llvm.ptr<i32>
    %493 = llvm.getelementptr %442[%491] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %494 = llvm.bitcast %492 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %494, %493 : !llvm.ptr<ptr<i8>>
    %495 = llvm.mlir.constant(13 : i32) : i32
    %496 = llvm.getelementptr %440[%438, 13] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %425, %496 : !llvm.ptr<ptr<ptr<i8>>>
    %497 = llvm.getelementptr %442[%495] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %498 = llvm.bitcast %496 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %498, %497 : !llvm.ptr<ptr<i8>>
    %499 = llvm.mlir.addressof @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void : !llvm.ptr<array<101 x i8>>
    %500 = llvm.mlir.constant(0 : index) : i64
    %501 = llvm.getelementptr %499[%500, %500] : (!llvm.ptr<array<101 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %501, %442) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %502 = llvm.icmp "eq" %366, %6 : i64
    %503 = llvm.sub %366, %5  : i64
    %504 = llvm.udiv %503, %1  : i64
    %505 = llvm.add %504, %5  : i64
    %506 = llvm.select %502, %6, %505 : i1, i64
    %507 = llvm.mul %506, %2  : i64
    %508 = llvm.icmp "sle" %507, %6 : i64
    %509 = llvm.sub %6, %507  : i64
    %510 = llvm.sub %507, %5  : i64
    %511 = llvm.select %508, %509, %510 : i1, i64
    %512 = llvm.sdiv %511, %2  : i64
    %513 = llvm.sub %6, %512  : i64
    %514 = llvm.add %512, %5  : i64
    %515 = llvm.select %508, %513, %514 : i1, i64
    %516 = llvm.mlir.addressof @main_kernel_0_blob_gpu.binary : !llvm.ptr<array<2320 x i8>>
    %517 = llvm.mlir.constant(0 : index) : i64
    %518 = llvm.getelementptr %516[%517, %517] : (!llvm.ptr<array<2320 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %519 = llvm.mlir.constant(1 : i32) : i32
    %520 = llvm.alloca %519 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %521 = llvm.mlir.constant(0 : i32) : i32
    %522 = llvm.getelementptr %520[%521] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %518, %522 : !llvm.ptr<ptr<i8>>
    %523 = llvm.mlir.constant(1 : i64) : i64
    %524 = llvm.mlir.addressof @main_kernel_0_main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_1_kernel_name : !llvm.ptr<array<59 x i8>>
    %525 = llvm.mlir.constant(0 : index) : i64
    %526 = llvm.getelementptr %524[%525, %525] : (!llvm.ptr<array<59 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %527 = llvm.extractvalue %119[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %528 = llvm.extractvalue %119[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %529 = llvm.extractvalue %119[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %530 = llvm.extractvalue %119[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %531 = llvm.extractvalue %119[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %532 = llvm.extractvalue %119[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %533 = llvm.extractvalue %119[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %534 = llvm.extractvalue %261[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %535 = llvm.extractvalue %261[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %536 = llvm.extractvalue %261[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %537 = llvm.extractvalue %261[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %538 = llvm.extractvalue %261[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %539 = llvm.extractvalue %261[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %540 = llvm.extractvalue %261[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %541 = llvm.extractvalue %404[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %542 = llvm.extractvalue %404[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %543 = llvm.extractvalue %404[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %544 = llvm.extractvalue %404[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %545 = llvm.extractvalue %404[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %546 = llvm.mlir.constant(1 : i32) : i32
    %547 = llvm.alloca %546 x !llvm.struct<".11", (i64, i64, i64, ptr, ptr, ptr)> : (i32) -> !llvm.ptr<struct<".11", (i64, i64, i64, ptr, ptr, ptr)>>
    %548 = llvm.mlir.constant(6 : i32) : i32
    %549 = llvm.alloca %548 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %550 = llvm.mlir.constant(0 : i32) : i32
    %551 = llvm.mlir.constant(0 : i32) : i32
    %552 = llvm.getelementptr %547[%550, 0] : (!llvm.ptr<struct<".11", (i64, i64, i64, ptr, ptr, ptr)>>, i32) -> !llvm.ptr<i64>
    llvm.store %2, %552 : !llvm.ptr<i64>
    %553 = llvm.getelementptr %549[%551] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %554 = llvm.bitcast %552 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %554, %553 : !llvm.ptr<ptr<i8>>
    %555 = llvm.mlir.constant(1 : i32) : i32
    %556 = llvm.getelementptr %547[%550, 1] : (!llvm.ptr<struct<".11", (i64, i64, i64, ptr, ptr, ptr)>>, i32) -> !llvm.ptr<i64>
    llvm.store %507, %556 : !llvm.ptr<i64>
    %557 = llvm.getelementptr %549[%555] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %558 = llvm.bitcast %556 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %558, %557 : !llvm.ptr<ptr<i8>>
    %559 = llvm.mlir.constant(2 : i32) : i32
    %560 = llvm.getelementptr %547[%550, 2] : (!llvm.ptr<struct<".11", (i64, i64, i64, ptr, ptr, ptr)>>, i32) -> !llvm.ptr<i64>
    llvm.store %366, %560 : !llvm.ptr<i64>
    %561 = llvm.getelementptr %549[%559] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %562 = llvm.bitcast %560 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %562, %561 : !llvm.ptr<ptr<i8>>
    %563 = llvm.mlir.constant(3 : i32) : i32
    %564 = llvm.getelementptr %547[%550, 3] : (!llvm.ptr<struct<".11", (i64, i64, i64, ptr, ptr, ptr)>>, i32) -> !llvm.ptr<ptr>
    llvm.store %528, %564 : !llvm.ptr<ptr>
    %565 = llvm.getelementptr %549[%563] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %566 = llvm.bitcast %564 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %566, %565 : !llvm.ptr<ptr<i8>>
    %567 = llvm.mlir.constant(4 : i32) : i32
    %568 = llvm.getelementptr %547[%550, 4] : (!llvm.ptr<struct<".11", (i64, i64, i64, ptr, ptr, ptr)>>, i32) -> !llvm.ptr<ptr>
    llvm.store %535, %568 : !llvm.ptr<ptr>
    %569 = llvm.getelementptr %549[%567] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %570 = llvm.bitcast %568 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %570, %569 : !llvm.ptr<ptr<i8>>
    %571 = llvm.mlir.constant(5 : i32) : i32
    %572 = llvm.getelementptr %547[%550, 5] : (!llvm.ptr<struct<".11", (i64, i64, i64, ptr, ptr, ptr)>>, i32) -> !llvm.ptr<ptr>
    llvm.store %542, %572 : !llvm.ptr<ptr>
    %573 = llvm.getelementptr %549[%571] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %574 = llvm.bitcast %572 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %574, %573 : !llvm.ptr<ptr<i8>>
    %575 = llvm.mlir.constant(0 : i32) : i32
    %576 = llvm.mlir.constant(6 : i32) : i32
    %577 = llvm.inttoptr %575 : i32 to !llvm.ptr<i8>
    %578 = llvm.mlir.constant(0 : i32) : i32
    %579 = llvm.mlir.constant(1 : i32) : i32
    %580 = llvm.alloca %579 x !llvm.struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)> : (i32) -> !llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>
    %581 = llvm.mlir.constant(14 : i32) : i32
    %582 = llvm.alloca %581 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %583 = llvm.mlir.constant(0 : i32) : i32
    %584 = llvm.getelementptr %580[%578, 0] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %584 : !llvm.ptr<ptr<i8>>
    %585 = llvm.getelementptr %582[%583] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %586 = llvm.bitcast %584 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %586, %585 : !llvm.ptr<ptr<i8>>
    %587 = llvm.mlir.constant(1 : i32) : i32
    %588 = llvm.getelementptr %580[%578, 1] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %520, %588 : !llvm.ptr<ptr<ptr<i8>>>
    %589 = llvm.getelementptr %582[%587] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %590 = llvm.bitcast %588 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %590, %589 : !llvm.ptr<ptr<i8>>
    %591 = llvm.mlir.constant(2 : i32) : i32
    %592 = llvm.getelementptr %580[%578, 2] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %523, %592 : !llvm.ptr<i64>
    %593 = llvm.getelementptr %582[%591] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %594 = llvm.bitcast %592 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %594, %593 : !llvm.ptr<ptr<i8>>
    %595 = llvm.mlir.constant(3 : i32) : i32
    %596 = llvm.getelementptr %580[%578, 3] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %526, %596 : !llvm.ptr<ptr<i8>>
    %597 = llvm.getelementptr %582[%595] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %598 = llvm.bitcast %596 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %598, %597 : !llvm.ptr<ptr<i8>>
    %599 = llvm.mlir.constant(4 : i32) : i32
    %600 = llvm.getelementptr %580[%578, 4] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %515, %600 : !llvm.ptr<i64>
    %601 = llvm.getelementptr %582[%599] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %602 = llvm.bitcast %600 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %602, %601 : !llvm.ptr<ptr<i8>>
    %603 = llvm.mlir.constant(5 : i32) : i32
    %604 = llvm.getelementptr %580[%578, 5] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %604 : !llvm.ptr<i64>
    %605 = llvm.getelementptr %582[%603] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %606 = llvm.bitcast %604 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %606, %605 : !llvm.ptr<ptr<i8>>
    %607 = llvm.mlir.constant(6 : i32) : i32
    %608 = llvm.getelementptr %580[%578, 6] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %608 : !llvm.ptr<i64>
    %609 = llvm.getelementptr %582[%607] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %610 = llvm.bitcast %608 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %610, %609 : !llvm.ptr<ptr<i8>>
    %611 = llvm.mlir.constant(7 : i32) : i32
    %612 = llvm.getelementptr %580[%578, 7] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %2, %612 : !llvm.ptr<i64>
    %613 = llvm.getelementptr %582[%611] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %614 = llvm.bitcast %612 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %614, %613 : !llvm.ptr<ptr<i8>>
    %615 = llvm.mlir.constant(8 : i32) : i32
    %616 = llvm.getelementptr %580[%578, 8] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %616 : !llvm.ptr<i64>
    %617 = llvm.getelementptr %582[%615] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %618 = llvm.bitcast %616 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %618, %617 : !llvm.ptr<ptr<i8>>
    %619 = llvm.mlir.constant(9 : i32) : i32
    %620 = llvm.getelementptr %580[%578, 9] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %620 : !llvm.ptr<i64>
    %621 = llvm.getelementptr %582[%619] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %622 = llvm.bitcast %620 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %622, %621 : !llvm.ptr<ptr<i8>>
    %623 = llvm.mlir.constant(10 : i32) : i32
    %624 = llvm.getelementptr %580[%578, 10] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %575, %624 : !llvm.ptr<i32>
    %625 = llvm.getelementptr %582[%623] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %626 = llvm.bitcast %624 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %626, %625 : !llvm.ptr<ptr<i8>>
    %627 = llvm.mlir.constant(11 : i32) : i32
    %628 = llvm.getelementptr %580[%578, 11] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %577, %628 : !llvm.ptr<ptr<i8>>
    %629 = llvm.getelementptr %582[%627] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %630 = llvm.bitcast %628 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %630, %629 : !llvm.ptr<ptr<i8>>
    %631 = llvm.mlir.constant(12 : i32) : i32
    %632 = llvm.getelementptr %580[%578, 12] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %576, %632 : !llvm.ptr<i32>
    %633 = llvm.getelementptr %582[%631] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %634 = llvm.bitcast %632 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %634, %633 : !llvm.ptr<ptr<i8>>
    %635 = llvm.mlir.constant(13 : i32) : i32
    %636 = llvm.getelementptr %580[%578, 13] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %549, %636 : !llvm.ptr<ptr<ptr<i8>>>
    %637 = llvm.getelementptr %582[%635] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %638 = llvm.bitcast %636 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %638, %637 : !llvm.ptr<ptr<i8>>
    %639 = llvm.mlir.addressof @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void : !llvm.ptr<array<101 x i8>>
    %640 = llvm.mlir.constant(0 : index) : i64
    %641 = llvm.getelementptr %639[%640, %640] : (!llvm.ptr<array<101 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %641, %582) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    llvm.br ^bb7
  ^bb3:  // pred: ^bb1
    %642 = llvm.mlir.addressof @main_kernel_1_blob_gpu.binary : !llvm.ptr<array<1112 x i8>>
    %643 = llvm.mlir.constant(0 : index) : i64
    %644 = llvm.getelementptr %642[%643, %643] : (!llvm.ptr<array<1112 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %645 = llvm.mlir.constant(1 : i32) : i32
    %646 = llvm.alloca %645 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %647 = llvm.mlir.constant(0 : i32) : i32
    %648 = llvm.getelementptr %646[%647] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %644, %648 : !llvm.ptr<ptr<i8>>
    %649 = llvm.mlir.constant(1 : i64) : i64
    %650 = llvm.mlir.addressof @main_kernel_1_main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_kernel_name : !llvm.ptr<array<56 x i8>>
    %651 = llvm.mlir.constant(0 : index) : i64
    %652 = llvm.getelementptr %650[%651, %651] : (!llvm.ptr<array<56 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %653 = llvm.extractvalue %404[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %654 = llvm.extractvalue %404[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %655 = llvm.extractvalue %404[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %656 = llvm.extractvalue %404[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %657 = llvm.extractvalue %404[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %658 = llvm.mlir.constant(1 : i32) : i32
    %659 = llvm.alloca %658 x !llvm.struct<".22", (i64, ptr)> : (i32) -> !llvm.ptr<struct<".22", (i64, ptr)>>
    %660 = llvm.mlir.constant(2 : i32) : i32
    %661 = llvm.alloca %660 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %662 = llvm.mlir.constant(0 : i32) : i32
    %663 = llvm.mlir.constant(0 : i32) : i32
    %664 = llvm.getelementptr %659[%662, 0] : (!llvm.ptr<struct<".22", (i64, ptr)>>, i32) -> !llvm.ptr<i64>
    llvm.store %0, %664 : !llvm.ptr<i64>
    %665 = llvm.getelementptr %661[%663] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %666 = llvm.bitcast %664 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %666, %665 : !llvm.ptr<ptr<i8>>
    %667 = llvm.mlir.constant(1 : i32) : i32
    %668 = llvm.getelementptr %659[%662, 1] : (!llvm.ptr<struct<".22", (i64, ptr)>>, i32) -> !llvm.ptr<ptr>
    llvm.store %654, %668 : !llvm.ptr<ptr>
    %669 = llvm.getelementptr %661[%667] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %670 = llvm.bitcast %668 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %670, %669 : !llvm.ptr<ptr<i8>>
    %671 = llvm.mlir.constant(0 : i32) : i32
    %672 = llvm.mlir.constant(2 : i32) : i32
    %673 = llvm.inttoptr %671 : i32 to !llvm.ptr<i8>
    %674 = llvm.mlir.constant(0 : i32) : i32
    %675 = llvm.mlir.constant(1 : i32) : i32
    %676 = llvm.alloca %675 x !llvm.struct<".23", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)> : (i32) -> !llvm.ptr<struct<".23", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>
    %677 = llvm.mlir.constant(14 : i32) : i32
    %678 = llvm.alloca %677 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %679 = llvm.mlir.constant(0 : i32) : i32
    %680 = llvm.getelementptr %676[%674, 0] : (!llvm.ptr<struct<".23", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %680 : !llvm.ptr<ptr<i8>>
    %681 = llvm.getelementptr %678[%679] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %682 = llvm.bitcast %680 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %682, %681 : !llvm.ptr<ptr<i8>>
    %683 = llvm.mlir.constant(1 : i32) : i32
    %684 = llvm.getelementptr %676[%674, 1] : (!llvm.ptr<struct<".23", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %646, %684 : !llvm.ptr<ptr<ptr<i8>>>
    %685 = llvm.getelementptr %678[%683] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %686 = llvm.bitcast %684 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %686, %685 : !llvm.ptr<ptr<i8>>
    %687 = llvm.mlir.constant(2 : i32) : i32
    %688 = llvm.getelementptr %676[%674, 2] : (!llvm.ptr<struct<".23", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %649, %688 : !llvm.ptr<i64>
    %689 = llvm.getelementptr %678[%687] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %690 = llvm.bitcast %688 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %690, %689 : !llvm.ptr<ptr<i8>>
    %691 = llvm.mlir.constant(3 : i32) : i32
    %692 = llvm.getelementptr %676[%674, 3] : (!llvm.ptr<struct<".23", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %652, %692 : !llvm.ptr<ptr<i8>>
    %693 = llvm.getelementptr %678[%691] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %694 = llvm.bitcast %692 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %694, %693 : !llvm.ptr<ptr<i8>>
    %695 = llvm.mlir.constant(4 : i32) : i32
    %696 = llvm.getelementptr %676[%674, 4] : (!llvm.ptr<struct<".23", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %696 : !llvm.ptr<i64>
    %697 = llvm.getelementptr %678[%695] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %698 = llvm.bitcast %696 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %698, %697 : !llvm.ptr<ptr<i8>>
    %699 = llvm.mlir.constant(5 : i32) : i32
    %700 = llvm.getelementptr %676[%674, 5] : (!llvm.ptr<struct<".23", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %700 : !llvm.ptr<i64>
    %701 = llvm.getelementptr %678[%699] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %702 = llvm.bitcast %700 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %702, %701 : !llvm.ptr<ptr<i8>>
    %703 = llvm.mlir.constant(6 : i32) : i32
    %704 = llvm.getelementptr %676[%674, 6] : (!llvm.ptr<struct<".23", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %704 : !llvm.ptr<i64>
    %705 = llvm.getelementptr %678[%703] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %706 = llvm.bitcast %704 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %706, %705 : !llvm.ptr<ptr<i8>>
    %707 = llvm.mlir.constant(7 : i32) : i32
    %708 = llvm.getelementptr %676[%674, 7] : (!llvm.ptr<struct<".23", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %0, %708 : !llvm.ptr<i64>
    %709 = llvm.getelementptr %678[%707] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %710 = llvm.bitcast %708 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %710, %709 : !llvm.ptr<ptr<i8>>
    %711 = llvm.mlir.constant(8 : i32) : i32
    %712 = llvm.getelementptr %676[%674, 8] : (!llvm.ptr<struct<".23", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %712 : !llvm.ptr<i64>
    %713 = llvm.getelementptr %678[%711] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %714 = llvm.bitcast %712 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %714, %713 : !llvm.ptr<ptr<i8>>
    %715 = llvm.mlir.constant(9 : i32) : i32
    %716 = llvm.getelementptr %676[%674, 9] : (!llvm.ptr<struct<".23", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %716 : !llvm.ptr<i64>
    %717 = llvm.getelementptr %678[%715] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %718 = llvm.bitcast %716 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %718, %717 : !llvm.ptr<ptr<i8>>
    %719 = llvm.mlir.constant(10 : i32) : i32
    %720 = llvm.getelementptr %676[%674, 10] : (!llvm.ptr<struct<".23", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %671, %720 : !llvm.ptr<i32>
    %721 = llvm.getelementptr %678[%719] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %722 = llvm.bitcast %720 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %722, %721 : !llvm.ptr<ptr<i8>>
    %723 = llvm.mlir.constant(11 : i32) : i32
    %724 = llvm.getelementptr %676[%674, 11] : (!llvm.ptr<struct<".23", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %673, %724 : !llvm.ptr<ptr<i8>>
    %725 = llvm.getelementptr %678[%723] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %726 = llvm.bitcast %724 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %726, %725 : !llvm.ptr<ptr<i8>>
    %727 = llvm.mlir.constant(12 : i32) : i32
    %728 = llvm.getelementptr %676[%674, 12] : (!llvm.ptr<struct<".23", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %672, %728 : !llvm.ptr<i32>
    %729 = llvm.getelementptr %678[%727] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %730 = llvm.bitcast %728 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %730, %729 : !llvm.ptr<ptr<i8>>
    %731 = llvm.mlir.constant(13 : i32) : i32
    %732 = llvm.getelementptr %676[%674, 13] : (!llvm.ptr<struct<".23", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %661, %732 : !llvm.ptr<ptr<ptr<i8>>>
    %733 = llvm.getelementptr %678[%731] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %734 = llvm.bitcast %732 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %734, %733 : !llvm.ptr<ptr<i8>>
    %735 = llvm.mlir.addressof @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void : !llvm.ptr<array<101 x i8>>
    %736 = llvm.mlir.constant(0 : index) : i64
    %737 = llvm.getelementptr %735[%736, %736] : (!llvm.ptr<array<101 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %737, %678) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %738 = llvm.icmp "eq" %366, %6 : i64
    %739 = llvm.sub %366, %5  : i64
    %740 = llvm.udiv %739, %2  : i64
    %741 = llvm.add %740, %5  : i64
    %742 = llvm.select %738, %6, %741 : i1, i64
    %743 = llvm.mul %742, %0  : i64
    %744 = llvm.icmp "sle" %743, %6 : i64
    %745 = llvm.sub %6, %743  : i64
    %746 = llvm.sub %743, %5  : i64
    %747 = llvm.select %744, %745, %746 : i1, i64
    %748 = llvm.sdiv %747, %0  : i64
    %749 = llvm.sub %6, %748  : i64
    %750 = llvm.add %748, %5  : i64
    %751 = llvm.select %744, %749, %750 : i1, i64
    %752 = llvm.mlir.addressof @main_kernel_2_blob_gpu.binary : !llvm.ptr<array<3000 x i8>>
    %753 = llvm.mlir.constant(0 : index) : i64
    %754 = llvm.getelementptr %752[%753, %753] : (!llvm.ptr<array<3000 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %755 = llvm.mlir.constant(1 : i32) : i32
    %756 = llvm.alloca %755 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %757 = llvm.mlir.constant(0 : i32) : i32
    %758 = llvm.getelementptr %756[%757] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %754, %758 : !llvm.ptr<ptr<i8>>
    %759 = llvm.mlir.constant(1 : i64) : i64
    %760 = llvm.mlir.addressof @main_kernel_2_main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1_kernel_name : !llvm.ptr<array<58 x i8>>
    %761 = llvm.mlir.constant(0 : index) : i64
    %762 = llvm.getelementptr %760[%761, %761] : (!llvm.ptr<array<58 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %763 = llvm.extractvalue %119[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %764 = llvm.extractvalue %119[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %765 = llvm.extractvalue %119[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %766 = llvm.extractvalue %119[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %767 = llvm.extractvalue %119[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %768 = llvm.extractvalue %119[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %769 = llvm.extractvalue %119[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %770 = llvm.extractvalue %261[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %771 = llvm.extractvalue %261[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %772 = llvm.extractvalue %261[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %773 = llvm.extractvalue %261[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %774 = llvm.extractvalue %261[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %775 = llvm.extractvalue %261[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %776 = llvm.extractvalue %261[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %777 = llvm.extractvalue %404[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %778 = llvm.extractvalue %404[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %779 = llvm.extractvalue %404[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %780 = llvm.extractvalue %404[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %781 = llvm.extractvalue %404[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %782 = llvm.mlir.constant(1 : i32) : i32
    %783 = llvm.alloca %782 x !llvm.struct<".24", (i64, i64, i64, ptr, ptr, ptr)> : (i32) -> !llvm.ptr<struct<".24", (i64, i64, i64, ptr, ptr, ptr)>>
    %784 = llvm.mlir.constant(6 : i32) : i32
    %785 = llvm.alloca %784 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %786 = llvm.mlir.constant(0 : i32) : i32
    %787 = llvm.mlir.constant(0 : i32) : i32
    %788 = llvm.getelementptr %783[%786, 0] : (!llvm.ptr<struct<".24", (i64, i64, i64, ptr, ptr, ptr)>>, i32) -> !llvm.ptr<i64>
    llvm.store %0, %788 : !llvm.ptr<i64>
    %789 = llvm.getelementptr %785[%787] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %790 = llvm.bitcast %788 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %790, %789 : !llvm.ptr<ptr<i8>>
    %791 = llvm.mlir.constant(1 : i32) : i32
    %792 = llvm.getelementptr %783[%786, 1] : (!llvm.ptr<struct<".24", (i64, i64, i64, ptr, ptr, ptr)>>, i32) -> !llvm.ptr<i64>
    llvm.store %743, %792 : !llvm.ptr<i64>
    %793 = llvm.getelementptr %785[%791] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %794 = llvm.bitcast %792 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %794, %793 : !llvm.ptr<ptr<i8>>
    %795 = llvm.mlir.constant(2 : i32) : i32
    %796 = llvm.getelementptr %783[%786, 2] : (!llvm.ptr<struct<".24", (i64, i64, i64, ptr, ptr, ptr)>>, i32) -> !llvm.ptr<i64>
    llvm.store %366, %796 : !llvm.ptr<i64>
    %797 = llvm.getelementptr %785[%795] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %798 = llvm.bitcast %796 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %798, %797 : !llvm.ptr<ptr<i8>>
    %799 = llvm.mlir.constant(3 : i32) : i32
    %800 = llvm.getelementptr %783[%786, 3] : (!llvm.ptr<struct<".24", (i64, i64, i64, ptr, ptr, ptr)>>, i32) -> !llvm.ptr<ptr>
    llvm.store %764, %800 : !llvm.ptr<ptr>
    %801 = llvm.getelementptr %785[%799] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %802 = llvm.bitcast %800 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %802, %801 : !llvm.ptr<ptr<i8>>
    %803 = llvm.mlir.constant(4 : i32) : i32
    %804 = llvm.getelementptr %783[%786, 4] : (!llvm.ptr<struct<".24", (i64, i64, i64, ptr, ptr, ptr)>>, i32) -> !llvm.ptr<ptr>
    llvm.store %771, %804 : !llvm.ptr<ptr>
    %805 = llvm.getelementptr %785[%803] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %806 = llvm.bitcast %804 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %806, %805 : !llvm.ptr<ptr<i8>>
    %807 = llvm.mlir.constant(5 : i32) : i32
    %808 = llvm.getelementptr %783[%786, 5] : (!llvm.ptr<struct<".24", (i64, i64, i64, ptr, ptr, ptr)>>, i32) -> !llvm.ptr<ptr>
    llvm.store %778, %808 : !llvm.ptr<ptr>
    %809 = llvm.getelementptr %785[%807] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %810 = llvm.bitcast %808 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %810, %809 : !llvm.ptr<ptr<i8>>
    %811 = llvm.mlir.constant(0 : i32) : i32
    %812 = llvm.mlir.constant(6 : i32) : i32
    %813 = llvm.inttoptr %811 : i32 to !llvm.ptr<i8>
    %814 = llvm.mlir.constant(0 : i32) : i32
    %815 = llvm.mlir.constant(1 : i32) : i32
    %816 = llvm.alloca %815 x !llvm.struct<".25", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)> : (i32) -> !llvm.ptr<struct<".25", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>
    %817 = llvm.mlir.constant(14 : i32) : i32
    %818 = llvm.alloca %817 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %819 = llvm.mlir.constant(0 : i32) : i32
    %820 = llvm.getelementptr %816[%814, 0] : (!llvm.ptr<struct<".25", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %820 : !llvm.ptr<ptr<i8>>
    %821 = llvm.getelementptr %818[%819] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %822 = llvm.bitcast %820 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %822, %821 : !llvm.ptr<ptr<i8>>
    %823 = llvm.mlir.constant(1 : i32) : i32
    %824 = llvm.getelementptr %816[%814, 1] : (!llvm.ptr<struct<".25", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %756, %824 : !llvm.ptr<ptr<ptr<i8>>>
    %825 = llvm.getelementptr %818[%823] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %826 = llvm.bitcast %824 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %826, %825 : !llvm.ptr<ptr<i8>>
    %827 = llvm.mlir.constant(2 : i32) : i32
    %828 = llvm.getelementptr %816[%814, 2] : (!llvm.ptr<struct<".25", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %759, %828 : !llvm.ptr<i64>
    %829 = llvm.getelementptr %818[%827] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %830 = llvm.bitcast %828 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %830, %829 : !llvm.ptr<ptr<i8>>
    %831 = llvm.mlir.constant(3 : i32) : i32
    %832 = llvm.getelementptr %816[%814, 3] : (!llvm.ptr<struct<".25", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %762, %832 : !llvm.ptr<ptr<i8>>
    %833 = llvm.getelementptr %818[%831] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %834 = llvm.bitcast %832 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %834, %833 : !llvm.ptr<ptr<i8>>
    %835 = llvm.mlir.constant(4 : i32) : i32
    %836 = llvm.getelementptr %816[%814, 4] : (!llvm.ptr<struct<".25", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %751, %836 : !llvm.ptr<i64>
    %837 = llvm.getelementptr %818[%835] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %838 = llvm.bitcast %836 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %838, %837 : !llvm.ptr<ptr<i8>>
    %839 = llvm.mlir.constant(5 : i32) : i32
    %840 = llvm.getelementptr %816[%814, 5] : (!llvm.ptr<struct<".25", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %840 : !llvm.ptr<i64>
    %841 = llvm.getelementptr %818[%839] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %842 = llvm.bitcast %840 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %842, %841 : !llvm.ptr<ptr<i8>>
    %843 = llvm.mlir.constant(6 : i32) : i32
    %844 = llvm.getelementptr %816[%814, 6] : (!llvm.ptr<struct<".25", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %844 : !llvm.ptr<i64>
    %845 = llvm.getelementptr %818[%843] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %846 = llvm.bitcast %844 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %846, %845 : !llvm.ptr<ptr<i8>>
    %847 = llvm.mlir.constant(7 : i32) : i32
    %848 = llvm.getelementptr %816[%814, 7] : (!llvm.ptr<struct<".25", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %0, %848 : !llvm.ptr<i64>
    %849 = llvm.getelementptr %818[%847] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %850 = llvm.bitcast %848 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %850, %849 : !llvm.ptr<ptr<i8>>
    %851 = llvm.mlir.constant(8 : i32) : i32
    %852 = llvm.getelementptr %816[%814, 8] : (!llvm.ptr<struct<".25", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %852 : !llvm.ptr<i64>
    %853 = llvm.getelementptr %818[%851] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %854 = llvm.bitcast %852 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %854, %853 : !llvm.ptr<ptr<i8>>
    %855 = llvm.mlir.constant(9 : i32) : i32
    %856 = llvm.getelementptr %816[%814, 9] : (!llvm.ptr<struct<".25", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %856 : !llvm.ptr<i64>
    %857 = llvm.getelementptr %818[%855] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %858 = llvm.bitcast %856 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %858, %857 : !llvm.ptr<ptr<i8>>
    %859 = llvm.mlir.constant(10 : i32) : i32
    %860 = llvm.getelementptr %816[%814, 10] : (!llvm.ptr<struct<".25", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %811, %860 : !llvm.ptr<i32>
    %861 = llvm.getelementptr %818[%859] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %862 = llvm.bitcast %860 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %862, %861 : !llvm.ptr<ptr<i8>>
    %863 = llvm.mlir.constant(11 : i32) : i32
    %864 = llvm.getelementptr %816[%814, 11] : (!llvm.ptr<struct<".25", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %813, %864 : !llvm.ptr<ptr<i8>>
    %865 = llvm.getelementptr %818[%863] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %866 = llvm.bitcast %864 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %866, %865 : !llvm.ptr<ptr<i8>>
    %867 = llvm.mlir.constant(12 : i32) : i32
    %868 = llvm.getelementptr %816[%814, 12] : (!llvm.ptr<struct<".25", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %812, %868 : !llvm.ptr<i32>
    %869 = llvm.getelementptr %818[%867] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %870 = llvm.bitcast %868 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %870, %869 : !llvm.ptr<ptr<i8>>
    %871 = llvm.mlir.constant(13 : i32) : i32
    %872 = llvm.getelementptr %816[%814, 13] : (!llvm.ptr<struct<".25", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %785, %872 : !llvm.ptr<ptr<ptr<i8>>>
    %873 = llvm.getelementptr %818[%871] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %874 = llvm.bitcast %872 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %874, %873 : !llvm.ptr<ptr<i8>>
    %875 = llvm.mlir.addressof @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void : !llvm.ptr<array<101 x i8>>
    %876 = llvm.mlir.constant(0 : index) : i64
    %877 = llvm.getelementptr %875[%876, %876] : (!llvm.ptr<array<101 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %877, %818) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    llvm.br ^bb7
  ^bb4:  // pred: ^bb0
    %878 = llvm.icmp "slt" %366, %5 : i64
    llvm.cond_br %878, ^bb5, ^bb6
  ^bb5:  // pred: ^bb4
    %879 = llvm.mlir.addressof @main_kernel_3_blob_gpu.binary : !llvm.ptr<array<1112 x i8>>
    %880 = llvm.mlir.constant(0 : index) : i64
    %881 = llvm.getelementptr %879[%880, %880] : (!llvm.ptr<array<1112 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %882 = llvm.mlir.constant(1 : i32) : i32
    %883 = llvm.alloca %882 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %884 = llvm.mlir.constant(0 : i32) : i32
    %885 = llvm.getelementptr %883[%884] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %881, %885 : !llvm.ptr<ptr<i8>>
    %886 = llvm.mlir.constant(1 : i64) : i64
    %887 = llvm.mlir.addressof @main_kernel_3_main_kColReduction_reduce__6_1_0___thread_tile_h32_kernel_name : !llvm.ptr<array<51 x i8>>
    %888 = llvm.mlir.constant(0 : index) : i64
    %889 = llvm.getelementptr %887[%888, %888] : (!llvm.ptr<array<51 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %890 = llvm.extractvalue %404[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %891 = llvm.extractvalue %404[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %892 = llvm.extractvalue %404[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %893 = llvm.extractvalue %404[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %894 = llvm.extractvalue %404[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %895 = llvm.mlir.constant(1 : i32) : i32
    %896 = llvm.alloca %895 x !llvm.struct<".26", (i64, ptr)> : (i32) -> !llvm.ptr<struct<".26", (i64, ptr)>>
    %897 = llvm.mlir.constant(2 : i32) : i32
    %898 = llvm.alloca %897 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %899 = llvm.mlir.constant(0 : i32) : i32
    %900 = llvm.mlir.constant(0 : i32) : i32
    %901 = llvm.getelementptr %896[%899, 0] : (!llvm.ptr<struct<".26", (i64, ptr)>>, i32) -> !llvm.ptr<i64>
    llvm.store %2, %901 : !llvm.ptr<i64>
    %902 = llvm.getelementptr %898[%900] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %903 = llvm.bitcast %901 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %903, %902 : !llvm.ptr<ptr<i8>>
    %904 = llvm.mlir.constant(1 : i32) : i32
    %905 = llvm.getelementptr %896[%899, 1] : (!llvm.ptr<struct<".26", (i64, ptr)>>, i32) -> !llvm.ptr<ptr>
    llvm.store %891, %905 : !llvm.ptr<ptr>
    %906 = llvm.getelementptr %898[%904] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %907 = llvm.bitcast %905 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %907, %906 : !llvm.ptr<ptr<i8>>
    %908 = llvm.mlir.constant(0 : i32) : i32
    %909 = llvm.mlir.constant(2 : i32) : i32
    %910 = llvm.inttoptr %908 : i32 to !llvm.ptr<i8>
    %911 = llvm.mlir.constant(0 : i32) : i32
    %912 = llvm.mlir.constant(1 : i32) : i32
    %913 = llvm.alloca %912 x !llvm.struct<".27", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)> : (i32) -> !llvm.ptr<struct<".27", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>
    %914 = llvm.mlir.constant(14 : i32) : i32
    %915 = llvm.alloca %914 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %916 = llvm.mlir.constant(0 : i32) : i32
    %917 = llvm.getelementptr %913[%911, 0] : (!llvm.ptr<struct<".27", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %917 : !llvm.ptr<ptr<i8>>
    %918 = llvm.getelementptr %915[%916] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %919 = llvm.bitcast %917 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %919, %918 : !llvm.ptr<ptr<i8>>
    %920 = llvm.mlir.constant(1 : i32) : i32
    %921 = llvm.getelementptr %913[%911, 1] : (!llvm.ptr<struct<".27", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %883, %921 : !llvm.ptr<ptr<ptr<i8>>>
    %922 = llvm.getelementptr %915[%920] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %923 = llvm.bitcast %921 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %923, %922 : !llvm.ptr<ptr<i8>>
    %924 = llvm.mlir.constant(2 : i32) : i32
    %925 = llvm.getelementptr %913[%911, 2] : (!llvm.ptr<struct<".27", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %886, %925 : !llvm.ptr<i64>
    %926 = llvm.getelementptr %915[%924] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %927 = llvm.bitcast %925 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %927, %926 : !llvm.ptr<ptr<i8>>
    %928 = llvm.mlir.constant(3 : i32) : i32
    %929 = llvm.getelementptr %913[%911, 3] : (!llvm.ptr<struct<".27", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %889, %929 : !llvm.ptr<ptr<i8>>
    %930 = llvm.getelementptr %915[%928] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %931 = llvm.bitcast %929 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %931, %930 : !llvm.ptr<ptr<i8>>
    %932 = llvm.mlir.constant(4 : i32) : i32
    %933 = llvm.getelementptr %913[%911, 4] : (!llvm.ptr<struct<".27", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %933 : !llvm.ptr<i64>
    %934 = llvm.getelementptr %915[%932] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %935 = llvm.bitcast %933 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %935, %934 : !llvm.ptr<ptr<i8>>
    %936 = llvm.mlir.constant(5 : i32) : i32
    %937 = llvm.getelementptr %913[%911, 5] : (!llvm.ptr<struct<".27", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %937 : !llvm.ptr<i64>
    %938 = llvm.getelementptr %915[%936] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %939 = llvm.bitcast %937 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %939, %938 : !llvm.ptr<ptr<i8>>
    %940 = llvm.mlir.constant(6 : i32) : i32
    %941 = llvm.getelementptr %913[%911, 6] : (!llvm.ptr<struct<".27", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %941 : !llvm.ptr<i64>
    %942 = llvm.getelementptr %915[%940] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %943 = llvm.bitcast %941 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %943, %942 : !llvm.ptr<ptr<i8>>
    %944 = llvm.mlir.constant(7 : i32) : i32
    %945 = llvm.getelementptr %913[%911, 7] : (!llvm.ptr<struct<".27", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %2, %945 : !llvm.ptr<i64>
    %946 = llvm.getelementptr %915[%944] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %947 = llvm.bitcast %945 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %947, %946 : !llvm.ptr<ptr<i8>>
    %948 = llvm.mlir.constant(8 : i32) : i32
    %949 = llvm.getelementptr %913[%911, 8] : (!llvm.ptr<struct<".27", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %949 : !llvm.ptr<i64>
    %950 = llvm.getelementptr %915[%948] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %951 = llvm.bitcast %949 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %951, %950 : !llvm.ptr<ptr<i8>>
    %952 = llvm.mlir.constant(9 : i32) : i32
    %953 = llvm.getelementptr %913[%911, 9] : (!llvm.ptr<struct<".27", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %953 : !llvm.ptr<i64>
    %954 = llvm.getelementptr %915[%952] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %955 = llvm.bitcast %953 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %955, %954 : !llvm.ptr<ptr<i8>>
    %956 = llvm.mlir.constant(10 : i32) : i32
    %957 = llvm.getelementptr %913[%911, 10] : (!llvm.ptr<struct<".27", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %908, %957 : !llvm.ptr<i32>
    %958 = llvm.getelementptr %915[%956] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %959 = llvm.bitcast %957 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %959, %958 : !llvm.ptr<ptr<i8>>
    %960 = llvm.mlir.constant(11 : i32) : i32
    %961 = llvm.getelementptr %913[%911, 11] : (!llvm.ptr<struct<".27", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %910, %961 : !llvm.ptr<ptr<i8>>
    %962 = llvm.getelementptr %915[%960] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %963 = llvm.bitcast %961 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %963, %962 : !llvm.ptr<ptr<i8>>
    %964 = llvm.mlir.constant(12 : i32) : i32
    %965 = llvm.getelementptr %913[%911, 12] : (!llvm.ptr<struct<".27", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %909, %965 : !llvm.ptr<i32>
    %966 = llvm.getelementptr %915[%964] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %967 = llvm.bitcast %965 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %967, %966 : !llvm.ptr<ptr<i8>>
    %968 = llvm.mlir.constant(13 : i32) : i32
    %969 = llvm.getelementptr %913[%911, 13] : (!llvm.ptr<struct<".27", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %898, %969 : !llvm.ptr<ptr<ptr<i8>>>
    %970 = llvm.getelementptr %915[%968] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %971 = llvm.bitcast %969 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %971, %970 : !llvm.ptr<ptr<i8>>
    %972 = llvm.mlir.addressof @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void : !llvm.ptr<array<101 x i8>>
    %973 = llvm.mlir.constant(0 : index) : i64
    %974 = llvm.getelementptr %972[%973, %973] : (!llvm.ptr<array<101 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %974, %915) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %975 = llvm.icmp "eq" %366, %6 : i64
    %976 = llvm.sub %366, %5  : i64
    %977 = llvm.udiv %976, %1  : i64
    %978 = llvm.add %977, %5  : i64
    %979 = llvm.select %975, %6, %978 : i1, i64
    %980 = llvm.mul %979, %2  : i64
    %981 = llvm.icmp "sle" %980, %6 : i64
    %982 = llvm.sub %6, %980  : i64
    %983 = llvm.sub %980, %5  : i64
    %984 = llvm.select %981, %982, %983 : i1, i64
    %985 = llvm.sdiv %984, %2  : i64
    %986 = llvm.sub %6, %985  : i64
    %987 = llvm.add %985, %5  : i64
    %988 = llvm.select %981, %986, %987 : i1, i64
    %989 = llvm.mlir.addressof @main_kernel_4_blob_gpu.binary : !llvm.ptr<array<3368 x i8>>
    %990 = llvm.mlir.constant(0 : index) : i64
    %991 = llvm.getelementptr %989[%990, %990] : (!llvm.ptr<array<3368 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %992 = llvm.mlir.constant(1 : i32) : i32
    %993 = llvm.alloca %992 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %994 = llvm.mlir.constant(0 : i32) : i32
    %995 = llvm.getelementptr %993[%994] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %991, %995 : !llvm.ptr<ptr<i8>>
    %996 = llvm.mlir.constant(1 : i64) : i64
    %997 = llvm.mlir.addressof @main_kernel_4_main_kColReduction_reduce__6_1_0___thread_tile_h32_1_kernel_name : !llvm.ptr<array<53 x i8>>
    %998 = llvm.mlir.constant(0 : index) : i64
    %999 = llvm.getelementptr %997[%998, %998] : (!llvm.ptr<array<53 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %1000 = llvm.extractvalue %119[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %1001 = llvm.extractvalue %119[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %1002 = llvm.extractvalue %119[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %1003 = llvm.extractvalue %119[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %1004 = llvm.extractvalue %119[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %1005 = llvm.extractvalue %119[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %1006 = llvm.extractvalue %119[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %1007 = llvm.extractvalue %261[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %1008 = llvm.extractvalue %261[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %1009 = llvm.extractvalue %261[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %1010 = llvm.extractvalue %261[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %1011 = llvm.extractvalue %261[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %1012 = llvm.extractvalue %261[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %1013 = llvm.extractvalue %261[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %1014 = llvm.extractvalue %404[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %1015 = llvm.extractvalue %404[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %1016 = llvm.extractvalue %404[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %1017 = llvm.extractvalue %404[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %1018 = llvm.extractvalue %404[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %1019 = llvm.mlir.constant(1 : i32) : i32
    %1020 = llvm.alloca %1019 x !llvm.struct<".28", (i64, i64, i64, i64, i64, ptr, ptr, ptr)> : (i32) -> !llvm.ptr<struct<".28", (i64, i64, i64, i64, i64, ptr, ptr, ptr)>>
    %1021 = llvm.mlir.constant(8 : i32) : i32
    %1022 = llvm.alloca %1021 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %1023 = llvm.mlir.constant(0 : i32) : i32
    %1024 = llvm.mlir.constant(0 : i32) : i32
    %1025 = llvm.getelementptr %1020[%1023, 0] : (!llvm.ptr<struct<".28", (i64, i64, i64, i64, i64, ptr, ptr, ptr)>>, i32) -> !llvm.ptr<i64>
    llvm.store %49, %1025 : !llvm.ptr<i64>
    %1026 = llvm.getelementptr %1022[%1024] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1027 = llvm.bitcast %1025 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1027, %1026 : !llvm.ptr<ptr<i8>>
    %1028 = llvm.mlir.constant(1 : i32) : i32
    %1029 = llvm.getelementptr %1020[%1023, 1] : (!llvm.ptr<struct<".28", (i64, i64, i64, i64, i64, ptr, ptr, ptr)>>, i32) -> !llvm.ptr<i64>
    llvm.store %50, %1029 : !llvm.ptr<i64>
    %1030 = llvm.getelementptr %1022[%1028] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1031 = llvm.bitcast %1029 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1031, %1030 : !llvm.ptr<ptr<i8>>
    %1032 = llvm.mlir.constant(2 : i32) : i32
    %1033 = llvm.getelementptr %1020[%1023, 2] : (!llvm.ptr<struct<".28", (i64, i64, i64, i64, i64, ptr, ptr, ptr)>>, i32) -> !llvm.ptr<i64>
    llvm.store %2, %1033 : !llvm.ptr<i64>
    %1034 = llvm.getelementptr %1022[%1032] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1035 = llvm.bitcast %1033 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1035, %1034 : !llvm.ptr<ptr<i8>>
    %1036 = llvm.mlir.constant(3 : i32) : i32
    %1037 = llvm.getelementptr %1020[%1023, 3] : (!llvm.ptr<struct<".28", (i64, i64, i64, i64, i64, ptr, ptr, ptr)>>, i32) -> !llvm.ptr<i64>
    llvm.store %980, %1037 : !llvm.ptr<i64>
    %1038 = llvm.getelementptr %1022[%1036] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1039 = llvm.bitcast %1037 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1039, %1038 : !llvm.ptr<ptr<i8>>
    %1040 = llvm.mlir.constant(4 : i32) : i32
    %1041 = llvm.getelementptr %1020[%1023, 4] : (!llvm.ptr<struct<".28", (i64, i64, i64, i64, i64, ptr, ptr, ptr)>>, i32) -> !llvm.ptr<i64>
    llvm.store %366, %1041 : !llvm.ptr<i64>
    %1042 = llvm.getelementptr %1022[%1040] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1043 = llvm.bitcast %1041 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1043, %1042 : !llvm.ptr<ptr<i8>>
    %1044 = llvm.mlir.constant(5 : i32) : i32
    %1045 = llvm.getelementptr %1020[%1023, 5] : (!llvm.ptr<struct<".28", (i64, i64, i64, i64, i64, ptr, ptr, ptr)>>, i32) -> !llvm.ptr<ptr>
    llvm.store %1001, %1045 : !llvm.ptr<ptr>
    %1046 = llvm.getelementptr %1022[%1044] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1047 = llvm.bitcast %1045 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %1047, %1046 : !llvm.ptr<ptr<i8>>
    %1048 = llvm.mlir.constant(6 : i32) : i32
    %1049 = llvm.getelementptr %1020[%1023, 6] : (!llvm.ptr<struct<".28", (i64, i64, i64, i64, i64, ptr, ptr, ptr)>>, i32) -> !llvm.ptr<ptr>
    llvm.store %1008, %1049 : !llvm.ptr<ptr>
    %1050 = llvm.getelementptr %1022[%1048] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1051 = llvm.bitcast %1049 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %1051, %1050 : !llvm.ptr<ptr<i8>>
    %1052 = llvm.mlir.constant(7 : i32) : i32
    %1053 = llvm.getelementptr %1020[%1023, 7] : (!llvm.ptr<struct<".28", (i64, i64, i64, i64, i64, ptr, ptr, ptr)>>, i32) -> !llvm.ptr<ptr>
    llvm.store %1015, %1053 : !llvm.ptr<ptr>
    %1054 = llvm.getelementptr %1022[%1052] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1055 = llvm.bitcast %1053 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %1055, %1054 : !llvm.ptr<ptr<i8>>
    %1056 = llvm.mlir.constant(0 : i32) : i32
    %1057 = llvm.mlir.constant(8 : i32) : i32
    %1058 = llvm.inttoptr %1056 : i32 to !llvm.ptr<i8>
    %1059 = llvm.mlir.constant(0 : i32) : i32
    %1060 = llvm.mlir.constant(1 : i32) : i32
    %1061 = llvm.alloca %1060 x !llvm.struct<".29", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)> : (i32) -> !llvm.ptr<struct<".29", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>
    %1062 = llvm.mlir.constant(14 : i32) : i32
    %1063 = llvm.alloca %1062 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %1064 = llvm.mlir.constant(0 : i32) : i32
    %1065 = llvm.getelementptr %1061[%1059, 0] : (!llvm.ptr<struct<".29", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %1065 : !llvm.ptr<ptr<i8>>
    %1066 = llvm.getelementptr %1063[%1064] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1067 = llvm.bitcast %1065 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %1067, %1066 : !llvm.ptr<ptr<i8>>
    %1068 = llvm.mlir.constant(1 : i32) : i32
    %1069 = llvm.getelementptr %1061[%1059, 1] : (!llvm.ptr<struct<".29", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %993, %1069 : !llvm.ptr<ptr<ptr<i8>>>
    %1070 = llvm.getelementptr %1063[%1068] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1071 = llvm.bitcast %1069 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %1071, %1070 : !llvm.ptr<ptr<i8>>
    %1072 = llvm.mlir.constant(2 : i32) : i32
    %1073 = llvm.getelementptr %1061[%1059, 2] : (!llvm.ptr<struct<".29", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %996, %1073 : !llvm.ptr<i64>
    %1074 = llvm.getelementptr %1063[%1072] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1075 = llvm.bitcast %1073 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1075, %1074 : !llvm.ptr<ptr<i8>>
    %1076 = llvm.mlir.constant(3 : i32) : i32
    %1077 = llvm.getelementptr %1061[%1059, 3] : (!llvm.ptr<struct<".29", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %999, %1077 : !llvm.ptr<ptr<i8>>
    %1078 = llvm.getelementptr %1063[%1076] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1079 = llvm.bitcast %1077 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %1079, %1078 : !llvm.ptr<ptr<i8>>
    %1080 = llvm.mlir.constant(4 : i32) : i32
    %1081 = llvm.getelementptr %1061[%1059, 4] : (!llvm.ptr<struct<".29", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %988, %1081 : !llvm.ptr<i64>
    %1082 = llvm.getelementptr %1063[%1080] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1083 = llvm.bitcast %1081 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1083, %1082 : !llvm.ptr<ptr<i8>>
    %1084 = llvm.mlir.constant(5 : i32) : i32
    %1085 = llvm.getelementptr %1061[%1059, 5] : (!llvm.ptr<struct<".29", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %1085 : !llvm.ptr<i64>
    %1086 = llvm.getelementptr %1063[%1084] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1087 = llvm.bitcast %1085 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1087, %1086 : !llvm.ptr<ptr<i8>>
    %1088 = llvm.mlir.constant(6 : i32) : i32
    %1089 = llvm.getelementptr %1061[%1059, 6] : (!llvm.ptr<struct<".29", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %1089 : !llvm.ptr<i64>
    %1090 = llvm.getelementptr %1063[%1088] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1091 = llvm.bitcast %1089 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1091, %1090 : !llvm.ptr<ptr<i8>>
    %1092 = llvm.mlir.constant(7 : i32) : i32
    %1093 = llvm.getelementptr %1061[%1059, 7] : (!llvm.ptr<struct<".29", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %2, %1093 : !llvm.ptr<i64>
    %1094 = llvm.getelementptr %1063[%1092] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1095 = llvm.bitcast %1093 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1095, %1094 : !llvm.ptr<ptr<i8>>
    %1096 = llvm.mlir.constant(8 : i32) : i32
    %1097 = llvm.getelementptr %1061[%1059, 8] : (!llvm.ptr<struct<".29", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %1097 : !llvm.ptr<i64>
    %1098 = llvm.getelementptr %1063[%1096] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1099 = llvm.bitcast %1097 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1099, %1098 : !llvm.ptr<ptr<i8>>
    %1100 = llvm.mlir.constant(9 : i32) : i32
    %1101 = llvm.getelementptr %1061[%1059, 9] : (!llvm.ptr<struct<".29", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %1101 : !llvm.ptr<i64>
    %1102 = llvm.getelementptr %1063[%1100] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1103 = llvm.bitcast %1101 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1103, %1102 : !llvm.ptr<ptr<i8>>
    %1104 = llvm.mlir.constant(10 : i32) : i32
    %1105 = llvm.getelementptr %1061[%1059, 10] : (!llvm.ptr<struct<".29", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %1056, %1105 : !llvm.ptr<i32>
    %1106 = llvm.getelementptr %1063[%1104] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1107 = llvm.bitcast %1105 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %1107, %1106 : !llvm.ptr<ptr<i8>>
    %1108 = llvm.mlir.constant(11 : i32) : i32
    %1109 = llvm.getelementptr %1061[%1059, 11] : (!llvm.ptr<struct<".29", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %1058, %1109 : !llvm.ptr<ptr<i8>>
    %1110 = llvm.getelementptr %1063[%1108] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1111 = llvm.bitcast %1109 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %1111, %1110 : !llvm.ptr<ptr<i8>>
    %1112 = llvm.mlir.constant(12 : i32) : i32
    %1113 = llvm.getelementptr %1061[%1059, 12] : (!llvm.ptr<struct<".29", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %1057, %1113 : !llvm.ptr<i32>
    %1114 = llvm.getelementptr %1063[%1112] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1115 = llvm.bitcast %1113 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %1115, %1114 : !llvm.ptr<ptr<i8>>
    %1116 = llvm.mlir.constant(13 : i32) : i32
    %1117 = llvm.getelementptr %1061[%1059, 13] : (!llvm.ptr<struct<".29", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %1022, %1117 : !llvm.ptr<ptr<ptr<i8>>>
    %1118 = llvm.getelementptr %1063[%1116] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1119 = llvm.bitcast %1117 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %1119, %1118 : !llvm.ptr<ptr<i8>>
    %1120 = llvm.mlir.addressof @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void : !llvm.ptr<array<101 x i8>>
    %1121 = llvm.mlir.constant(0 : index) : i64
    %1122 = llvm.getelementptr %1120[%1121, %1121] : (!llvm.ptr<array<101 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %1122, %1063) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    llvm.br ^bb7
  ^bb6:  // pred: ^bb4
    %1123 = llvm.mlir.addressof @main_kernel_5_blob_gpu.binary : !llvm.ptr<array<1112 x i8>>
    %1124 = llvm.mlir.constant(0 : index) : i64
    %1125 = llvm.getelementptr %1123[%1124, %1124] : (!llvm.ptr<array<1112 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %1126 = llvm.mlir.constant(1 : i32) : i32
    %1127 = llvm.alloca %1126 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %1128 = llvm.mlir.constant(0 : i32) : i32
    %1129 = llvm.getelementptr %1127[%1128] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %1125, %1129 : !llvm.ptr<ptr<i8>>
    %1130 = llvm.mlir.constant(1 : i64) : i64
    %1131 = llvm.mlir.addressof @main_kernel_5_main_kColReduction_reduce__6_1_0___block_tile_h64_kernel_name : !llvm.ptr<array<50 x i8>>
    %1132 = llvm.mlir.constant(0 : index) : i64
    %1133 = llvm.getelementptr %1131[%1132, %1132] : (!llvm.ptr<array<50 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %1134 = llvm.extractvalue %404[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %1135 = llvm.extractvalue %404[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %1136 = llvm.extractvalue %404[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %1137 = llvm.extractvalue %404[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %1138 = llvm.extractvalue %404[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %1139 = llvm.mlir.constant(1 : i32) : i32
    %1140 = llvm.alloca %1139 x !llvm.struct<".30", (i64, ptr)> : (i32) -> !llvm.ptr<struct<".30", (i64, ptr)>>
    %1141 = llvm.mlir.constant(2 : i32) : i32
    %1142 = llvm.alloca %1141 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %1143 = llvm.mlir.constant(0 : i32) : i32
    %1144 = llvm.mlir.constant(0 : i32) : i32
    %1145 = llvm.getelementptr %1140[%1143, 0] : (!llvm.ptr<struct<".30", (i64, ptr)>>, i32) -> !llvm.ptr<i64>
    llvm.store %0, %1145 : !llvm.ptr<i64>
    %1146 = llvm.getelementptr %1142[%1144] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1147 = llvm.bitcast %1145 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1147, %1146 : !llvm.ptr<ptr<i8>>
    %1148 = llvm.mlir.constant(1 : i32) : i32
    %1149 = llvm.getelementptr %1140[%1143, 1] : (!llvm.ptr<struct<".30", (i64, ptr)>>, i32) -> !llvm.ptr<ptr>
    llvm.store %1135, %1149 : !llvm.ptr<ptr>
    %1150 = llvm.getelementptr %1142[%1148] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1151 = llvm.bitcast %1149 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %1151, %1150 : !llvm.ptr<ptr<i8>>
    %1152 = llvm.mlir.constant(0 : i32) : i32
    %1153 = llvm.mlir.constant(2 : i32) : i32
    %1154 = llvm.inttoptr %1152 : i32 to !llvm.ptr<i8>
    %1155 = llvm.mlir.constant(0 : i32) : i32
    %1156 = llvm.mlir.constant(1 : i32) : i32
    %1157 = llvm.alloca %1156 x !llvm.struct<".31", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)> : (i32) -> !llvm.ptr<struct<".31", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>
    %1158 = llvm.mlir.constant(14 : i32) : i32
    %1159 = llvm.alloca %1158 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %1160 = llvm.mlir.constant(0 : i32) : i32
    %1161 = llvm.getelementptr %1157[%1155, 0] : (!llvm.ptr<struct<".31", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %1161 : !llvm.ptr<ptr<i8>>
    %1162 = llvm.getelementptr %1159[%1160] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1163 = llvm.bitcast %1161 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %1163, %1162 : !llvm.ptr<ptr<i8>>
    %1164 = llvm.mlir.constant(1 : i32) : i32
    %1165 = llvm.getelementptr %1157[%1155, 1] : (!llvm.ptr<struct<".31", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %1127, %1165 : !llvm.ptr<ptr<ptr<i8>>>
    %1166 = llvm.getelementptr %1159[%1164] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1167 = llvm.bitcast %1165 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %1167, %1166 : !llvm.ptr<ptr<i8>>
    %1168 = llvm.mlir.constant(2 : i32) : i32
    %1169 = llvm.getelementptr %1157[%1155, 2] : (!llvm.ptr<struct<".31", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %1130, %1169 : !llvm.ptr<i64>
    %1170 = llvm.getelementptr %1159[%1168] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1171 = llvm.bitcast %1169 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1171, %1170 : !llvm.ptr<ptr<i8>>
    %1172 = llvm.mlir.constant(3 : i32) : i32
    %1173 = llvm.getelementptr %1157[%1155, 3] : (!llvm.ptr<struct<".31", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %1133, %1173 : !llvm.ptr<ptr<i8>>
    %1174 = llvm.getelementptr %1159[%1172] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1175 = llvm.bitcast %1173 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %1175, %1174 : !llvm.ptr<ptr<i8>>
    %1176 = llvm.mlir.constant(4 : i32) : i32
    %1177 = llvm.getelementptr %1157[%1155, 4] : (!llvm.ptr<struct<".31", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %1177 : !llvm.ptr<i64>
    %1178 = llvm.getelementptr %1159[%1176] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1179 = llvm.bitcast %1177 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1179, %1178 : !llvm.ptr<ptr<i8>>
    %1180 = llvm.mlir.constant(5 : i32) : i32
    %1181 = llvm.getelementptr %1157[%1155, 5] : (!llvm.ptr<struct<".31", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %1181 : !llvm.ptr<i64>
    %1182 = llvm.getelementptr %1159[%1180] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1183 = llvm.bitcast %1181 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1183, %1182 : !llvm.ptr<ptr<i8>>
    %1184 = llvm.mlir.constant(6 : i32) : i32
    %1185 = llvm.getelementptr %1157[%1155, 6] : (!llvm.ptr<struct<".31", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %1185 : !llvm.ptr<i64>
    %1186 = llvm.getelementptr %1159[%1184] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1187 = llvm.bitcast %1185 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1187, %1186 : !llvm.ptr<ptr<i8>>
    %1188 = llvm.mlir.constant(7 : i32) : i32
    %1189 = llvm.getelementptr %1157[%1155, 7] : (!llvm.ptr<struct<".31", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %0, %1189 : !llvm.ptr<i64>
    %1190 = llvm.getelementptr %1159[%1188] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1191 = llvm.bitcast %1189 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1191, %1190 : !llvm.ptr<ptr<i8>>
    %1192 = llvm.mlir.constant(8 : i32) : i32
    %1193 = llvm.getelementptr %1157[%1155, 8] : (!llvm.ptr<struct<".31", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %1193 : !llvm.ptr<i64>
    %1194 = llvm.getelementptr %1159[%1192] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1195 = llvm.bitcast %1193 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1195, %1194 : !llvm.ptr<ptr<i8>>
    %1196 = llvm.mlir.constant(9 : i32) : i32
    %1197 = llvm.getelementptr %1157[%1155, 9] : (!llvm.ptr<struct<".31", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %1197 : !llvm.ptr<i64>
    %1198 = llvm.getelementptr %1159[%1196] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1199 = llvm.bitcast %1197 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1199, %1198 : !llvm.ptr<ptr<i8>>
    %1200 = llvm.mlir.constant(10 : i32) : i32
    %1201 = llvm.getelementptr %1157[%1155, 10] : (!llvm.ptr<struct<".31", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %1152, %1201 : !llvm.ptr<i32>
    %1202 = llvm.getelementptr %1159[%1200] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1203 = llvm.bitcast %1201 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %1203, %1202 : !llvm.ptr<ptr<i8>>
    %1204 = llvm.mlir.constant(11 : i32) : i32
    %1205 = llvm.getelementptr %1157[%1155, 11] : (!llvm.ptr<struct<".31", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %1154, %1205 : !llvm.ptr<ptr<i8>>
    %1206 = llvm.getelementptr %1159[%1204] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1207 = llvm.bitcast %1205 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %1207, %1206 : !llvm.ptr<ptr<i8>>
    %1208 = llvm.mlir.constant(12 : i32) : i32
    %1209 = llvm.getelementptr %1157[%1155, 12] : (!llvm.ptr<struct<".31", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %1153, %1209 : !llvm.ptr<i32>
    %1210 = llvm.getelementptr %1159[%1208] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1211 = llvm.bitcast %1209 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %1211, %1210 : !llvm.ptr<ptr<i8>>
    %1212 = llvm.mlir.constant(13 : i32) : i32
    %1213 = llvm.getelementptr %1157[%1155, 13] : (!llvm.ptr<struct<".31", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %1142, %1213 : !llvm.ptr<ptr<ptr<i8>>>
    %1214 = llvm.getelementptr %1159[%1212] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1215 = llvm.bitcast %1213 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %1215, %1214 : !llvm.ptr<ptr<i8>>
    %1216 = llvm.mlir.addressof @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void : !llvm.ptr<array<101 x i8>>
    %1217 = llvm.mlir.constant(0 : index) : i64
    %1218 = llvm.getelementptr %1216[%1217, %1217] : (!llvm.ptr<array<101 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %1218, %1159) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %1219 = llvm.icmp "eq" %366, %6 : i64
    %1220 = llvm.sub %366, %5  : i64
    %1221 = llvm.udiv %1220, %2  : i64
    %1222 = llvm.add %1221, %5  : i64
    %1223 = llvm.select %1219, %6, %1222 : i1, i64
    %1224 = llvm.mul %1223, %0  : i64
    %1225 = llvm.icmp "sle" %1224, %6 : i64
    %1226 = llvm.sub %6, %1224  : i64
    %1227 = llvm.sub %1224, %5  : i64
    %1228 = llvm.select %1225, %1226, %1227 : i1, i64
    %1229 = llvm.sdiv %1228, %0  : i64
    %1230 = llvm.sub %6, %1229  : i64
    %1231 = llvm.add %1229, %5  : i64
    %1232 = llvm.select %1225, %1230, %1231 : i1, i64
    %1233 = llvm.mlir.addressof @main_kernel_6_blob_gpu.binary : !llvm.ptr<array<2024 x i8>>
    %1234 = llvm.mlir.constant(0 : index) : i64
    %1235 = llvm.getelementptr %1233[%1234, %1234] : (!llvm.ptr<array<2024 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %1236 = llvm.mlir.constant(1 : i32) : i32
    %1237 = llvm.alloca %1236 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %1238 = llvm.mlir.constant(0 : i32) : i32
    %1239 = llvm.getelementptr %1237[%1238] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %1235, %1239 : !llvm.ptr<ptr<i8>>
    %1240 = llvm.mlir.constant(1 : i64) : i64
    %1241 = llvm.mlir.addressof @main_kernel_6_main_kColReduction_reduce__6_1_0___block_tile_h64_1_kernel_name : !llvm.ptr<array<52 x i8>>
    %1242 = llvm.mlir.constant(0 : index) : i64
    %1243 = llvm.getelementptr %1241[%1242, %1242] : (!llvm.ptr<array<52 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %1244 = llvm.extractvalue %119[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %1245 = llvm.extractvalue %119[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %1246 = llvm.extractvalue %119[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %1247 = llvm.extractvalue %119[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %1248 = llvm.extractvalue %119[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %1249 = llvm.extractvalue %119[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %1250 = llvm.extractvalue %119[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %1251 = llvm.extractvalue %261[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %1252 = llvm.extractvalue %261[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %1253 = llvm.extractvalue %261[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %1254 = llvm.extractvalue %261[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %1255 = llvm.extractvalue %261[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %1256 = llvm.extractvalue %261[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %1257 = llvm.extractvalue %261[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %1258 = llvm.extractvalue %404[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %1259 = llvm.extractvalue %404[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %1260 = llvm.extractvalue %404[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %1261 = llvm.extractvalue %404[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %1262 = llvm.extractvalue %404[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %1263 = llvm.mlir.constant(1 : i32) : i32
    %1264 = llvm.alloca %1263 x !llvm.struct<".32", (i64, i64, i64, i64, i64, ptr, ptr, ptr)> : (i32) -> !llvm.ptr<struct<".32", (i64, i64, i64, i64, i64, ptr, ptr, ptr)>>
    %1265 = llvm.mlir.constant(8 : i32) : i32
    %1266 = llvm.alloca %1265 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %1267 = llvm.mlir.constant(0 : i32) : i32
    %1268 = llvm.mlir.constant(0 : i32) : i32
    %1269 = llvm.getelementptr %1264[%1267, 0] : (!llvm.ptr<struct<".32", (i64, i64, i64, i64, i64, ptr, ptr, ptr)>>, i32) -> !llvm.ptr<i64>
    llvm.store %49, %1269 : !llvm.ptr<i64>
    %1270 = llvm.getelementptr %1266[%1268] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1271 = llvm.bitcast %1269 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1271, %1270 : !llvm.ptr<ptr<i8>>
    %1272 = llvm.mlir.constant(1 : i32) : i32
    %1273 = llvm.getelementptr %1264[%1267, 1] : (!llvm.ptr<struct<".32", (i64, i64, i64, i64, i64, ptr, ptr, ptr)>>, i32) -> !llvm.ptr<i64>
    llvm.store %50, %1273 : !llvm.ptr<i64>
    %1274 = llvm.getelementptr %1266[%1272] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1275 = llvm.bitcast %1273 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1275, %1274 : !llvm.ptr<ptr<i8>>
    %1276 = llvm.mlir.constant(2 : i32) : i32
    %1277 = llvm.getelementptr %1264[%1267, 2] : (!llvm.ptr<struct<".32", (i64, i64, i64, i64, i64, ptr, ptr, ptr)>>, i32) -> !llvm.ptr<i64>
    llvm.store %0, %1277 : !llvm.ptr<i64>
    %1278 = llvm.getelementptr %1266[%1276] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1279 = llvm.bitcast %1277 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1279, %1278 : !llvm.ptr<ptr<i8>>
    %1280 = llvm.mlir.constant(3 : i32) : i32
    %1281 = llvm.getelementptr %1264[%1267, 3] : (!llvm.ptr<struct<".32", (i64, i64, i64, i64, i64, ptr, ptr, ptr)>>, i32) -> !llvm.ptr<i64>
    llvm.store %1224, %1281 : !llvm.ptr<i64>
    %1282 = llvm.getelementptr %1266[%1280] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1283 = llvm.bitcast %1281 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1283, %1282 : !llvm.ptr<ptr<i8>>
    %1284 = llvm.mlir.constant(4 : i32) : i32
    %1285 = llvm.getelementptr %1264[%1267, 4] : (!llvm.ptr<struct<".32", (i64, i64, i64, i64, i64, ptr, ptr, ptr)>>, i32) -> !llvm.ptr<i64>
    llvm.store %366, %1285 : !llvm.ptr<i64>
    %1286 = llvm.getelementptr %1266[%1284] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1287 = llvm.bitcast %1285 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1287, %1286 : !llvm.ptr<ptr<i8>>
    %1288 = llvm.mlir.constant(5 : i32) : i32
    %1289 = llvm.getelementptr %1264[%1267, 5] : (!llvm.ptr<struct<".32", (i64, i64, i64, i64, i64, ptr, ptr, ptr)>>, i32) -> !llvm.ptr<ptr>
    llvm.store %1245, %1289 : !llvm.ptr<ptr>
    %1290 = llvm.getelementptr %1266[%1288] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1291 = llvm.bitcast %1289 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %1291, %1290 : !llvm.ptr<ptr<i8>>
    %1292 = llvm.mlir.constant(6 : i32) : i32
    %1293 = llvm.getelementptr %1264[%1267, 6] : (!llvm.ptr<struct<".32", (i64, i64, i64, i64, i64, ptr, ptr, ptr)>>, i32) -> !llvm.ptr<ptr>
    llvm.store %1252, %1293 : !llvm.ptr<ptr>
    %1294 = llvm.getelementptr %1266[%1292] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1295 = llvm.bitcast %1293 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %1295, %1294 : !llvm.ptr<ptr<i8>>
    %1296 = llvm.mlir.constant(7 : i32) : i32
    %1297 = llvm.getelementptr %1264[%1267, 7] : (!llvm.ptr<struct<".32", (i64, i64, i64, i64, i64, ptr, ptr, ptr)>>, i32) -> !llvm.ptr<ptr>
    llvm.store %1259, %1297 : !llvm.ptr<ptr>
    %1298 = llvm.getelementptr %1266[%1296] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1299 = llvm.bitcast %1297 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %1299, %1298 : !llvm.ptr<ptr<i8>>
    %1300 = llvm.mlir.constant(0 : i32) : i32
    %1301 = llvm.mlir.constant(8 : i32) : i32
    %1302 = llvm.inttoptr %1300 : i32 to !llvm.ptr<i8>
    %1303 = llvm.mlir.constant(0 : i32) : i32
    %1304 = llvm.mlir.constant(1 : i32) : i32
    %1305 = llvm.alloca %1304 x !llvm.struct<".33", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)> : (i32) -> !llvm.ptr<struct<".33", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>
    %1306 = llvm.mlir.constant(14 : i32) : i32
    %1307 = llvm.alloca %1306 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %1308 = llvm.mlir.constant(0 : i32) : i32
    %1309 = llvm.getelementptr %1305[%1303, 0] : (!llvm.ptr<struct<".33", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %1309 : !llvm.ptr<ptr<i8>>
    %1310 = llvm.getelementptr %1307[%1308] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1311 = llvm.bitcast %1309 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %1311, %1310 : !llvm.ptr<ptr<i8>>
    %1312 = llvm.mlir.constant(1 : i32) : i32
    %1313 = llvm.getelementptr %1305[%1303, 1] : (!llvm.ptr<struct<".33", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %1237, %1313 : !llvm.ptr<ptr<ptr<i8>>>
    %1314 = llvm.getelementptr %1307[%1312] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1315 = llvm.bitcast %1313 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %1315, %1314 : !llvm.ptr<ptr<i8>>
    %1316 = llvm.mlir.constant(2 : i32) : i32
    %1317 = llvm.getelementptr %1305[%1303, 2] : (!llvm.ptr<struct<".33", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %1240, %1317 : !llvm.ptr<i64>
    %1318 = llvm.getelementptr %1307[%1316] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1319 = llvm.bitcast %1317 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1319, %1318 : !llvm.ptr<ptr<i8>>
    %1320 = llvm.mlir.constant(3 : i32) : i32
    %1321 = llvm.getelementptr %1305[%1303, 3] : (!llvm.ptr<struct<".33", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %1243, %1321 : !llvm.ptr<ptr<i8>>
    %1322 = llvm.getelementptr %1307[%1320] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1323 = llvm.bitcast %1321 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %1323, %1322 : !llvm.ptr<ptr<i8>>
    %1324 = llvm.mlir.constant(4 : i32) : i32
    %1325 = llvm.getelementptr %1305[%1303, 4] : (!llvm.ptr<struct<".33", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %1232, %1325 : !llvm.ptr<i64>
    %1326 = llvm.getelementptr %1307[%1324] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1327 = llvm.bitcast %1325 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1327, %1326 : !llvm.ptr<ptr<i8>>
    %1328 = llvm.mlir.constant(5 : i32) : i32
    %1329 = llvm.getelementptr %1305[%1303, 5] : (!llvm.ptr<struct<".33", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %1329 : !llvm.ptr<i64>
    %1330 = llvm.getelementptr %1307[%1328] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1331 = llvm.bitcast %1329 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1331, %1330 : !llvm.ptr<ptr<i8>>
    %1332 = llvm.mlir.constant(6 : i32) : i32
    %1333 = llvm.getelementptr %1305[%1303, 6] : (!llvm.ptr<struct<".33", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %1333 : !llvm.ptr<i64>
    %1334 = llvm.getelementptr %1307[%1332] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1335 = llvm.bitcast %1333 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1335, %1334 : !llvm.ptr<ptr<i8>>
    %1336 = llvm.mlir.constant(7 : i32) : i32
    %1337 = llvm.getelementptr %1305[%1303, 7] : (!llvm.ptr<struct<".33", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %0, %1337 : !llvm.ptr<i64>
    %1338 = llvm.getelementptr %1307[%1336] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1339 = llvm.bitcast %1337 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1339, %1338 : !llvm.ptr<ptr<i8>>
    %1340 = llvm.mlir.constant(8 : i32) : i32
    %1341 = llvm.getelementptr %1305[%1303, 8] : (!llvm.ptr<struct<".33", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %1341 : !llvm.ptr<i64>
    %1342 = llvm.getelementptr %1307[%1340] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1343 = llvm.bitcast %1341 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1343, %1342 : !llvm.ptr<ptr<i8>>
    %1344 = llvm.mlir.constant(9 : i32) : i32
    %1345 = llvm.getelementptr %1305[%1303, 9] : (!llvm.ptr<struct<".33", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %1345 : !llvm.ptr<i64>
    %1346 = llvm.getelementptr %1307[%1344] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1347 = llvm.bitcast %1345 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1347, %1346 : !llvm.ptr<ptr<i8>>
    %1348 = llvm.mlir.constant(10 : i32) : i32
    %1349 = llvm.getelementptr %1305[%1303, 10] : (!llvm.ptr<struct<".33", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %1300, %1349 : !llvm.ptr<i32>
    %1350 = llvm.getelementptr %1307[%1348] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1351 = llvm.bitcast %1349 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %1351, %1350 : !llvm.ptr<ptr<i8>>
    %1352 = llvm.mlir.constant(11 : i32) : i32
    %1353 = llvm.getelementptr %1305[%1303, 11] : (!llvm.ptr<struct<".33", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %1302, %1353 : !llvm.ptr<ptr<i8>>
    %1354 = llvm.getelementptr %1307[%1352] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1355 = llvm.bitcast %1353 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %1355, %1354 : !llvm.ptr<ptr<i8>>
    %1356 = llvm.mlir.constant(12 : i32) : i32
    %1357 = llvm.getelementptr %1305[%1303, 12] : (!llvm.ptr<struct<".33", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %1301, %1357 : !llvm.ptr<i32>
    %1358 = llvm.getelementptr %1307[%1356] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1359 = llvm.bitcast %1357 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %1359, %1358 : !llvm.ptr<ptr<i8>>
    %1360 = llvm.mlir.constant(13 : i32) : i32
    %1361 = llvm.getelementptr %1305[%1303, 13] : (!llvm.ptr<struct<".33", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %1266, %1361 : !llvm.ptr<ptr<ptr<i8>>>
    %1362 = llvm.getelementptr %1307[%1360] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1363 = llvm.bitcast %1361 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %1363, %1362 : !llvm.ptr<ptr<i8>>
    %1364 = llvm.mlir.addressof @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void : !llvm.ptr<array<101 x i8>>
    %1365 = llvm.mlir.constant(0 : index) : i64
    %1366 = llvm.getelementptr %1364[%1365, %1365] : (!llvm.ptr<array<101 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %1366, %1307) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    llvm.br ^bb7
  ^bb7:  // 4 preds: ^bb2, ^bb3, ^bb5, ^bb6
    %1367 = llvm.extractvalue %261[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %1368 = llvm.bitcast %1367 : !llvm.ptr to !llvm.ptr
    %1369 = llvm.mlir.constant(0 : i32) : i32
    %1370 = llvm.mlir.constant(1 : i32) : i32
    %1371 = llvm.alloca %1370 x !llvm.struct<".13", (ptr<i8>, ptr)> : (i32) -> !llvm.ptr<struct<".13", (ptr<i8>, ptr)>>
    %1372 = llvm.mlir.constant(2 : i32) : i32
    %1373 = llvm.alloca %1372 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %1374 = llvm.mlir.constant(0 : i32) : i32
    %1375 = llvm.getelementptr %1371[%1369, 0] : (!llvm.ptr<struct<".13", (ptr<i8>, ptr)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %1375 : !llvm.ptr<ptr<i8>>
    %1376 = llvm.getelementptr %1373[%1374] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1377 = llvm.bitcast %1375 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %1377, %1376 : !llvm.ptr<ptr<i8>>
    %1378 = llvm.mlir.constant(1 : i32) : i32
    %1379 = llvm.getelementptr %1371[%1369, 1] : (!llvm.ptr<struct<".13", (ptr<i8>, ptr)>>, i32) -> !llvm.ptr<ptr>
    llvm.store %1368, %1379 : !llvm.ptr<ptr>
    %1380 = llvm.getelementptr %1373[%1378] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1381 = llvm.bitcast %1379 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %1381, %1380 : !llvm.ptr<ptr<i8>>
    %1382 = llvm.mlir.addressof @dealloc___gpu___pvoid_pvoid___void : !llvm.ptr<array<35 x i8>>
    %1383 = llvm.mlir.constant(0 : index) : i64
    %1384 = llvm.getelementptr %1382[%1383, %1383] : (!llvm.ptr<array<35 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %1384, %1373) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %1385 = llvm.extractvalue %119[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %1386 = llvm.bitcast %1385 : !llvm.ptr to !llvm.ptr
    %1387 = llvm.mlir.constant(0 : i32) : i32
    %1388 = llvm.mlir.constant(1 : i32) : i32
    %1389 = llvm.alloca %1388 x !llvm.struct<".14", (ptr<i8>, ptr)> : (i32) -> !llvm.ptr<struct<".14", (ptr<i8>, ptr)>>
    %1390 = llvm.mlir.constant(2 : i32) : i32
    %1391 = llvm.alloca %1390 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %1392 = llvm.mlir.constant(0 : i32) : i32
    %1393 = llvm.getelementptr %1389[%1387, 0] : (!llvm.ptr<struct<".14", (ptr<i8>, ptr)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %1393 : !llvm.ptr<ptr<i8>>
    %1394 = llvm.getelementptr %1391[%1392] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1395 = llvm.bitcast %1393 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %1395, %1394 : !llvm.ptr<ptr<i8>>
    %1396 = llvm.mlir.constant(1 : i32) : i32
    %1397 = llvm.getelementptr %1389[%1387, 1] : (!llvm.ptr<struct<".14", (ptr<i8>, ptr)>>, i32) -> !llvm.ptr<ptr>
    llvm.store %1386, %1397 : !llvm.ptr<ptr>
    %1398 = llvm.getelementptr %1391[%1396] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1399 = llvm.bitcast %1397 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %1399, %1398 : !llvm.ptr<ptr<i8>>
    %1400 = llvm.mlir.addressof @dealloc___gpu___pvoid_pvoid___void : !llvm.ptr<array<35 x i8>>
    %1401 = llvm.mlir.constant(0 : index) : i64
    %1402 = llvm.getelementptr %1400[%1401, %1401] : (!llvm.ptr<array<35 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %1402, %1391) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %1403 = llvm.mlir.constant(0 : index) : i64
    %1404 = llvm.mlir.constant(1 : index) : i64
    %1405 = llvm.alloca %1404 x i64 : (i64) -> !llvm.ptr
    %1406 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %1407 = llvm.insertvalue %1405, %1406[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %1408 = llvm.insertvalue %1405, %1407[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %1409 = llvm.mlir.constant(0 : index) : i64
    %1410 = llvm.insertvalue %1409, %1408[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %1411 = llvm.insertvalue %1403, %1410[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %1412 = llvm.insertvalue %1404, %1411[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %1413 = llvm.mlir.constant(0 : i32) : i32
    %1414 = llvm.mlir.constant(1 : i32) : i32
    %1415 = llvm.extractvalue %404[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %1416 = llvm.extractvalue %404[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %1417 = llvm.extractvalue %404[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %1418 = llvm.extractvalue %404[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %1419 = llvm.extractvalue %404[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %1420 = llvm.extractvalue %1412[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %1421 = llvm.extractvalue %1412[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %1422 = llvm.extractvalue %1412[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %1423 = llvm.extractvalue %1412[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %1424 = llvm.extractvalue %1412[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %1425 = llvm.alloca %1414 x !llvm.struct<".15", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, ptr, ptr, i64, i64, i64, struct<(ptr, ptr, i64)>)> : (i32) -> !llvm.ptr<struct<".15", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, ptr, ptr, i64, i64, i64, struct<(ptr, ptr, i64)>)>>
    %1426 = llvm.mlir.constant(13 : i32) : i32
    %1427 = llvm.alloca %1426 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %1428 = llvm.mlir.constant(0 : i32) : i32
    %1429 = llvm.getelementptr %1425[%1413, 0] : (!llvm.ptr<struct<".15", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, ptr, ptr, i64, i64, i64, struct<(ptr, ptr, i64)>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %1429 : !llvm.ptr<ptr<i8>>
    %1430 = llvm.getelementptr %1427[%1428] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1431 = llvm.bitcast %1429 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %1431, %1430 : !llvm.ptr<ptr<i8>>
    %1432 = llvm.mlir.constant(1 : i32) : i32
    %1433 = llvm.getelementptr %1425[%1413, 1] : (!llvm.ptr<struct<".15", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, ptr, ptr, i64, i64, i64, struct<(ptr, ptr, i64)>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %120, %1433 : !llvm.ptr<ptr<i8>>
    %1434 = llvm.getelementptr %1427[%1432] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1435 = llvm.bitcast %1433 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %1435, %1434 : !llvm.ptr<ptr<i8>>
    %1436 = llvm.mlir.constant(2 : i32) : i32
    %1437 = llvm.getelementptr %1425[%1413, 2] : (!llvm.ptr<struct<".15", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, ptr, ptr, i64, i64, i64, struct<(ptr, ptr, i64)>)>>, i32) -> !llvm.ptr<ptr>
    llvm.store %1415, %1437 : !llvm.ptr<ptr>
    %1438 = llvm.getelementptr %1427[%1436] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1439 = llvm.bitcast %1437 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %1439, %1438 : !llvm.ptr<ptr<i8>>
    %1440 = llvm.mlir.constant(3 : i32) : i32
    %1441 = llvm.getelementptr %1425[%1413, 3] : (!llvm.ptr<struct<".15", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, ptr, ptr, i64, i64, i64, struct<(ptr, ptr, i64)>)>>, i32) -> !llvm.ptr<ptr>
    llvm.store %1416, %1441 : !llvm.ptr<ptr>
    %1442 = llvm.getelementptr %1427[%1440] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1443 = llvm.bitcast %1441 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %1443, %1442 : !llvm.ptr<ptr<i8>>
    %1444 = llvm.mlir.constant(4 : i32) : i32
    %1445 = llvm.getelementptr %1425[%1413, 4] : (!llvm.ptr<struct<".15", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, ptr, ptr, i64, i64, i64, struct<(ptr, ptr, i64)>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %1417, %1445 : !llvm.ptr<i64>
    %1446 = llvm.getelementptr %1427[%1444] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1447 = llvm.bitcast %1445 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1447, %1446 : !llvm.ptr<ptr<i8>>
    %1448 = llvm.mlir.constant(5 : i32) : i32
    %1449 = llvm.getelementptr %1425[%1413, 5] : (!llvm.ptr<struct<".15", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, ptr, ptr, i64, i64, i64, struct<(ptr, ptr, i64)>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %1418, %1449 : !llvm.ptr<i64>
    %1450 = llvm.getelementptr %1427[%1448] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1451 = llvm.bitcast %1449 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1451, %1450 : !llvm.ptr<ptr<i8>>
    %1452 = llvm.mlir.constant(6 : i32) : i32
    %1453 = llvm.getelementptr %1425[%1413, 6] : (!llvm.ptr<struct<".15", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, ptr, ptr, i64, i64, i64, struct<(ptr, ptr, i64)>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %1419, %1453 : !llvm.ptr<i64>
    %1454 = llvm.getelementptr %1427[%1452] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1455 = llvm.bitcast %1453 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1455, %1454 : !llvm.ptr<ptr<i8>>
    %1456 = llvm.mlir.constant(7 : i32) : i32
    %1457 = llvm.getelementptr %1425[%1413, 7] : (!llvm.ptr<struct<".15", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, ptr, ptr, i64, i64, i64, struct<(ptr, ptr, i64)>)>>, i32) -> !llvm.ptr<ptr>
    llvm.store %1420, %1457 : !llvm.ptr<ptr>
    %1458 = llvm.getelementptr %1427[%1456] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1459 = llvm.bitcast %1457 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %1459, %1458 : !llvm.ptr<ptr<i8>>
    %1460 = llvm.mlir.constant(8 : i32) : i32
    %1461 = llvm.getelementptr %1425[%1413, 8] : (!llvm.ptr<struct<".15", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, ptr, ptr, i64, i64, i64, struct<(ptr, ptr, i64)>)>>, i32) -> !llvm.ptr<ptr>
    llvm.store %1421, %1461 : !llvm.ptr<ptr>
    %1462 = llvm.getelementptr %1427[%1460] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1463 = llvm.bitcast %1461 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %1463, %1462 : !llvm.ptr<ptr<i8>>
    %1464 = llvm.mlir.constant(9 : i32) : i32
    %1465 = llvm.getelementptr %1425[%1413, 9] : (!llvm.ptr<struct<".15", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, ptr, ptr, i64, i64, i64, struct<(ptr, ptr, i64)>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %1422, %1465 : !llvm.ptr<i64>
    %1466 = llvm.getelementptr %1427[%1464] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1467 = llvm.bitcast %1465 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1467, %1466 : !llvm.ptr<ptr<i8>>
    %1468 = llvm.mlir.constant(10 : i32) : i32
    %1469 = llvm.getelementptr %1425[%1413, 10] : (!llvm.ptr<struct<".15", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, ptr, ptr, i64, i64, i64, struct<(ptr, ptr, i64)>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %1423, %1469 : !llvm.ptr<i64>
    %1470 = llvm.getelementptr %1427[%1468] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1471 = llvm.bitcast %1469 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1471, %1470 : !llvm.ptr<ptr<i8>>
    %1472 = llvm.mlir.constant(11 : i32) : i32
    %1473 = llvm.getelementptr %1425[%1413, 11] : (!llvm.ptr<struct<".15", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, ptr, ptr, i64, i64, i64, struct<(ptr, ptr, i64)>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %1424, %1473 : !llvm.ptr<i64>
    %1474 = llvm.getelementptr %1427[%1472] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1475 = llvm.bitcast %1473 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1475, %1474 : !llvm.ptr<ptr<i8>>
    %1476 = llvm.mlir.constant(12 : i32) : i32
    %1477 = llvm.getelementptr %1425[%1413, 12] : (!llvm.ptr<struct<".15", (ptr<i8>, ptr<i8>, ptr, ptr, i64, i64, i64, ptr, ptr, i64, i64, i64, struct<(ptr, ptr, i64)>)>>, i32) -> !llvm.ptr<struct<(ptr, ptr, i64)>>
    %1478 = llvm.getelementptr %1427[%1476] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1479 = llvm.bitcast %1477 : !llvm.ptr<struct<(ptr, ptr, i64)>> to !llvm.ptr<i8>
    llvm.store %1479, %1478 : !llvm.ptr<ptr<i8>>
    %1480 = llvm.mlir.addressof @inc_ref___gpu___pvoid_pvoid_m1df32_m1di64___m0df32 : !llvm.ptr<array<51 x i8>>
    %1481 = llvm.mlir.constant(0 : index) : i64
    %1482 = llvm.getelementptr %1480[%1481, %1481] : (!llvm.ptr<array<51 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %1482, %1427) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %1483 = llvm.load %1477 : !llvm.ptr<struct<(ptr, ptr, i64)>>
    %1484 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %1485 = llvm.extractvalue %1483[0] : !llvm.struct<(ptr, ptr, i64)> 
    %1486 = llvm.extractvalue %1483[1] : !llvm.struct<(ptr, ptr, i64)> 
    %1487 = llvm.insertvalue %1485, %1484[0] : !llvm.struct<(ptr, ptr, i64)> 
    %1488 = llvm.insertvalue %1486, %1487[1] : !llvm.struct<(ptr, ptr, i64)> 
    %1489 = llvm.mlir.constant(0 : index) : i64
    %1490 = llvm.insertvalue %1489, %1488[2] : !llvm.struct<(ptr, ptr, i64)> 
    %1491 = llvm.extractvalue %404[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %1492 = llvm.bitcast %1491 : !llvm.ptr to !llvm.ptr
    %1493 = llvm.mlir.constant(0 : i32) : i32
    %1494 = llvm.mlir.constant(1 : i32) : i32
    %1495 = llvm.alloca %1494 x !llvm.struct<".16", (ptr<i8>, ptr)> : (i32) -> !llvm.ptr<struct<".16", (ptr<i8>, ptr)>>
    %1496 = llvm.mlir.constant(2 : i32) : i32
    %1497 = llvm.alloca %1496 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %1498 = llvm.mlir.constant(0 : i32) : i32
    %1499 = llvm.getelementptr %1495[%1493, 0] : (!llvm.ptr<struct<".16", (ptr<i8>, ptr)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %1499 : !llvm.ptr<ptr<i8>>
    %1500 = llvm.getelementptr %1497[%1498] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1501 = llvm.bitcast %1499 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %1501, %1500 : !llvm.ptr<ptr<i8>>
    %1502 = llvm.mlir.constant(1 : i32) : i32
    %1503 = llvm.getelementptr %1495[%1493, 1] : (!llvm.ptr<struct<".16", (ptr<i8>, ptr)>>, i32) -> !llvm.ptr<ptr>
    llvm.store %1492, %1503 : !llvm.ptr<ptr>
    %1504 = llvm.getelementptr %1497[%1502] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1505 = llvm.bitcast %1503 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %1505, %1504 : !llvm.ptr<ptr<i8>>
    %1506 = llvm.mlir.addressof @dealloc___gpu___pvoid_pvoid___void : !llvm.ptr<array<35 x i8>>
    %1507 = llvm.mlir.constant(0 : index) : i64
    %1508 = llvm.getelementptr %1506[%1507, %1507] : (!llvm.ptr<array<35 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %1508, %1497) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %1509 = llvm.mlir.constant(1 : index) : i64
    %1510 = llvm.mlir.null : !llvm.ptr
    %1511 = llvm.getelementptr %1510[%1509] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %1512 = llvm.ptrtoint %1511 : !llvm.ptr to i64
    %1513 = llvm.mlir.constant(0 : i32) : i32
    %1514 = llvm.mlir.constant(1 : i32) : i32
    %1515 = llvm.alloca %1514 x !llvm.struct<".17", (ptr<i8>, i64, ptr)> : (i32) -> !llvm.ptr<struct<".17", (ptr<i8>, i64, ptr)>>
    %1516 = llvm.mlir.constant(3 : i32) : i32
    %1517 = llvm.alloca %1516 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %1518 = llvm.mlir.constant(0 : i32) : i32
    %1519 = llvm.getelementptr %1515[%1513, 0] : (!llvm.ptr<struct<".17", (ptr<i8>, i64, ptr)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %1519 : !llvm.ptr<ptr<i8>>
    %1520 = llvm.getelementptr %1517[%1518] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1521 = llvm.bitcast %1519 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %1521, %1520 : !llvm.ptr<ptr<i8>>
    %1522 = llvm.mlir.constant(1 : i32) : i32
    %1523 = llvm.getelementptr %1515[%1513, 1] : (!llvm.ptr<struct<".17", (ptr<i8>, i64, ptr)>>, i32) -> !llvm.ptr<i64>
    llvm.store %1512, %1523 : !llvm.ptr<i64>
    %1524 = llvm.getelementptr %1517[%1522] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1525 = llvm.bitcast %1523 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1525, %1524 : !llvm.ptr<ptr<i8>>
    %1526 = llvm.mlir.constant(2 : i32) : i32
    %1527 = llvm.getelementptr %1515[%1513, 2] : (!llvm.ptr<struct<".17", (ptr<i8>, i64, ptr)>>, i32) -> !llvm.ptr<ptr>
    %1528 = llvm.getelementptr %1517[%1526] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1529 = llvm.bitcast %1527 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %1529, %1528 : !llvm.ptr<ptr<i8>>
    %1530 = llvm.mlir.addressof @alloc___cpu___pvoid_i64___pvoid : !llvm.ptr<array<32 x i8>>
    %1531 = llvm.mlir.constant(0 : index) : i64
    %1532 = llvm.getelementptr %1530[%1531, %1531] : (!llvm.ptr<array<32 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %1532, %1517) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %1533 = llvm.load %1527 : !llvm.ptr<ptr>
    %1534 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %1535 = llvm.bitcast %1533 : !llvm.ptr to !llvm.ptr
    %1536 = llvm.insertvalue %1535, %1534[0] : !llvm.struct<(ptr, ptr, i64)> 
    %1537 = llvm.insertvalue %1535, %1536[1] : !llvm.struct<(ptr, ptr, i64)> 
    %1538 = llvm.mlir.constant(0 : index) : i64
    %1539 = llvm.insertvalue %1538, %1537[2] : !llvm.struct<(ptr, ptr, i64)> 
    %1540 = llvm.mlir.constant(0 : i32) : i32
    %1541 = llvm.mlir.constant(1 : i32) : i32
    %1542 = llvm.extractvalue %1490[0] : !llvm.struct<(ptr, ptr, i64)> 
    %1543 = llvm.extractvalue %1490[1] : !llvm.struct<(ptr, ptr, i64)> 
    %1544 = llvm.extractvalue %1490[2] : !llvm.struct<(ptr, ptr, i64)> 
    %1545 = llvm.extractvalue %1539[0] : !llvm.struct<(ptr, ptr, i64)> 
    %1546 = llvm.extractvalue %1539[1] : !llvm.struct<(ptr, ptr, i64)> 
    %1547 = llvm.extractvalue %1539[2] : !llvm.struct<(ptr, ptr, i64)> 
    %1548 = llvm.alloca %1541 x !llvm.struct<".18", (ptr<i8>, ptr<i8>, ptr, ptr, i64, ptr, ptr, i64)> : (i32) -> !llvm.ptr<struct<".18", (ptr<i8>, ptr<i8>, ptr, ptr, i64, ptr, ptr, i64)>>
    %1549 = llvm.mlir.constant(8 : i32) : i32
    %1550 = llvm.alloca %1549 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %1551 = llvm.mlir.constant(0 : i32) : i32
    %1552 = llvm.getelementptr %1548[%1540, 0] : (!llvm.ptr<struct<".18", (ptr<i8>, ptr<i8>, ptr, ptr, i64, ptr, ptr, i64)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %1552 : !llvm.ptr<ptr<i8>>
    %1553 = llvm.getelementptr %1550[%1551] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1554 = llvm.bitcast %1552 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %1554, %1553 : !llvm.ptr<ptr<i8>>
    %1555 = llvm.mlir.constant(1 : i32) : i32
    %1556 = llvm.getelementptr %1548[%1540, 1] : (!llvm.ptr<struct<".18", (ptr<i8>, ptr<i8>, ptr, ptr, i64, ptr, ptr, i64)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %120, %1556 : !llvm.ptr<ptr<i8>>
    %1557 = llvm.getelementptr %1550[%1555] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1558 = llvm.bitcast %1556 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %1558, %1557 : !llvm.ptr<ptr<i8>>
    %1559 = llvm.mlir.constant(2 : i32) : i32
    %1560 = llvm.getelementptr %1548[%1540, 2] : (!llvm.ptr<struct<".18", (ptr<i8>, ptr<i8>, ptr, ptr, i64, ptr, ptr, i64)>>, i32) -> !llvm.ptr<ptr>
    llvm.store %1542, %1560 : !llvm.ptr<ptr>
    %1561 = llvm.getelementptr %1550[%1559] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1562 = llvm.bitcast %1560 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %1562, %1561 : !llvm.ptr<ptr<i8>>
    %1563 = llvm.mlir.constant(3 : i32) : i32
    %1564 = llvm.getelementptr %1548[%1540, 3] : (!llvm.ptr<struct<".18", (ptr<i8>, ptr<i8>, ptr, ptr, i64, ptr, ptr, i64)>>, i32) -> !llvm.ptr<ptr>
    llvm.store %1543, %1564 : !llvm.ptr<ptr>
    %1565 = llvm.getelementptr %1550[%1563] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1566 = llvm.bitcast %1564 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %1566, %1565 : !llvm.ptr<ptr<i8>>
    %1567 = llvm.mlir.constant(4 : i32) : i32
    %1568 = llvm.getelementptr %1548[%1540, 4] : (!llvm.ptr<struct<".18", (ptr<i8>, ptr<i8>, ptr, ptr, i64, ptr, ptr, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %1544, %1568 : !llvm.ptr<i64>
    %1569 = llvm.getelementptr %1550[%1567] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1570 = llvm.bitcast %1568 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1570, %1569 : !llvm.ptr<ptr<i8>>
    %1571 = llvm.mlir.constant(5 : i32) : i32
    %1572 = llvm.getelementptr %1548[%1540, 5] : (!llvm.ptr<struct<".18", (ptr<i8>, ptr<i8>, ptr, ptr, i64, ptr, ptr, i64)>>, i32) -> !llvm.ptr<ptr>
    llvm.store %1545, %1572 : !llvm.ptr<ptr>
    %1573 = llvm.getelementptr %1550[%1571] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1574 = llvm.bitcast %1572 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %1574, %1573 : !llvm.ptr<ptr<i8>>
    %1575 = llvm.mlir.constant(6 : i32) : i32
    %1576 = llvm.getelementptr %1548[%1540, 6] : (!llvm.ptr<struct<".18", (ptr<i8>, ptr<i8>, ptr, ptr, i64, ptr, ptr, i64)>>, i32) -> !llvm.ptr<ptr>
    llvm.store %1546, %1576 : !llvm.ptr<ptr>
    %1577 = llvm.getelementptr %1550[%1575] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1578 = llvm.bitcast %1576 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %1578, %1577 : !llvm.ptr<ptr<i8>>
    %1579 = llvm.mlir.constant(7 : i32) : i32
    %1580 = llvm.getelementptr %1548[%1540, 7] : (!llvm.ptr<struct<".18", (ptr<i8>, ptr<i8>, ptr, ptr, i64, ptr, ptr, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %1547, %1580 : !llvm.ptr<i64>
    %1581 = llvm.getelementptr %1550[%1579] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1582 = llvm.bitcast %1580 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1582, %1581 : !llvm.ptr<ptr<i8>>
    %1583 = llvm.mlir.addressof @d2h___gpu___pvoid_pvoid_m0df32_m0df32___void : !llvm.ptr<array<45 x i8>>
    %1584 = llvm.mlir.constant(0 : index) : i64
    %1585 = llvm.getelementptr %1583[%1584, %1584] : (!llvm.ptr<array<45 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %1585, %1550) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %1586 = llvm.mlir.constant(0 : i32) : i32
    %1587 = llvm.mlir.constant(1 : i32) : i32
    %1588 = llvm.alloca %1587 x !llvm.struct<".19", (ptr<i8>, ptr<i8>)> : (i32) -> !llvm.ptr<struct<".19", (ptr<i8>, ptr<i8>)>>
    %1589 = llvm.mlir.constant(2 : i32) : i32
    %1590 = llvm.alloca %1589 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %1591 = llvm.mlir.constant(0 : i32) : i32
    %1592 = llvm.getelementptr %1588[%1586, 0] : (!llvm.ptr<struct<".19", (ptr<i8>, ptr<i8>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %1592 : !llvm.ptr<ptr<i8>>
    %1593 = llvm.getelementptr %1590[%1591] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1594 = llvm.bitcast %1592 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %1594, %1593 : !llvm.ptr<ptr<i8>>
    %1595 = llvm.mlir.constant(1 : i32) : i32
    %1596 = llvm.getelementptr %1588[%1586, 1] : (!llvm.ptr<struct<".19", (ptr<i8>, ptr<i8>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %120, %1596 : !llvm.ptr<ptr<i8>>
    %1597 = llvm.getelementptr %1590[%1595] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1598 = llvm.bitcast %1596 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %1598, %1597 : !llvm.ptr<ptr<i8>>
    %1599 = llvm.mlir.addressof @sync_on_stream___gpu___pvoid_pvoid___void : !llvm.ptr<array<42 x i8>>
    %1600 = llvm.mlir.constant(0 : index) : i64
    %1601 = llvm.getelementptr %1599[%1600, %1600] : (!llvm.ptr<array<42 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %1601, %1590) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %1602 = llvm.extractvalue %1490[0] : !llvm.struct<(ptr, ptr, i64)> 
    %1603 = llvm.bitcast %1602 : !llvm.ptr to !llvm.ptr
    %1604 = llvm.mlir.constant(0 : i32) : i32
    %1605 = llvm.mlir.constant(1 : i32) : i32
    %1606 = llvm.alloca %1605 x !llvm.struct<".20", (ptr<i8>, ptr)> : (i32) -> !llvm.ptr<struct<".20", (ptr<i8>, ptr)>>
    %1607 = llvm.mlir.constant(2 : i32) : i32
    %1608 = llvm.alloca %1607 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %1609 = llvm.mlir.constant(0 : i32) : i32
    %1610 = llvm.getelementptr %1606[%1604, 0] : (!llvm.ptr<struct<".20", (ptr<i8>, ptr)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %1610 : !llvm.ptr<ptr<i8>>
    %1611 = llvm.getelementptr %1608[%1609] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1612 = llvm.bitcast %1610 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %1612, %1611 : !llvm.ptr<ptr<i8>>
    %1613 = llvm.mlir.constant(1 : i32) : i32
    %1614 = llvm.getelementptr %1606[%1604, 1] : (!llvm.ptr<struct<".20", (ptr<i8>, ptr)>>, i32) -> !llvm.ptr<ptr>
    llvm.store %1603, %1614 : !llvm.ptr<ptr>
    %1615 = llvm.getelementptr %1608[%1613] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1616 = llvm.bitcast %1614 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %1616, %1615 : !llvm.ptr<ptr<i8>>
    %1617 = llvm.mlir.addressof @dealloc___gpu___pvoid_pvoid___void : !llvm.ptr<array<35 x i8>>
    %1618 = llvm.mlir.constant(0 : index) : i64
    %1619 = llvm.getelementptr %1617[%1618, %1618] : (!llvm.ptr<array<35 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %1619, %1608) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %1620 = llvm.mlir.constant(0 : i32) : i32
    %1621 = llvm.mlir.constant(1 : i32) : i32
    %1622 = llvm.extractvalue %1539[0] : !llvm.struct<(ptr, ptr, i64)> 
    %1623 = llvm.extractvalue %1539[1] : !llvm.struct<(ptr, ptr, i64)> 
    %1624 = llvm.extractvalue %1539[2] : !llvm.struct<(ptr, ptr, i64)> 
    %1625 = llvm.alloca %1621 x !llvm.struct<".21", (ptr<i8>, i64, ptr, ptr, i64)> : (i32) -> !llvm.ptr<struct<".21", (ptr<i8>, i64, ptr, ptr, i64)>>
    %1626 = llvm.mlir.constant(5 : i32) : i32
    %1627 = llvm.alloca %1626 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %1628 = llvm.mlir.constant(0 : i32) : i32
    %1629 = llvm.getelementptr %1625[%1620, 0] : (!llvm.ptr<struct<".21", (ptr<i8>, i64, ptr, ptr, i64)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %1629 : !llvm.ptr<ptr<i8>>
    %1630 = llvm.getelementptr %1627[%1628] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1631 = llvm.bitcast %1629 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %1631, %1630 : !llvm.ptr<ptr<i8>>
    %1632 = llvm.mlir.constant(1 : i32) : i32
    %1633 = llvm.getelementptr %1625[%1620, 1] : (!llvm.ptr<struct<".21", (ptr<i8>, i64, ptr, ptr, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %6, %1633 : !llvm.ptr<i64>
    %1634 = llvm.getelementptr %1627[%1632] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1635 = llvm.bitcast %1633 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1635, %1634 : !llvm.ptr<ptr<i8>>
    %1636 = llvm.mlir.constant(2 : i32) : i32
    %1637 = llvm.getelementptr %1625[%1620, 2] : (!llvm.ptr<struct<".21", (ptr<i8>, i64, ptr, ptr, i64)>>, i32) -> !llvm.ptr<ptr>
    llvm.store %1622, %1637 : !llvm.ptr<ptr>
    %1638 = llvm.getelementptr %1627[%1636] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1639 = llvm.bitcast %1637 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %1639, %1638 : !llvm.ptr<ptr<i8>>
    %1640 = llvm.mlir.constant(3 : i32) : i32
    %1641 = llvm.getelementptr %1625[%1620, 3] : (!llvm.ptr<struct<".21", (ptr<i8>, i64, ptr, ptr, i64)>>, i32) -> !llvm.ptr<ptr>
    llvm.store %1623, %1641 : !llvm.ptr<ptr>
    %1642 = llvm.getelementptr %1627[%1640] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1643 = llvm.bitcast %1641 : !llvm.ptr<ptr> to !llvm.ptr<i8>
    llvm.store %1643, %1642 : !llvm.ptr<ptr<i8>>
    %1644 = llvm.mlir.constant(4 : i32) : i32
    %1645 = llvm.getelementptr %1625[%1620, 4] : (!llvm.ptr<struct<".21", (ptr<i8>, i64, ptr, ptr, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %1624, %1645 : !llvm.ptr<i64>
    %1646 = llvm.getelementptr %1627[%1644] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %1647 = llvm.bitcast %1645 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %1647, %1646 : !llvm.ptr<ptr<i8>>
    %1648 = llvm.mlir.addressof @ral_send_output___cpu___pvoid_i64_m0df32___void : !llvm.ptr<array<48 x i8>>
    %1649 = llvm.mlir.constant(0 : index) : i64
    %1650 = llvm.getelementptr %1648[%1649, %1649] : (!llvm.ptr<array<48 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %1650, %1627) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    llvm.return
  }
}


===-------------------------------------------------------------------------===
                         ... Execution time report ...
===-------------------------------------------------------------------------===
  Total Execution Time: 1.1254 seconds

  ----Wall Time----  ----Name----
    0.0002 (  0.0%)  'func.func' Pipeline
    0.0002 (  0.0%)    DiscAlgebraicSimplifierPass
    0.0001 (  0.0%)  DiscInputOutputAliasPass
    0.0001 (  0.0%)  DiscShapePropagatePass
    0.0006 (  0.1%)  Inliner
    0.0000 (  0.0%)    (A) CallGraph
    0.0002 (  0.0%)  'func.func' Pipeline
    0.0002 (  0.0%)    Canonicalizer
    0.0033 (  0.3%)  'func.func' Pipeline
    0.0001 (  0.0%)    DiscCollectiveOpsRewriterPass
    0.0001 (  0.0%)    MhloDecompositionRewriterPass
    0.0007 (  0.1%)    RemoveShapeConstraintsPass
    0.0007 (  0.1%)    Canonicalizer
    0.0001 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0001 (  0.0%)    Canonicalizer
    0.0001 (  0.0%)    DiscTranformWeightDataLayoutForWeightOnlyQuantPass
    0.0001 (  0.0%)    Canonicalizer
    0.0001 (  0.0%)    DiscCustomCallRewriterPass
    0.0001 (  0.0%)    DiscConvertFakeQuantOpPass
    0.0001 (  0.0%)    DiscLowerGpuQuantizeAndDequantizePass
    0.0011 (  0.1%)    ConvertShapeToStandardPass
    0.0028 (  0.2%)  DiscShapeOptimizationPass
    0.0035 (  0.3%)  'builtin.func' Pipeline
    0.0035 (  0.3%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0011 (  0.1%)  'func.func' Pipeline
    0.0001 (  0.0%)    ConvertTensorToStandardPass
    0.0001 (  0.0%)    ConvertHloToStandardPass
    0.0002 (  0.0%)    Canonicalizer
    0.0001 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0002 (  0.0%)    Canonicalizer
    0.0001 (  0.0%)    DiscAlgebraicSimplifierPass
    0.0001 (  0.0%)    SplitLargeOpsPass
    0.0001 (  0.0%)    DotRewriterPass
    0.0025 (  0.2%)  DiscShapeOptimizationPass
    0.0002 (  0.0%)  'func.func' Pipeline
    0.0002 (  0.0%)    DiscDotMergePass
    0.0025 (  0.2%)  DiscShapeOptimizationPass
    0.0010 (  0.1%)  'func.func' Pipeline
    0.0010 (  0.1%)    HloCanonicalizeReductionPass
    0.0044 (  0.4%)  DiscShapeOptimizationPass
    0.0015 (  0.1%)  DiscMarkShapeCalculationPass
    0.0017 (  0.2%)  PlaceOpsPass
    0.0009 (  0.1%)  'func.func' Pipeline
    0.0003 (  0.0%)    Canonicalizer
    0.0001 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0003 (  0.0%)    Canonicalizer
    0.0002 (  0.0%)    ElementTypeConverterPass
    0.0038 (  0.3%)  DiscShapeOptimizationPass
    0.0007 (  0.1%)  'func.func' Pipeline
    0.0002 (  0.0%)    ReductionRewriterPass
    0.0002 (  0.0%)    ConvRewriterPass
    0.0001 (  0.0%)    ConvRewriterPass
    0.0002 (  0.0%)    QuantizedDotRewriterPass
    0.0037 (  0.3%)  DiscShapeOptimizationPass
    0.0033 (  0.3%)  'func.func' Pipeline
    0.0003 (  0.0%)    Canonicalizer
    0.0001 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0003 (  0.0%)    Canonicalizer
    0.0025 (  0.2%)    TransposeSimplifierPass
    0.0001 (  0.0%)    GpuConvPaddingLegalizationPass
    0.0038 (  0.3%)  DiscShapeOptimizationPass
    0.0002 (  0.0%)  'func.func' Pipeline
    0.0002 (  0.0%)    DiscAlgebraicSimplifierPass
    0.0042 (  0.4%)  DiscShapeOptimizationPass
    0.0029 (  0.3%)  'func.func' Pipeline
    0.0020 (  0.2%)    Canonicalizer
    0.0002 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0006 (  0.1%)    Canonicalizer
    0.0023 (  0.2%)  FuncBufferize
    0.0033 (  0.3%)  DiscHloLegalizeToLhloPass
    0.0043 (  0.4%)  HloLegalizeToLhloPass
    0.0026 (  0.2%)  'func.func' Pipeline
    0.0026 (  0.2%)    Canonicalizer
    0.0004 (  0.0%)  DiscLhloRewriterPass
    0.0066 (  0.6%)  'func.func' Pipeline
    0.0004 (  0.0%)    Canonicalizer
    0.0003 (  0.0%)    ConvertShapeToStandardPass
    0.0004 (  0.0%)    Canonicalizer
    0.0003 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0004 (  0.0%)    Canonicalizer
    0.0022 (  0.2%)    LegalizeToTensorOpPass
    0.0022 (  0.2%)    Canonicalizer
    0.0003 (  0.0%)    StdBufferizePass
    0.0003 (  0.0%)  ArithBufferize
    0.0090 (  0.8%)  'func.func' Pipeline
    0.0022 (  0.2%)    TensorBufferize
    0.0003 (  0.0%)    FinalizingBufferize
    0.0022 (  0.2%)    Canonicalizer
    0.0021 (  0.2%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0004 (  0.0%)    Canonicalizer
    0.0018 (  0.2%)    DiscMemrefCanonicalizer
    0.0024 (  0.2%)  DiscAssignMemorySpacePass
    0.0182 (  1.6%)  'func.func' Pipeline
    0.0003 (  0.0%)    DiscDuplicateComputationForFusionPass
    0.0019 (  0.2%)    PromoteBuffersToStack
    0.0006 (  0.1%)    DiscMemRefLoadStoreSimplifierPass
    0.0025 (  0.2%)    DiscFusionPass
    0.0002 (  0.0%)    DiscFuseSplatConstPass
    0.0067 (  0.6%)    DiscSpecializeFusionWithSpeculationPass
    0.0049 (  0.4%)    Canonicalizer
    0.0004 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0006 (  0.1%)    Canonicalizer
    0.0005 (  0.0%)  DiscOptimizationBarrierExpandPass
    0.0016 (  0.1%)  'func.func' Pipeline
    0.0006 (  0.1%)    Canonicalizer
    0.0004 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0006 (  0.1%)    Canonicalizer
    0.0004 (  0.0%)  DiscOpSchedulePass
    0.0092 (  0.8%)  'func.func' Pipeline
    0.0043 (  0.4%)    DiscReduceBufferLiveRangePass
    0.0045 (  0.4%)    BufferDeallocation
    0.0004 (  0.0%)    DiscBufferDeallocationPass
    0.0054 (  0.5%)  RalInjectExecutionContextPass
    0.0056 (  0.5%)  'func.func' Pipeline
    0.0056 (  0.5%)    DiscLowerToLibraryCallPass
    0.0006 (  0.1%)  DiscConstToRALPass
    0.1459 ( 13.0%)  'func.func' Pipeline
    0.0009 (  0.1%)    DiscMemRefLoadStoreSimplifierPass
    0.0099 (  0.9%)    DiscLhloLegalizeRootsToParallelLoopsPass
    0.0013 (  0.1%)    ExpandOps
    0.0015 (  0.1%)    UnhandledAtomicRMWConverterPass
    0.0091 (  0.8%)    InputInlineFusionPass
    0.0011 (  0.1%)    ForLoopUnrollInterleave
    0.0015 (  0.1%)    DiscBF16ExpansionPass
    0.0086 (  0.8%)    ArithExpandOps
    0.0088 (  0.8%)    FoldMemRefAliasOps
    0.0144 (  1.3%)    DiscFlattenMemrefAccessPass
    0.0128 (  1.1%)    Canonicalizer
    0.0091 (  0.8%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0018 (  0.2%)    Canonicalizer
    0.0036 (  0.3%)    DiscMemRefCSEPass
    0.0085 (  0.8%)    ConvertShapeToStandardPass
    0.0086 (  0.8%)    Canonicalizer
    0.0012 (  0.1%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0018 (  0.2%)    Canonicalizer
    0.0084 (  0.8%)    ParallelLoopCollapsing
    0.0100 (  0.9%)    SCFParallelLoopTiling
    0.0103 (  0.9%)    GpuMapParallelLoopsPass
    0.0124 (  1.1%)    ConvertParallelLoopToGpu
    0.0024 (  0.2%)  'func' Pipeline
    0.0024 (  0.2%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0136 (  1.2%)  GpuLaunchSinkIndexComputations
    0.0162 (  1.4%)  GpuKernelOutlining
    0.0157 (  1.4%)  AssignKernelNamePass
    0.0047 (  0.4%)  'func.func' Pipeline
    0.0047 (  0.4%)    LhloFusionInlinerPass
    0.0141 (  1.2%)  DiscArgsMutationExpandPass
    0.0018 (  0.2%)  DiscCompIntensFusionToCUDASourcePass
    0.0122 (  1.1%)  ReviseGpuKernelOutliningPass
    0.4710 ( 41.9%)  'gpu.module' Pipeline
    0.0072 (  0.6%)    LoopInvariantCodeMotion
    0.0022 (  0.2%)    'gpu.func' Pipeline
    0.0021 (  0.2%)      SideEffectLoopInvariantCodeMotionPass
    0.0014 (  0.1%)    LoopInvariantCodeMotion
    0.0086 (  0.8%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0034 (  0.3%)    'gpu.func' Pipeline
    0.0014 (  0.1%)      DiscEraseBufferDeallocationPass
    0.0020 (  0.2%)      ExpandStridedMetadata
    0.0095 (  0.8%)    SCFToControlFlow
    0.0095 (  0.8%)    ConvertAffineToStandard
    0.0089 (  0.8%)    StripDebugInfo
    0.0231 (  2.1%)    DiscLowerGpuOpsToNVVMOpsPass
    0.0109 (  1.0%)    'llvm.func' Pipeline
    0.0109 (  1.0%)      LLVMInsertValueSimplifierPass
    0.0104 (  0.9%)    FunctionDeadArgumentEliminationPass
    0.3756 ( 33.4%)    GpuKernelToBlobPass
    0.0020 (  0.2%)  DiscGPUSourceToLibPass
    0.0059 (  0.5%)  'func.func' Pipeline
    0.0038 (  0.3%)    Canonicalizer
    0.0004 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0007 (  0.1%)    Canonicalizer
    0.0004 (  0.0%)    RemoveDeadBufferPass
    0.0006 (  0.1%)    LinalgLowerToLoops
    0.0465 (  4.1%)  SCFToControlFlow
    0.0085 (  0.8%)  'func.func' Pipeline
    0.0036 (  0.3%)    ExpandStridedMetadata
    0.0038 (  0.3%)    Canonicalizer
    0.0004 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0007 (  0.1%)    Canonicalizer
    0.0478 (  4.2%)  ConvertAffineToStandard
    0.0474 (  4.2%)  StripDebugInfo
    0.0467 (  4.1%)  DiscStripShapeConstraintOpsPass
    0.0958 (  8.5%)  DiscToLLVMPass
    0.0046 (  0.4%)  Rest
    1.1254 (100.0%)  Total
[DISC] LowerHLOToLLVM takes: 1.126533e+00 s.
before optimize llvm module:
; ModuleID = 'LLVMDialectModule'
source_filename = "LLVMDialectModule"

%0 = type { ptr, i64, { ptr, ptr, i64, [2 x i64], [2 x i64] } }
%.1 = type { ptr, i64, { ptr, ptr, i64, [2 x i64], [2 x i64] } }
%.2 = type { ptr, i64, ptr }
%.3 = type { ptr, ptr, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64 }
%.4 = type { ptr, ptr }
%.5 = type { ptr, i64, ptr }
%.6 = type { ptr, ptr, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64 }
%.7 = type { ptr, ptr }
%.8 = type { ptr, i64, ptr }
%.9 = type { i64, ptr }
%.10 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.11 = type { i64, i64, i64, ptr, ptr, ptr }
%.12 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.22 = type { i64, ptr }
%.23 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.24 = type { i64, i64, i64, ptr, ptr, ptr }
%.25 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.26 = type { i64, ptr }
%.27 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.28 = type { i64, i64, i64, i64, i64, ptr, ptr, ptr }
%.29 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.30 = type { i64, ptr }
%.31 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.32 = type { i64, i64, i64, i64, i64, ptr, ptr, ptr }
%.33 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.13 = type { ptr, ptr }
%.14 = type { ptr, ptr }
%.15 = type { ptr, ptr, ptr, ptr, i64, i64, i64, ptr, ptr, i64, i64, i64, { ptr, ptr, i64 } }
%.16 = type { ptr, ptr }
%.17 = type { ptr, i64, ptr }
%.18 = type { ptr, ptr, ptr, ptr, i64, ptr, ptr, i64 }
%.19 = type { ptr, ptr }
%.20 = type { ptr, ptr }
%.21 = type { ptr, i64, ptr, ptr, i64 }

@main_kernel_6_main_kColReduction_reduce__6_1_0___block_tile_h64_1_kernel_name = internal constant [52 x i8] c"main_kColReduction_reduce__6_1_0___block_tile_h64_1\00"
@main_kernel_6_blob_gpu.binary = internal constant [2024 x i8] c"P\EDU\BA\01\00\10\00\D8\07\00\00\00\00\00\00\02\00\01\01@\00\00\00\98\07\00\00\00\00\00\00\93\07\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\A8\12\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00g\02\00\BE\00r\00\01\00\22\12\00\01\00\11\0F\06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\0Ce__6_1_0___block_tile_h64_1C\00\0F=\00&oshared?\00&\9Fconstant0B\00#\FA\01debug_frame\00.rel\11\00!nv\14\00\11aN\00\0FW\01 \0F\93\00 \0F\8B\01\A4\8F$____wg_<\00 \00\15\00/28\CD\010o_param\D4\01\1C\0F\01\00\09\8Cf\00\00\00\03\00\0A\00\01\00\11\DD\18\00,\0B\00\01\00 ^\01\18\00,\09\00\01\00\11\A7\18\00,\04\00\01\00\11\C5\18\00,\07\00\01\00\102\E3\03\17\10x\00\03#\00f\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0\06\07\00 \00\04\9B\00R\04\14\00\00\00E\002\04p\01\18\000/\08\00#\00\10\19\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\A8\04\F3\08\015\00\00\04\0A\08\00\03\00\00\00`\010\00\03\190\00\04\17\0C\CB\00U(\00\00\F0!\10\009\06\00 \10\009\05\00\18\10\00u\04\00\10\00\00\F0\11\10\009\03\00\0C\10\009\02\00\08\10\009\01\00\04\10\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\00\D0\05\00\00 \06\00\00\04\1E\94\01#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\11\02h\01\0F\01\00\FF\98@$v\01\FFw\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%s\02a\0E\00\19y\03\00\01\00\10!-\00\F0\04\0E\00$z\00\00\00Z\00\00\03\02\8E\07\00\CA\1F\00\0C\10\00\C5[\00\00p`\F0\03\00\DA\0F\00M\93\04\B0\80\03\00\EA\0F\00\19x\03\FF\1F\0F\00\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\00\EB\04Q\E2\0F\00Eyy\03\020\00a\E4\0F\00\11r\04q\00\C1\FF@\8F\07\00\C8\0F\00\12x\03\04Y\04\10\C0p\00@\0F\00$x\BC\02`\00\00\03\0A\8E\070\00c$r\03\FF\FF\00\C0\00\10\C60\00 \02\00p\00\22\FF\C0 \00P\19x\05\FF\05Q\000\16\01\00\10\00@\0Cr\00\020\001pR\F2p\00P\0Cx\00\00\7F\10\000@\F0\03\90\00@$x\02\02\D9\02\10\05\E0\00\12\C8\10\00\14\04`\00\96\CC\0F\00G\19\00\00@\03\E0\00c$v\03\FF\00X \00\10\E2p\001\04\FF\08\E3\04A\01\00\00\C8`\00$\03\01p\00u\E2\0F\04$x\04\04`\00\01\C0\00\16\05\C0\00\91\E2\0F\00\07z\03\03\00Y`\00\02@\00Uz\00\03\00X\B0\00\11\04\10\00\10Y\10\00\12\F4\B0\00\08\00\01\10\C4\B0\004\08\04@`\00\11\CA@\00\81\08\00\\\00\00pb\FA@\00@\10x\0A\08\90\00B\FF\E0\FF\07\10\006\10\08\02\10\00\000\00\12\0A0\00#\FC\03\10\00\12\10\10\00\11\F8\10\00T\10x\13\08\030\00\91\C6\0F\00\07\D2\06\08\FF\00\B0\00\11\04\E0\00H\D4\09\FF\04\D0\00#\D2\08 \00#\00\05P\00\12\13P\00\11\F6\C0\00\A3%\D6\06\06\00`\00\00\09\020\00S\E8\0C\0A\01\000\00\10\C6 \00H\08\08\00^ \006\0A\0A\01p\00P\00\81\D9\06\06p\00\BA\00\19\1E\0C\00\A4\00\00$\E4\0B\80\00S\C8\0E\10\02\00P\00u\E2\0F\04\81\D9\12\080\00\87\A2\02\00\07\C8\10\10\02P\00@%\E6\0C\0Cp\00\14\0Bp\001\B8\11\13\EF\01\03\90\00:$\C4\15`\00E\B8\07\13\03@\00\86\1F\00%\E6\0A\0A\00`@\00E\81\E9\14\0Cp\00p\E6\00\00%\C6\0E\0E`\001\15\02\8Ep\01E\81\E9\0A\0A \00\87\E4\0E\00$\B4\16\FF\04\D0\01c%\C6\08\10\00`0\00u\E2/\00\81\C9\0E\0E0\00p&\0F\00%\B6\10\11P\00\12\16P\00F\08\81\C9\08\E0\00\10$ \00D\0C\07\00` \00e\1F\00\81\B9\10\10 \00fh\0F\00\81\B9\0C\A0\00\10b\E0\014\05\05\04\E0\01\81\E2\0F\00#\D2\03\12\06\0C\07P\00\00\00\C6O\D0\02\10\05`\021pR\FA\C0\01T#\E2\03\14\0A \00\85\C8\8F\00#\C2\03\0E\08\10\00v\0F\01#\B2\03\10\0C\10\00p\02GY\00\00P\FD\D1\03\11\83@\03\14Ap\04\03P\03A\88s\00\02,\00\00\96\06f\E8\0F\00\1D{\00\01\00\84\EC\0F\00\84\89\04\02\00 \00\12\E2\C0\03\10?\90\000@\F2\03\A0\01c\84\89\05\02\00\10 \00\93$\0E\00!\82\05\04\05\00\01\00\8F\CA\1F\00\88\83\00\02\05`\00\09\1E\99`\00\14\1F \04\00`\00!\99\07\07\08\05`\00H\92\07\04\07`\00O\93\00\02\07\C0\00\0A\1A\03`\00\10r\EC\01\03\E0\03\13\C6\E0\00\18\04\C0\00J\03\03\04\00\C0\00\0F \01\099M\19\00P\01'\84yp\00\96\22\0E\00$v\04\FF\00b`\02c$v\05\FF\00c\10\00a\CA\0F\00\8Ey\00\81\05@\84\E7\10\0C\D0\02\1BM\A0\01cGy\00\00\F0\FF\C0\01f\C0\0F\00\18y\00\01\00\0F\10\00\A0\0F\01\00-\14\01\DC\02\0B\01\00\22@\00\01\00=W\01\000\00\08\01\00\1F\0B@\00\04\13\97)\00\1F\D4@\00\0C\13\13t\09\0C\01\00\13pU\00*\A8\00\98\09\22\08\00\01\00\22\18\00\01\00.*\01T\00\00\01\00\13\18u\02/p\00\80\00\0B\1F)'\00\03#\00\88@\00\04\10\0C\04\E4\00*\04\00\01\00\1Fl@\00\04\13\B8)\00&\B8\00@\00\1F\0A@\00\00!H\01D\01\0D@\00\13p\F5\03*\D8\00\01\00\1B\08\08\00?7\01\00F\0D\00Q\00\00H\06\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\E8\14\01\0C\84\01\13X@\00\17\901\01\0F\C0\00\01\132@\00\15\06R\00\0B\A1\00\16\07\80\00j\06\00\00\19\80\00\01\00\13\A9+\00+\03\00\01\00\04\DB\02\1F\04\80\00\0B\11\06\D9\06\07\E8\11\0B\01\00\1B\A8\08\00\04\97\00\13\018\00\04\E8\00\0D8\00\1A\08\08\00\088\00\18\06\A0\00\0F\01\00\05\03\A9\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00"
@main_kernel_5_main_kColReduction_reduce__6_1_0___block_tile_h64_kernel_name = internal constant [50 x i8] c"main_kColReduction_reduce__6_1_0___block_tile_h64\00"
@main_kernel_5_blob_gpu.binary = internal constant [1112 x i8] c"P\EDU\BA\01\00\10\00H\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\08\04\00\00\00\00\00\00\01\04\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\0Ae__6_1_0___block_tile_h64A\00\0F;\00$oshared=\00$\9Fconstant0@\00!\FA\01debug_frame\00.rel\11\00!nv\14\00\11aL\00\0FO\01 \0F\91\00\1E\0F\81\01\DEo_param\88\01\1C\0F\01\00\05\8Cd\00\00\00\03\00\0A\00\01\00 \14\01\18\00,\09\00\01\00\11[\18\00,\04\00\01\00\11y\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\1C\00\00\00E\00#\04\10\18\00\80/\08\00\05\00\00\00\06\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\048\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C$\00u\01\00\08\00\00\F0!\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00p\00\00\00\C0\00\01\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\00#\02b,\00\00\00H\00\01\00\00 \01/\05\00\01\00\FFp@$v\01\FF\CF\03\A3\FF\00\8E\07\00\C4\0F\00\C3y\AC\03\A2%\00\00\00\22\0E\00\19y\00\01\00\F3\0E!\00\00\00b\0E\00\90r\05?\04\00\00\80?\E0\FF\0F\00\C6\1F\00\B9z\04\00\00X\9E\02p\E4\0F\00\A4r\04\05B\00\B1?\02\8E\0F\00\CC\0F\00\0C|\00\FE\03\95pR\F0\0B\00\DA/\00M#\04\F3\0D\80\03\00\EA\0F\00\02z\02\00\00Z\00\00\00\0F\00\00\00\E2\0F\00$v\03\FF\00[\90\00\22\E2\0F`\001F\00\00s\04\80\C8\0F\00\86y\00\02\FFp\030\19\10\0C \00*MyP\00PGy\00\00\F0\F9\03\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\80\0F\01\00-\00\C7\04.\03\00\01\00\22@\00\01\00=O\01\000\00\08\01\00\1F\0B@\00\04\13\8F)\00\1F\88@\00\0C\13\13\CC\03\0C\01\00\13\18U\00*\90\00\F0\03\04\0D\04\22\18\00\01\00.\22\01T\00\00\01\00\13\A8@\00/p\00\80\00\0B\1F)'\00\03A\00\18\04\00\01\00\04\00\06\04\E4\00*\04\00\01\00\1Fj@\00\04\13H)\00&L\00@\00\1F\0A@\00\00!@\01D\01\0D@\00\13\98)\00*\D8\00\01\00\1B\08\08\00?/\01\006\07\003\00\00p\1D\05\17\10\80\00\17\048\00\04\18\00\13\E2\14\01\0C\84\01*\80\05\E8\06\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\80\07\12\03\C0\01:\06\80\00\01\00\13\06\D8\01\04(\0B\0D\C8\01\1A\00\08\00\04\F8\00\13\018\00\04\A8\00\0D\01\00)\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00\00\00\00\00\00\00"
@main_kernel_4_main_kColReduction_reduce__6_1_0___thread_tile_h32_1_kernel_name = internal constant [53 x i8] c"main_kColReduction_reduce__6_1_0___thread_tile_h32_1\00"
@main_kernel_4_blob_gpu.binary = internal constant [3368 x i8] c"P\EDU\BA\01\00\10\00\18\0D\00\00\00\00\00\00\02\00\01\01@\00\00\00\D8\0C\00\00\00\00\00\00\D8\0C\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8!\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\00!@!\07\001\00\80\1E\07\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\0De__6_1_0___thread_tile_h32_1D\00\0F>\00'oshared@\00'\9Fconstant0C\00$\FA\01debug_frame\00.rel\11\00!nv\14\00\11aO\00\0F[\01 \0F\94\00!\0F\90\01\EAo_param\97\01\1C\0F\01\00\0A\8Cg\00\00\00\03\00\0A\00\01\00  \01\18\00,\09\00\01\00\11j\18\00,\04\00\01\00\11\88\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\16\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\18\00\00\00E\002\04H\05\18\000/\08\00\0A\00\10\22\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04X\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\010\00\03\190\00\04\17\0C$\00u\07\00(\00\00\F0!\10\009\06\00 \10\009\05\00\18\10\00u\04\00\10\00\00\F0\11\10\009\03\00\0C\10\009\02\00\08\10\009\01\00\04\10\00\01\01\00\F1\02\F0\11\00\03\1B\FF\00\04\1C\08\00`\00\00\00\90\15\16\003\00K\00\01\00`\02\02\08\10\0A/\E7\00\11\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00\80\01/\05\00\01\00\FF\F0@$v\01\FF\AF\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\CB\02Q\0E\00\19y\03\0F\00\10\00\08\08\F0\15$\0E\00$z\00\00\00Z\00\00\03\02\8E\07\00\CA\1F\00\12x\FF\00\FF\01\00\00\FF\C0\80\07\00\C8\0F\00\0C \00\C5[\00\00pdp\00\00\DA\0F\00M\F3\04\B0\80\03\00\EA\0F\00\19x\03\FF\1F\0F\00\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\003\05a\C6\0F\00\11r\03q\00@\FFH\8F\07 \00c$v\00\FF\00X\A0\00\12\C6@\00\00S\00\10\03@\00P\E4\0F\00\0Cx$\03`\00\00pR\F0\03 \00c$x\03\03 \000\00@\E2\0F\00\07\90\00\22Y\00\01\00p\C8\0F\00\10x\0A\030\00\F0\05\FF\E0\FF\07\00\E4\0F\04\0Cz\00\03\00\\\00\00pb\F4\03\10\00Z\10x\0C\03\02 \00\12\0A \00\10\FC \00f\00\10x\12\03\03 \00\00\F0\00\12\0C \00C\FA\03\00\C4\00\01\13X\90\00\02@\00\01\80\00ApR\F2\03\D0\00G\A4\09\FF\04\A0\00B\0Cz\00\12@\00\13\F6`\004\02\03\04`\00i\E2\0F\04$\E4\0B0\00c\07\A2\04\03\FF\00\FF\04c\E4\0F\04\07\A2\08\10\00 \80\04 \00<$\D4\0D`\00\12\02`\00\11\F8\80\00@%\A6\04\04\E9\04#\09\02P\00D\E8\06\0A\01@\00\10\C4\10\004\0A\0A\01`\00\010\00G\08\08\00^0\00F\D8\0E\0C\02p\00A\04\81\A9\05#\06\D8\00\19\1E\0C\00\A2\00\00\07\D8\0C\0C\02@\000\E6\06\06@\00\12\0B@\00f\08\07\B8\10\12\03@\00U\00\81\A9\00\08@\00\95\A4\02\00%\E6\0A\0A\00`0\00g\00\07\B8\12\12\03P\00E\81\E9\07\060\00p\E4\0E\00%\D6\0E\0E`\00\14\0D0\008\C2\13\02\10\01F\81\E9\14\0A0\00V\08\00$\B4\15 \01\96\C4\0F\00%\D6\0C\0C\00`@\00E\81\D9\0F\0E0\00pf\0F\00%\B6\10\10`\00\14\15\C0\00F\C2\0A\02\FF\90\00U\01\81\D9\0C\0C0\00\10d0\00C\08\12\00`0\00u\E4/\00\81\B9\11\10 \00i$\03\00$\C4\0B\80\006\81\B9\08\00\01u$\0F\00%\C6\12\130\01\10\C8\10\00\07\10\01u\E4\0F\00\81\C9\13\120\00\10(\10\00\16\0A\E0\00\84\22\0F\00$r\02\FF\FF`\00\10\E2P\025\04\03\05\B0\02u\1F\04\10x\10\03\07`\02Q/\04#\A2\02\08\06\01\D4\00\84\E2O\00\10x\05\03\06 \00\02\D0\02\15\04 \03\83\E2\0F\00#\E2\02\07\14@\060\00\E2\8F \00\15\05 \03\84\C6\0F\00#\D2\02\0F\0C \00\85\C8\0F\02#\B2\02\11\080\00j\0F\01\0Cz\00\10\F0\028\11\03\08\F0\02\0B \03W\07\A8\08\04\01\F0\02V\10x\00\03\090\00f\00#\C2\02\13\0A`\00\00\90\00\15\11\F0\02\10\E4@\007\0A\04\01\10\02*$\E4 \03Y\07\E8\06\05\02\00\039\0C\05\02\00\03'\0A\0A\00\03\11\08\D0\03\04\F0\03\00`\008\B8\0E\10\D0\02\010\03\08`\03G\B8\10\10\03P\006\81\A9\05\B0\01%\A4\00 \03\07\C0\02)\11\FF \03\19\04 \039$\B4\17@\02*%\E6\C0\02_\07\C2\12\11\FF0\03\09\12\B60\03\12\17P\02Y\08\81\E9\14\0C \03&\C4\16`\00\10\E4\D0\02C\0A\10\00`0\00W\E2\1F\00\81\B9 \03\10b\10\04\11\15\80\05\04@\011%\C6\10\C0\02\14\16`\009\B9\0A\0A \03\12\C6 \03\01 \00 \E2/0\04(\00\01P\03\18\C90\03\1A\01\D0\04\01\00\03\0B0\035\D6\12\15@\01\1B\C8\D0\03\000\00\1B\D90\03\08\B0\03\10\22\80\029\10\03\0C \034\00\03\0E\10\00\00`\02E\A2\02\05\04`\02\11OP\03\17\0A \03\000\03\14\0B\10\00\1F\E40\03\0Cw\C8\8F\00#\B2\02\0F\B0\02\16\02@\03\12\F6@\03\1F\C20\03\05\02\D0\02\000\03\17\0D\A0\00\0CP\06\00\F0\02\14\02\90\01\02\80\06\18\11\C0\06\00P\03\17\02 \02Z#\D2\02\13\0C@\03\06\E0\02\00`\00\1B\B40\037\B8\06\05\F0\02\1B\04 \03d\00\07\B8\0C\05\03`\00\00P\03O\C2\0E\10\FF \03\09O\C2\10\10\FF \03\09\19\B6 \03i\08\07\E8\13\11\01\80\06\0B \03\17\C4 \03\01\C0\02\0B \03H\E8\12\11\01p\00\1B\B9 \03\17\C6 \03\12\E2\E0\02\17\02\C0\00+\81\B90\03*\E4\16\90\03\1B\C60\03\1B\C9P\06\17\E6 \03\00\D0\00I\D8\0C\00\02\00\03\0A0\03\09\D0\07\00\D0\00\1A\E6@\03'\81\E90\03*&\01\10\03\00`\06\18\E90\03\1F\03 \03\22\1B\11 \03\14\13\10\00\0F \03\04\14\0F \00\03 \03\14\10\10\00\0F \03\01\1F\B2P\06\05\05\10\03\0C0\03\06\10\03\1C\CE\00\03\11/\00\03\08\A0\02*#\E2P\03\00@\03\17\12\90\00\00\10\03\16\03\80\01\0F\10\03\01\1F\11\10\03\0A:\B2\06\05\E0\02\090\06\07P\03\11\FC \0A3\07\B2\0C0\00\1F\00\10\03\03:\C8\0E\10\E0\02)\05\0A`\09H\C8\10\10\01@\00\08 \03\00p\05\0F\10\03\1DH\D8\13\11\02p\00\08\10\03\88\E2\0E\00\07\D8\12\11\02p\00\08 \03\1F\C4\10\03\00\1B\D4@\06\0B\10\03H\07\E8\15\00\A0\01\09 \03\10d\C0\05\0B \03O\E8\0C\00\03 \03\08'%\D6\10\03[\C4/\00\81\D9@\06)\E4\0D\90\00+\81\D9@\06\1B\E6@\06\08P\07\000\00\1B\E9 \03\1E\E9 \03\1B\16 \03\14\18\10\00\0F \03\04\1B\14 \03\14\15\10\00\0F \03=\11\C6\D0\02\0D\00\03\14\17p\00\1F\C6@\06\00O\A2\0A\04\FF@\06\02\03\F0\08G\A2\08\04\FF\10\02/#\E2@\06\05\1F\FC@\06\06\1F\01@\06\0C\18\01@\06F\C8\0E\10\020\00\0F0\03\00?\10\10\02@\06\19O\D8\13\11\03@\06)O\D8\12\11\03@\06\05.$\07@\06G\E2\15\00\FF\C0\00\090\03*$\0B0\03\0F@\06\00H\07\E2\00\00p\01\090\03*$\010\03\00\00\04\08 \03\10$\F0\0A\17\06\D0\00\1A\8F0\03\1B\E20\03%\E6\0E\10\03\22\06\02\90\00\090\03\00 \07S\E6\0C\00\00` \009\E4\0F\02 \03\1Bh \03\10b\E0\026\00\03\1A\E0\02d\04\10x\06\03\1B\10\00\01\00\034\0E\03\1C\10\00/\E2\1F0\03\007\05\03\19\B0\02\09\10\03;\C8\0F\01\00\03\00\80\02\15\06@\03\0D\00\03\17\8F\A0\02\12\FA \00\0A\C0\02\16\02`\03\04\F0\0F9\0A\03\1D\C0\009\08\03\1E\10\007\07\03\1F\A0\002\07\D8\16L\00\00\B0\01\02\D0\03\15\0E\B0\03\02\10\00\15\0A\A0\03\00P\03G\E8\10\05\010\00W\07\A8\0D\06\03\10\001\03x\0C\C0\01\01\01\00\02\90\0D\18\08\90\03H\0Cz\00\07\B0\00\06\C0\00\12\F4`\00H\B2\0B\0E\FF`\00&\C8\09\E0\0F\10\CA\B0\00(\04\08\B0\00H\07\E8\03\07\80\00\060\01\01\D0\10d\00\07\A8\14\05\01\D0\02\00 \000r\00\0C`\00$pRp\00%\0E\0E\F0\02\01\C0\03(\0A\0A0\00T\07\D8\08\08\02\10\00\000\01H\88\12\00\02 \00H\E8\00\07\03\10\008\A8\18\06\10\00\06\E0\00\03P\11\1A\84\B0\039$\A4\1A\10\00E%\86\12\12\90\03f\D0\0F\00$\94\11 \00\00P\03&\89\07\C0\02p\A6\00\00%\96\14\140\00\14\11\10\06!\96\10\90\03#\11\020\005\99\05\140\00v\E6\02\00%\86\16\16\B0\03\00 \00\17\06P\03`\08\00%\A6\18\18P\00\14\1A \005\89\0C\16 \00f\A6\0A\00$\B4\1C\90\00\00\00\04S\A6\1A\0D\00`0\00\00\80\06:\A9\0D\18\D0\04\02`\04\15\1C\F0\07&\1A\1A \00V\0E\00$\C4\1DP\00\01\C0\04 \14\0BP\00\14\1C\F0\03\08\C0\0Dl\A6\0E\00$\D4\1F\80\04\01\90\05\13\1D0\05H\81\B9\14\140\00c%\C6\10\09\00` \00\10\E4\90\07\19\0Bp\041%\D6\120\12\11\1F \00\00\00\0B*\10\10\90\04\090\0BA\02%\D6\08\90\12\14\1F \01\08\90\0A\010\0B)\18\00\E0\04\08\90\04\00\00\08U\E6\16\03\00` \00F\00\81\E9\190\01\01\90\04(\16\16\90\04c$v\04\FF\00b\80\00\00\C0\06H\92\02\05\06\A0\0D5\82\02\07\10\04 \C8O\90\04%\0D\1A\10\00\01\80\04\16\0F\90\07\00P\00C\05\FF\00cP\00\02\90\07(\0B\10\A0\04H\D2\02\13\08\10\11D\E2\02\19\16\10\00a\CA\0F\00\8Ey\00\90\0D@\84\E7\10\0CP\00\14M\80\15\030\15PGy\00\00\F0\A9\19\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\B0\0F\01\00-\00\\\04.\03\00\01\00\22@\00\01\00=[\01\000\00\08\01\00\1F\0B@\00\04\13\9B)\00\1F\97@\00\0C\14\13\CC\01\0B\01\00\138U\00\11\90\06\00\06p\19\22\08\00\01\00\22\18\00\01\00..\01T\00\00\01\00\13\C8@\00/p\00\80\00\0B\1F)'\00\03!\008\F5\02$\00\00\E0\1B\04\E4\00*\04\00\01\00\1Fm@\00\04\13h)\00&\AC\00@\00\1F\0A@\00\00!L\01D\01\0D@\001\18\05\00\01\00*\D8\00\01\00\1B\08\08\00?;\01\00\16\1D\003\00\00\F0@\00&\10\00\80\00\17\048\00\04\18\00\13\EB\14\01\0D\84\01!\06\00\01\00\17\901\01\0F\C0\00\01\132@\00+\06\00\01\00\1A\08`\1D\12\03\C0\01>\22\80\00g\00\17\05(!\0C\01\00*\A8\00\08\00\04\C0\00\13\018\00\0Ey\00\03x\00\1A\18\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00"
@main_kernel_3_main_kColReduction_reduce__6_1_0___thread_tile_h32_kernel_name = internal constant [51 x i8] c"main_kColReduction_reduce__6_1_0___thread_tile_h32\00"
@main_kernel_3_blob_gpu.binary = internal constant [1112 x i8] c"P\EDU\BA\01\00\10\00H\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\08\04\00\00\00\00\00\00\01\04\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\0Be__6_1_0___thread_tile_h32B\00\0F<\00%oshared>\00%\9Fconstant0A\00\22\FA\01debug_frame\00.rel\11\00!nv\14\00\11aM\00\0FS\01 \0F\92\00\1F\0F\86\01\E2o_param\8D\01\1C\0F\01\00\04\8Ce\00\00\00\03\00\0A\00\01\00 \18\01\18\00,\09\00\01\00\11`\18\00,\04\00\01\00\11~\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\1C\00\00\00E\00#\04\10\18\00\80/\08\00\05\00\00\00\06\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04@\04\91\015\00\00\04\0A\08\00\02\FC\00\91\10\00\03\19\10\00\04\17\0C$\00u\01\00\08\00\00\F0!\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00p\00\00\00\C0\00\01\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00 \01/\05\00\01\00\FFh@$v\01\FF\C7\03\A3\FF\00\8E\07\00\C4\0F\00\C3y\A4\03\A2%\00\00\00\22\0E\00\19y\00\01\00\F3\0E!\00\00\00b\0E\00\90r\05?\04\00\00\80?\E0\FF\0F\00\C6\1F\00\B9z\04\00\00X\96\02p\E4\0F\00\A4r\04\05B\00\B1?\02\8E\0F\00\CC\0F\00\0C|\00\F6\03\95pR\F0\0B\00\DA/\00M\1B\04\F3\0D\80\03\00\EA\0F\00\02z\02\00\00Z\00\00\00\0F\00\00\00\E2\0F\00$v\03\FF\00[\90\00\22\E2\0F`\001F\00\00k\04\80\C8\0F\00\86y\00\02\FFh\030\19\10\0C \00*MyP\00PGy\00\00\F0\F1\03\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\80\0F\01\00-\00\BF\04.\03\00\01\00\22@\00\01\00=S\01\000\00\08\01\00\1F\0B@\00\04\13\93)\00\1F\8D@\00\0C\13\13\C4\03\0C\01\00\13 U\00*\90\00\E8\03\04\05\04\22\18\00\01\00.&\01T\00\00\01\00\13\B0@\00/p\00\80\00\0B\1F)'\00\03A\00 \04\00\01\00\04\F8\05\04\E4\00*\04\00\01\00\1Fk@\00\04\13P)\00&L\00@\00\1F\0A@\00\00!D\01D\01\0D@\00\13\A0)\00*\D8\00\01\00\1B\08\08\00?3\01\00.\07\003\00\00x\15\05\17\10\80\00\17\048\00\04\18\00\13\E5\14\01\0C\84\01*\88\05\E0\06\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07x\07\12\03\C0\01:\06\80\00\01\00\13\06\D8\01\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\13\018\00\04\A8\00\0C\01\00*\F8\02\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00\00\00\00\00\00\00"
@main_kernel_2_main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1_kernel_name = internal constant [58 x i8] c"main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1\00"
@main_kernel_2_blob_gpu.binary = internal constant [3000 x i8] c"P\EDU\BA\01\00\10\00\A8\0B\00\00\00\00\00\00\02\00\01\01@\00\00\00h\0B\00\00\00\00\00\00c\0B\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\A8#\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00g\02\00\BE\00r\00\01\00\22#\00\01\00\11 \06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\12e__6_1_0___no_ibXblock_tile_h64_1I\00\0FC\00,osharedE\00,\9Fconstant0H\00)\FA\01debug_frame\00.rel\11\00!nv\14\00\11aT\00\0Fo\01 \0F\99\00&\0F\A9\01\B6\8F$____wg_B\00&\00\1B\00/26\F1\016o_param\F8\01\1C\0F\01\00\05\8Cl\00\00\00\03\00\0A\00\01\00\11\EF\18\00,\0B\00\01\00 |\01\18\00,\09\00\01\00\11\CB\18\00,\04\00\01\00\11\E9\18\00,\07\00\01\00g2\00\00\00\12\10x\00\11\18\06\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0\17\07\00 \00\04\9B\00R\04\14\00\00\00E\002\04\C0\05\18\00C/\08\00\06\7F\00\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E0\04\F1\08\015\00\00\04\0A\08\00\03\00\00\00`\01(\00\03\19(\00\04\17\0C$\00\10\05\ED\04%\F0!\10\009\04\00\18\10\009\03\00\10\10\00u\02\00\08\00\00\F0\11\10\009\01\00\04\10\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\00\10\17\00\00`\17\00\00\04\1Et\01#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\005\02\00\00\84\01\0F\01\00\FFz@$v\01\FF?\04\A3\FF\00\8E\07\00\C4\0F\00\19y\A6\01\10%[\02a\0E\00\19y\03\00\01\00\10!-\00\F5\14\0E\00$z\06\06\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\06\00Y\00\00p`\F0\03\00\DA\0F\00M[\04\B0\80\03\00\EA\0F\00\19x\03\FF\1F\FC\01\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\00\B3\04\93\E2\0F\00Ey\00\00@\150\00\93\E2\0F\00$r\08\FF\FF\00\90\00p\E2\0F\00\11r\03\03\92\00\C1\FF@\8F\07\00\C8\0F\00\12x\05\031\04\10\C0\80\00p\0F\00$x\06\06\01\B4\03\12\0A\10\00@\12x\05\06p\00\01 \00p\E4\0F\04\19x\00\FF*\04\C0\06\16\01\00\00\E4\0F\00\0Cr\00\05`\00@pR\F2\03 \00P\0Cx\00\06\7F\10\00\22@\F0\80\002x\07\05\C9\02\11\8Ep\00T$x\07\07\04\90\00\9A\CC\0F\00G\19\00\00\80\14\E0\00\000\00\10\03\E0\00\01\90\00*\03\03@\004\09\03@@\00q\C8\0F\00%x\04\09P\00\11\02\E0\00P\04\10x\00\09\C0\001\FF\E0\FF\B0\00\B0\0Cz\00\09\00Z\00\00pb\F4\A0\00P\00\12x\04\04P\00!\FF\FC\D0\00\00p\01\12\00 \00\11\F6 \00`\10z\02\04\00\\@\00\11\F3@\00`\10z\04\04\00^\10\00\11\F9\D0\01\00`\00\00|\03\03`\00\D2\00\10z\03\05\00]\00\00\FF\E4\FF\000\00@\05\05\00_\10\00*\7F\02`\00\11\F8\10\01\F4\06\81\B9\0D\02\04\FC\FF\FF\00\19\1E\0C\00\A2\0E\00\10x\0A\09\03P\00\93\C6\0F\00\81\A9\00\02\04\F8 \00 \E2\0E@\00\12\0A@\00\22\FA\03 \00%\0B\04 \00u\E8\0E\00\81\B9\0A\04P\00p\A8\0E\00\81\C9\0F\02\10\01\01\10\00y(\0F\00\81\C9\0C\04\10\00S\D9\11\02\04\04\10\00\10h\10\00%\0E\04\10\00\10b\E0\004\10\09\04\90\00\01\F0\00:\12\09\05P\01\12\10\90\00#\FC\03\E0\00\12\12\10\00\12\F2@\01Dx\10\09\060\00a\E2\0F\04#\A2\086\07\00$\021\00\E2\8F@\01\14\07 \00q\CE\0F\00\81\E9\0B\02\91\01\06\F0\00\15\10\B0\01\93\E2\0F\00#\B2\08\0D\0A\00=\05!\E2O0\01\16\08`\00`\00\81\99\0D\02\04\17\06\03P\01\09\D0\01\000\018\E9\00\04`\00U#\C2\08\0F\0CP\00&\0F\01`\01\11\F8\C0\00H\81\99\0A\04P\00\00\D0\00\19\09\A0\01C\0F\02\04\10 \00\86\22\0F\00#\D2\08\11\0EP\00\16\02\C0\00\12\FAP\00W\A9\0C\04\04\10\80\01W\B9\11\02\04\14\80\019\B9\0E\04\10\00X\C9\13\02\04\18\10\00)\10\04\10\00X\D9\15\02\04\1C\10\00H\12\04\04\1C\C0\017\14\09\0A \01U#\E2\08\0B\00\A0\00\02\80\01\17\0B\E0\01E\0Cz\00\14\D0\01\000\00\1A\92p\01\06P\01\04\E0\017\00\09\0C\10\02\00\90\01\18\0Dp\00\1D\A2`\01\15\00\D0\01\13\CA\F0\01\18 `\018\0C\09\0F`\01\00\B0\01\07 \00-#\B2`\01\19\0A\B0\03Lx\0A\09\0E\10\02\19$\F0\01$\13\10@\00\02\B0\02\06\F0\01\13\E2\D0\01\17(\A0\01\00\00\02\08@\00F\D2\08\15\12\80\00\00@\00\15\0C\E0\01\13\C4\D0\01\17,\90\01\00\F0\01\08P\00\00\E0\01\1B,\E0\01\1B0\E0\01\1B0\E0\01\1B4\E0\01\1A4\E0\01\1F\10\E0\01\06\11O\A0\01\1F\11\E0\01\16\1A\8F\E0\01\13\D6\A0\01\188\A0\01;\00\09\12\F0\01\1F\13\F0\01\15\13\E4\D0\01\1F8\D0\01\1B\16\14`\00\11\04\D0\01\16<\90\01X\10x\0E\09\15\80\00\08\E0\01\1B\E2\E0\01\04\B0\03\1B@\E0\01\1F<\E0\01\0A\1D\0E\C0\03\1B@\C0\03\1BD\C0\03\1BD\E0\01\1BH\E0\01\1BH\E0\01\1BL\E0\01\1AL\E0\01\1F\16\E0\01\0C\1F\17\E0\01-\1AP\E0\01\1F\18\D0\01\08\00\90\01\16\19\00\02\1F\00\E0\01\02\1FP\E0\01\17\01\D0\01\18T\D0\017\0A\09\1A\D0\00\00\10\04\1F\1B\E0\01\15\13\E4\E0\01\1BX\E0\01\1FT\C0\03\1C\1B\\\C0\03\1BX\C0\03\1B\\\E0\01\1B`\E0\01\1B`\E0\01\1Bd\E0\01\1Ad\E0\01\1F\1C\E0\01\0C\1F\1D\E0\01\05\13\DA\C0\01\17hp\01\0F\F0\01\09\01P\03;\00\09\1E\C0\03\1F\1F\C0\03\1D\1Fh\E0\01\14\01\C0\01<\0A\09 \C0\03\1Al\C0\03\1F!\C0\03\1D\1Bp\E0\01\1Fl\C0\03\1C\1Bp\C0\03\1Bt\C0\03\1Bt\E0\01\1Bx\E0\01\1Bx\E0\01\1B|\E0\01\1A|\E0\01\1F\22\E0\01\0C\1F#\C0\03-\16\80\90\01\00P\00\1F$\C0\03\0C\1F%\C0\03\0D\1F\80\C0\03\1C\18\84\D0\017\0A\09&\D0\00\00\C0\03\1F'\C0\03\1D\1B\88\E0\01\1F\84\C0\03\1C\1B\8C\C0\03\1B\88\C0\03\1B\8C\E0\01\1B\90\E0\01\1B\90\E0\01\1B\94\E0\01\1A\94\E0\01\1F(\E0\01\0C\1F)\C0\03\0D\1F\98\C0\03\1B\1B*\C0\03\1F+\C0\03\1D\1F\98\C0\03\1B\1C,\C0\03\1A\9C\C0\03\1F-\C0\03\1D\1B\A0\E0\01\1F\9C\C0\03\1C\1B\A0\C0\03\1B\A4\C0\03\1B\A4\E0\01\1B\A8\E0\01\1B\A8\E0\01\1B\AC\E0\01\1A\AC\E0\01\1F.\E0\01\0C\1F/\C0\03-\16\B0\90\01\00P\00\1F0\C0\03\0C\1F1\C0\03\0D\1F\B0\C0\03\1C\18\B4\D0\017\0A\092\D0\00\00\C0\03\1F3\C0\03\1D\1B\B8\E0\01\1F\B4\C0\03\1C\1B\BC\C0\03\1B\B8\C0\03\1B\BC\E0\01\1B\C0\E0\01\1B\C0\E0\01\1B\C4\E0\01\1A\C4\E0\01\1F4\E0\01\0C\1F5\C0\03\0D\1F\C8\C0\03\1B\1B6\C0\03\1F7\C0\03\1D\1F\C8\C0\03\1B\1C8\C0\03\1A\CC\C0\03\1F9\C0\03\1D\1B\D0\E0\01\1F\CC\C0\03\1C\1B\D0\C0\03\1B\D4\C0\03\1B\D4\E0\01\1B\D8\E0\01\1B\D8\E0\01\1B\DC\E0\01\1A\DC\E0\01\1F:\E0\01\0C\1F;\C0\03-\16\E0\90\01\00P\00\1F<\C0\03\0C\1F=\C0\03\0D\1F\E0\C0\03\17\00P\00\17>\C0\00\00\B0\03\17?`\00g\81\99\09\02\04\E4\A0\01\0F\C0\03\0D\00\D0\01\040\00\00P\12Y\A9\0D\02\04\E8\E0\10\0F\C0\03\0B\00\E0\01\18\E8\E0\01K\0F\02\04\EC\E0\01\1B\EC\D0\01\18\F0\10\00K\11\02\04\F0\D0\01\18\F4\10\00%\13\02\10\00\1Bb\D0\01 \C8O\B0\01\15\09\B0\01 \C8\8F\80\01\15\0D\80\01\86\C8\0F\01#\B2\08\0F\0E\10\00f\02#\C2\08\11\10\10\00\00\E0\00\15\13\E0\00P\C4\0F\00Ay\09\00\06\90\14c\88s\00\07\08\00!\00f\E8\0F\00\1D{\00\01\00S\EC\0F\00\84\89\CD\19\13\08 \01Ax\00\06?\00\15\11\F2@\03c\84\89\03\07\00\10 \00s$\0E\00!\82\00\00\02\16\10\00\F0\15'\88\83@\00\0F`\00\01-\99\02`\00\14\1F`\15\00`\00W\99\03\07\00\08`\009\92\02\02`\00O\93\00\07\02\C0\00\0A\1A\03`\005r\00\06\D0\15\01\C0\00H\04\07\00\04\C0\00J\04\03\04\00\C0\00\1F\04`\00\089M\19\00P\016\84y\07p\00\93\22\0E\00$v\02\FF\00`\D0\15\93\C4\0F\00$v\03\FF\00a\10\00a\CA\0F\00\8Ey\00\01\01\9B\84\E7\10\0C\00\E2\1F\00M\A0\01PGy\00\00\F0\C0\16\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00`\0F\01\00-'\01\00 \02\07\01\00\22@\00\01\00=o\01\000\00\08\01\00\1F\0B@\00\04\13\AF)\00\1F\F8@\00\0C\13\13\\\1A\0C\01\00\13\A8U\00*\A8\00\80\1A\22\08\00\01\00\22\18\00\01\00.B\01T\00\00\01\00\13P5\02/p\00\80\00\0B/)\00'\00\02#\00\C0@\00\00,\09\17\00\E4\00*\04\00\01\00\1Fr@\00\04\13\F0)\00&\98\00@\00\1F\0A@\00\00!`\01D\01\0D@\001\88\05\00\01\00*\D8\00\01\00\1B\08\08\00?O\01\00\0E\1E\00Q\00\00`\06\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\FA\14\01\0C\84\01\13p@\00\17\881\01\0F\C0\00\01\132@\00\15\06R\00\0B\A1\00\16\18\80\00j\06\00\00\18\80\00\01\00\13\B5+\00+\03\00\01\00\00\D5\0F\1A\00q\00\0F\80\00\01\11\06\F0\1D\07\E8\22\0CH\02\1A\00\08\00\04\97\00\13\018\00\04\E8\00\0C\01\00*\90\19\08\00\088\00\18\06\A0\00\0F\01\00\05\03\A9\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00"
@main_kernel_1_main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_kernel_name = internal constant [56 x i8] c"main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64\00"
@main_kernel_1_blob_gpu.binary = internal constant [1112 x i8] c"P\EDU\BA\01\00\10\00H\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\08\04\00\00\00\00\00\00\06\04\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00h\0C\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\002\C0\0B\00\01\00\11\09\06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\10e__6_1_0___no_ibXblock_tile_h64G\00\0FA\00*osharedC\00*\9Fconstant0F\00'\FA\01debug_frame\00.rel\11\00!nv\14\00\11aR\00\0Fg\01 \0F\97\00$\0F\9F\01\F6o_param\A6\01\1C\0F\01\00\07\8Cj\00\00\00\03\00\0A\00\01\00 ,\01\18\00,\09\00\01\00\11y\18\00,\04\00\01\00\11\97\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\1C\00\00\00E\00#\04\10\18\00\80/\08\00\05\00\00\00\06\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04p\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C$\00u\01\00\08\00\00\F0!\10\00\01\01\00\C0\F0\11\00\03\1B\FF\00\04\1C\08\00p\A8\04\12\00\01\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00 \01/\05\00\01\00\FF\B8@$v\01\FF\17\04\A3\FF\00\8E\07\00\C4\0F\00\C3y\F4\03\A2%\00\00\00\22\0E\00\19y\00\01\00\F3\0E!\00\00\00b\0E\00\90r\05?\04\00\00\80?\E0\FF\0F\00\C6\1F\00\B9z\04\00\00X\E6\02p\E4\0F\00\A4r\04\05B\00\B1?\02\8E\0F\00\CC\0F\00\0C|\00F\04\95pR\F0\0B\00\DA/\00Mk\04\F3\0D\80\03\00\EA\0F\00\02z\02\00\00Z\00\00\00\0F\00\00\00\E2\0F\00$v\03\FF\00[\90\00\22\E2\0F`\001F\00\00\BB\04\80\C8\0F\00\86y\00\02\FF\B8\030\19\10\0C \00*MyP\00PGy\00\00\F0A\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\80\0F\01\00-\00\0F\05.\03\00\01\00\22@\00\01\00=g\01\000\00\08\01\00\1F\0B@\00\04\13\A7)\00\1F\A6@\00\0C\13\13\14\04\0C\01\00\13PU\00*\90\008\04\04U\04\22\18\00\01\00.:\01T\00\00\01\00\13\E0@\00/p\00\80\00\0B\1F)'\00\03A\00P\04\00\01\00\04H\06\04\E4\00*\04\00\01\00\1Fp@\00\04\13\80)\00&L\00@\00\1F\0A@\00\00!X\01D\01\0D@\00\13\D0)\00*\D8\00\01\00\1B\08\08\00%G\01\DB\0A\09\01\00\13\A8e\05\17\10\80\00\17\048\00\04\18\00\13\F4\14\01\0C\84\01*\B8\050\07\1F\00\C0\00\04\132@\00*\06\00\01\00*\80\07\C8\07\12\03\C0\01:\06\80\00\01\00\13\06\D8\01\05\A8\0B\0C\C8\00\1A\00\08\00\04\F8\00\13\018\00\04\A8\00\0C\01\009H\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00\00"
@ral_send_output___cpu___pvoid_i64_m0df32___void = internal constant [48 x i8] c"ral_send_output___cpu___pvoid_i64_m0df32___void\00"
@d2h___gpu___pvoid_pvoid_m0df32_m0df32___void = internal constant [45 x i8] c"d2h___gpu___pvoid_pvoid_m0df32_m0df32___void\00"
@alloc___cpu___pvoid_i64___pvoid = internal constant [32 x i8] c"alloc___cpu___pvoid_i64___pvoid\00"
@inc_ref___gpu___pvoid_pvoid_m1df32_m1di64___m0df32 = internal constant [51 x i8] c"inc_ref___gpu___pvoid_pvoid_m1df32_m1di64___m0df32\00"
@dealloc___gpu___pvoid_pvoid___void = internal constant [35 x i8] c"dealloc___gpu___pvoid_pvoid___void\00"
@main_kernel_0_main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_1_kernel_name = internal constant [59 x i8] c"main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_1\00"
@main_kernel_0_blob_gpu.binary = internal constant [2320 x i8] c"P\EDU\BA\01\00\10\00\00\09\00\00\00\00\00\00\02\00\01\01@\00\00\00\C0\08\00\00\00\00\00\00\BF\08\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00h\17\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\002\C0\16\00\01\00\11\14\06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\13e__6_1_0___no_ibXthread_tile_h32_1J\00\0FD\00-osharedF\00-\9Fconstant0I\00*\FA\01debug_frame\00.rel\11\00!nv\14\00\11aU\00\0Fs\01 \0F\9A\00'\0F\AE\01\FF\03o_param\B5\01\1C\0F\01\00\04\8Cm\00\00\00\03\00\0A\00\01\00 8\01\18\00,\09\00\01\00\11\88\18\00,\04\00\01\00\11\A6\18\00,\07\00\01\00g2\00\00\00\12\10`\00\11\0C\06\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0\0B\07\00 \00\04\9B\00 \04\18i\00\01:\002\04\B8\02\18\00p/\08\00\05\00\00\00\1A\00\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\88\04\80\015\00\00\04\0A\08\00F\00\A2`\01(\00\03\19(\00\04\17\C5\00u\05\00 \00\00\F0!\10\009\04\00\18\10\009\03\00\10\10\00u\02\00\08\00\00\F0\11\10\009\01\00\04\10\00\01\01\00\F3\01\F0\11\00\03\1B\FF\00\04\1C\08\00`\00\00\00P\D8\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\00\D2\05b,\00\00\00H\00\01\00\00`\01/\05\00\01\00\FF\E0@$v\01\FF\7F\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\BB\02Q\0E\00\19y\03\0F\00 \00!-\00\F0\14\0E\00$z\00\00\00X\00\00\03\02\8E\07\00\CA\1F\00\12x\FF\00\FF\01\00\00\FF\C0\80\07\00\C8\0F\00\0C \00\C5Y\00\00pdp\00\00\DA\0F\00M\C3\04\B0\80\03\00\EA\0F\00\19x\03\FF\1F\0F\00\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\00\03\05a\C6\0F\00\11r\03q\001\FFH\8FP\00\000\00\00C\00\10\030\00\93\CA\0F\00$x\07\03 \00\B0\00p\C8\0F\00%x\04\07s\04\10\FF\90\00\F0\09\E2\0F\04\0Cz\00\07\00Z\00\00pb\F4\03\00\E4\0F\04\10x\00\07\01 \00\B0\E0\FF\07\00\E4\0F\00\12x\04\04\01\031\FF\FC\8E\10\00\01\B0\00\010\00\10\F60\00p\00\10z\02\04\00\\0\00!\F1\07@\00Pz\04\04\00^\10\00\11\F3 \01\00P\00\00,\03\04P\00\C2\10z\03\05\00]\00\00\FF\E4\7F\000\00@\05\05\00_\10\00*\FF\00`\00 \F8\03\F0\00\F4\06\81\B9\0B\02\04\FC\FF\FF\00\19\1E\0C\00\A2\0E\00\10x\06\07\03P\00\93\C6\0F\00\81\A9\00\02\04\F8 \00u\E8\0E\00\81\A9\09\04\10\00 \E2\0EP\00\12\06P\00!\FA\030\005\B9\08\04P\00p\A8\0E\00\81\C9\0D\02\10\01\01\10\00y(\0F\00\81\C9\0A\04\10\00S\D9\0F\02\04\04\10\00\10h\10\00%\0C\04\10\00!b\0F\90\00\14\04\90\00\01\F0\006\0E\07\05\F0\00\16\04\80\00\12\FC0\01Jx\10\07\06 \00\12\0E \00\13\F0 \00:\0E\07\07 \00\12\10 \00\11\F2\10\01T$r\06\FF\FF\D0\01\12\E2P\00\14\0A0\00a\E2\0F\04#\A2\06\06\07\00$\00F\00\E2\8F\00`\00\12\F4P\01S\E9\09\02\04\08\C0\00!\E2\0Ep\00\18\08P\018\E9\00\04 \00@#\B2\06\0B\0F\00\11\06P\00\17OP\00\02\00\02P\81\89\0B\02\04\0F\06\05\A0\018\0E\07\09P\008\89\08\04 \00T#\C2\06\0D\0AP\00\93\C6\0F\01\81\99\0D\02\04\10 \00'\22\0F`\00\12\F8\B0\01W\99\0A\04\04\10\90\01X\A9\11\02\04\14\10\00F\0E\04\04\14@\00U#\D2\06\0F\0C\B0\00&\0F\02@\01 \FA\03\A0\01g\81\B9\0F\02\04\18\D0\019\B9\0C\04\10\00X\C9\13\02\04\1C\10\00)\10\04\10\00X\D9\15\02\04 \10\00H\12\04\04 \10\026\14\07\0B\90\01e\00#\E2\06\09\00\90\00\11\8F\10\03\17\0C0\02E\0Cz\00\14 \02\000\00\1D\82p\01\1A\00 \027\00\07\0D \02X\10x\08\07\0Ep\00\17\92`\016\E2\0F\01@\00C\F2\03\00\CA\00\025$\00\00\00\03\00\F0\01\17$\A0\01F\A2\06\11\0E@\00\00\80\00\17\08 \04\02\F0\01\18(@\028\08\07\0F\F0\01\00\D0\01\17,\90\01\1D\B2\90\01\19\08@\02\00\E0\01\070\00\\\10x\0C\07\10@\02\07p\00Z#\C2\06\13\10\A0\00\15\0C0\02\00P\00X\A9\0F\02\040\90\018\10\07\11P\008\A9\0C\04 \00Z#\D2\06\15\12P\00\060\02\00P\00W\B9\11\02\044\F0\01[\B9\0E\04\0440\02\1B80\02\1B80\02\1B<0\02\1A<0\02\1F\120\02\06\11O\F0\01\1F\130\02\16\1A\8F0\02\13\D6\F0\01\18@\A0\03?\00\07\140\02\08\00\D0\01\16\15`\02\19\00@\02\000\00\1F\A2\D0\01\05\11\F4 \00\01P\02\09p\009\08\07\16\B0\027\0A\07\17\80\00\01P\02\19Dp\04\0F\80\02\04\12\F6\C0\03\00\10\02\040\00\00\F0\05\00p\02\17Hp\02\0E \02\19\0A \02\00p\02\070\00\00\B0\04\19\18 \02O\0F\02\04L \02\0A\19\0E \02\00P\02\17L\E0\01\000\02\1BP0\02\1BP0\02\1BT0\02\1BT0\02\1BX0\02\1AX0\02\1F\190\02\05*\C6O \02\030\04\18\1A0\00\0B0\02\00\A0\01\14\1B \00\13\D20\02\16\\\C0\01\0FP\04\0A\03`\00\1A\1C\B0\06\1A\08\B0\048\08\07\1D\80\00\0E@\02\15\00@\02\13\C4@\02\18\\@\028\0A\07\1E\A0\01_\99\0B\02\04`0\02\14\01\F0\01(\08\040\00\00`\04\18\1FP\00_\89\07\02\04d@\02\18X\89\0A\04\04d\80\06O\0D\02\04h0\02\0A\15\0C0\02\13\C40\02\17h\E0\01\00\90\06\1Bl0\02\1Bl \02\18p\10\00K\11\02\04p \02\18t\10\00%\13\02\10\00\1Bb \02 \C8O\D0\01\06\00\02 \C8\8F\10\02%\07\0A\10\00v\0F\01#\A2\06\0D\0C\10\00\10\02\C0\05\08`\01c$v\08\FF\00`\80\08\10\E4\10\00C\09\FF\00a\10\00\11\E2@\01&\11\10@\00\00\10\01\15\13\10\01p\CA\0F\00\8Ey\00\08\0C\00@\84\E7\10\0C0\00*My\F0\0APGy\00\00\F09\0F\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00p\0F\01\00-\11\01h\0F\0E\01\00\22@\00\01\00=s\01\000\00\08\01\00\1F\0B@\00\04\13\B3)\00\1F\B5@\00\0C\13\13\BC\0E\0C\01\00\13hU\00*\90\00\E0\0E\22\08\00\01\00\22\18\00\01\00.F\01T\00\00\01\00\13\F8@\00/p\00\80\00\0B\1F)'\00\03A\00h\04\00\01\00\00\9B\07\17\00\E4\00*\04\00\01\00\1Fs@\00\04\13\98)\00&\8C\00@\00\1F\0A@\00\00!d\01D\01\0D@\001(\05\00\01\00*\D8\00\01\00\1B\08\08\00?S\01\00f\12\00\04\E1\02\10\00B\11\05\80\00\17\048\00\04\18\00\13\FD\14\01\0C\84\01&\10\06\B0\12\04\01\00\0F\C0\00\01\132@\00+\06\00\01\00\1A\08\B0\12\12\03\C0\01>\18\80\00\A7\00\18\05\A8\16\0B\01\00*\A8\00\08\00\04W\00\13\018\00\04\A8\00\0D\D0\12)\0D\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00"
@ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void = internal constant [101 x i8] c"ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void\00"
@main_kernel_main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_kernel_name = internal constant [57 x i8] c"main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32\00"
@main_kernel_blob_gpu.binary = internal constant [1112 x i8] c"P\EDU\BA\01\00\10\00H\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\08\04\00\00\00\00\00\00\08\04\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00h\0C\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\002\C0\0B\00\01\00\11\09\06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\11e__6_1_0___no_ibXthread_tile_h32H\00\0FB\00+osharedD\00+\9Fconstant0G\00(\FA\01debug_frame\00.rel\11\00!nv\14\00\11aS\00\0Fk\01 \0F\98\00%\0F\A4\01\FAo_param\AB\01\1C\0F\01\00\06\8Ck\00\00\00\03\00\0A\00\01\00 0\01\18\00,\09\00\01\00\11~\18\00,\04\00\01\00\11\9C\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\1C\00\00\00E\00#\04\10\18\00\80/\08\00\05\00\00\00\06\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04x\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C$\00u\01\00\08\00\00\F0!\10\00\01\01\00\C0\F0\11\00\03\1B\FF\00\04\1C\08\00p\B0\04\12\00\01\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00 \01/\05\00\01\00\FF\B0@$v\01\FF\0F\04\A3\FF\00\8E\07\00\C4\0F\00\C3y\EC\03\A2%\00\00\00\22\0E\00\19y\00\01\00\F3\0E!\00\00\00b\0E\00\90r\05?\04\00\00\80?\E0\FF\0F\00\C6\1F\00\B9z\04\00\00X\DE\02p\E4\0F\00\A4r\04\05B\00\B1?\02\8E\0F\00\CC\0F\00\0C|\00>\04\95pR\F0\0B\00\DA/\00Mc\04\F3\0D\80\03\00\EA\0F\00\02z\02\00\00Z\00\00\00\0F\00\00\00\E2\0F\00$v\03\FF\00[\90\00\22\E2\0F`\001F\00\00\B3\04\80\C8\0F\00\86y\00\02\FF\B0\030\19\10\0C \00*MyP\00PGy\00\00\F09\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\80\0F\01\00-\00\07\05.\03\00\01\00\22@\00\01\00=k\01\000\00\08\01\00\1F\0B@\00\04*\AB\01\08\00\0F@\00\05\13\13\0C\04\0C\01\00\13XU\00*\90\000\04\04M\04\22\18\00\01\00.>\01T\00\00\01\00\13\E8@\00/p\00\80\00\0B\1F)'\00\03A\00X\04\00\01\00\04@\06\04\E4\00*\04\00\01\00\1Fq@\00\04\13\88)\00&L\00@\00\1F\0A@\00\00!\\\01D\01\0D@\00\13\D8)\00*\D8\00\01\00\1B\08\08\00%K\01\DB\0A\09\01\00\13\B0]\05\17\10\80\00\17\048\00\04\18\00\13\F7\14\01\0C\84\01*\C0\05(\07\1F\00\C0\00\04\132@\00*\06\00\01\00*\80\07\C0\07\12\03\C0\01:\06\80\00\01\00\13\06\D8\01\05\A8\0B\0B\01\00*\A8\00\08\00\04\F8\00\14\018\00/\05\00\01\00\029@\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00"
@sync_on_stream___gpu___pvoid_pvoid___void = internal constant [42 x i8] c"sync_on_stream___gpu___pvoid_pvoid___void\00"
@h2d___gpu___pvoid_pvoid_m2df32_m2df32___void = internal constant [45 x i8] c"h2d___gpu___pvoid_pvoid_m2df32_m2df32___void\00"
@alloc___gpu___pvoid_i64___pvoid = internal constant [32 x i8] c"alloc___gpu___pvoid_i64___pvoid\00"
@ral_recv_input___cpu___pvoid_i64___m2df32 = internal constant [42 x i8] c"ral_recv_input___cpu___pvoid_i64___m2df32\00"

declare ptr @malloc(i64)

declare void @free(ptr)

declare void @disc_ral_call(ptr, ptr, ptr)

define void @main(ptr %0) {
  %2 = alloca %0, align 8
  %3 = alloca ptr, i32 3, align 8
  %4 = getelementptr %0, ptr %2, i32 0, i32 0
  store ptr %0, ptr %4, align 8
  %5 = getelementptr ptr, ptr %3, i32 0
  store ptr %4, ptr %5, align 8
  %6 = getelementptr %0, ptr %2, i32 0, i32 1
  store i64 0, ptr %6, align 4
  %7 = getelementptr ptr, ptr %3, i32 1
  store ptr %6, ptr %7, align 8
  %8 = getelementptr %0, ptr %2, i32 0, i32 2
  %9 = getelementptr ptr, ptr %3, i32 2
  store ptr %8, ptr %9, align 8
  call void @disc_ral_call(ptr %0, ptr @ral_recv_input___cpu___pvoid_i64___m2df32, ptr %3)
  %10 = load { ptr, ptr, i64, [2 x i64], [2 x i64] }, ptr %8, align 8
  %11 = alloca %.1, align 8
  %12 = alloca ptr, i32 3, align 8
  %13 = getelementptr %.1, ptr %11, i32 0, i32 0
  store ptr %0, ptr %13, align 8
  %14 = getelementptr ptr, ptr %12, i32 0
  store ptr %13, ptr %14, align 8
  %15 = getelementptr %.1, ptr %11, i32 0, i32 1
  store i64 1, ptr %15, align 4
  %16 = getelementptr ptr, ptr %12, i32 1
  store ptr %15, ptr %16, align 8
  %17 = getelementptr %.1, ptr %11, i32 0, i32 2
  %18 = getelementptr ptr, ptr %12, i32 2
  store ptr %17, ptr %18, align 8
  call void @disc_ral_call(ptr %0, ptr @ral_recv_input___cpu___pvoid_i64___m2df32, ptr %12)
  %19 = load { ptr, ptr, i64, [2 x i64], [2 x i64] }, ptr %17, align 8
  %20 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %19, 3, 0
  %21 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %10, 3, 0
  %22 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %10, 0
  %23 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %10, 1
  %24 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } undef, ptr %22, 0
  %25 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %24, ptr %23, 1
  %26 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %25, i64 0, 2
  %27 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %26, i64 %21, 3, 0
  %28 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %27, i64 4, 4, 0
  %29 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %28, i64 4, 3, 1
  %30 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %29, i64 1, 4, 1
  %31 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %19, 0
  %32 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %19, 1
  %33 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } undef, ptr %31, 0
  %34 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %33, ptr %32, 1
  %35 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %34, i64 0, 2
  %36 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %35, i64 %20, 3, 0
  %37 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %36, i64 4, 4, 0
  %38 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %37, i64 4, 3, 1
  %39 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %38, i64 1, 4, 1
  %40 = icmp eq i64 %20, 1
  %41 = select i1 %40, i64 %21, i64 %20
  %42 = mul i64 4, %21
  %43 = getelementptr float, ptr null, i64 %42
  %44 = ptrtoint ptr %43 to i64
  %45 = alloca %.2, align 8
  %46 = alloca ptr, i32 3, align 8
  %47 = getelementptr %.2, ptr %45, i32 0, i32 0
  store ptr %0, ptr %47, align 8
  %48 = getelementptr ptr, ptr %46, i32 0
  store ptr %47, ptr %48, align 8
  %49 = getelementptr %.2, ptr %45, i32 0, i32 1
  store i64 %44, ptr %49, align 4
  %50 = getelementptr ptr, ptr %46, i32 1
  store ptr %49, ptr %50, align 8
  %51 = getelementptr %.2, ptr %45, i32 0, i32 2
  %52 = getelementptr ptr, ptr %46, i32 2
  store ptr %51, ptr %52, align 8
  call void @disc_ral_call(ptr %0, ptr @alloc___gpu___pvoid_i64___pvoid, ptr %46)
  %53 = load ptr, ptr %51, align 8
  %54 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } undef, ptr %53, 0
  %55 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %54, ptr %53, 1
  %56 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %55, i64 0, 2
  %57 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %56, i64 4, 3, 1
  %58 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %57, i64 1, 4, 1
  %59 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %58, i64 %21, 3, 0
  %60 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %59, i64 4, 4, 0
  %61 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %30, 0
  %62 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %30, 1
  %63 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %30, 2
  %64 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %30, 3, 0
  %65 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %30, 3, 1
  %66 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %30, 4, 0
  %67 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %30, 4, 1
  %68 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %60, 0
  %69 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %60, 1
  %70 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %60, 2
  %71 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %60, 3, 0
  %72 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %60, 3, 1
  %73 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %60, 4, 0
  %74 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %60, 4, 1
  %75 = alloca %.3, align 8
  %76 = alloca ptr, i32 16, align 8
  %77 = getelementptr %.3, ptr %75, i32 0, i32 0
  store ptr %0, ptr %77, align 8
  %78 = getelementptr ptr, ptr %76, i32 0
  store ptr %77, ptr %78, align 8
  %79 = getelementptr %.3, ptr %75, i32 0, i32 1
  store ptr null, ptr %79, align 8
  %80 = getelementptr ptr, ptr %76, i32 1
  store ptr %79, ptr %80, align 8
  %81 = getelementptr %.3, ptr %75, i32 0, i32 2
  store ptr %61, ptr %81, align 8
  %82 = getelementptr ptr, ptr %76, i32 2
  store ptr %81, ptr %82, align 8
  %83 = getelementptr %.3, ptr %75, i32 0, i32 3
  store ptr %62, ptr %83, align 8
  %84 = getelementptr ptr, ptr %76, i32 3
  store ptr %83, ptr %84, align 8
  %85 = getelementptr %.3, ptr %75, i32 0, i32 4
  store i64 %63, ptr %85, align 4
  %86 = getelementptr ptr, ptr %76, i32 4
  store ptr %85, ptr %86, align 8
  %87 = getelementptr %.3, ptr %75, i32 0, i32 5
  store i64 %64, ptr %87, align 4
  %88 = getelementptr ptr, ptr %76, i32 5
  store ptr %87, ptr %88, align 8
  %89 = getelementptr %.3, ptr %75, i32 0, i32 6
  store i64 %65, ptr %89, align 4
  %90 = getelementptr ptr, ptr %76, i32 6
  store ptr %89, ptr %90, align 8
  %91 = getelementptr %.3, ptr %75, i32 0, i32 7
  store i64 %66, ptr %91, align 4
  %92 = getelementptr ptr, ptr %76, i32 7
  store ptr %91, ptr %92, align 8
  %93 = getelementptr %.3, ptr %75, i32 0, i32 8
  store i64 %67, ptr %93, align 4
  %94 = getelementptr ptr, ptr %76, i32 8
  store ptr %93, ptr %94, align 8
  %95 = getelementptr %.3, ptr %75, i32 0, i32 9
  store ptr %68, ptr %95, align 8
  %96 = getelementptr ptr, ptr %76, i32 9
  store ptr %95, ptr %96, align 8
  %97 = getelementptr %.3, ptr %75, i32 0, i32 10
  store ptr %69, ptr %97, align 8
  %98 = getelementptr ptr, ptr %76, i32 10
  store ptr %97, ptr %98, align 8
  %99 = getelementptr %.3, ptr %75, i32 0, i32 11
  store i64 %70, ptr %99, align 4
  %100 = getelementptr ptr, ptr %76, i32 11
  store ptr %99, ptr %100, align 8
  %101 = getelementptr %.3, ptr %75, i32 0, i32 12
  store i64 %71, ptr %101, align 4
  %102 = getelementptr ptr, ptr %76, i32 12
  store ptr %101, ptr %102, align 8
  %103 = getelementptr %.3, ptr %75, i32 0, i32 13
  store i64 %72, ptr %103, align 4
  %104 = getelementptr ptr, ptr %76, i32 13
  store ptr %103, ptr %104, align 8
  %105 = getelementptr %.3, ptr %75, i32 0, i32 14
  store i64 %73, ptr %105, align 4
  %106 = getelementptr ptr, ptr %76, i32 14
  store ptr %105, ptr %106, align 8
  %107 = getelementptr %.3, ptr %75, i32 0, i32 15
  store i64 %74, ptr %107, align 4
  %108 = getelementptr ptr, ptr %76, i32 15
  store ptr %107, ptr %108, align 8
  call void @disc_ral_call(ptr %0, ptr @h2d___gpu___pvoid_pvoid_m2df32_m2df32___void, ptr %76)
  %109 = alloca %.4, align 8
  %110 = alloca ptr, i32 2, align 8
  %111 = getelementptr %.4, ptr %109, i32 0, i32 0
  store ptr %0, ptr %111, align 8
  %112 = getelementptr ptr, ptr %110, i32 0
  store ptr %111, ptr %112, align 8
  %113 = getelementptr %.4, ptr %109, i32 0, i32 1
  store ptr null, ptr %113, align 8
  %114 = getelementptr ptr, ptr %110, i32 1
  store ptr %113, ptr %114, align 8
  call void @disc_ral_call(ptr %0, ptr @sync_on_stream___gpu___pvoid_pvoid___void, ptr %110)
  %115 = mul i64 4, %20
  %116 = getelementptr float, ptr null, i64 %115
  %117 = ptrtoint ptr %116 to i64
  %118 = alloca %.5, align 8
  %119 = alloca ptr, i32 3, align 8
  %120 = getelementptr %.5, ptr %118, i32 0, i32 0
  store ptr %0, ptr %120, align 8
  %121 = getelementptr ptr, ptr %119, i32 0
  store ptr %120, ptr %121, align 8
  %122 = getelementptr %.5, ptr %118, i32 0, i32 1
  store i64 %117, ptr %122, align 4
  %123 = getelementptr ptr, ptr %119, i32 1
  store ptr %122, ptr %123, align 8
  %124 = getelementptr %.5, ptr %118, i32 0, i32 2
  %125 = getelementptr ptr, ptr %119, i32 2
  store ptr %124, ptr %125, align 8
  call void @disc_ral_call(ptr %0, ptr @alloc___gpu___pvoid_i64___pvoid, ptr %119)
  %126 = load ptr, ptr %124, align 8
  %127 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } undef, ptr %126, 0
  %128 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %127, ptr %126, 1
  %129 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %128, i64 0, 2
  %130 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %129, i64 4, 3, 1
  %131 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %130, i64 1, 4, 1
  %132 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %131, i64 %20, 3, 0
  %133 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %132, i64 4, 4, 0
  %134 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %39, 0
  %135 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %39, 1
  %136 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %39, 2
  %137 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %39, 3, 0
  %138 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %39, 3, 1
  %139 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %39, 4, 0
  %140 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %39, 4, 1
  %141 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %133, 0
  %142 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %133, 1
  %143 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %133, 2
  %144 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %133, 3, 0
  %145 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %133, 3, 1
  %146 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %133, 4, 0
  %147 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %133, 4, 1
  %148 = alloca %.6, align 8
  %149 = alloca ptr, i32 16, align 8
  %150 = getelementptr %.6, ptr %148, i32 0, i32 0
  store ptr %0, ptr %150, align 8
  %151 = getelementptr ptr, ptr %149, i32 0
  store ptr %150, ptr %151, align 8
  %152 = getelementptr %.6, ptr %148, i32 0, i32 1
  store ptr null, ptr %152, align 8
  %153 = getelementptr ptr, ptr %149, i32 1
  store ptr %152, ptr %153, align 8
  %154 = getelementptr %.6, ptr %148, i32 0, i32 2
  store ptr %134, ptr %154, align 8
  %155 = getelementptr ptr, ptr %149, i32 2
  store ptr %154, ptr %155, align 8
  %156 = getelementptr %.6, ptr %148, i32 0, i32 3
  store ptr %135, ptr %156, align 8
  %157 = getelementptr ptr, ptr %149, i32 3
  store ptr %156, ptr %157, align 8
  %158 = getelementptr %.6, ptr %148, i32 0, i32 4
  store i64 %136, ptr %158, align 4
  %159 = getelementptr ptr, ptr %149, i32 4
  store ptr %158, ptr %159, align 8
  %160 = getelementptr %.6, ptr %148, i32 0, i32 5
  store i64 %137, ptr %160, align 4
  %161 = getelementptr ptr, ptr %149, i32 5
  store ptr %160, ptr %161, align 8
  %162 = getelementptr %.6, ptr %148, i32 0, i32 6
  store i64 %138, ptr %162, align 4
  %163 = getelementptr ptr, ptr %149, i32 6
  store ptr %162, ptr %163, align 8
  %164 = getelementptr %.6, ptr %148, i32 0, i32 7
  store i64 %139, ptr %164, align 4
  %165 = getelementptr ptr, ptr %149, i32 7
  store ptr %164, ptr %165, align 8
  %166 = getelementptr %.6, ptr %148, i32 0, i32 8
  store i64 %140, ptr %166, align 4
  %167 = getelementptr ptr, ptr %149, i32 8
  store ptr %166, ptr %167, align 8
  %168 = getelementptr %.6, ptr %148, i32 0, i32 9
  store ptr %141, ptr %168, align 8
  %169 = getelementptr ptr, ptr %149, i32 9
  store ptr %168, ptr %169, align 8
  %170 = getelementptr %.6, ptr %148, i32 0, i32 10
  store ptr %142, ptr %170, align 8
  %171 = getelementptr ptr, ptr %149, i32 10
  store ptr %170, ptr %171, align 8
  %172 = getelementptr %.6, ptr %148, i32 0, i32 11
  store i64 %143, ptr %172, align 4
  %173 = getelementptr ptr, ptr %149, i32 11
  store ptr %172, ptr %173, align 8
  %174 = getelementptr %.6, ptr %148, i32 0, i32 12
  store i64 %144, ptr %174, align 4
  %175 = getelementptr ptr, ptr %149, i32 12
  store ptr %174, ptr %175, align 8
  %176 = getelementptr %.6, ptr %148, i32 0, i32 13
  store i64 %145, ptr %176, align 4
  %177 = getelementptr ptr, ptr %149, i32 13
  store ptr %176, ptr %177, align 8
  %178 = getelementptr %.6, ptr %148, i32 0, i32 14
  store i64 %146, ptr %178, align 4
  %179 = getelementptr ptr, ptr %149, i32 14
  store ptr %178, ptr %179, align 8
  %180 = getelementptr %.6, ptr %148, i32 0, i32 15
  store i64 %147, ptr %180, align 4
  %181 = getelementptr ptr, ptr %149, i32 15
  store ptr %180, ptr %181, align 8
  call void @disc_ral_call(ptr %0, ptr @h2d___gpu___pvoid_pvoid_m2df32_m2df32___void, ptr %149)
  %182 = alloca %.7, align 8
  %183 = alloca ptr, i32 2, align 8
  %184 = getelementptr %.7, ptr %182, i32 0, i32 0
  store ptr %0, ptr %184, align 8
  %185 = getelementptr ptr, ptr %183, i32 0
  store ptr %184, ptr %185, align 8
  %186 = getelementptr %.7, ptr %182, i32 0, i32 1
  store ptr null, ptr %186, align 8
  %187 = getelementptr ptr, ptr %183, i32 1
  store ptr %186, ptr %187, align 8
  call void @disc_ral_call(ptr %0, ptr @sync_on_stream___gpu___pvoid_pvoid___void, ptr %183)
  %188 = trunc i64 %41 to i32
  %189 = mul i32 %188, 4
  %190 = sext i32 %189 to i64
  %191 = icmp eq i64 %21, %41
  %192 = icmp eq i64 %20, %41
  %193 = and i1 %192, %191
  %194 = alloca %.8, align 8
  %195 = alloca ptr, i32 3, align 8
  %196 = getelementptr %.8, ptr %194, i32 0, i32 0
  store ptr %0, ptr %196, align 8
  %197 = getelementptr ptr, ptr %195, i32 0
  store ptr %196, ptr %197, align 8
  %198 = getelementptr %.8, ptr %194, i32 0, i32 1
  store i64 ptrtoint (ptr getelementptr (float, ptr null, i64 1) to i64), ptr %198, align 4
  %199 = getelementptr ptr, ptr %195, i32 1
  store ptr %198, ptr %199, align 8
  %200 = getelementptr %.8, ptr %194, i32 0, i32 2
  %201 = getelementptr ptr, ptr %195, i32 2
  store ptr %200, ptr %201, align 8
  call void @disc_ral_call(ptr %0, ptr @alloc___gpu___pvoid_i64___pvoid, ptr %195)
  %202 = load ptr, ptr %200, align 8
  %203 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } undef, ptr %202, 0
  %204 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %203, ptr %202, 1
  %205 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %204, i64 0, 2
  %206 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %205, i64 1, 3, 0
  %207 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %206, i64 1, 4, 0
  br i1 %193, label %208, label %456

208:                                              ; preds = %1
  %209 = icmp slt i64 %190, 1
  br i1 %209, label %210, label %333

210:                                              ; preds = %208
  %211 = alloca ptr, align 8
  %212 = getelementptr ptr, ptr %211, i32 0
  store ptr @main_kernel_blob_gpu.binary, ptr %212, align 8
  %213 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 0
  %214 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 1
  %215 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 2
  %216 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 3, 0
  %217 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 4, 0
  %218 = alloca %.9, align 8
  %219 = alloca ptr, i32 2, align 8
  %220 = getelementptr %.9, ptr %218, i32 0, i32 0
  store i64 512, ptr %220, align 4
  %221 = getelementptr ptr, ptr %219, i32 0
  store ptr %220, ptr %221, align 8
  %222 = getelementptr %.9, ptr %218, i32 0, i32 1
  store ptr %214, ptr %222, align 8
  %223 = getelementptr ptr, ptr %219, i32 1
  store ptr %222, ptr %223, align 8
  %224 = alloca %.10, align 8
  %225 = alloca ptr, i32 14, align 8
  %226 = getelementptr %.10, ptr %224, i32 0, i32 0
  store ptr %0, ptr %226, align 8
  %227 = getelementptr ptr, ptr %225, i32 0
  store ptr %226, ptr %227, align 8
  %228 = getelementptr %.10, ptr %224, i32 0, i32 1
  store ptr %211, ptr %228, align 8
  %229 = getelementptr ptr, ptr %225, i32 1
  store ptr %228, ptr %229, align 8
  %230 = getelementptr %.10, ptr %224, i32 0, i32 2
  store i64 1, ptr %230, align 4
  %231 = getelementptr ptr, ptr %225, i32 2
  store ptr %230, ptr %231, align 8
  %232 = getelementptr %.10, ptr %224, i32 0, i32 3
  store ptr @main_kernel_main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_kernel_name, ptr %232, align 8
  %233 = getelementptr ptr, ptr %225, i32 3
  store ptr %232, ptr %233, align 8
  %234 = getelementptr %.10, ptr %224, i32 0, i32 4
  store i64 1, ptr %234, align 4
  %235 = getelementptr ptr, ptr %225, i32 4
  store ptr %234, ptr %235, align 8
  %236 = getelementptr %.10, ptr %224, i32 0, i32 5
  store i64 1, ptr %236, align 4
  %237 = getelementptr ptr, ptr %225, i32 5
  store ptr %236, ptr %237, align 8
  %238 = getelementptr %.10, ptr %224, i32 0, i32 6
  store i64 1, ptr %238, align 4
  %239 = getelementptr ptr, ptr %225, i32 6
  store ptr %238, ptr %239, align 8
  %240 = getelementptr %.10, ptr %224, i32 0, i32 7
  store i64 512, ptr %240, align 4
  %241 = getelementptr ptr, ptr %225, i32 7
  store ptr %240, ptr %241, align 8
  %242 = getelementptr %.10, ptr %224, i32 0, i32 8
  store i64 1, ptr %242, align 4
  %243 = getelementptr ptr, ptr %225, i32 8
  store ptr %242, ptr %243, align 8
  %244 = getelementptr %.10, ptr %224, i32 0, i32 9
  store i64 1, ptr %244, align 4
  %245 = getelementptr ptr, ptr %225, i32 9
  store ptr %244, ptr %245, align 8
  %246 = getelementptr %.10, ptr %224, i32 0, i32 10
  store i32 0, ptr %246, align 4
  %247 = getelementptr ptr, ptr %225, i32 10
  store ptr %246, ptr %247, align 8
  %248 = getelementptr %.10, ptr %224, i32 0, i32 11
  store ptr null, ptr %248, align 8
  %249 = getelementptr ptr, ptr %225, i32 11
  store ptr %248, ptr %249, align 8
  %250 = getelementptr %.10, ptr %224, i32 0, i32 12
  store i32 2, ptr %250, align 4
  %251 = getelementptr ptr, ptr %225, i32 12
  store ptr %250, ptr %251, align 8
  %252 = getelementptr %.10, ptr %224, i32 0, i32 13
  store ptr %219, ptr %252, align 8
  %253 = getelementptr ptr, ptr %225, i32 13
  store ptr %252, ptr %253, align 8
  call void @disc_ral_call(ptr %0, ptr @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr %225)
  %254 = icmp eq i64 %190, 0
  %255 = sub i64 %190, 1
  %256 = udiv i64 %255, 32
  %257 = add i64 %256, 1
  %258 = select i1 %254, i64 0, i64 %257
  %259 = mul i64 %258, 512
  %260 = icmp sle i64 %259, 0
  %261 = sub i64 0, %259
  %262 = sub i64 %259, 1
  %263 = select i1 %260, i64 %261, i64 %262
  %264 = sdiv i64 %263, 512
  %265 = sub i64 0, %264
  %266 = add i64 %264, 1
  %267 = select i1 %260, i64 %265, i64 %266
  %268 = alloca ptr, align 8
  %269 = getelementptr ptr, ptr %268, i32 0
  store ptr @main_kernel_0_blob_gpu.binary, ptr %269, align 8
  %270 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %60, 0
  %271 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %60, 1
  %272 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %60, 2
  %273 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %60, 3, 0
  %274 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %60, 3, 1
  %275 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %60, 4, 0
  %276 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %60, 4, 1
  %277 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %133, 0
  %278 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %133, 1
  %279 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %133, 2
  %280 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %133, 3, 0
  %281 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %133, 3, 1
  %282 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %133, 4, 0
  %283 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %133, 4, 1
  %284 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 0
  %285 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 1
  %286 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 2
  %287 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 3, 0
  %288 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 4, 0
  %289 = alloca %.11, align 8
  %290 = alloca ptr, i32 6, align 8
  %291 = getelementptr %.11, ptr %289, i32 0, i32 0
  store i64 512, ptr %291, align 4
  %292 = getelementptr ptr, ptr %290, i32 0
  store ptr %291, ptr %292, align 8
  %293 = getelementptr %.11, ptr %289, i32 0, i32 1
  store i64 %259, ptr %293, align 4
  %294 = getelementptr ptr, ptr %290, i32 1
  store ptr %293, ptr %294, align 8
  %295 = getelementptr %.11, ptr %289, i32 0, i32 2
  store i64 %190, ptr %295, align 4
  %296 = getelementptr ptr, ptr %290, i32 2
  store ptr %295, ptr %296, align 8
  %297 = getelementptr %.11, ptr %289, i32 0, i32 3
  store ptr %271, ptr %297, align 8
  %298 = getelementptr ptr, ptr %290, i32 3
  store ptr %297, ptr %298, align 8
  %299 = getelementptr %.11, ptr %289, i32 0, i32 4
  store ptr %278, ptr %299, align 8
  %300 = getelementptr ptr, ptr %290, i32 4
  store ptr %299, ptr %300, align 8
  %301 = getelementptr %.11, ptr %289, i32 0, i32 5
  store ptr %285, ptr %301, align 8
  %302 = getelementptr ptr, ptr %290, i32 5
  store ptr %301, ptr %302, align 8
  %303 = alloca %.12, align 8
  %304 = alloca ptr, i32 14, align 8
  %305 = getelementptr %.12, ptr %303, i32 0, i32 0
  store ptr %0, ptr %305, align 8
  %306 = getelementptr ptr, ptr %304, i32 0
  store ptr %305, ptr %306, align 8
  %307 = getelementptr %.12, ptr %303, i32 0, i32 1
  store ptr %268, ptr %307, align 8
  %308 = getelementptr ptr, ptr %304, i32 1
  store ptr %307, ptr %308, align 8
  %309 = getelementptr %.12, ptr %303, i32 0, i32 2
  store i64 1, ptr %309, align 4
  %310 = getelementptr ptr, ptr %304, i32 2
  store ptr %309, ptr %310, align 8
  %311 = getelementptr %.12, ptr %303, i32 0, i32 3
  store ptr @main_kernel_0_main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_1_kernel_name, ptr %311, align 8
  %312 = getelementptr ptr, ptr %304, i32 3
  store ptr %311, ptr %312, align 8
  %313 = getelementptr %.12, ptr %303, i32 0, i32 4
  store i64 %267, ptr %313, align 4
  %314 = getelementptr ptr, ptr %304, i32 4
  store ptr %313, ptr %314, align 8
  %315 = getelementptr %.12, ptr %303, i32 0, i32 5
  store i64 1, ptr %315, align 4
  %316 = getelementptr ptr, ptr %304, i32 5
  store ptr %315, ptr %316, align 8
  %317 = getelementptr %.12, ptr %303, i32 0, i32 6
  store i64 1, ptr %317, align 4
  %318 = getelementptr ptr, ptr %304, i32 6
  store ptr %317, ptr %318, align 8
  %319 = getelementptr %.12, ptr %303, i32 0, i32 7
  store i64 512, ptr %319, align 4
  %320 = getelementptr ptr, ptr %304, i32 7
  store ptr %319, ptr %320, align 8
  %321 = getelementptr %.12, ptr %303, i32 0, i32 8
  store i64 1, ptr %321, align 4
  %322 = getelementptr ptr, ptr %304, i32 8
  store ptr %321, ptr %322, align 8
  %323 = getelementptr %.12, ptr %303, i32 0, i32 9
  store i64 1, ptr %323, align 4
  %324 = getelementptr ptr, ptr %304, i32 9
  store ptr %323, ptr %324, align 8
  %325 = getelementptr %.12, ptr %303, i32 0, i32 10
  store i32 0, ptr %325, align 4
  %326 = getelementptr ptr, ptr %304, i32 10
  store ptr %325, ptr %326, align 8
  %327 = getelementptr %.12, ptr %303, i32 0, i32 11
  store ptr null, ptr %327, align 8
  %328 = getelementptr ptr, ptr %304, i32 11
  store ptr %327, ptr %328, align 8
  %329 = getelementptr %.12, ptr %303, i32 0, i32 12
  store i32 6, ptr %329, align 4
  %330 = getelementptr ptr, ptr %304, i32 12
  store ptr %329, ptr %330, align 8
  %331 = getelementptr %.12, ptr %303, i32 0, i32 13
  store ptr %290, ptr %331, align 8
  %332 = getelementptr ptr, ptr %304, i32 13
  store ptr %331, ptr %332, align 8
  call void @disc_ral_call(ptr %0, ptr @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr %304)
  br label %712

333:                                              ; preds = %208
  %334 = alloca ptr, align 8
  %335 = getelementptr ptr, ptr %334, i32 0
  store ptr @main_kernel_1_blob_gpu.binary, ptr %335, align 8
  %336 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 0
  %337 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 1
  %338 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 2
  %339 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 3, 0
  %340 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 4, 0
  %341 = alloca %.22, align 8
  %342 = alloca ptr, i32 2, align 8
  %343 = getelementptr %.22, ptr %341, i32 0, i32 0
  store i64 256, ptr %343, align 4
  %344 = getelementptr ptr, ptr %342, i32 0
  store ptr %343, ptr %344, align 8
  %345 = getelementptr %.22, ptr %341, i32 0, i32 1
  store ptr %337, ptr %345, align 8
  %346 = getelementptr ptr, ptr %342, i32 1
  store ptr %345, ptr %346, align 8
  %347 = alloca %.23, align 8
  %348 = alloca ptr, i32 14, align 8
  %349 = getelementptr %.23, ptr %347, i32 0, i32 0
  store ptr %0, ptr %349, align 8
  %350 = getelementptr ptr, ptr %348, i32 0
  store ptr %349, ptr %350, align 8
  %351 = getelementptr %.23, ptr %347, i32 0, i32 1
  store ptr %334, ptr %351, align 8
  %352 = getelementptr ptr, ptr %348, i32 1
  store ptr %351, ptr %352, align 8
  %353 = getelementptr %.23, ptr %347, i32 0, i32 2
  store i64 1, ptr %353, align 4
  %354 = getelementptr ptr, ptr %348, i32 2
  store ptr %353, ptr %354, align 8
  %355 = getelementptr %.23, ptr %347, i32 0, i32 3
  store ptr @main_kernel_1_main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_kernel_name, ptr %355, align 8
  %356 = getelementptr ptr, ptr %348, i32 3
  store ptr %355, ptr %356, align 8
  %357 = getelementptr %.23, ptr %347, i32 0, i32 4
  store i64 1, ptr %357, align 4
  %358 = getelementptr ptr, ptr %348, i32 4
  store ptr %357, ptr %358, align 8
  %359 = getelementptr %.23, ptr %347, i32 0, i32 5
  store i64 1, ptr %359, align 4
  %360 = getelementptr ptr, ptr %348, i32 5
  store ptr %359, ptr %360, align 8
  %361 = getelementptr %.23, ptr %347, i32 0, i32 6
  store i64 1, ptr %361, align 4
  %362 = getelementptr ptr, ptr %348, i32 6
  store ptr %361, ptr %362, align 8
  %363 = getelementptr %.23, ptr %347, i32 0, i32 7
  store i64 256, ptr %363, align 4
  %364 = getelementptr ptr, ptr %348, i32 7
  store ptr %363, ptr %364, align 8
  %365 = getelementptr %.23, ptr %347, i32 0, i32 8
  store i64 1, ptr %365, align 4
  %366 = getelementptr ptr, ptr %348, i32 8
  store ptr %365, ptr %366, align 8
  %367 = getelementptr %.23, ptr %347, i32 0, i32 9
  store i64 1, ptr %367, align 4
  %368 = getelementptr ptr, ptr %348, i32 9
  store ptr %367, ptr %368, align 8
  %369 = getelementptr %.23, ptr %347, i32 0, i32 10
  store i32 0, ptr %369, align 4
  %370 = getelementptr ptr, ptr %348, i32 10
  store ptr %369, ptr %370, align 8
  %371 = getelementptr %.23, ptr %347, i32 0, i32 11
  store ptr null, ptr %371, align 8
  %372 = getelementptr ptr, ptr %348, i32 11
  store ptr %371, ptr %372, align 8
  %373 = getelementptr %.23, ptr %347, i32 0, i32 12
  store i32 2, ptr %373, align 4
  %374 = getelementptr ptr, ptr %348, i32 12
  store ptr %373, ptr %374, align 8
  %375 = getelementptr %.23, ptr %347, i32 0, i32 13
  store ptr %342, ptr %375, align 8
  %376 = getelementptr ptr, ptr %348, i32 13
  store ptr %375, ptr %376, align 8
  call void @disc_ral_call(ptr %0, ptr @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr %348)
  %377 = icmp eq i64 %190, 0
  %378 = sub i64 %190, 1
  %379 = udiv i64 %378, 512
  %380 = add i64 %379, 1
  %381 = select i1 %377, i64 0, i64 %380
  %382 = mul i64 %381, 256
  %383 = icmp sle i64 %382, 0
  %384 = sub i64 0, %382
  %385 = sub i64 %382, 1
  %386 = select i1 %383, i64 %384, i64 %385
  %387 = sdiv i64 %386, 256
  %388 = sub i64 0, %387
  %389 = add i64 %387, 1
  %390 = select i1 %383, i64 %388, i64 %389
  %391 = alloca ptr, align 8
  %392 = getelementptr ptr, ptr %391, i32 0
  store ptr @main_kernel_2_blob_gpu.binary, ptr %392, align 8
  %393 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %60, 0
  %394 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %60, 1
  %395 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %60, 2
  %396 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %60, 3, 0
  %397 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %60, 3, 1
  %398 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %60, 4, 0
  %399 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %60, 4, 1
  %400 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %133, 0
  %401 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %133, 1
  %402 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %133, 2
  %403 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %133, 3, 0
  %404 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %133, 3, 1
  %405 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %133, 4, 0
  %406 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %133, 4, 1
  %407 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 0
  %408 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 1
  %409 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 2
  %410 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 3, 0
  %411 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 4, 0
  %412 = alloca %.24, align 8
  %413 = alloca ptr, i32 6, align 8
  %414 = getelementptr %.24, ptr %412, i32 0, i32 0
  store i64 256, ptr %414, align 4
  %415 = getelementptr ptr, ptr %413, i32 0
  store ptr %414, ptr %415, align 8
  %416 = getelementptr %.24, ptr %412, i32 0, i32 1
  store i64 %382, ptr %416, align 4
  %417 = getelementptr ptr, ptr %413, i32 1
  store ptr %416, ptr %417, align 8
  %418 = getelementptr %.24, ptr %412, i32 0, i32 2
  store i64 %190, ptr %418, align 4
  %419 = getelementptr ptr, ptr %413, i32 2
  store ptr %418, ptr %419, align 8
  %420 = getelementptr %.24, ptr %412, i32 0, i32 3
  store ptr %394, ptr %420, align 8
  %421 = getelementptr ptr, ptr %413, i32 3
  store ptr %420, ptr %421, align 8
  %422 = getelementptr %.24, ptr %412, i32 0, i32 4
  store ptr %401, ptr %422, align 8
  %423 = getelementptr ptr, ptr %413, i32 4
  store ptr %422, ptr %423, align 8
  %424 = getelementptr %.24, ptr %412, i32 0, i32 5
  store ptr %408, ptr %424, align 8
  %425 = getelementptr ptr, ptr %413, i32 5
  store ptr %424, ptr %425, align 8
  %426 = alloca %.25, align 8
  %427 = alloca ptr, i32 14, align 8
  %428 = getelementptr %.25, ptr %426, i32 0, i32 0
  store ptr %0, ptr %428, align 8
  %429 = getelementptr ptr, ptr %427, i32 0
  store ptr %428, ptr %429, align 8
  %430 = getelementptr %.25, ptr %426, i32 0, i32 1
  store ptr %391, ptr %430, align 8
  %431 = getelementptr ptr, ptr %427, i32 1
  store ptr %430, ptr %431, align 8
  %432 = getelementptr %.25, ptr %426, i32 0, i32 2
  store i64 1, ptr %432, align 4
  %433 = getelementptr ptr, ptr %427, i32 2
  store ptr %432, ptr %433, align 8
  %434 = getelementptr %.25, ptr %426, i32 0, i32 3
  store ptr @main_kernel_2_main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1_kernel_name, ptr %434, align 8
  %435 = getelementptr ptr, ptr %427, i32 3
  store ptr %434, ptr %435, align 8
  %436 = getelementptr %.25, ptr %426, i32 0, i32 4
  store i64 %390, ptr %436, align 4
  %437 = getelementptr ptr, ptr %427, i32 4
  store ptr %436, ptr %437, align 8
  %438 = getelementptr %.25, ptr %426, i32 0, i32 5
  store i64 1, ptr %438, align 4
  %439 = getelementptr ptr, ptr %427, i32 5
  store ptr %438, ptr %439, align 8
  %440 = getelementptr %.25, ptr %426, i32 0, i32 6
  store i64 1, ptr %440, align 4
  %441 = getelementptr ptr, ptr %427, i32 6
  store ptr %440, ptr %441, align 8
  %442 = getelementptr %.25, ptr %426, i32 0, i32 7
  store i64 256, ptr %442, align 4
  %443 = getelementptr ptr, ptr %427, i32 7
  store ptr %442, ptr %443, align 8
  %444 = getelementptr %.25, ptr %426, i32 0, i32 8
  store i64 1, ptr %444, align 4
  %445 = getelementptr ptr, ptr %427, i32 8
  store ptr %444, ptr %445, align 8
  %446 = getelementptr %.25, ptr %426, i32 0, i32 9
  store i64 1, ptr %446, align 4
  %447 = getelementptr ptr, ptr %427, i32 9
  store ptr %446, ptr %447, align 8
  %448 = getelementptr %.25, ptr %426, i32 0, i32 10
  store i32 0, ptr %448, align 4
  %449 = getelementptr ptr, ptr %427, i32 10
  store ptr %448, ptr %449, align 8
  %450 = getelementptr %.25, ptr %426, i32 0, i32 11
  store ptr null, ptr %450, align 8
  %451 = getelementptr ptr, ptr %427, i32 11
  store ptr %450, ptr %451, align 8
  %452 = getelementptr %.25, ptr %426, i32 0, i32 12
  store i32 6, ptr %452, align 4
  %453 = getelementptr ptr, ptr %427, i32 12
  store ptr %452, ptr %453, align 8
  %454 = getelementptr %.25, ptr %426, i32 0, i32 13
  store ptr %413, ptr %454, align 8
  %455 = getelementptr ptr, ptr %427, i32 13
  store ptr %454, ptr %455, align 8
  call void @disc_ral_call(ptr %0, ptr @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr %427)
  br label %712

456:                                              ; preds = %1
  %457 = icmp slt i64 %190, 1
  br i1 %457, label %458, label %585

458:                                              ; preds = %456
  %459 = alloca ptr, align 8
  %460 = getelementptr ptr, ptr %459, i32 0
  store ptr @main_kernel_3_blob_gpu.binary, ptr %460, align 8
  %461 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 0
  %462 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 1
  %463 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 2
  %464 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 3, 0
  %465 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 4, 0
  %466 = alloca %.26, align 8
  %467 = alloca ptr, i32 2, align 8
  %468 = getelementptr %.26, ptr %466, i32 0, i32 0
  store i64 512, ptr %468, align 4
  %469 = getelementptr ptr, ptr %467, i32 0
  store ptr %468, ptr %469, align 8
  %470 = getelementptr %.26, ptr %466, i32 0, i32 1
  store ptr %462, ptr %470, align 8
  %471 = getelementptr ptr, ptr %467, i32 1
  store ptr %470, ptr %471, align 8
  %472 = alloca %.27, align 8
  %473 = alloca ptr, i32 14, align 8
  %474 = getelementptr %.27, ptr %472, i32 0, i32 0
  store ptr %0, ptr %474, align 8
  %475 = getelementptr ptr, ptr %473, i32 0
  store ptr %474, ptr %475, align 8
  %476 = getelementptr %.27, ptr %472, i32 0, i32 1
  store ptr %459, ptr %476, align 8
  %477 = getelementptr ptr, ptr %473, i32 1
  store ptr %476, ptr %477, align 8
  %478 = getelementptr %.27, ptr %472, i32 0, i32 2
  store i64 1, ptr %478, align 4
  %479 = getelementptr ptr, ptr %473, i32 2
  store ptr %478, ptr %479, align 8
  %480 = getelementptr %.27, ptr %472, i32 0, i32 3
  store ptr @main_kernel_3_main_kColReduction_reduce__6_1_0___thread_tile_h32_kernel_name, ptr %480, align 8
  %481 = getelementptr ptr, ptr %473, i32 3
  store ptr %480, ptr %481, align 8
  %482 = getelementptr %.27, ptr %472, i32 0, i32 4
  store i64 1, ptr %482, align 4
  %483 = getelementptr ptr, ptr %473, i32 4
  store ptr %482, ptr %483, align 8
  %484 = getelementptr %.27, ptr %472, i32 0, i32 5
  store i64 1, ptr %484, align 4
  %485 = getelementptr ptr, ptr %473, i32 5
  store ptr %484, ptr %485, align 8
  %486 = getelementptr %.27, ptr %472, i32 0, i32 6
  store i64 1, ptr %486, align 4
  %487 = getelementptr ptr, ptr %473, i32 6
  store ptr %486, ptr %487, align 8
  %488 = getelementptr %.27, ptr %472, i32 0, i32 7
  store i64 512, ptr %488, align 4
  %489 = getelementptr ptr, ptr %473, i32 7
  store ptr %488, ptr %489, align 8
  %490 = getelementptr %.27, ptr %472, i32 0, i32 8
  store i64 1, ptr %490, align 4
  %491 = getelementptr ptr, ptr %473, i32 8
  store ptr %490, ptr %491, align 8
  %492 = getelementptr %.27, ptr %472, i32 0, i32 9
  store i64 1, ptr %492, align 4
  %493 = getelementptr ptr, ptr %473, i32 9
  store ptr %492, ptr %493, align 8
  %494 = getelementptr %.27, ptr %472, i32 0, i32 10
  store i32 0, ptr %494, align 4
  %495 = getelementptr ptr, ptr %473, i32 10
  store ptr %494, ptr %495, align 8
  %496 = getelementptr %.27, ptr %472, i32 0, i32 11
  store ptr null, ptr %496, align 8
  %497 = getelementptr ptr, ptr %473, i32 11
  store ptr %496, ptr %497, align 8
  %498 = getelementptr %.27, ptr %472, i32 0, i32 12
  store i32 2, ptr %498, align 4
  %499 = getelementptr ptr, ptr %473, i32 12
  store ptr %498, ptr %499, align 8
  %500 = getelementptr %.27, ptr %472, i32 0, i32 13
  store ptr %467, ptr %500, align 8
  %501 = getelementptr ptr, ptr %473, i32 13
  store ptr %500, ptr %501, align 8
  call void @disc_ral_call(ptr %0, ptr @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr %473)
  %502 = icmp eq i64 %190, 0
  %503 = sub i64 %190, 1
  %504 = udiv i64 %503, 32
  %505 = add i64 %504, 1
  %506 = select i1 %502, i64 0, i64 %505
  %507 = mul i64 %506, 512
  %508 = icmp sle i64 %507, 0
  %509 = sub i64 0, %507
  %510 = sub i64 %507, 1
  %511 = select i1 %508, i64 %509, i64 %510
  %512 = sdiv i64 %511, 512
  %513 = sub i64 0, %512
  %514 = add i64 %512, 1
  %515 = select i1 %508, i64 %513, i64 %514
  %516 = alloca ptr, align 8
  %517 = getelementptr ptr, ptr %516, i32 0
  store ptr @main_kernel_4_blob_gpu.binary, ptr %517, align 8
  %518 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %60, 0
  %519 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %60, 1
  %520 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %60, 2
  %521 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %60, 3, 0
  %522 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %60, 3, 1
  %523 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %60, 4, 0
  %524 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %60, 4, 1
  %525 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %133, 0
  %526 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %133, 1
  %527 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %133, 2
  %528 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %133, 3, 0
  %529 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %133, 3, 1
  %530 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %133, 4, 0
  %531 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %133, 4, 1
  %532 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 0
  %533 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 1
  %534 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 2
  %535 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 3, 0
  %536 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 4, 0
  %537 = alloca %.28, align 8
  %538 = alloca ptr, i32 8, align 8
  %539 = getelementptr %.28, ptr %537, i32 0, i32 0
  store i64 %20, ptr %539, align 4
  %540 = getelementptr ptr, ptr %538, i32 0
  store ptr %539, ptr %540, align 8
  %541 = getelementptr %.28, ptr %537, i32 0, i32 1
  store i64 %21, ptr %541, align 4
  %542 = getelementptr ptr, ptr %538, i32 1
  store ptr %541, ptr %542, align 8
  %543 = getelementptr %.28, ptr %537, i32 0, i32 2
  store i64 512, ptr %543, align 4
  %544 = getelementptr ptr, ptr %538, i32 2
  store ptr %543, ptr %544, align 8
  %545 = getelementptr %.28, ptr %537, i32 0, i32 3
  store i64 %507, ptr %545, align 4
  %546 = getelementptr ptr, ptr %538, i32 3
  store ptr %545, ptr %546, align 8
  %547 = getelementptr %.28, ptr %537, i32 0, i32 4
  store i64 %190, ptr %547, align 4
  %548 = getelementptr ptr, ptr %538, i32 4
  store ptr %547, ptr %548, align 8
  %549 = getelementptr %.28, ptr %537, i32 0, i32 5
  store ptr %519, ptr %549, align 8
  %550 = getelementptr ptr, ptr %538, i32 5
  store ptr %549, ptr %550, align 8
  %551 = getelementptr %.28, ptr %537, i32 0, i32 6
  store ptr %526, ptr %551, align 8
  %552 = getelementptr ptr, ptr %538, i32 6
  store ptr %551, ptr %552, align 8
  %553 = getelementptr %.28, ptr %537, i32 0, i32 7
  store ptr %533, ptr %553, align 8
  %554 = getelementptr ptr, ptr %538, i32 7
  store ptr %553, ptr %554, align 8
  %555 = alloca %.29, align 8
  %556 = alloca ptr, i32 14, align 8
  %557 = getelementptr %.29, ptr %555, i32 0, i32 0
  store ptr %0, ptr %557, align 8
  %558 = getelementptr ptr, ptr %556, i32 0
  store ptr %557, ptr %558, align 8
  %559 = getelementptr %.29, ptr %555, i32 0, i32 1
  store ptr %516, ptr %559, align 8
  %560 = getelementptr ptr, ptr %556, i32 1
  store ptr %559, ptr %560, align 8
  %561 = getelementptr %.29, ptr %555, i32 0, i32 2
  store i64 1, ptr %561, align 4
  %562 = getelementptr ptr, ptr %556, i32 2
  store ptr %561, ptr %562, align 8
  %563 = getelementptr %.29, ptr %555, i32 0, i32 3
  store ptr @main_kernel_4_main_kColReduction_reduce__6_1_0___thread_tile_h32_1_kernel_name, ptr %563, align 8
  %564 = getelementptr ptr, ptr %556, i32 3
  store ptr %563, ptr %564, align 8
  %565 = getelementptr %.29, ptr %555, i32 0, i32 4
  store i64 %515, ptr %565, align 4
  %566 = getelementptr ptr, ptr %556, i32 4
  store ptr %565, ptr %566, align 8
  %567 = getelementptr %.29, ptr %555, i32 0, i32 5
  store i64 1, ptr %567, align 4
  %568 = getelementptr ptr, ptr %556, i32 5
  store ptr %567, ptr %568, align 8
  %569 = getelementptr %.29, ptr %555, i32 0, i32 6
  store i64 1, ptr %569, align 4
  %570 = getelementptr ptr, ptr %556, i32 6
  store ptr %569, ptr %570, align 8
  %571 = getelementptr %.29, ptr %555, i32 0, i32 7
  store i64 512, ptr %571, align 4
  %572 = getelementptr ptr, ptr %556, i32 7
  store ptr %571, ptr %572, align 8
  %573 = getelementptr %.29, ptr %555, i32 0, i32 8
  store i64 1, ptr %573, align 4
  %574 = getelementptr ptr, ptr %556, i32 8
  store ptr %573, ptr %574, align 8
  %575 = getelementptr %.29, ptr %555, i32 0, i32 9
  store i64 1, ptr %575, align 4
  %576 = getelementptr ptr, ptr %556, i32 9
  store ptr %575, ptr %576, align 8
  %577 = getelementptr %.29, ptr %555, i32 0, i32 10
  store i32 0, ptr %577, align 4
  %578 = getelementptr ptr, ptr %556, i32 10
  store ptr %577, ptr %578, align 8
  %579 = getelementptr %.29, ptr %555, i32 0, i32 11
  store ptr null, ptr %579, align 8
  %580 = getelementptr ptr, ptr %556, i32 11
  store ptr %579, ptr %580, align 8
  %581 = getelementptr %.29, ptr %555, i32 0, i32 12
  store i32 8, ptr %581, align 4
  %582 = getelementptr ptr, ptr %556, i32 12
  store ptr %581, ptr %582, align 8
  %583 = getelementptr %.29, ptr %555, i32 0, i32 13
  store ptr %538, ptr %583, align 8
  %584 = getelementptr ptr, ptr %556, i32 13
  store ptr %583, ptr %584, align 8
  call void @disc_ral_call(ptr %0, ptr @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr %556)
  br label %712

585:                                              ; preds = %456
  %586 = alloca ptr, align 8
  %587 = getelementptr ptr, ptr %586, i32 0
  store ptr @main_kernel_5_blob_gpu.binary, ptr %587, align 8
  %588 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 0
  %589 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 1
  %590 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 2
  %591 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 3, 0
  %592 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 4, 0
  %593 = alloca %.30, align 8
  %594 = alloca ptr, i32 2, align 8
  %595 = getelementptr %.30, ptr %593, i32 0, i32 0
  store i64 256, ptr %595, align 4
  %596 = getelementptr ptr, ptr %594, i32 0
  store ptr %595, ptr %596, align 8
  %597 = getelementptr %.30, ptr %593, i32 0, i32 1
  store ptr %589, ptr %597, align 8
  %598 = getelementptr ptr, ptr %594, i32 1
  store ptr %597, ptr %598, align 8
  %599 = alloca %.31, align 8
  %600 = alloca ptr, i32 14, align 8
  %601 = getelementptr %.31, ptr %599, i32 0, i32 0
  store ptr %0, ptr %601, align 8
  %602 = getelementptr ptr, ptr %600, i32 0
  store ptr %601, ptr %602, align 8
  %603 = getelementptr %.31, ptr %599, i32 0, i32 1
  store ptr %586, ptr %603, align 8
  %604 = getelementptr ptr, ptr %600, i32 1
  store ptr %603, ptr %604, align 8
  %605 = getelementptr %.31, ptr %599, i32 0, i32 2
  store i64 1, ptr %605, align 4
  %606 = getelementptr ptr, ptr %600, i32 2
  store ptr %605, ptr %606, align 8
  %607 = getelementptr %.31, ptr %599, i32 0, i32 3
  store ptr @main_kernel_5_main_kColReduction_reduce__6_1_0___block_tile_h64_kernel_name, ptr %607, align 8
  %608 = getelementptr ptr, ptr %600, i32 3
  store ptr %607, ptr %608, align 8
  %609 = getelementptr %.31, ptr %599, i32 0, i32 4
  store i64 1, ptr %609, align 4
  %610 = getelementptr ptr, ptr %600, i32 4
  store ptr %609, ptr %610, align 8
  %611 = getelementptr %.31, ptr %599, i32 0, i32 5
  store i64 1, ptr %611, align 4
  %612 = getelementptr ptr, ptr %600, i32 5
  store ptr %611, ptr %612, align 8
  %613 = getelementptr %.31, ptr %599, i32 0, i32 6
  store i64 1, ptr %613, align 4
  %614 = getelementptr ptr, ptr %600, i32 6
  store ptr %613, ptr %614, align 8
  %615 = getelementptr %.31, ptr %599, i32 0, i32 7
  store i64 256, ptr %615, align 4
  %616 = getelementptr ptr, ptr %600, i32 7
  store ptr %615, ptr %616, align 8
  %617 = getelementptr %.31, ptr %599, i32 0, i32 8
  store i64 1, ptr %617, align 4
  %618 = getelementptr ptr, ptr %600, i32 8
  store ptr %617, ptr %618, align 8
  %619 = getelementptr %.31, ptr %599, i32 0, i32 9
  store i64 1, ptr %619, align 4
  %620 = getelementptr ptr, ptr %600, i32 9
  store ptr %619, ptr %620, align 8
  %621 = getelementptr %.31, ptr %599, i32 0, i32 10
  store i32 0, ptr %621, align 4
  %622 = getelementptr ptr, ptr %600, i32 10
  store ptr %621, ptr %622, align 8
  %623 = getelementptr %.31, ptr %599, i32 0, i32 11
  store ptr null, ptr %623, align 8
  %624 = getelementptr ptr, ptr %600, i32 11
  store ptr %623, ptr %624, align 8
  %625 = getelementptr %.31, ptr %599, i32 0, i32 12
  store i32 2, ptr %625, align 4
  %626 = getelementptr ptr, ptr %600, i32 12
  store ptr %625, ptr %626, align 8
  %627 = getelementptr %.31, ptr %599, i32 0, i32 13
  store ptr %594, ptr %627, align 8
  %628 = getelementptr ptr, ptr %600, i32 13
  store ptr %627, ptr %628, align 8
  call void @disc_ral_call(ptr %0, ptr @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr %600)
  %629 = icmp eq i64 %190, 0
  %630 = sub i64 %190, 1
  %631 = udiv i64 %630, 512
  %632 = add i64 %631, 1
  %633 = select i1 %629, i64 0, i64 %632
  %634 = mul i64 %633, 256
  %635 = icmp sle i64 %634, 0
  %636 = sub i64 0, %634
  %637 = sub i64 %634, 1
  %638 = select i1 %635, i64 %636, i64 %637
  %639 = sdiv i64 %638, 256
  %640 = sub i64 0, %639
  %641 = add i64 %639, 1
  %642 = select i1 %635, i64 %640, i64 %641
  %643 = alloca ptr, align 8
  %644 = getelementptr ptr, ptr %643, i32 0
  store ptr @main_kernel_6_blob_gpu.binary, ptr %644, align 8
  %645 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %60, 0
  %646 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %60, 1
  %647 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %60, 2
  %648 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %60, 3, 0
  %649 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %60, 3, 1
  %650 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %60, 4, 0
  %651 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %60, 4, 1
  %652 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %133, 0
  %653 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %133, 1
  %654 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %133, 2
  %655 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %133, 3, 0
  %656 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %133, 3, 1
  %657 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %133, 4, 0
  %658 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %133, 4, 1
  %659 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 0
  %660 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 1
  %661 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 2
  %662 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 3, 0
  %663 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 4, 0
  %664 = alloca %.32, align 8
  %665 = alloca ptr, i32 8, align 8
  %666 = getelementptr %.32, ptr %664, i32 0, i32 0
  store i64 %20, ptr %666, align 4
  %667 = getelementptr ptr, ptr %665, i32 0
  store ptr %666, ptr %667, align 8
  %668 = getelementptr %.32, ptr %664, i32 0, i32 1
  store i64 %21, ptr %668, align 4
  %669 = getelementptr ptr, ptr %665, i32 1
  store ptr %668, ptr %669, align 8
  %670 = getelementptr %.32, ptr %664, i32 0, i32 2
  store i64 256, ptr %670, align 4
  %671 = getelementptr ptr, ptr %665, i32 2
  store ptr %670, ptr %671, align 8
  %672 = getelementptr %.32, ptr %664, i32 0, i32 3
  store i64 %634, ptr %672, align 4
  %673 = getelementptr ptr, ptr %665, i32 3
  store ptr %672, ptr %673, align 8
  %674 = getelementptr %.32, ptr %664, i32 0, i32 4
  store i64 %190, ptr %674, align 4
  %675 = getelementptr ptr, ptr %665, i32 4
  store ptr %674, ptr %675, align 8
  %676 = getelementptr %.32, ptr %664, i32 0, i32 5
  store ptr %646, ptr %676, align 8
  %677 = getelementptr ptr, ptr %665, i32 5
  store ptr %676, ptr %677, align 8
  %678 = getelementptr %.32, ptr %664, i32 0, i32 6
  store ptr %653, ptr %678, align 8
  %679 = getelementptr ptr, ptr %665, i32 6
  store ptr %678, ptr %679, align 8
  %680 = getelementptr %.32, ptr %664, i32 0, i32 7
  store ptr %660, ptr %680, align 8
  %681 = getelementptr ptr, ptr %665, i32 7
  store ptr %680, ptr %681, align 8
  %682 = alloca %.33, align 8
  %683 = alloca ptr, i32 14, align 8
  %684 = getelementptr %.33, ptr %682, i32 0, i32 0
  store ptr %0, ptr %684, align 8
  %685 = getelementptr ptr, ptr %683, i32 0
  store ptr %684, ptr %685, align 8
  %686 = getelementptr %.33, ptr %682, i32 0, i32 1
  store ptr %643, ptr %686, align 8
  %687 = getelementptr ptr, ptr %683, i32 1
  store ptr %686, ptr %687, align 8
  %688 = getelementptr %.33, ptr %682, i32 0, i32 2
  store i64 1, ptr %688, align 4
  %689 = getelementptr ptr, ptr %683, i32 2
  store ptr %688, ptr %689, align 8
  %690 = getelementptr %.33, ptr %682, i32 0, i32 3
  store ptr @main_kernel_6_main_kColReduction_reduce__6_1_0___block_tile_h64_1_kernel_name, ptr %690, align 8
  %691 = getelementptr ptr, ptr %683, i32 3
  store ptr %690, ptr %691, align 8
  %692 = getelementptr %.33, ptr %682, i32 0, i32 4
  store i64 %642, ptr %692, align 4
  %693 = getelementptr ptr, ptr %683, i32 4
  store ptr %692, ptr %693, align 8
  %694 = getelementptr %.33, ptr %682, i32 0, i32 5
  store i64 1, ptr %694, align 4
  %695 = getelementptr ptr, ptr %683, i32 5
  store ptr %694, ptr %695, align 8
  %696 = getelementptr %.33, ptr %682, i32 0, i32 6
  store i64 1, ptr %696, align 4
  %697 = getelementptr ptr, ptr %683, i32 6
  store ptr %696, ptr %697, align 8
  %698 = getelementptr %.33, ptr %682, i32 0, i32 7
  store i64 256, ptr %698, align 4
  %699 = getelementptr ptr, ptr %683, i32 7
  store ptr %698, ptr %699, align 8
  %700 = getelementptr %.33, ptr %682, i32 0, i32 8
  store i64 1, ptr %700, align 4
  %701 = getelementptr ptr, ptr %683, i32 8
  store ptr %700, ptr %701, align 8
  %702 = getelementptr %.33, ptr %682, i32 0, i32 9
  store i64 1, ptr %702, align 4
  %703 = getelementptr ptr, ptr %683, i32 9
  store ptr %702, ptr %703, align 8
  %704 = getelementptr %.33, ptr %682, i32 0, i32 10
  store i32 0, ptr %704, align 4
  %705 = getelementptr ptr, ptr %683, i32 10
  store ptr %704, ptr %705, align 8
  %706 = getelementptr %.33, ptr %682, i32 0, i32 11
  store ptr null, ptr %706, align 8
  %707 = getelementptr ptr, ptr %683, i32 11
  store ptr %706, ptr %707, align 8
  %708 = getelementptr %.33, ptr %682, i32 0, i32 12
  store i32 8, ptr %708, align 4
  %709 = getelementptr ptr, ptr %683, i32 12
  store ptr %708, ptr %709, align 8
  %710 = getelementptr %.33, ptr %682, i32 0, i32 13
  store ptr %665, ptr %710, align 8
  %711 = getelementptr ptr, ptr %683, i32 13
  store ptr %710, ptr %711, align 8
  call void @disc_ral_call(ptr %0, ptr @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr %683)
  br label %712

712:                                              ; preds = %210, %333, %458, %585
  %713 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %133, 0
  %714 = alloca %.13, align 8
  %715 = alloca ptr, i32 2, align 8
  %716 = getelementptr %.13, ptr %714, i32 0, i32 0
  store ptr %0, ptr %716, align 8
  %717 = getelementptr ptr, ptr %715, i32 0
  store ptr %716, ptr %717, align 8
  %718 = getelementptr %.13, ptr %714, i32 0, i32 1
  store ptr %713, ptr %718, align 8
  %719 = getelementptr ptr, ptr %715, i32 1
  store ptr %718, ptr %719, align 8
  call void @disc_ral_call(ptr %0, ptr @dealloc___gpu___pvoid_pvoid___void, ptr %715)
  %720 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %60, 0
  %721 = alloca %.14, align 8
  %722 = alloca ptr, i32 2, align 8
  %723 = getelementptr %.14, ptr %721, i32 0, i32 0
  store ptr %0, ptr %723, align 8
  %724 = getelementptr ptr, ptr %722, i32 0
  store ptr %723, ptr %724, align 8
  %725 = getelementptr %.14, ptr %721, i32 0, i32 1
  store ptr %720, ptr %725, align 8
  %726 = getelementptr ptr, ptr %722, i32 1
  store ptr %725, ptr %726, align 8
  call void @disc_ral_call(ptr %0, ptr @dealloc___gpu___pvoid_pvoid___void, ptr %722)
  %727 = alloca i64, i64 1, align 8
  %728 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } undef, ptr %727, 0
  %729 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %728, ptr %727, 1
  %730 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %729, i64 0, 2
  %731 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %730, i64 0, 3, 0
  %732 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %731, i64 1, 4, 0
  %733 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 0
  %734 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 1
  %735 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 2
  %736 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 3, 0
  %737 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 4, 0
  %738 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %732, 0
  %739 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %732, 1
  %740 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %732, 2
  %741 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %732, 3, 0
  %742 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %732, 4, 0
  %743 = alloca %.15, align 8
  %744 = alloca ptr, i32 13, align 8
  %745 = getelementptr %.15, ptr %743, i32 0, i32 0
  store ptr %0, ptr %745, align 8
  %746 = getelementptr ptr, ptr %744, i32 0
  store ptr %745, ptr %746, align 8
  %747 = getelementptr %.15, ptr %743, i32 0, i32 1
  store ptr null, ptr %747, align 8
  %748 = getelementptr ptr, ptr %744, i32 1
  store ptr %747, ptr %748, align 8
  %749 = getelementptr %.15, ptr %743, i32 0, i32 2
  store ptr %733, ptr %749, align 8
  %750 = getelementptr ptr, ptr %744, i32 2
  store ptr %749, ptr %750, align 8
  %751 = getelementptr %.15, ptr %743, i32 0, i32 3
  store ptr %734, ptr %751, align 8
  %752 = getelementptr ptr, ptr %744, i32 3
  store ptr %751, ptr %752, align 8
  %753 = getelementptr %.15, ptr %743, i32 0, i32 4
  store i64 %735, ptr %753, align 4
  %754 = getelementptr ptr, ptr %744, i32 4
  store ptr %753, ptr %754, align 8
  %755 = getelementptr %.15, ptr %743, i32 0, i32 5
  store i64 %736, ptr %755, align 4
  %756 = getelementptr ptr, ptr %744, i32 5
  store ptr %755, ptr %756, align 8
  %757 = getelementptr %.15, ptr %743, i32 0, i32 6
  store i64 %737, ptr %757, align 4
  %758 = getelementptr ptr, ptr %744, i32 6
  store ptr %757, ptr %758, align 8
  %759 = getelementptr %.15, ptr %743, i32 0, i32 7
  store ptr %738, ptr %759, align 8
  %760 = getelementptr ptr, ptr %744, i32 7
  store ptr %759, ptr %760, align 8
  %761 = getelementptr %.15, ptr %743, i32 0, i32 8
  store ptr %739, ptr %761, align 8
  %762 = getelementptr ptr, ptr %744, i32 8
  store ptr %761, ptr %762, align 8
  %763 = getelementptr %.15, ptr %743, i32 0, i32 9
  store i64 %740, ptr %763, align 4
  %764 = getelementptr ptr, ptr %744, i32 9
  store ptr %763, ptr %764, align 8
  %765 = getelementptr %.15, ptr %743, i32 0, i32 10
  store i64 %741, ptr %765, align 4
  %766 = getelementptr ptr, ptr %744, i32 10
  store ptr %765, ptr %766, align 8
  %767 = getelementptr %.15, ptr %743, i32 0, i32 11
  store i64 %742, ptr %767, align 4
  %768 = getelementptr ptr, ptr %744, i32 11
  store ptr %767, ptr %768, align 8
  %769 = getelementptr %.15, ptr %743, i32 0, i32 12
  %770 = getelementptr ptr, ptr %744, i32 12
  store ptr %769, ptr %770, align 8
  call void @disc_ral_call(ptr %0, ptr @inc_ref___gpu___pvoid_pvoid_m1df32_m1di64___m0df32, ptr %744)
  %771 = load { ptr, ptr, i64 }, ptr %769, align 8
  %772 = extractvalue { ptr, ptr, i64 } %771, 0
  %773 = extractvalue { ptr, ptr, i64 } %771, 1
  %774 = insertvalue { ptr, ptr, i64 } undef, ptr %772, 0
  %775 = insertvalue { ptr, ptr, i64 } %774, ptr %773, 1
  %776 = insertvalue { ptr, ptr, i64 } %775, i64 0, 2
  %777 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %207, 0
  %778 = alloca %.16, align 8
  %779 = alloca ptr, i32 2, align 8
  %780 = getelementptr %.16, ptr %778, i32 0, i32 0
  store ptr %0, ptr %780, align 8
  %781 = getelementptr ptr, ptr %779, i32 0
  store ptr %780, ptr %781, align 8
  %782 = getelementptr %.16, ptr %778, i32 0, i32 1
  store ptr %777, ptr %782, align 8
  %783 = getelementptr ptr, ptr %779, i32 1
  store ptr %782, ptr %783, align 8
  call void @disc_ral_call(ptr %0, ptr @dealloc___gpu___pvoid_pvoid___void, ptr %779)
  %784 = alloca %.17, align 8
  %785 = alloca ptr, i32 3, align 8
  %786 = getelementptr %.17, ptr %784, i32 0, i32 0
  store ptr %0, ptr %786, align 8
  %787 = getelementptr ptr, ptr %785, i32 0
  store ptr %786, ptr %787, align 8
  %788 = getelementptr %.17, ptr %784, i32 0, i32 1
  store i64 ptrtoint (ptr getelementptr (float, ptr null, i64 1) to i64), ptr %788, align 4
  %789 = getelementptr ptr, ptr %785, i32 1
  store ptr %788, ptr %789, align 8
  %790 = getelementptr %.17, ptr %784, i32 0, i32 2
  %791 = getelementptr ptr, ptr %785, i32 2
  store ptr %790, ptr %791, align 8
  call void @disc_ral_call(ptr %0, ptr @alloc___cpu___pvoid_i64___pvoid, ptr %785)
  %792 = load ptr, ptr %790, align 8
  %793 = insertvalue { ptr, ptr, i64 } undef, ptr %792, 0
  %794 = insertvalue { ptr, ptr, i64 } %793, ptr %792, 1
  %795 = insertvalue { ptr, ptr, i64 } %794, i64 0, 2
  %796 = extractvalue { ptr, ptr, i64 } %776, 0
  %797 = extractvalue { ptr, ptr, i64 } %776, 1
  %798 = extractvalue { ptr, ptr, i64 } %776, 2
  %799 = extractvalue { ptr, ptr, i64 } %795, 0
  %800 = extractvalue { ptr, ptr, i64 } %795, 1
  %801 = extractvalue { ptr, ptr, i64 } %795, 2
  %802 = alloca %.18, align 8
  %803 = alloca ptr, i32 8, align 8
  %804 = getelementptr %.18, ptr %802, i32 0, i32 0
  store ptr %0, ptr %804, align 8
  %805 = getelementptr ptr, ptr %803, i32 0
  store ptr %804, ptr %805, align 8
  %806 = getelementptr %.18, ptr %802, i32 0, i32 1
  store ptr null, ptr %806, align 8
  %807 = getelementptr ptr, ptr %803, i32 1
  store ptr %806, ptr %807, align 8
  %808 = getelementptr %.18, ptr %802, i32 0, i32 2
  store ptr %796, ptr %808, align 8
  %809 = getelementptr ptr, ptr %803, i32 2
  store ptr %808, ptr %809, align 8
  %810 = getelementptr %.18, ptr %802, i32 0, i32 3
  store ptr %797, ptr %810, align 8
  %811 = getelementptr ptr, ptr %803, i32 3
  store ptr %810, ptr %811, align 8
  %812 = getelementptr %.18, ptr %802, i32 0, i32 4
  store i64 %798, ptr %812, align 4
  %813 = getelementptr ptr, ptr %803, i32 4
  store ptr %812, ptr %813, align 8
  %814 = getelementptr %.18, ptr %802, i32 0, i32 5
  store ptr %799, ptr %814, align 8
  %815 = getelementptr ptr, ptr %803, i32 5
  store ptr %814, ptr %815, align 8
  %816 = getelementptr %.18, ptr %802, i32 0, i32 6
  store ptr %800, ptr %816, align 8
  %817 = getelementptr ptr, ptr %803, i32 6
  store ptr %816, ptr %817, align 8
  %818 = getelementptr %.18, ptr %802, i32 0, i32 7
  store i64 %801, ptr %818, align 4
  %819 = getelementptr ptr, ptr %803, i32 7
  store ptr %818, ptr %819, align 8
  call void @disc_ral_call(ptr %0, ptr @d2h___gpu___pvoid_pvoid_m0df32_m0df32___void, ptr %803)
  %820 = alloca %.19, align 8
  %821 = alloca ptr, i32 2, align 8
  %822 = getelementptr %.19, ptr %820, i32 0, i32 0
  store ptr %0, ptr %822, align 8
  %823 = getelementptr ptr, ptr %821, i32 0
  store ptr %822, ptr %823, align 8
  %824 = getelementptr %.19, ptr %820, i32 0, i32 1
  store ptr null, ptr %824, align 8
  %825 = getelementptr ptr, ptr %821, i32 1
  store ptr %824, ptr %825, align 8
  call void @disc_ral_call(ptr %0, ptr @sync_on_stream___gpu___pvoid_pvoid___void, ptr %821)
  %826 = extractvalue { ptr, ptr, i64 } %776, 0
  %827 = alloca %.20, align 8
  %828 = alloca ptr, i32 2, align 8
  %829 = getelementptr %.20, ptr %827, i32 0, i32 0
  store ptr %0, ptr %829, align 8
  %830 = getelementptr ptr, ptr %828, i32 0
  store ptr %829, ptr %830, align 8
  %831 = getelementptr %.20, ptr %827, i32 0, i32 1
  store ptr %826, ptr %831, align 8
  %832 = getelementptr ptr, ptr %828, i32 1
  store ptr %831, ptr %832, align 8
  call void @disc_ral_call(ptr %0, ptr @dealloc___gpu___pvoid_pvoid___void, ptr %828)
  %833 = extractvalue { ptr, ptr, i64 } %795, 0
  %834 = extractvalue { ptr, ptr, i64 } %795, 1
  %835 = extractvalue { ptr, ptr, i64 } %795, 2
  %836 = alloca %.21, align 8
  %837 = alloca ptr, i32 5, align 8
  %838 = getelementptr %.21, ptr %836, i32 0, i32 0
  store ptr %0, ptr %838, align 8
  %839 = getelementptr ptr, ptr %837, i32 0
  store ptr %838, ptr %839, align 8
  %840 = getelementptr %.21, ptr %836, i32 0, i32 1
  store i64 0, ptr %840, align 4
  %841 = getelementptr ptr, ptr %837, i32 1
  store ptr %840, ptr %841, align 8
  %842 = getelementptr %.21, ptr %836, i32 0, i32 2
  store ptr %833, ptr %842, align 8
  %843 = getelementptr ptr, ptr %837, i32 2
  store ptr %842, ptr %843, align 8
  %844 = getelementptr %.21, ptr %836, i32 0, i32 3
  store ptr %834, ptr %844, align 8
  %845 = getelementptr ptr, ptr %837, i32 3
  store ptr %844, ptr %845, align 8
  %846 = getelementptr %.21, ptr %836, i32 0, i32 4
  store i64 %835, ptr %846, align 4
  %847 = getelementptr ptr, ptr %837, i32 4
  store ptr %846, ptr %847, align 8
  call void @disc_ral_call(ptr %0, ptr @ral_send_output___cpu___pvoid_i64_m0df32___void, ptr %837)
  ret void
}

host default target triple: x86_64-unknown-linux-gnu
host cpu name: icelake-server
host cpu features: +xsaves,+sse2,-hreset,+avx512cd,+sha,+xsaveopt,-kl,-avxvnni,-mwaitx,-clzero,+sse4.2,+bmi,-cldemote,-widekl,+avx512f,-raoint,+xsavec,+lzcnt,-serialize,-avxvnniint8,+fsgsbase,+aes,+sse,-sse4a,-rdpru,-tbm,-avx512bf16,-rtm,+fma,-waitpkg,-amx-fp16,+avx512ifma,-avx512vp2intersect,+popcnt,+vaes,-prefetchi,+f16c,+avx2,+sahf,+xsave,-uintr,+fxsr,-sgx,+pconfig,-avx512er,-avx512fp16,+gfni,+rdseed,+bmi2,-movdir64b,+avx512vl,+pku,-xop,+avx512bw,+avx512vbmi,+prfchw,+rdpid,+sse3,+cx16,+vpclmulqdq,+avx512vbmi2,-enqcmd,-amx-bf16,+64bit,-amx-int8,-avx512pf,-ptwrite,-amx-tile,-lwp,+avx512vpopcntdq,+avx512dq,-avxneconvert,+mmx,-fma4,+avx512vnni,-avxifma,+avx,+cmov,+sse4.1,+movbe,+invpcid,+adx,+clwb,-prefetchwt1,-cmpccxadd,+ssse3,+cx8,+clflushopt,-tsxldtrk,+pclmul,+crc32,+rdrnd,+avx512bitalg,-shstk,-movdiri,+wbnoinvd
after optimize llvm module:
; ModuleID = 'LLVMDialectModule'
source_filename = "LLVMDialectModule"
target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

%0 = type { ptr, i64, { ptr, ptr, i64, [2 x i64], [2 x i64] } }
%.1 = type { ptr, i64, { ptr, ptr, i64, [2 x i64], [2 x i64] } }
%.2 = type { ptr, i64, ptr }
%.3 = type { ptr, ptr, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64 }
%.4 = type { ptr, ptr }
%.5 = type { ptr, i64, ptr }
%.6 = type { ptr, ptr, ptr, ptr, i64, i64, i64, i64, i64, ptr, ptr, i64, i64, i64, i64, i64 }
%.7 = type { ptr, ptr }
%.8 = type { ptr, i64, ptr }
%.9 = type { i64, ptr }
%.10 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.11 = type { i64, i64, i64, ptr, ptr, ptr }
%.12 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.22 = type { i64, ptr }
%.23 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.24 = type { i64, i64, i64, ptr, ptr, ptr }
%.25 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.26 = type { i64, ptr }
%.27 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.28 = type { i64, i64, i64, i64, i64, ptr, ptr, ptr }
%.29 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.30 = type { i64, ptr }
%.31 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.32 = type { i64, i64, i64, i64, i64, ptr, ptr, ptr }
%.33 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.13 = type { ptr, ptr }
%.14 = type { ptr, ptr }
%.15 = type { ptr, ptr, ptr, ptr, i64, i64, i64, ptr, ptr, i64, i64, i64, { ptr, ptr, i64 } }
%.16 = type { ptr, ptr }
%.17 = type { ptr, i64, ptr }
%.18 = type { ptr, ptr, ptr, ptr, i64, ptr, ptr, i64 }
%.19 = type { ptr, ptr }
%.20 = type { ptr, ptr }
%.21 = type { ptr, i64, ptr, ptr, i64 }

@main_kernel_6_main_kColReduction_reduce__6_1_0___block_tile_h64_1_kernel_name = internal constant [52 x i8] c"main_kColReduction_reduce__6_1_0___block_tile_h64_1\00"
@main_kernel_6_blob_gpu.binary = internal constant [2024 x i8] c"P\EDU\BA\01\00\10\00\D8\07\00\00\00\00\00\00\02\00\01\01@\00\00\00\98\07\00\00\00\00\00\00\93\07\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\A8\12\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00g\02\00\BE\00r\00\01\00\22\12\00\01\00\11\0F\06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\0Ce__6_1_0___block_tile_h64_1C\00\0F=\00&oshared?\00&\9Fconstant0B\00#\FA\01debug_frame\00.rel\11\00!nv\14\00\11aN\00\0FW\01 \0F\93\00 \0F\8B\01\A4\8F$____wg_<\00 \00\15\00/28\CD\010o_param\D4\01\1C\0F\01\00\09\8Cf\00\00\00\03\00\0A\00\01\00\11\DD\18\00,\0B\00\01\00 ^\01\18\00,\09\00\01\00\11\A7\18\00,\04\00\01\00\11\C5\18\00,\07\00\01\00\102\E3\03\17\10x\00\03#\00f\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0\06\07\00 \00\04\9B\00R\04\14\00\00\00E\002\04p\01\18\000/\08\00#\00\10\19\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\A8\04\F3\08\015\00\00\04\0A\08\00\03\00\00\00`\010\00\03\190\00\04\17\0C\CB\00U(\00\00\F0!\10\009\06\00 \10\009\05\00\18\10\00u\04\00\10\00\00\F0\11\10\009\03\00\0C\10\009\02\00\08\10\009\01\00\04\10\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\00\D0\05\00\00 \06\00\00\04\1E\94\01#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\11\02h\01\0F\01\00\FF\98@$v\01\FFw\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%s\02a\0E\00\19y\03\00\01\00\10!-\00\F0\04\0E\00$z\00\00\00Z\00\00\03\02\8E\07\00\CA\1F\00\0C\10\00\C5[\00\00p`\F0\03\00\DA\0F\00M\93\04\B0\80\03\00\EA\0F\00\19x\03\FF\1F\0F\00\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\00\EB\04Q\E2\0F\00Eyy\03\020\00a\E4\0F\00\11r\04q\00\C1\FF@\8F\07\00\C8\0F\00\12x\03\04Y\04\10\C0p\00@\0F\00$x\BC\02`\00\00\03\0A\8E\070\00c$r\03\FF\FF\00\C0\00\10\C60\00 \02\00p\00\22\FF\C0 \00P\19x\05\FF\05Q\000\16\01\00\10\00@\0Cr\00\020\001pR\F2p\00P\0Cx\00\00\7F\10\000@\F0\03\90\00@$x\02\02\D9\02\10\05\E0\00\12\C8\10\00\14\04`\00\96\CC\0F\00G\19\00\00@\03\E0\00c$v\03\FF\00X \00\10\E2p\001\04\FF\08\E3\04A\01\00\00\C8`\00$\03\01p\00u\E2\0F\04$x\04\04`\00\01\C0\00\16\05\C0\00\91\E2\0F\00\07z\03\03\00Y`\00\02@\00Uz\00\03\00X\B0\00\11\04\10\00\10Y\10\00\12\F4\B0\00\08\00\01\10\C4\B0\004\08\04@`\00\11\CA@\00\81\08\00\\\00\00pb\FA@\00@\10x\0A\08\90\00B\FF\E0\FF\07\10\006\10\08\02\10\00\000\00\12\0A0\00#\FC\03\10\00\12\10\10\00\11\F8\10\00T\10x\13\08\030\00\91\C6\0F\00\07\D2\06\08\FF\00\B0\00\11\04\E0\00H\D4\09\FF\04\D0\00#\D2\08 \00#\00\05P\00\12\13P\00\11\F6\C0\00\A3%\D6\06\06\00`\00\00\09\020\00S\E8\0C\0A\01\000\00\10\C6 \00H\08\08\00^ \006\0A\0A\01p\00P\00\81\D9\06\06p\00\BA\00\19\1E\0C\00\A4\00\00$\E4\0B\80\00S\C8\0E\10\02\00P\00u\E2\0F\04\81\D9\12\080\00\87\A2\02\00\07\C8\10\10\02P\00@%\E6\0C\0Cp\00\14\0Bp\001\B8\11\13\EF\01\03\90\00:$\C4\15`\00E\B8\07\13\03@\00\86\1F\00%\E6\0A\0A\00`@\00E\81\E9\14\0Cp\00p\E6\00\00%\C6\0E\0E`\001\15\02\8Ep\01E\81\E9\0A\0A \00\87\E4\0E\00$\B4\16\FF\04\D0\01c%\C6\08\10\00`0\00u\E2/\00\81\C9\0E\0E0\00p&\0F\00%\B6\10\11P\00\12\16P\00F\08\81\C9\08\E0\00\10$ \00D\0C\07\00` \00e\1F\00\81\B9\10\10 \00fh\0F\00\81\B9\0C\A0\00\10b\E0\014\05\05\04\E0\01\81\E2\0F\00#\D2\03\12\06\0C\07P\00\00\00\C6O\D0\02\10\05`\021pR\FA\C0\01T#\E2\03\14\0A \00\85\C8\8F\00#\C2\03\0E\08\10\00v\0F\01#\B2\03\10\0C\10\00p\02GY\00\00P\FD\D1\03\11\83@\03\14Ap\04\03P\03A\88s\00\02,\00\00\96\06f\E8\0F\00\1D{\00\01\00\84\EC\0F\00\84\89\04\02\00 \00\12\E2\C0\03\10?\90\000@\F2\03\A0\01c\84\89\05\02\00\10 \00\93$\0E\00!\82\05\04\05\00\01\00\8F\CA\1F\00\88\83\00\02\05`\00\09\1E\99`\00\14\1F \04\00`\00!\99\07\07\08\05`\00H\92\07\04\07`\00O\93\00\02\07\C0\00\0A\1A\03`\00\10r\EC\01\03\E0\03\13\C6\E0\00\18\04\C0\00J\03\03\04\00\C0\00\0F \01\099M\19\00P\01'\84yp\00\96\22\0E\00$v\04\FF\00b`\02c$v\05\FF\00c\10\00a\CA\0F\00\8Ey\00\81\05@\84\E7\10\0C\D0\02\1BM\A0\01cGy\00\00\F0\FF\C0\01f\C0\0F\00\18y\00\01\00\0F\10\00\A0\0F\01\00-\14\01\DC\02\0B\01\00\22@\00\01\00=W\01\000\00\08\01\00\1F\0B@\00\04\13\97)\00\1F\D4@\00\0C\13\13t\09\0C\01\00\13pU\00*\A8\00\98\09\22\08\00\01\00\22\18\00\01\00.*\01T\00\00\01\00\13\18u\02/p\00\80\00\0B\1F)'\00\03#\00\88@\00\04\10\0C\04\E4\00*\04\00\01\00\1Fl@\00\04\13\B8)\00&\B8\00@\00\1F\0A@\00\00!H\01D\01\0D@\00\13p\F5\03*\D8\00\01\00\1B\08\08\00?7\01\00F\0D\00Q\00\00H\06\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\E8\14\01\0C\84\01\13X@\00\17\901\01\0F\C0\00\01\132@\00\15\06R\00\0B\A1\00\16\07\80\00j\06\00\00\19\80\00\01\00\13\A9+\00+\03\00\01\00\04\DB\02\1F\04\80\00\0B\11\06\D9\06\07\E8\11\0B\01\00\1B\A8\08\00\04\97\00\13\018\00\04\E8\00\0D8\00\1A\08\08\00\088\00\18\06\A0\00\0F\01\00\05\03\A9\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00"
@main_kernel_5_main_kColReduction_reduce__6_1_0___block_tile_h64_kernel_name = internal constant [50 x i8] c"main_kColReduction_reduce__6_1_0___block_tile_h64\00"
@main_kernel_5_blob_gpu.binary = internal constant [1112 x i8] c"P\EDU\BA\01\00\10\00H\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\08\04\00\00\00\00\00\00\01\04\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\0Ae__6_1_0___block_tile_h64A\00\0F;\00$oshared=\00$\9Fconstant0@\00!\FA\01debug_frame\00.rel\11\00!nv\14\00\11aL\00\0FO\01 \0F\91\00\1E\0F\81\01\DEo_param\88\01\1C\0F\01\00\05\8Cd\00\00\00\03\00\0A\00\01\00 \14\01\18\00,\09\00\01\00\11[\18\00,\04\00\01\00\11y\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\1C\00\00\00E\00#\04\10\18\00\80/\08\00\05\00\00\00\06\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\048\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C$\00u\01\00\08\00\00\F0!\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00p\00\00\00\C0\00\01\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\00#\02b,\00\00\00H\00\01\00\00 \01/\05\00\01\00\FFp@$v\01\FF\CF\03\A3\FF\00\8E\07\00\C4\0F\00\C3y\AC\03\A2%\00\00\00\22\0E\00\19y\00\01\00\F3\0E!\00\00\00b\0E\00\90r\05?\04\00\00\80?\E0\FF\0F\00\C6\1F\00\B9z\04\00\00X\9E\02p\E4\0F\00\A4r\04\05B\00\B1?\02\8E\0F\00\CC\0F\00\0C|\00\FE\03\95pR\F0\0B\00\DA/\00M#\04\F3\0D\80\03\00\EA\0F\00\02z\02\00\00Z\00\00\00\0F\00\00\00\E2\0F\00$v\03\FF\00[\90\00\22\E2\0F`\001F\00\00s\04\80\C8\0F\00\86y\00\02\FFp\030\19\10\0C \00*MyP\00PGy\00\00\F0\F9\03\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\80\0F\01\00-\00\C7\04.\03\00\01\00\22@\00\01\00=O\01\000\00\08\01\00\1F\0B@\00\04\13\8F)\00\1F\88@\00\0C\13\13\CC\03\0C\01\00\13\18U\00*\90\00\F0\03\04\0D\04\22\18\00\01\00.\22\01T\00\00\01\00\13\A8@\00/p\00\80\00\0B\1F)'\00\03A\00\18\04\00\01\00\04\00\06\04\E4\00*\04\00\01\00\1Fj@\00\04\13H)\00&L\00@\00\1F\0A@\00\00!@\01D\01\0D@\00\13\98)\00*\D8\00\01\00\1B\08\08\00?/\01\006\07\003\00\00p\1D\05\17\10\80\00\17\048\00\04\18\00\13\E2\14\01\0C\84\01*\80\05\E8\06\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\80\07\12\03\C0\01:\06\80\00\01\00\13\06\D8\01\04(\0B\0D\C8\01\1A\00\08\00\04\F8\00\13\018\00\04\A8\00\0D\01\00)\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00\00\00\00\00\00\00"
@main_kernel_4_main_kColReduction_reduce__6_1_0___thread_tile_h32_1_kernel_name = internal constant [53 x i8] c"main_kColReduction_reduce__6_1_0___thread_tile_h32_1\00"
@main_kernel_4_blob_gpu.binary = internal constant [3368 x i8] c"P\EDU\BA\01\00\10\00\18\0D\00\00\00\00\00\00\02\00\01\01@\00\00\00\D8\0C\00\00\00\00\00\00\D8\0C\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8!\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\00!@!\07\001\00\80\1E\07\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\0De__6_1_0___thread_tile_h32_1D\00\0F>\00'oshared@\00'\9Fconstant0C\00$\FA\01debug_frame\00.rel\11\00!nv\14\00\11aO\00\0F[\01 \0F\94\00!\0F\90\01\EAo_param\97\01\1C\0F\01\00\0A\8Cg\00\00\00\03\00\0A\00\01\00  \01\18\00,\09\00\01\00\11j\18\00,\04\00\01\00\11\88\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\16\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\18\00\00\00E\002\04H\05\18\000/\08\00\0A\00\10\22\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04X\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\010\00\03\190\00\04\17\0C$\00u\07\00(\00\00\F0!\10\009\06\00 \10\009\05\00\18\10\00u\04\00\10\00\00\F0\11\10\009\03\00\0C\10\009\02\00\08\10\009\01\00\04\10\00\01\01\00\F1\02\F0\11\00\03\1B\FF\00\04\1C\08\00`\00\00\00\90\15\16\003\00K\00\01\00`\02\02\08\10\0A/\E7\00\11\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00\80\01/\05\00\01\00\FF\F0@$v\01\FF\AF\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\CB\02Q\0E\00\19y\03\0F\00\10\00\08\08\F0\15$\0E\00$z\00\00\00Z\00\00\03\02\8E\07\00\CA\1F\00\12x\FF\00\FF\01\00\00\FF\C0\80\07\00\C8\0F\00\0C \00\C5[\00\00pdp\00\00\DA\0F\00M\F3\04\B0\80\03\00\EA\0F\00\19x\03\FF\1F\0F\00\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\003\05a\C6\0F\00\11r\03q\00@\FFH\8F\07 \00c$v\00\FF\00X\A0\00\12\C6@\00\00S\00\10\03@\00P\E4\0F\00\0Cx$\03`\00\00pR\F0\03 \00c$x\03\03 \000\00@\E2\0F\00\07\90\00\22Y\00\01\00p\C8\0F\00\10x\0A\030\00\F0\05\FF\E0\FF\07\00\E4\0F\04\0Cz\00\03\00\\\00\00pb\F4\03\10\00Z\10x\0C\03\02 \00\12\0A \00\10\FC \00f\00\10x\12\03\03 \00\00\F0\00\12\0C \00C\FA\03\00\C4\00\01\13X\90\00\02@\00\01\80\00ApR\F2\03\D0\00G\A4\09\FF\04\A0\00B\0Cz\00\12@\00\13\F6`\004\02\03\04`\00i\E2\0F\04$\E4\0B0\00c\07\A2\04\03\FF\00\FF\04c\E4\0F\04\07\A2\08\10\00 \80\04 \00<$\D4\0D`\00\12\02`\00\11\F8\80\00@%\A6\04\04\E9\04#\09\02P\00D\E8\06\0A\01@\00\10\C4\10\004\0A\0A\01`\00\010\00G\08\08\00^0\00F\D8\0E\0C\02p\00A\04\81\A9\05#\06\D8\00\19\1E\0C\00\A2\00\00\07\D8\0C\0C\02@\000\E6\06\06@\00\12\0B@\00f\08\07\B8\10\12\03@\00U\00\81\A9\00\08@\00\95\A4\02\00%\E6\0A\0A\00`0\00g\00\07\B8\12\12\03P\00E\81\E9\07\060\00p\E4\0E\00%\D6\0E\0E`\00\14\0D0\008\C2\13\02\10\01F\81\E9\14\0A0\00V\08\00$\B4\15 \01\96\C4\0F\00%\D6\0C\0C\00`@\00E\81\D9\0F\0E0\00pf\0F\00%\B6\10\10`\00\14\15\C0\00F\C2\0A\02\FF\90\00U\01\81\D9\0C\0C0\00\10d0\00C\08\12\00`0\00u\E4/\00\81\B9\11\10 \00i$\03\00$\C4\0B\80\006\81\B9\08\00\01u$\0F\00%\C6\12\130\01\10\C8\10\00\07\10\01u\E4\0F\00\81\C9\13\120\00\10(\10\00\16\0A\E0\00\84\22\0F\00$r\02\FF\FF`\00\10\E2P\025\04\03\05\B0\02u\1F\04\10x\10\03\07`\02Q/\04#\A2\02\08\06\01\D4\00\84\E2O\00\10x\05\03\06 \00\02\D0\02\15\04 \03\83\E2\0F\00#\E2\02\07\14@\060\00\E2\8F \00\15\05 \03\84\C6\0F\00#\D2\02\0F\0C \00\85\C8\0F\02#\B2\02\11\080\00j\0F\01\0Cz\00\10\F0\028\11\03\08\F0\02\0B \03W\07\A8\08\04\01\F0\02V\10x\00\03\090\00f\00#\C2\02\13\0A`\00\00\90\00\15\11\F0\02\10\E4@\007\0A\04\01\10\02*$\E4 \03Y\07\E8\06\05\02\00\039\0C\05\02\00\03'\0A\0A\00\03\11\08\D0\03\04\F0\03\00`\008\B8\0E\10\D0\02\010\03\08`\03G\B8\10\10\03P\006\81\A9\05\B0\01%\A4\00 \03\07\C0\02)\11\FF \03\19\04 \039$\B4\17@\02*%\E6\C0\02_\07\C2\12\11\FF0\03\09\12\B60\03\12\17P\02Y\08\81\E9\14\0C \03&\C4\16`\00\10\E4\D0\02C\0A\10\00`0\00W\E2\1F\00\81\B9 \03\10b\10\04\11\15\80\05\04@\011%\C6\10\C0\02\14\16`\009\B9\0A\0A \03\12\C6 \03\01 \00 \E2/0\04(\00\01P\03\18\C90\03\1A\01\D0\04\01\00\03\0B0\035\D6\12\15@\01\1B\C8\D0\03\000\00\1B\D90\03\08\B0\03\10\22\80\029\10\03\0C \034\00\03\0E\10\00\00`\02E\A2\02\05\04`\02\11OP\03\17\0A \03\000\03\14\0B\10\00\1F\E40\03\0Cw\C8\8F\00#\B2\02\0F\B0\02\16\02@\03\12\F6@\03\1F\C20\03\05\02\D0\02\000\03\17\0D\A0\00\0CP\06\00\F0\02\14\02\90\01\02\80\06\18\11\C0\06\00P\03\17\02 \02Z#\D2\02\13\0C@\03\06\E0\02\00`\00\1B\B40\037\B8\06\05\F0\02\1B\04 \03d\00\07\B8\0C\05\03`\00\00P\03O\C2\0E\10\FF \03\09O\C2\10\10\FF \03\09\19\B6 \03i\08\07\E8\13\11\01\80\06\0B \03\17\C4 \03\01\C0\02\0B \03H\E8\12\11\01p\00\1B\B9 \03\17\C6 \03\12\E2\E0\02\17\02\C0\00+\81\B90\03*\E4\16\90\03\1B\C60\03\1B\C9P\06\17\E6 \03\00\D0\00I\D8\0C\00\02\00\03\0A0\03\09\D0\07\00\D0\00\1A\E6@\03'\81\E90\03*&\01\10\03\00`\06\18\E90\03\1F\03 \03\22\1B\11 \03\14\13\10\00\0F \03\04\14\0F \00\03 \03\14\10\10\00\0F \03\01\1F\B2P\06\05\05\10\03\0C0\03\06\10\03\1C\CE\00\03\11/\00\03\08\A0\02*#\E2P\03\00@\03\17\12\90\00\00\10\03\16\03\80\01\0F\10\03\01\1F\11\10\03\0A:\B2\06\05\E0\02\090\06\07P\03\11\FC \0A3\07\B2\0C0\00\1F\00\10\03\03:\C8\0E\10\E0\02)\05\0A`\09H\C8\10\10\01@\00\08 \03\00p\05\0F\10\03\1DH\D8\13\11\02p\00\08\10\03\88\E2\0E\00\07\D8\12\11\02p\00\08 \03\1F\C4\10\03\00\1B\D4@\06\0B\10\03H\07\E8\15\00\A0\01\09 \03\10d\C0\05\0B \03O\E8\0C\00\03 \03\08'%\D6\10\03[\C4/\00\81\D9@\06)\E4\0D\90\00+\81\D9@\06\1B\E6@\06\08P\07\000\00\1B\E9 \03\1E\E9 \03\1B\16 \03\14\18\10\00\0F \03\04\1B\14 \03\14\15\10\00\0F \03=\11\C6\D0\02\0D\00\03\14\17p\00\1F\C6@\06\00O\A2\0A\04\FF@\06\02\03\F0\08G\A2\08\04\FF\10\02/#\E2@\06\05\1F\FC@\06\06\1F\01@\06\0C\18\01@\06F\C8\0E\10\020\00\0F0\03\00?\10\10\02@\06\19O\D8\13\11\03@\06)O\D8\12\11\03@\06\05.$\07@\06G\E2\15\00\FF\C0\00\090\03*$\0B0\03\0F@\06\00H\07\E2\00\00p\01\090\03*$\010\03\00\00\04\08 \03\10$\F0\0A\17\06\D0\00\1A\8F0\03\1B\E20\03%\E6\0E\10\03\22\06\02\90\00\090\03\00 \07S\E6\0C\00\00` \009\E4\0F\02 \03\1Bh \03\10b\E0\026\00\03\1A\E0\02d\04\10x\06\03\1B\10\00\01\00\034\0E\03\1C\10\00/\E2\1F0\03\007\05\03\19\B0\02\09\10\03;\C8\0F\01\00\03\00\80\02\15\06@\03\0D\00\03\17\8F\A0\02\12\FA \00\0A\C0\02\16\02`\03\04\F0\0F9\0A\03\1D\C0\009\08\03\1E\10\007\07\03\1F\A0\002\07\D8\16L\00\00\B0\01\02\D0\03\15\0E\B0\03\02\10\00\15\0A\A0\03\00P\03G\E8\10\05\010\00W\07\A8\0D\06\03\10\001\03x\0C\C0\01\01\01\00\02\90\0D\18\08\90\03H\0Cz\00\07\B0\00\06\C0\00\12\F4`\00H\B2\0B\0E\FF`\00&\C8\09\E0\0F\10\CA\B0\00(\04\08\B0\00H\07\E8\03\07\80\00\060\01\01\D0\10d\00\07\A8\14\05\01\D0\02\00 \000r\00\0C`\00$pRp\00%\0E\0E\F0\02\01\C0\03(\0A\0A0\00T\07\D8\08\08\02\10\00\000\01H\88\12\00\02 \00H\E8\00\07\03\10\008\A8\18\06\10\00\06\E0\00\03P\11\1A\84\B0\039$\A4\1A\10\00E%\86\12\12\90\03f\D0\0F\00$\94\11 \00\00P\03&\89\07\C0\02p\A6\00\00%\96\14\140\00\14\11\10\06!\96\10\90\03#\11\020\005\99\05\140\00v\E6\02\00%\86\16\16\B0\03\00 \00\17\06P\03`\08\00%\A6\18\18P\00\14\1A \005\89\0C\16 \00f\A6\0A\00$\B4\1C\90\00\00\00\04S\A6\1A\0D\00`0\00\00\80\06:\A9\0D\18\D0\04\02`\04\15\1C\F0\07&\1A\1A \00V\0E\00$\C4\1DP\00\01\C0\04 \14\0BP\00\14\1C\F0\03\08\C0\0Dl\A6\0E\00$\D4\1F\80\04\01\90\05\13\1D0\05H\81\B9\14\140\00c%\C6\10\09\00` \00\10\E4\90\07\19\0Bp\041%\D6\120\12\11\1F \00\00\00\0B*\10\10\90\04\090\0BA\02%\D6\08\90\12\14\1F \01\08\90\0A\010\0B)\18\00\E0\04\08\90\04\00\00\08U\E6\16\03\00` \00F\00\81\E9\190\01\01\90\04(\16\16\90\04c$v\04\FF\00b\80\00\00\C0\06H\92\02\05\06\A0\0D5\82\02\07\10\04 \C8O\90\04%\0D\1A\10\00\01\80\04\16\0F\90\07\00P\00C\05\FF\00cP\00\02\90\07(\0B\10\A0\04H\D2\02\13\08\10\11D\E2\02\19\16\10\00a\CA\0F\00\8Ey\00\90\0D@\84\E7\10\0CP\00\14M\80\15\030\15PGy\00\00\F0\A9\19\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\B0\0F\01\00-\00\\\04.\03\00\01\00\22@\00\01\00=[\01\000\00\08\01\00\1F\0B@\00\04\13\9B)\00\1F\97@\00\0C\14\13\CC\01\0B\01\00\138U\00\11\90\06\00\06p\19\22\08\00\01\00\22\18\00\01\00..\01T\00\00\01\00\13\C8@\00/p\00\80\00\0B\1F)'\00\03!\008\F5\02$\00\00\E0\1B\04\E4\00*\04\00\01\00\1Fm@\00\04\13h)\00&\AC\00@\00\1F\0A@\00\00!L\01D\01\0D@\001\18\05\00\01\00*\D8\00\01\00\1B\08\08\00?;\01\00\16\1D\003\00\00\F0@\00&\10\00\80\00\17\048\00\04\18\00\13\EB\14\01\0D\84\01!\06\00\01\00\17\901\01\0F\C0\00\01\132@\00+\06\00\01\00\1A\08`\1D\12\03\C0\01>\22\80\00g\00\17\05(!\0C\01\00*\A8\00\08\00\04\C0\00\13\018\00\0Ey\00\03x\00\1A\18\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00"
@main_kernel_3_main_kColReduction_reduce__6_1_0___thread_tile_h32_kernel_name = internal constant [51 x i8] c"main_kColReduction_reduce__6_1_0___thread_tile_h32\00"
@main_kernel_3_blob_gpu.binary = internal constant [1112 x i8] c"P\EDU\BA\01\00\10\00H\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\08\04\00\00\00\00\00\00\01\04\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\0Be__6_1_0___thread_tile_h32B\00\0F<\00%oshared>\00%\9Fconstant0A\00\22\FA\01debug_frame\00.rel\11\00!nv\14\00\11aM\00\0FS\01 \0F\92\00\1F\0F\86\01\E2o_param\8D\01\1C\0F\01\00\04\8Ce\00\00\00\03\00\0A\00\01\00 \18\01\18\00,\09\00\01\00\11`\18\00,\04\00\01\00\11~\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\1C\00\00\00E\00#\04\10\18\00\80/\08\00\05\00\00\00\06\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04@\04\91\015\00\00\04\0A\08\00\02\FC\00\91\10\00\03\19\10\00\04\17\0C$\00u\01\00\08\00\00\F0!\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00p\00\00\00\C0\00\01\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00 \01/\05\00\01\00\FFh@$v\01\FF\C7\03\A3\FF\00\8E\07\00\C4\0F\00\C3y\A4\03\A2%\00\00\00\22\0E\00\19y\00\01\00\F3\0E!\00\00\00b\0E\00\90r\05?\04\00\00\80?\E0\FF\0F\00\C6\1F\00\B9z\04\00\00X\96\02p\E4\0F\00\A4r\04\05B\00\B1?\02\8E\0F\00\CC\0F\00\0C|\00\F6\03\95pR\F0\0B\00\DA/\00M\1B\04\F3\0D\80\03\00\EA\0F\00\02z\02\00\00Z\00\00\00\0F\00\00\00\E2\0F\00$v\03\FF\00[\90\00\22\E2\0F`\001F\00\00k\04\80\C8\0F\00\86y\00\02\FFh\030\19\10\0C \00*MyP\00PGy\00\00\F0\F1\03\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\80\0F\01\00-\00\BF\04.\03\00\01\00\22@\00\01\00=S\01\000\00\08\01\00\1F\0B@\00\04\13\93)\00\1F\8D@\00\0C\13\13\C4\03\0C\01\00\13 U\00*\90\00\E8\03\04\05\04\22\18\00\01\00.&\01T\00\00\01\00\13\B0@\00/p\00\80\00\0B\1F)'\00\03A\00 \04\00\01\00\04\F8\05\04\E4\00*\04\00\01\00\1Fk@\00\04\13P)\00&L\00@\00\1F\0A@\00\00!D\01D\01\0D@\00\13\A0)\00*\D8\00\01\00\1B\08\08\00?3\01\00.\07\003\00\00x\15\05\17\10\80\00\17\048\00\04\18\00\13\E5\14\01\0C\84\01*\88\05\E0\06\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07x\07\12\03\C0\01:\06\80\00\01\00\13\06\D8\01\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\13\018\00\04\A8\00\0C\01\00*\F8\02\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00\00\00\00\00\00\00"
@main_kernel_2_main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1_kernel_name = internal constant [58 x i8] c"main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1\00"
@main_kernel_2_blob_gpu.binary = internal constant [3000 x i8] c"P\EDU\BA\01\00\10\00\A8\0B\00\00\00\00\00\00\02\00\01\01@\00\00\00h\0B\00\00\00\00\00\00c\0B\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\A8#\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00g\02\00\BE\00r\00\01\00\22#\00\01\00\11 \06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\12e__6_1_0___no_ibXblock_tile_h64_1I\00\0FC\00,osharedE\00,\9Fconstant0H\00)\FA\01debug_frame\00.rel\11\00!nv\14\00\11aT\00\0Fo\01 \0F\99\00&\0F\A9\01\B6\8F$____wg_B\00&\00\1B\00/26\F1\016o_param\F8\01\1C\0F\01\00\05\8Cl\00\00\00\03\00\0A\00\01\00\11\EF\18\00,\0B\00\01\00 |\01\18\00,\09\00\01\00\11\CB\18\00,\04\00\01\00\11\E9\18\00,\07\00\01\00g2\00\00\00\12\10x\00\11\18\06\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0\17\07\00 \00\04\9B\00R\04\14\00\00\00E\002\04\C0\05\18\00C/\08\00\06\7F\00\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E0\04\F1\08\015\00\00\04\0A\08\00\03\00\00\00`\01(\00\03\19(\00\04\17\0C$\00\10\05\ED\04%\F0!\10\009\04\00\18\10\009\03\00\10\10\00u\02\00\08\00\00\F0\11\10\009\01\00\04\10\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\00\10\17\00\00`\17\00\00\04\1Et\01#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\005\02\00\00\84\01\0F\01\00\FFz@$v\01\FF?\04\A3\FF\00\8E\07\00\C4\0F\00\19y\A6\01\10%[\02a\0E\00\19y\03\00\01\00\10!-\00\F5\14\0E\00$z\06\06\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\06\00Y\00\00p`\F0\03\00\DA\0F\00M[\04\B0\80\03\00\EA\0F\00\19x\03\FF\1F\FC\01\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\00\B3\04\93\E2\0F\00Ey\00\00@\150\00\93\E2\0F\00$r\08\FF\FF\00\90\00p\E2\0F\00\11r\03\03\92\00\C1\FF@\8F\07\00\C8\0F\00\12x\05\031\04\10\C0\80\00p\0F\00$x\06\06\01\B4\03\12\0A\10\00@\12x\05\06p\00\01 \00p\E4\0F\04\19x\00\FF*\04\C0\06\16\01\00\00\E4\0F\00\0Cr\00\05`\00@pR\F2\03 \00P\0Cx\00\06\7F\10\00\22@\F0\80\002x\07\05\C9\02\11\8Ep\00T$x\07\07\04\90\00\9A\CC\0F\00G\19\00\00\80\14\E0\00\000\00\10\03\E0\00\01\90\00*\03\03@\004\09\03@@\00q\C8\0F\00%x\04\09P\00\11\02\E0\00P\04\10x\00\09\C0\001\FF\E0\FF\B0\00\B0\0Cz\00\09\00Z\00\00pb\F4\A0\00P\00\12x\04\04P\00!\FF\FC\D0\00\00p\01\12\00 \00\11\F6 \00`\10z\02\04\00\\@\00\11\F3@\00`\10z\04\04\00^\10\00\11\F9\D0\01\00`\00\00|\03\03`\00\D2\00\10z\03\05\00]\00\00\FF\E4\FF\000\00@\05\05\00_\10\00*\7F\02`\00\11\F8\10\01\F4\06\81\B9\0D\02\04\FC\FF\FF\00\19\1E\0C\00\A2\0E\00\10x\0A\09\03P\00\93\C6\0F\00\81\A9\00\02\04\F8 \00 \E2\0E@\00\12\0A@\00\22\FA\03 \00%\0B\04 \00u\E8\0E\00\81\B9\0A\04P\00p\A8\0E\00\81\C9\0F\02\10\01\01\10\00y(\0F\00\81\C9\0C\04\10\00S\D9\11\02\04\04\10\00\10h\10\00%\0E\04\10\00\10b\E0\004\10\09\04\90\00\01\F0\00:\12\09\05P\01\12\10\90\00#\FC\03\E0\00\12\12\10\00\12\F2@\01Dx\10\09\060\00a\E2\0F\04#\A2\086\07\00$\021\00\E2\8F@\01\14\07 \00q\CE\0F\00\81\E9\0B\02\91\01\06\F0\00\15\10\B0\01\93\E2\0F\00#\B2\08\0D\0A\00=\05!\E2O0\01\16\08`\00`\00\81\99\0D\02\04\17\06\03P\01\09\D0\01\000\018\E9\00\04`\00U#\C2\08\0F\0CP\00&\0F\01`\01\11\F8\C0\00H\81\99\0A\04P\00\00\D0\00\19\09\A0\01C\0F\02\04\10 \00\86\22\0F\00#\D2\08\11\0EP\00\16\02\C0\00\12\FAP\00W\A9\0C\04\04\10\80\01W\B9\11\02\04\14\80\019\B9\0E\04\10\00X\C9\13\02\04\18\10\00)\10\04\10\00X\D9\15\02\04\1C\10\00H\12\04\04\1C\C0\017\14\09\0A \01U#\E2\08\0B\00\A0\00\02\80\01\17\0B\E0\01E\0Cz\00\14\D0\01\000\00\1A\92p\01\06P\01\04\E0\017\00\09\0C\10\02\00\90\01\18\0Dp\00\1D\A2`\01\15\00\D0\01\13\CA\F0\01\18 `\018\0C\09\0F`\01\00\B0\01\07 \00-#\B2`\01\19\0A\B0\03Lx\0A\09\0E\10\02\19$\F0\01$\13\10@\00\02\B0\02\06\F0\01\13\E2\D0\01\17(\A0\01\00\00\02\08@\00F\D2\08\15\12\80\00\00@\00\15\0C\E0\01\13\C4\D0\01\17,\90\01\00\F0\01\08P\00\00\E0\01\1B,\E0\01\1B0\E0\01\1B0\E0\01\1B4\E0\01\1A4\E0\01\1F\10\E0\01\06\11O\A0\01\1F\11\E0\01\16\1A\8F\E0\01\13\D6\A0\01\188\A0\01;\00\09\12\F0\01\1F\13\F0\01\15\13\E4\D0\01\1F8\D0\01\1B\16\14`\00\11\04\D0\01\16<\90\01X\10x\0E\09\15\80\00\08\E0\01\1B\E2\E0\01\04\B0\03\1B@\E0\01\1F<\E0\01\0A\1D\0E\C0\03\1B@\C0\03\1BD\C0\03\1BD\E0\01\1BH\E0\01\1BH\E0\01\1BL\E0\01\1AL\E0\01\1F\16\E0\01\0C\1F\17\E0\01-\1AP\E0\01\1F\18\D0\01\08\00\90\01\16\19\00\02\1F\00\E0\01\02\1FP\E0\01\17\01\D0\01\18T\D0\017\0A\09\1A\D0\00\00\10\04\1F\1B\E0\01\15\13\E4\E0\01\1BX\E0\01\1FT\C0\03\1C\1B\\\C0\03\1BX\C0\03\1B\\\E0\01\1B`\E0\01\1B`\E0\01\1Bd\E0\01\1Ad\E0\01\1F\1C\E0\01\0C\1F\1D\E0\01\05\13\DA\C0\01\17hp\01\0F\F0\01\09\01P\03;\00\09\1E\C0\03\1F\1F\C0\03\1D\1Fh\E0\01\14\01\C0\01<\0A\09 \C0\03\1Al\C0\03\1F!\C0\03\1D\1Bp\E0\01\1Fl\C0\03\1C\1Bp\C0\03\1Bt\C0\03\1Bt\E0\01\1Bx\E0\01\1Bx\E0\01\1B|\E0\01\1A|\E0\01\1F\22\E0\01\0C\1F#\C0\03-\16\80\90\01\00P\00\1F$\C0\03\0C\1F%\C0\03\0D\1F\80\C0\03\1C\18\84\D0\017\0A\09&\D0\00\00\C0\03\1F'\C0\03\1D\1B\88\E0\01\1F\84\C0\03\1C\1B\8C\C0\03\1B\88\C0\03\1B\8C\E0\01\1B\90\E0\01\1B\90\E0\01\1B\94\E0\01\1A\94\E0\01\1F(\E0\01\0C\1F)\C0\03\0D\1F\98\C0\03\1B\1B*\C0\03\1F+\C0\03\1D\1F\98\C0\03\1B\1C,\C0\03\1A\9C\C0\03\1F-\C0\03\1D\1B\A0\E0\01\1F\9C\C0\03\1C\1B\A0\C0\03\1B\A4\C0\03\1B\A4\E0\01\1B\A8\E0\01\1B\A8\E0\01\1B\AC\E0\01\1A\AC\E0\01\1F.\E0\01\0C\1F/\C0\03-\16\B0\90\01\00P\00\1F0\C0\03\0C\1F1\C0\03\0D\1F\B0\C0\03\1C\18\B4\D0\017\0A\092\D0\00\00\C0\03\1F3\C0\03\1D\1B\B8\E0\01\1F\B4\C0\03\1C\1B\BC\C0\03\1B\B8\C0\03\1B\BC\E0\01\1B\C0\E0\01\1B\C0\E0\01\1B\C4\E0\01\1A\C4\E0\01\1F4\E0\01\0C\1F5\C0\03\0D\1F\C8\C0\03\1B\1B6\C0\03\1F7\C0\03\1D\1F\C8\C0\03\1B\1C8\C0\03\1A\CC\C0\03\1F9\C0\03\1D\1B\D0\E0\01\1F\CC\C0\03\1C\1B\D0\C0\03\1B\D4\C0\03\1B\D4\E0\01\1B\D8\E0\01\1B\D8\E0\01\1B\DC\E0\01\1A\DC\E0\01\1F:\E0\01\0C\1F;\C0\03-\16\E0\90\01\00P\00\1F<\C0\03\0C\1F=\C0\03\0D\1F\E0\C0\03\17\00P\00\17>\C0\00\00\B0\03\17?`\00g\81\99\09\02\04\E4\A0\01\0F\C0\03\0D\00\D0\01\040\00\00P\12Y\A9\0D\02\04\E8\E0\10\0F\C0\03\0B\00\E0\01\18\E8\E0\01K\0F\02\04\EC\E0\01\1B\EC\D0\01\18\F0\10\00K\11\02\04\F0\D0\01\18\F4\10\00%\13\02\10\00\1Bb\D0\01 \C8O\B0\01\15\09\B0\01 \C8\8F\80\01\15\0D\80\01\86\C8\0F\01#\B2\08\0F\0E\10\00f\02#\C2\08\11\10\10\00\00\E0\00\15\13\E0\00P\C4\0F\00Ay\09\00\06\90\14c\88s\00\07\08\00!\00f\E8\0F\00\1D{\00\01\00S\EC\0F\00\84\89\CD\19\13\08 \01Ax\00\06?\00\15\11\F2@\03c\84\89\03\07\00\10 \00s$\0E\00!\82\00\00\02\16\10\00\F0\15'\88\83@\00\0F`\00\01-\99\02`\00\14\1F`\15\00`\00W\99\03\07\00\08`\009\92\02\02`\00O\93\00\07\02\C0\00\0A\1A\03`\005r\00\06\D0\15\01\C0\00H\04\07\00\04\C0\00J\04\03\04\00\C0\00\1F\04`\00\089M\19\00P\016\84y\07p\00\93\22\0E\00$v\02\FF\00`\D0\15\93\C4\0F\00$v\03\FF\00a\10\00a\CA\0F\00\8Ey\00\01\01\9B\84\E7\10\0C\00\E2\1F\00M\A0\01PGy\00\00\F0\C0\16\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00`\0F\01\00-'\01\00 \02\07\01\00\22@\00\01\00=o\01\000\00\08\01\00\1F\0B@\00\04\13\AF)\00\1F\F8@\00\0C\13\13\\\1A\0C\01\00\13\A8U\00*\A8\00\80\1A\22\08\00\01\00\22\18\00\01\00.B\01T\00\00\01\00\13P5\02/p\00\80\00\0B/)\00'\00\02#\00\C0@\00\00,\09\17\00\E4\00*\04\00\01\00\1Fr@\00\04\13\F0)\00&\98\00@\00\1F\0A@\00\00!`\01D\01\0D@\001\88\05\00\01\00*\D8\00\01\00\1B\08\08\00?O\01\00\0E\1E\00Q\00\00`\06\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\FA\14\01\0C\84\01\13p@\00\17\881\01\0F\C0\00\01\132@\00\15\06R\00\0B\A1\00\16\18\80\00j\06\00\00\18\80\00\01\00\13\B5+\00+\03\00\01\00\00\D5\0F\1A\00q\00\0F\80\00\01\11\06\F0\1D\07\E8\22\0CH\02\1A\00\08\00\04\97\00\13\018\00\04\E8\00\0C\01\00*\90\19\08\00\088\00\18\06\A0\00\0F\01\00\05\03\A9\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00"
@main_kernel_1_main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_kernel_name = internal constant [56 x i8] c"main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64\00"
@main_kernel_1_blob_gpu.binary = internal constant [1112 x i8] c"P\EDU\BA\01\00\10\00H\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\08\04\00\00\00\00\00\00\06\04\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00h\0C\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\002\C0\0B\00\01\00\11\09\06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\10e__6_1_0___no_ibXblock_tile_h64G\00\0FA\00*osharedC\00*\9Fconstant0F\00'\FA\01debug_frame\00.rel\11\00!nv\14\00\11aR\00\0Fg\01 \0F\97\00$\0F\9F\01\F6o_param\A6\01\1C\0F\01\00\07\8Cj\00\00\00\03\00\0A\00\01\00 ,\01\18\00,\09\00\01\00\11y\18\00,\04\00\01\00\11\97\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\1C\00\00\00E\00#\04\10\18\00\80/\08\00\05\00\00\00\06\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04p\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C$\00u\01\00\08\00\00\F0!\10\00\01\01\00\C0\F0\11\00\03\1B\FF\00\04\1C\08\00p\A8\04\12\00\01\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00 \01/\05\00\01\00\FF\B8@$v\01\FF\17\04\A3\FF\00\8E\07\00\C4\0F\00\C3y\F4\03\A2%\00\00\00\22\0E\00\19y\00\01\00\F3\0E!\00\00\00b\0E\00\90r\05?\04\00\00\80?\E0\FF\0F\00\C6\1F\00\B9z\04\00\00X\E6\02p\E4\0F\00\A4r\04\05B\00\B1?\02\8E\0F\00\CC\0F\00\0C|\00F\04\95pR\F0\0B\00\DA/\00Mk\04\F3\0D\80\03\00\EA\0F\00\02z\02\00\00Z\00\00\00\0F\00\00\00\E2\0F\00$v\03\FF\00[\90\00\22\E2\0F`\001F\00\00\BB\04\80\C8\0F\00\86y\00\02\FF\B8\030\19\10\0C \00*MyP\00PGy\00\00\F0A\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\80\0F\01\00-\00\0F\05.\03\00\01\00\22@\00\01\00=g\01\000\00\08\01\00\1F\0B@\00\04\13\A7)\00\1F\A6@\00\0C\13\13\14\04\0C\01\00\13PU\00*\90\008\04\04U\04\22\18\00\01\00.:\01T\00\00\01\00\13\E0@\00/p\00\80\00\0B\1F)'\00\03A\00P\04\00\01\00\04H\06\04\E4\00*\04\00\01\00\1Fp@\00\04\13\80)\00&L\00@\00\1F\0A@\00\00!X\01D\01\0D@\00\13\D0)\00*\D8\00\01\00\1B\08\08\00%G\01\DB\0A\09\01\00\13\A8e\05\17\10\80\00\17\048\00\04\18\00\13\F4\14\01\0C\84\01*\B8\050\07\1F\00\C0\00\04\132@\00*\06\00\01\00*\80\07\C8\07\12\03\C0\01:\06\80\00\01\00\13\06\D8\01\05\A8\0B\0C\C8\00\1A\00\08\00\04\F8\00\13\018\00\04\A8\00\0C\01\009H\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00\00"
@ral_send_output___cpu___pvoid_i64_m0df32___void = internal constant [48 x i8] c"ral_send_output___cpu___pvoid_i64_m0df32___void\00"
@d2h___gpu___pvoid_pvoid_m0df32_m0df32___void = internal constant [45 x i8] c"d2h___gpu___pvoid_pvoid_m0df32_m0df32___void\00"
@alloc___cpu___pvoid_i64___pvoid = internal constant [32 x i8] c"alloc___cpu___pvoid_i64___pvoid\00"
@inc_ref___gpu___pvoid_pvoid_m1df32_m1di64___m0df32 = internal constant [51 x i8] c"inc_ref___gpu___pvoid_pvoid_m1df32_m1di64___m0df32\00"
@dealloc___gpu___pvoid_pvoid___void = internal constant [35 x i8] c"dealloc___gpu___pvoid_pvoid___void\00"
@main_kernel_0_main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_1_kernel_name = internal constant [59 x i8] c"main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_1\00"
@main_kernel_0_blob_gpu.binary = internal constant [2320 x i8] c"P\EDU\BA\01\00\10\00\00\09\00\00\00\00\00\00\02\00\01\01@\00\00\00\C0\08\00\00\00\00\00\00\BF\08\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00h\17\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\002\C0\16\00\01\00\11\14\06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\13e__6_1_0___no_ibXthread_tile_h32_1J\00\0FD\00-osharedF\00-\9Fconstant0I\00*\FA\01debug_frame\00.rel\11\00!nv\14\00\11aU\00\0Fs\01 \0F\9A\00'\0F\AE\01\FF\03o_param\B5\01\1C\0F\01\00\04\8Cm\00\00\00\03\00\0A\00\01\00 8\01\18\00,\09\00\01\00\11\88\18\00,\04\00\01\00\11\A6\18\00,\07\00\01\00g2\00\00\00\12\10`\00\11\0C\06\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0\0B\07\00 \00\04\9B\00 \04\18i\00\01:\002\04\B8\02\18\00p/\08\00\05\00\00\00\1A\00\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\88\04\80\015\00\00\04\0A\08\00F\00\A2`\01(\00\03\19(\00\04\17\C5\00u\05\00 \00\00\F0!\10\009\04\00\18\10\009\03\00\10\10\00u\02\00\08\00\00\F0\11\10\009\01\00\04\10\00\01\01\00\F3\01\F0\11\00\03\1B\FF\00\04\1C\08\00`\00\00\00P\D8\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\00\D2\05b,\00\00\00H\00\01\00\00`\01/\05\00\01\00\FF\E0@$v\01\FF\7F\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\BB\02Q\0E\00\19y\03\0F\00 \00!-\00\F0\14\0E\00$z\00\00\00X\00\00\03\02\8E\07\00\CA\1F\00\12x\FF\00\FF\01\00\00\FF\C0\80\07\00\C8\0F\00\0C \00\C5Y\00\00pdp\00\00\DA\0F\00M\C3\04\B0\80\03\00\EA\0F\00\19x\03\FF\1F\0F\00\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\00\03\05a\C6\0F\00\11r\03q\001\FFH\8FP\00\000\00\00C\00\10\030\00\93\CA\0F\00$x\07\03 \00\B0\00p\C8\0F\00%x\04\07s\04\10\FF\90\00\F0\09\E2\0F\04\0Cz\00\07\00Z\00\00pb\F4\03\00\E4\0F\04\10x\00\07\01 \00\B0\E0\FF\07\00\E4\0F\00\12x\04\04\01\031\FF\FC\8E\10\00\01\B0\00\010\00\10\F60\00p\00\10z\02\04\00\\0\00!\F1\07@\00Pz\04\04\00^\10\00\11\F3 \01\00P\00\00,\03\04P\00\C2\10z\03\05\00]\00\00\FF\E4\7F\000\00@\05\05\00_\10\00*\FF\00`\00 \F8\03\F0\00\F4\06\81\B9\0B\02\04\FC\FF\FF\00\19\1E\0C\00\A2\0E\00\10x\06\07\03P\00\93\C6\0F\00\81\A9\00\02\04\F8 \00u\E8\0E\00\81\A9\09\04\10\00 \E2\0EP\00\12\06P\00!\FA\030\005\B9\08\04P\00p\A8\0E\00\81\C9\0D\02\10\01\01\10\00y(\0F\00\81\C9\0A\04\10\00S\D9\0F\02\04\04\10\00\10h\10\00%\0C\04\10\00!b\0F\90\00\14\04\90\00\01\F0\006\0E\07\05\F0\00\16\04\80\00\12\FC0\01Jx\10\07\06 \00\12\0E \00\13\F0 \00:\0E\07\07 \00\12\10 \00\11\F2\10\01T$r\06\FF\FF\D0\01\12\E2P\00\14\0A0\00a\E2\0F\04#\A2\06\06\07\00$\00F\00\E2\8F\00`\00\12\F4P\01S\E9\09\02\04\08\C0\00!\E2\0Ep\00\18\08P\018\E9\00\04 \00@#\B2\06\0B\0F\00\11\06P\00\17OP\00\02\00\02P\81\89\0B\02\04\0F\06\05\A0\018\0E\07\09P\008\89\08\04 \00T#\C2\06\0D\0AP\00\93\C6\0F\01\81\99\0D\02\04\10 \00'\22\0F`\00\12\F8\B0\01W\99\0A\04\04\10\90\01X\A9\11\02\04\14\10\00F\0E\04\04\14@\00U#\D2\06\0F\0C\B0\00&\0F\02@\01 \FA\03\A0\01g\81\B9\0F\02\04\18\D0\019\B9\0C\04\10\00X\C9\13\02\04\1C\10\00)\10\04\10\00X\D9\15\02\04 \10\00H\12\04\04 \10\026\14\07\0B\90\01e\00#\E2\06\09\00\90\00\11\8F\10\03\17\0C0\02E\0Cz\00\14 \02\000\00\1D\82p\01\1A\00 \027\00\07\0D \02X\10x\08\07\0Ep\00\17\92`\016\E2\0F\01@\00C\F2\03\00\CA\00\025$\00\00\00\03\00\F0\01\17$\A0\01F\A2\06\11\0E@\00\00\80\00\17\08 \04\02\F0\01\18(@\028\08\07\0F\F0\01\00\D0\01\17,\90\01\1D\B2\90\01\19\08@\02\00\E0\01\070\00\\\10x\0C\07\10@\02\07p\00Z#\C2\06\13\10\A0\00\15\0C0\02\00P\00X\A9\0F\02\040\90\018\10\07\11P\008\A9\0C\04 \00Z#\D2\06\15\12P\00\060\02\00P\00W\B9\11\02\044\F0\01[\B9\0E\04\0440\02\1B80\02\1B80\02\1B<0\02\1A<0\02\1F\120\02\06\11O\F0\01\1F\130\02\16\1A\8F0\02\13\D6\F0\01\18@\A0\03?\00\07\140\02\08\00\D0\01\16\15`\02\19\00@\02\000\00\1F\A2\D0\01\05\11\F4 \00\01P\02\09p\009\08\07\16\B0\027\0A\07\17\80\00\01P\02\19Dp\04\0F\80\02\04\12\F6\C0\03\00\10\02\040\00\00\F0\05\00p\02\17Hp\02\0E \02\19\0A \02\00p\02\070\00\00\B0\04\19\18 \02O\0F\02\04L \02\0A\19\0E \02\00P\02\17L\E0\01\000\02\1BP0\02\1BP0\02\1BT0\02\1BT0\02\1BX0\02\1AX0\02\1F\190\02\05*\C6O \02\030\04\18\1A0\00\0B0\02\00\A0\01\14\1B \00\13\D20\02\16\\\C0\01\0FP\04\0A\03`\00\1A\1C\B0\06\1A\08\B0\048\08\07\1D\80\00\0E@\02\15\00@\02\13\C4@\02\18\\@\028\0A\07\1E\A0\01_\99\0B\02\04`0\02\14\01\F0\01(\08\040\00\00`\04\18\1FP\00_\89\07\02\04d@\02\18X\89\0A\04\04d\80\06O\0D\02\04h0\02\0A\15\0C0\02\13\C40\02\17h\E0\01\00\90\06\1Bl0\02\1Bl \02\18p\10\00K\11\02\04p \02\18t\10\00%\13\02\10\00\1Bb \02 \C8O\D0\01\06\00\02 \C8\8F\10\02%\07\0A\10\00v\0F\01#\A2\06\0D\0C\10\00\10\02\C0\05\08`\01c$v\08\FF\00`\80\08\10\E4\10\00C\09\FF\00a\10\00\11\E2@\01&\11\10@\00\00\10\01\15\13\10\01p\CA\0F\00\8Ey\00\08\0C\00@\84\E7\10\0C0\00*My\F0\0APGy\00\00\F09\0F\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00p\0F\01\00-\11\01h\0F\0E\01\00\22@\00\01\00=s\01\000\00\08\01\00\1F\0B@\00\04\13\B3)\00\1F\B5@\00\0C\13\13\BC\0E\0C\01\00\13hU\00*\90\00\E0\0E\22\08\00\01\00\22\18\00\01\00.F\01T\00\00\01\00\13\F8@\00/p\00\80\00\0B\1F)'\00\03A\00h\04\00\01\00\00\9B\07\17\00\E4\00*\04\00\01\00\1Fs@\00\04\13\98)\00&\8C\00@\00\1F\0A@\00\00!d\01D\01\0D@\001(\05\00\01\00*\D8\00\01\00\1B\08\08\00?S\01\00f\12\00\04\E1\02\10\00B\11\05\80\00\17\048\00\04\18\00\13\FD\14\01\0C\84\01&\10\06\B0\12\04\01\00\0F\C0\00\01\132@\00+\06\00\01\00\1A\08\B0\12\12\03\C0\01>\18\80\00\A7\00\18\05\A8\16\0B\01\00*\A8\00\08\00\04W\00\13\018\00\04\A8\00\0D\D0\12)\0D\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00"
@ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void = internal constant [101 x i8] c"ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void\00"
@main_kernel_main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_kernel_name = internal constant [57 x i8] c"main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32\00"
@main_kernel_blob_gpu.binary = internal constant [1112 x i8] c"P\EDU\BA\01\00\10\00H\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\08\04\00\00\00\00\00\00\08\04\00\00\00\00\00\00\07\00\01\00V\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00h\0C\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00r\00\01\002\C0\0B\00\01\00\11\09\06\00\F5\0E\00V\05V\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\11e__6_1_0___no_ibXthread_tile_h32H\00\0FB\00+osharedD\00+\9Fconstant0G\00(\FA\01debug_frame\00.rel\11\00!nv\14\00\11aS\00\0Fk\01 \0F\98\00%\0F\A4\01\FAo_param\AB\01\1C\0F\01\00\06\8Ck\00\00\00\03\00\0A\00\01\00 0\01\18\00,\09\00\01\00\11~\18\00,\04\00\01\00\11\9C\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\1C\00\00\00E\00#\04\10\18\00\80/\08\00\05\00\00\00\06\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04x\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C$\00u\01\00\08\00\00\F0!\10\00\01\01\00\C0\F0\11\00\03\1B\FF\00\04\1C\08\00p\B0\04\12\00\01\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00 \01/\05\00\01\00\FF\B0@$v\01\FF\0F\04\A3\FF\00\8E\07\00\C4\0F\00\C3y\EC\03\A2%\00\00\00\22\0E\00\19y\00\01\00\F3\0E!\00\00\00b\0E\00\90r\05?\04\00\00\80?\E0\FF\0F\00\C6\1F\00\B9z\04\00\00X\DE\02p\E4\0F\00\A4r\04\05B\00\B1?\02\8E\0F\00\CC\0F\00\0C|\00>\04\95pR\F0\0B\00\DA/\00Mc\04\F3\0D\80\03\00\EA\0F\00\02z\02\00\00Z\00\00\00\0F\00\00\00\E2\0F\00$v\03\FF\00[\90\00\22\E2\0F`\001F\00\00\B3\04\80\C8\0F\00\86y\00\02\FF\B0\030\19\10\0C \00*MyP\00PGy\00\00\F09\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\80\0F\01\00-\00\07\05.\03\00\01\00\22@\00\01\00=k\01\000\00\08\01\00\1F\0B@\00\04*\AB\01\08\00\0F@\00\05\13\13\0C\04\0C\01\00\13XU\00*\90\000\04\04M\04\22\18\00\01\00.>\01T\00\00\01\00\13\E8@\00/p\00\80\00\0B\1F)'\00\03A\00X\04\00\01\00\04@\06\04\E4\00*\04\00\01\00\1Fq@\00\04\13\88)\00&L\00@\00\1F\0A@\00\00!\\\01D\01\0D@\00\13\D8)\00*\D8\00\01\00\1B\08\08\00%K\01\DB\0A\09\01\00\13\B0]\05\17\10\80\00\17\048\00\04\18\00\13\F7\14\01\0C\84\01*\C0\05(\07\1F\00\C0\00\04\132@\00*\06\00\01\00*\80\07\C0\07\12\03\C0\01:\06\80\00\01\00\13\06\D8\01\05\A8\0B\0B\01\00*\A8\00\08\00\04\F8\00\14\018\00/\05\00\01\00\029@\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00"
@sync_on_stream___gpu___pvoid_pvoid___void = internal constant [42 x i8] c"sync_on_stream___gpu___pvoid_pvoid___void\00"
@h2d___gpu___pvoid_pvoid_m2df32_m2df32___void = internal constant [45 x i8] c"h2d___gpu___pvoid_pvoid_m2df32_m2df32___void\00"
@alloc___gpu___pvoid_i64___pvoid = internal constant [32 x i8] c"alloc___gpu___pvoid_i64___pvoid\00"
@ral_recv_input___cpu___pvoid_i64___m2df32 = internal constant [42 x i8] c"ral_recv_input___cpu___pvoid_i64___m2df32\00"

define void @disc_ral_call(ptr nocapture readonly %0, ptr %1, ptr %2) local_unnamed_addr {
entry:
  %3 = load ptr, ptr %0, align 8
  %4 = getelementptr ptr, ptr %0, i64 1
  %5 = load ptr, ptr %4, align 8
  %6 = load ptr, ptr %2, align 8
  store ptr %3, ptr %6, align 8
  tail call void %5(ptr %3, ptr %1, ptr nonnull %2)
  ret void
}

define void @main(ptr nocapture readonly %0) local_unnamed_addr {
  %2 = alloca %0, align 8
  %3 = alloca [3 x ptr], align 8
  store ptr %2, ptr %3, align 8
  %4 = getelementptr inbounds %0, ptr %2, i64 0, i32 1
  store i64 0, ptr %4, align 8
  %5 = getelementptr inbounds ptr, ptr %3, i64 1
  store ptr %4, ptr %5, align 8
  %6 = getelementptr inbounds %0, ptr %2, i64 0, i32 2
  %7 = getelementptr inbounds ptr, ptr %3, i64 2
  store ptr %6, ptr %7, align 8
  %8 = load ptr, ptr %0, align 8
  %9 = getelementptr ptr, ptr %0, i64 1
  %10 = load ptr, ptr %9, align 8
  store ptr %8, ptr %2, align 8
  call void %10(ptr %8, ptr nonnull @ral_recv_input___cpu___pvoid_i64___m2df32, ptr nonnull %3)
  %.fca.0.load2 = load ptr, ptr %6, align 8
  %.fca.1.gep4 = getelementptr inbounds %0, ptr %2, i64 0, i32 2, i32 1
  %.fca.1.load5 = load ptr, ptr %.fca.1.gep4, align 8
  %.fca.3.0.gep10 = getelementptr inbounds %0, ptr %2, i64 0, i32 2, i32 3
  %.fca.3.0.load11 = load i64, ptr %.fca.3.0.gep10, align 8
  %11 = alloca %.1, align 8
  %12 = alloca [3 x ptr], align 8
  store ptr %11, ptr %12, align 8
  %13 = getelementptr inbounds %.1, ptr %11, i64 0, i32 1
  store i64 1, ptr %13, align 8
  %14 = getelementptr inbounds ptr, ptr %12, i64 1
  store ptr %13, ptr %14, align 8
  %15 = getelementptr inbounds %.1, ptr %11, i64 0, i32 2
  %16 = getelementptr inbounds ptr, ptr %12, i64 2
  store ptr %15, ptr %16, align 8
  %17 = load ptr, ptr %0, align 8
  %18 = load ptr, ptr %9, align 8
  store ptr %17, ptr %11, align 8
  call void %18(ptr %17, ptr nonnull @ral_recv_input___cpu___pvoid_i64___m2df32, ptr nonnull %12)
  %.fca.0.load = load ptr, ptr %15, align 8
  %.fca.1.gep = getelementptr inbounds %.1, ptr %11, i64 0, i32 2, i32 1
  %.fca.1.load = load ptr, ptr %.fca.1.gep, align 8
  %.fca.3.0.gep = getelementptr inbounds %.1, ptr %11, i64 0, i32 2, i32 3
  %.fca.3.0.load = load i64, ptr %.fca.3.0.gep, align 8
  %19 = icmp eq i64 %.fca.3.0.load, 1
  %20 = select i1 %19, i64 %.fca.3.0.load11, i64 %.fca.3.0.load
  %.idx = shl i64 %.fca.3.0.load11, 4
  %21 = alloca %.2, align 8
  %22 = alloca [3 x ptr], align 8
  store ptr %21, ptr %22, align 8
  %23 = getelementptr inbounds %.2, ptr %21, i64 0, i32 1
  store i64 %.idx, ptr %23, align 8
  %24 = getelementptr inbounds ptr, ptr %22, i64 1
  store ptr %23, ptr %24, align 8
  %25 = getelementptr inbounds %.2, ptr %21, i64 0, i32 2
  %26 = getelementptr inbounds ptr, ptr %22, i64 2
  store ptr %25, ptr %26, align 8
  %27 = load ptr, ptr %0, align 8
  %28 = load ptr, ptr %9, align 8
  store ptr %27, ptr %21, align 8
  call void %28(ptr %27, ptr nonnull @alloc___gpu___pvoid_i64___pvoid, ptr nonnull %22)
  %29 = load ptr, ptr %25, align 8
  %30 = alloca %.3, align 8
  %31 = alloca [16 x ptr], align 8
  store ptr %30, ptr %31, align 8
  %32 = getelementptr inbounds %.3, ptr %30, i64 0, i32 1
  store ptr null, ptr %32, align 8
  %33 = getelementptr inbounds ptr, ptr %31, i64 1
  store ptr %32, ptr %33, align 8
  %34 = getelementptr inbounds %.3, ptr %30, i64 0, i32 2
  store ptr %.fca.0.load2, ptr %34, align 8
  %35 = getelementptr inbounds ptr, ptr %31, i64 2
  store ptr %34, ptr %35, align 8
  %36 = getelementptr inbounds %.3, ptr %30, i64 0, i32 3
  store ptr %.fca.1.load5, ptr %36, align 8
  %37 = getelementptr inbounds ptr, ptr %31, i64 3
  store ptr %36, ptr %37, align 8
  %38 = getelementptr inbounds %.3, ptr %30, i64 0, i32 4
  store i64 0, ptr %38, align 8
  %39 = getelementptr inbounds ptr, ptr %31, i64 4
  store ptr %38, ptr %39, align 8
  %40 = getelementptr inbounds %.3, ptr %30, i64 0, i32 5
  store i64 %.fca.3.0.load11, ptr %40, align 8
  %41 = getelementptr inbounds ptr, ptr %31, i64 5
  store ptr %40, ptr %41, align 8
  %42 = getelementptr inbounds %.3, ptr %30, i64 0, i32 6
  store i64 4, ptr %42, align 8
  %43 = getelementptr inbounds ptr, ptr %31, i64 6
  store ptr %42, ptr %43, align 8
  %44 = getelementptr inbounds %.3, ptr %30, i64 0, i32 7
  store i64 4, ptr %44, align 8
  %45 = getelementptr inbounds ptr, ptr %31, i64 7
  store ptr %44, ptr %45, align 8
  %46 = getelementptr inbounds %.3, ptr %30, i64 0, i32 8
  store i64 1, ptr %46, align 8
  %47 = getelementptr inbounds ptr, ptr %31, i64 8
  store ptr %46, ptr %47, align 8
  %48 = getelementptr inbounds %.3, ptr %30, i64 0, i32 9
  store ptr %29, ptr %48, align 8
  %49 = getelementptr inbounds ptr, ptr %31, i64 9
  store ptr %48, ptr %49, align 8
  %50 = getelementptr inbounds %.3, ptr %30, i64 0, i32 10
  store ptr %29, ptr %50, align 8
  %51 = getelementptr inbounds ptr, ptr %31, i64 10
  store ptr %50, ptr %51, align 8
  %52 = getelementptr inbounds %.3, ptr %30, i64 0, i32 11
  store i64 0, ptr %52, align 8
  %53 = getelementptr inbounds ptr, ptr %31, i64 11
  store ptr %52, ptr %53, align 8
  %54 = getelementptr inbounds %.3, ptr %30, i64 0, i32 12
  store i64 %.fca.3.0.load11, ptr %54, align 8
  %55 = getelementptr inbounds ptr, ptr %31, i64 12
  store ptr %54, ptr %55, align 8
  %56 = getelementptr inbounds %.3, ptr %30, i64 0, i32 13
  store i64 4, ptr %56, align 8
  %57 = getelementptr inbounds ptr, ptr %31, i64 13
  store ptr %56, ptr %57, align 8
  %58 = getelementptr inbounds %.3, ptr %30, i64 0, i32 14
  store i64 4, ptr %58, align 8
  %59 = getelementptr inbounds ptr, ptr %31, i64 14
  store ptr %58, ptr %59, align 8
  %60 = getelementptr inbounds %.3, ptr %30, i64 0, i32 15
  store i64 1, ptr %60, align 8
  %61 = getelementptr inbounds ptr, ptr %31, i64 15
  store ptr %60, ptr %61, align 8
  %62 = load ptr, ptr %0, align 8
  %63 = load ptr, ptr %9, align 8
  store ptr %62, ptr %30, align 8
  call void %63(ptr %62, ptr nonnull @h2d___gpu___pvoid_pvoid_m2df32_m2df32___void, ptr nonnull %31)
  %64 = alloca %.4, align 8
  %65 = alloca [2 x ptr], align 8
  store ptr %64, ptr %65, align 8
  %66 = getelementptr inbounds %.4, ptr %64, i64 0, i32 1
  store ptr null, ptr %66, align 8
  %67 = getelementptr inbounds ptr, ptr %65, i64 1
  store ptr %66, ptr %67, align 8
  %68 = load ptr, ptr %0, align 8
  %69 = load ptr, ptr %9, align 8
  store ptr %68, ptr %64, align 8
  call void %69(ptr %68, ptr nonnull @sync_on_stream___gpu___pvoid_pvoid___void, ptr nonnull %65)
  %.idx22 = shl i64 %.fca.3.0.load, 4
  %70 = alloca %.5, align 8
  %71 = alloca [3 x ptr], align 8
  store ptr %70, ptr %71, align 8
  %72 = getelementptr inbounds %.5, ptr %70, i64 0, i32 1
  store i64 %.idx22, ptr %72, align 8
  %73 = getelementptr inbounds ptr, ptr %71, i64 1
  store ptr %72, ptr %73, align 8
  %74 = getelementptr inbounds %.5, ptr %70, i64 0, i32 2
  %75 = getelementptr inbounds ptr, ptr %71, i64 2
  store ptr %74, ptr %75, align 8
  %76 = load ptr, ptr %0, align 8
  %77 = load ptr, ptr %9, align 8
  store ptr %76, ptr %70, align 8
  call void %77(ptr %76, ptr nonnull @alloc___gpu___pvoid_i64___pvoid, ptr nonnull %71)
  %78 = load ptr, ptr %74, align 8
  %79 = alloca %.6, align 8
  %80 = alloca [16 x ptr], align 8
  store ptr %79, ptr %80, align 8
  %81 = getelementptr inbounds %.6, ptr %79, i64 0, i32 1
  store ptr null, ptr %81, align 8
  %82 = getelementptr inbounds ptr, ptr %80, i64 1
  store ptr %81, ptr %82, align 8
  %83 = getelementptr inbounds %.6, ptr %79, i64 0, i32 2
  store ptr %.fca.0.load, ptr %83, align 8
  %84 = getelementptr inbounds ptr, ptr %80, i64 2
  store ptr %83, ptr %84, align 8
  %85 = getelementptr inbounds %.6, ptr %79, i64 0, i32 3
  store ptr %.fca.1.load, ptr %85, align 8
  %86 = getelementptr inbounds ptr, ptr %80, i64 3
  store ptr %85, ptr %86, align 8
  %87 = getelementptr inbounds %.6, ptr %79, i64 0, i32 4
  store i64 0, ptr %87, align 8
  %88 = getelementptr inbounds ptr, ptr %80, i64 4
  store ptr %87, ptr %88, align 8
  %89 = getelementptr inbounds %.6, ptr %79, i64 0, i32 5
  store i64 %.fca.3.0.load, ptr %89, align 8
  %90 = getelementptr inbounds ptr, ptr %80, i64 5
  store ptr %89, ptr %90, align 8
  %91 = getelementptr inbounds %.6, ptr %79, i64 0, i32 6
  store i64 4, ptr %91, align 8
  %92 = getelementptr inbounds ptr, ptr %80, i64 6
  store ptr %91, ptr %92, align 8
  %93 = getelementptr inbounds %.6, ptr %79, i64 0, i32 7
  store i64 4, ptr %93, align 8
  %94 = getelementptr inbounds ptr, ptr %80, i64 7
  store ptr %93, ptr %94, align 8
  %95 = getelementptr inbounds %.6, ptr %79, i64 0, i32 8
  store i64 1, ptr %95, align 8
  %96 = getelementptr inbounds ptr, ptr %80, i64 8
  store ptr %95, ptr %96, align 8
  %97 = getelementptr inbounds %.6, ptr %79, i64 0, i32 9
  store ptr %78, ptr %97, align 8
  %98 = getelementptr inbounds ptr, ptr %80, i64 9
  store ptr %97, ptr %98, align 8
  %99 = getelementptr inbounds %.6, ptr %79, i64 0, i32 10
  store ptr %78, ptr %99, align 8
  %100 = getelementptr inbounds ptr, ptr %80, i64 10
  store ptr %99, ptr %100, align 8
  %101 = getelementptr inbounds %.6, ptr %79, i64 0, i32 11
  store i64 0, ptr %101, align 8
  %102 = getelementptr inbounds ptr, ptr %80, i64 11
  store ptr %101, ptr %102, align 8
  %103 = getelementptr inbounds %.6, ptr %79, i64 0, i32 12
  store i64 %.fca.3.0.load, ptr %103, align 8
  %104 = getelementptr inbounds ptr, ptr %80, i64 12
  store ptr %103, ptr %104, align 8
  %105 = getelementptr inbounds %.6, ptr %79, i64 0, i32 13
  store i64 4, ptr %105, align 8
  %106 = getelementptr inbounds ptr, ptr %80, i64 13
  store ptr %105, ptr %106, align 8
  %107 = getelementptr inbounds %.6, ptr %79, i64 0, i32 14
  store i64 4, ptr %107, align 8
  %108 = getelementptr inbounds ptr, ptr %80, i64 14
  store ptr %107, ptr %108, align 8
  %109 = getelementptr inbounds %.6, ptr %79, i64 0, i32 15
  store i64 1, ptr %109, align 8
  %110 = getelementptr inbounds ptr, ptr %80, i64 15
  store ptr %109, ptr %110, align 8
  %111 = load ptr, ptr %0, align 8
  %112 = load ptr, ptr %9, align 8
  store ptr %111, ptr %79, align 8
  call void %112(ptr %111, ptr nonnull @h2d___gpu___pvoid_pvoid_m2df32_m2df32___void, ptr nonnull %80)
  %113 = alloca %.7, align 8
  %114 = alloca [2 x ptr], align 8
  store ptr %113, ptr %114, align 8
  %115 = getelementptr inbounds %.7, ptr %113, i64 0, i32 1
  store ptr null, ptr %115, align 8
  %116 = getelementptr inbounds ptr, ptr %114, i64 1
  store ptr %115, ptr %116, align 8
  %117 = load ptr, ptr %0, align 8
  %118 = load ptr, ptr %9, align 8
  store ptr %117, ptr %113, align 8
  call void %118(ptr %117, ptr nonnull @sync_on_stream___gpu___pvoid_pvoid___void, ptr nonnull %114)
  %119 = trunc i64 %20 to i32
  %120 = shl i32 %119, 2
  %121 = sext i32 %120 to i64
  %122 = icmp eq i64 %.fca.3.0.load11, %20
  %123 = icmp eq i64 %.fca.3.0.load, %20
  %124 = and i1 %123, %122
  %125 = alloca %.8, align 8
  %126 = alloca [3 x ptr], align 8
  store ptr %125, ptr %126, align 8
  %127 = getelementptr inbounds %.8, ptr %125, i64 0, i32 1
  store i64 4, ptr %127, align 8
  %128 = getelementptr inbounds ptr, ptr %126, i64 1
  store ptr %127, ptr %128, align 8
  %129 = getelementptr inbounds %.8, ptr %125, i64 0, i32 2
  %130 = getelementptr inbounds ptr, ptr %126, i64 2
  store ptr %129, ptr %130, align 8
  %131 = load ptr, ptr %0, align 8
  %132 = load ptr, ptr %9, align 8
  store ptr %131, ptr %125, align 8
  call void %132(ptr %131, ptr nonnull @alloc___gpu___pvoid_i64___pvoid, ptr nonnull %126)
  %133 = load ptr, ptr %129, align 8
  %134 = icmp slt i32 %120, 1
  %135 = alloca ptr, align 8
  br i1 %124, label %136, label %313

136:                                              ; preds = %1
  br i1 %134, label %137, label %229

137:                                              ; preds = %136
  store ptr @main_kernel_blob_gpu.binary, ptr %135, align 8
  %138 = alloca %.9, align 8
  %139 = alloca [2 x ptr], align 8
  store i64 512, ptr %138, align 8
  store ptr %138, ptr %139, align 8
  %140 = getelementptr inbounds %.9, ptr %138, i64 0, i32 1
  store ptr %133, ptr %140, align 8
  %141 = getelementptr inbounds ptr, ptr %139, i64 1
  store ptr %140, ptr %141, align 8
  %142 = alloca %.10, align 8
  %143 = alloca [14 x ptr], align 8
  store ptr %142, ptr %143, align 8
  %144 = getelementptr inbounds %.10, ptr %142, i64 0, i32 1
  store ptr %135, ptr %144, align 8
  %145 = getelementptr inbounds ptr, ptr %143, i64 1
  store ptr %144, ptr %145, align 8
  %146 = getelementptr inbounds %.10, ptr %142, i64 0, i32 2
  store i64 1, ptr %146, align 8
  %147 = getelementptr inbounds ptr, ptr %143, i64 2
  store ptr %146, ptr %147, align 8
  %148 = getelementptr inbounds %.10, ptr %142, i64 0, i32 3
  store ptr @main_kernel_main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_kernel_name, ptr %148, align 8
  %149 = getelementptr inbounds ptr, ptr %143, i64 3
  store ptr %148, ptr %149, align 8
  %150 = getelementptr inbounds %.10, ptr %142, i64 0, i32 4
  store i64 1, ptr %150, align 8
  %151 = getelementptr inbounds ptr, ptr %143, i64 4
  store ptr %150, ptr %151, align 8
  %152 = getelementptr inbounds %.10, ptr %142, i64 0, i32 5
  store i64 1, ptr %152, align 8
  %153 = getelementptr inbounds ptr, ptr %143, i64 5
  store ptr %152, ptr %153, align 8
  %154 = getelementptr inbounds %.10, ptr %142, i64 0, i32 6
  store i64 1, ptr %154, align 8
  %155 = getelementptr inbounds ptr, ptr %143, i64 6
  store ptr %154, ptr %155, align 8
  %156 = getelementptr inbounds %.10, ptr %142, i64 0, i32 7
  store i64 512, ptr %156, align 8
  %157 = getelementptr inbounds ptr, ptr %143, i64 7
  store ptr %156, ptr %157, align 8
  %158 = getelementptr inbounds %.10, ptr %142, i64 0, i32 8
  store i64 1, ptr %158, align 8
  %159 = getelementptr inbounds ptr, ptr %143, i64 8
  store ptr %158, ptr %159, align 8
  %160 = getelementptr inbounds %.10, ptr %142, i64 0, i32 9
  store i64 1, ptr %160, align 8
  %161 = getelementptr inbounds ptr, ptr %143, i64 9
  store ptr %160, ptr %161, align 8
  %162 = getelementptr inbounds %.10, ptr %142, i64 0, i32 10
  store i32 0, ptr %162, align 8
  %163 = getelementptr inbounds ptr, ptr %143, i64 10
  store ptr %162, ptr %163, align 8
  %164 = getelementptr inbounds %.10, ptr %142, i64 0, i32 11
  store ptr null, ptr %164, align 8
  %165 = getelementptr inbounds ptr, ptr %143, i64 11
  store ptr %164, ptr %165, align 8
  %166 = getelementptr inbounds %.10, ptr %142, i64 0, i32 12
  store i32 2, ptr %166, align 8
  %167 = getelementptr inbounds ptr, ptr %143, i64 12
  store ptr %166, ptr %167, align 8
  %168 = getelementptr inbounds %.10, ptr %142, i64 0, i32 13
  store ptr %139, ptr %168, align 8
  %169 = getelementptr inbounds ptr, ptr %143, i64 13
  store ptr %168, ptr %169, align 8
  %170 = load ptr, ptr %0, align 8
  %171 = load ptr, ptr %9, align 8
  store ptr %170, ptr %142, align 8
  call void %171(ptr %170, ptr nonnull @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr nonnull %143)
  %172 = icmp eq i32 %120, 0
  %173 = shl nsw i64 %121, 4
  %174 = add nsw i64 %173, -16
  %175 = and i64 %174, -512
  %176 = add nsw i64 %175, 512
  %177 = select i1 %172, i64 0, i64 %176
  %178 = icmp slt i64 %177, 1
  %179 = sub nsw i64 0, %177
  %180 = add nsw i64 %177, -1
  %181 = select i1 %178, i64 %179, i64 %180
  %182 = sdiv i64 %181, 512
  %183 = sub nsw i64 0, %182
  %184 = add nsw i64 %182, 1
  %185 = select i1 %178, i64 %183, i64 %184
  %186 = alloca ptr, align 8
  store ptr @main_kernel_0_blob_gpu.binary, ptr %186, align 8
  %187 = alloca %.11, align 8
  %188 = alloca [6 x ptr], align 8
  store i64 512, ptr %187, align 8
  store ptr %187, ptr %188, align 8
  %189 = getelementptr inbounds %.11, ptr %187, i64 0, i32 1
  store i64 %177, ptr %189, align 8
  %190 = getelementptr inbounds ptr, ptr %188, i64 1
  store ptr %189, ptr %190, align 8
  %191 = getelementptr inbounds %.11, ptr %187, i64 0, i32 2
  store i64 %121, ptr %191, align 8
  %192 = getelementptr inbounds ptr, ptr %188, i64 2
  store ptr %191, ptr %192, align 8
  %193 = getelementptr inbounds %.11, ptr %187, i64 0, i32 3
  store ptr %29, ptr %193, align 8
  %194 = getelementptr inbounds ptr, ptr %188, i64 3
  store ptr %193, ptr %194, align 8
  %195 = getelementptr inbounds %.11, ptr %187, i64 0, i32 4
  store ptr %78, ptr %195, align 8
  %196 = getelementptr inbounds ptr, ptr %188, i64 4
  store ptr %195, ptr %196, align 8
  %197 = getelementptr inbounds %.11, ptr %187, i64 0, i32 5
  store ptr %133, ptr %197, align 8
  %198 = getelementptr inbounds ptr, ptr %188, i64 5
  store ptr %197, ptr %198, align 8
  %199 = alloca %.12, align 8
  %200 = alloca [14 x ptr], align 8
  store ptr %199, ptr %200, align 8
  %201 = getelementptr inbounds %.12, ptr %199, i64 0, i32 1
  store ptr %186, ptr %201, align 8
  %202 = getelementptr inbounds ptr, ptr %200, i64 1
  store ptr %201, ptr %202, align 8
  %203 = getelementptr inbounds %.12, ptr %199, i64 0, i32 2
  store i64 1, ptr %203, align 8
  %204 = getelementptr inbounds ptr, ptr %200, i64 2
  store ptr %203, ptr %204, align 8
  %205 = getelementptr inbounds %.12, ptr %199, i64 0, i32 3
  store ptr @main_kernel_0_main_kColReduction_reduce__6_1_0___no_ibXthread_tile_h32_1_kernel_name, ptr %205, align 8
  %206 = getelementptr inbounds ptr, ptr %200, i64 3
  store ptr %205, ptr %206, align 8
  %207 = getelementptr inbounds %.12, ptr %199, i64 0, i32 4
  store i64 %185, ptr %207, align 8
  %208 = getelementptr inbounds ptr, ptr %200, i64 4
  store ptr %207, ptr %208, align 8
  %209 = getelementptr inbounds %.12, ptr %199, i64 0, i32 5
  store i64 1, ptr %209, align 8
  %210 = getelementptr inbounds ptr, ptr %200, i64 5
  store ptr %209, ptr %210, align 8
  %211 = getelementptr inbounds %.12, ptr %199, i64 0, i32 6
  store i64 1, ptr %211, align 8
  %212 = getelementptr inbounds ptr, ptr %200, i64 6
  store ptr %211, ptr %212, align 8
  %213 = getelementptr inbounds %.12, ptr %199, i64 0, i32 7
  store i64 512, ptr %213, align 8
  %214 = getelementptr inbounds ptr, ptr %200, i64 7
  store ptr %213, ptr %214, align 8
  %215 = getelementptr inbounds %.12, ptr %199, i64 0, i32 8
  store i64 1, ptr %215, align 8
  %216 = getelementptr inbounds ptr, ptr %200, i64 8
  store ptr %215, ptr %216, align 8
  %217 = getelementptr inbounds %.12, ptr %199, i64 0, i32 9
  store i64 1, ptr %217, align 8
  %218 = getelementptr inbounds ptr, ptr %200, i64 9
  store ptr %217, ptr %218, align 8
  %219 = getelementptr inbounds %.12, ptr %199, i64 0, i32 10
  store i32 0, ptr %219, align 8
  %220 = getelementptr inbounds ptr, ptr %200, i64 10
  store ptr %219, ptr %220, align 8
  %221 = getelementptr inbounds %.12, ptr %199, i64 0, i32 11
  store ptr null, ptr %221, align 8
  %222 = getelementptr inbounds ptr, ptr %200, i64 11
  store ptr %221, ptr %222, align 8
  %223 = getelementptr inbounds %.12, ptr %199, i64 0, i32 12
  store i32 6, ptr %223, align 8
  %224 = getelementptr inbounds ptr, ptr %200, i64 12
  store ptr %223, ptr %224, align 8
  %225 = getelementptr inbounds %.12, ptr %199, i64 0, i32 13
  store ptr %188, ptr %225, align 8
  %226 = getelementptr inbounds ptr, ptr %200, i64 13
  store ptr %225, ptr %226, align 8
  %227 = load ptr, ptr %0, align 8
  %228 = load ptr, ptr %9, align 8
  store ptr %227, ptr %199, align 8
  call void %228(ptr %227, ptr nonnull @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr nonnull %200)
  br label %498

229:                                              ; preds = %136
  store ptr @main_kernel_1_blob_gpu.binary, ptr %135, align 8
  %230 = alloca %.22, align 8
  %231 = alloca [2 x ptr], align 8
  store i64 256, ptr %230, align 8
  store ptr %230, ptr %231, align 8
  %232 = getelementptr inbounds %.22, ptr %230, i64 0, i32 1
  store ptr %133, ptr %232, align 8
  %233 = getelementptr inbounds ptr, ptr %231, i64 1
  store ptr %232, ptr %233, align 8
  %234 = alloca %.23, align 8
  %235 = alloca [14 x ptr], align 8
  store ptr %234, ptr %235, align 8
  %236 = getelementptr inbounds %.23, ptr %234, i64 0, i32 1
  store ptr %135, ptr %236, align 8
  %237 = getelementptr inbounds ptr, ptr %235, i64 1
  store ptr %236, ptr %237, align 8
  %238 = getelementptr inbounds %.23, ptr %234, i64 0, i32 2
  store i64 1, ptr %238, align 8
  %239 = getelementptr inbounds ptr, ptr %235, i64 2
  store ptr %238, ptr %239, align 8
  %240 = getelementptr inbounds %.23, ptr %234, i64 0, i32 3
  store ptr @main_kernel_1_main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_kernel_name, ptr %240, align 8
  %241 = getelementptr inbounds ptr, ptr %235, i64 3
  store ptr %240, ptr %241, align 8
  %242 = getelementptr inbounds %.23, ptr %234, i64 0, i32 4
  store i64 1, ptr %242, align 8
  %243 = getelementptr inbounds ptr, ptr %235, i64 4
  store ptr %242, ptr %243, align 8
  %244 = getelementptr inbounds %.23, ptr %234, i64 0, i32 5
  store i64 1, ptr %244, align 8
  %245 = getelementptr inbounds ptr, ptr %235, i64 5
  store ptr %244, ptr %245, align 8
  %246 = getelementptr inbounds %.23, ptr %234, i64 0, i32 6
  store i64 1, ptr %246, align 8
  %247 = getelementptr inbounds ptr, ptr %235, i64 6
  store ptr %246, ptr %247, align 8
  %248 = getelementptr inbounds %.23, ptr %234, i64 0, i32 7
  store i64 256, ptr %248, align 8
  %249 = getelementptr inbounds ptr, ptr %235, i64 7
  store ptr %248, ptr %249, align 8
  %250 = getelementptr inbounds %.23, ptr %234, i64 0, i32 8
  store i64 1, ptr %250, align 8
  %251 = getelementptr inbounds ptr, ptr %235, i64 8
  store ptr %250, ptr %251, align 8
  %252 = getelementptr inbounds %.23, ptr %234, i64 0, i32 9
  store i64 1, ptr %252, align 8
  %253 = getelementptr inbounds ptr, ptr %235, i64 9
  store ptr %252, ptr %253, align 8
  %254 = getelementptr inbounds %.23, ptr %234, i64 0, i32 10
  store i32 0, ptr %254, align 8
  %255 = getelementptr inbounds ptr, ptr %235, i64 10
  store ptr %254, ptr %255, align 8
  %256 = getelementptr inbounds %.23, ptr %234, i64 0, i32 11
  store ptr null, ptr %256, align 8
  %257 = getelementptr inbounds ptr, ptr %235, i64 11
  store ptr %256, ptr %257, align 8
  %258 = getelementptr inbounds %.23, ptr %234, i64 0, i32 12
  store i32 2, ptr %258, align 8
  %259 = getelementptr inbounds ptr, ptr %235, i64 12
  store ptr %258, ptr %259, align 8
  %260 = getelementptr inbounds %.23, ptr %234, i64 0, i32 13
  store ptr %231, ptr %260, align 8
  %261 = getelementptr inbounds ptr, ptr %235, i64 13
  store ptr %260, ptr %261, align 8
  %262 = load ptr, ptr %0, align 8
  %263 = load ptr, ptr %9, align 8
  store ptr %262, ptr %234, align 8
  call void %263(ptr %262, ptr nonnull @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr nonnull %235)
  %264 = add nsw i64 %121, -1
  %265 = lshr i64 %264, 1
  %266 = and i64 %265, 9223372036854775552
  %267 = add nuw i64 %266, 256
  %268 = lshr i64 %264, 9
  %269 = add nuw nsw i64 %268, 1
  %270 = alloca ptr, align 8
  store ptr @main_kernel_2_blob_gpu.binary, ptr %270, align 8
  %271 = alloca %.24, align 8
  %272 = alloca [6 x ptr], align 8
  store i64 256, ptr %271, align 8
  store ptr %271, ptr %272, align 8
  %273 = getelementptr inbounds %.24, ptr %271, i64 0, i32 1
  store i64 %267, ptr %273, align 8
  %274 = getelementptr inbounds ptr, ptr %272, i64 1
  store ptr %273, ptr %274, align 8
  %275 = getelementptr inbounds %.24, ptr %271, i64 0, i32 2
  store i64 %121, ptr %275, align 8
  %276 = getelementptr inbounds ptr, ptr %272, i64 2
  store ptr %275, ptr %276, align 8
  %277 = getelementptr inbounds %.24, ptr %271, i64 0, i32 3
  store ptr %29, ptr %277, align 8
  %278 = getelementptr inbounds ptr, ptr %272, i64 3
  store ptr %277, ptr %278, align 8
  %279 = getelementptr inbounds %.24, ptr %271, i64 0, i32 4
  store ptr %78, ptr %279, align 8
  %280 = getelementptr inbounds ptr, ptr %272, i64 4
  store ptr %279, ptr %280, align 8
  %281 = getelementptr inbounds %.24, ptr %271, i64 0, i32 5
  store ptr %133, ptr %281, align 8
  %282 = getelementptr inbounds ptr, ptr %272, i64 5
  store ptr %281, ptr %282, align 8
  %283 = alloca %.25, align 8
  %284 = alloca [14 x ptr], align 8
  store ptr %283, ptr %284, align 8
  %285 = getelementptr inbounds %.25, ptr %283, i64 0, i32 1
  store ptr %270, ptr %285, align 8
  %286 = getelementptr inbounds ptr, ptr %284, i64 1
  store ptr %285, ptr %286, align 8
  %287 = getelementptr inbounds %.25, ptr %283, i64 0, i32 2
  store i64 1, ptr %287, align 8
  %288 = getelementptr inbounds ptr, ptr %284, i64 2
  store ptr %287, ptr %288, align 8
  %289 = getelementptr inbounds %.25, ptr %283, i64 0, i32 3
  store ptr @main_kernel_2_main_kColReduction_reduce__6_1_0___no_ibXblock_tile_h64_1_kernel_name, ptr %289, align 8
  %290 = getelementptr inbounds ptr, ptr %284, i64 3
  store ptr %289, ptr %290, align 8
  %291 = getelementptr inbounds %.25, ptr %283, i64 0, i32 4
  store i64 %269, ptr %291, align 8
  %292 = getelementptr inbounds ptr, ptr %284, i64 4
  store ptr %291, ptr %292, align 8
  %293 = getelementptr inbounds %.25, ptr %283, i64 0, i32 5
  store i64 1, ptr %293, align 8
  %294 = getelementptr inbounds ptr, ptr %284, i64 5
  store ptr %293, ptr %294, align 8
  %295 = getelementptr inbounds %.25, ptr %283, i64 0, i32 6
  store i64 1, ptr %295, align 8
  %296 = getelementptr inbounds ptr, ptr %284, i64 6
  store ptr %295, ptr %296, align 8
  %297 = getelementptr inbounds %.25, ptr %283, i64 0, i32 7
  store i64 256, ptr %297, align 8
  %298 = getelementptr inbounds ptr, ptr %284, i64 7
  store ptr %297, ptr %298, align 8
  %299 = getelementptr inbounds %.25, ptr %283, i64 0, i32 8
  store i64 1, ptr %299, align 8
  %300 = getelementptr inbounds ptr, ptr %284, i64 8
  store ptr %299, ptr %300, align 8
  %301 = getelementptr inbounds %.25, ptr %283, i64 0, i32 9
  store i64 1, ptr %301, align 8
  %302 = getelementptr inbounds ptr, ptr %284, i64 9
  store ptr %301, ptr %302, align 8
  %303 = getelementptr inbounds %.25, ptr %283, i64 0, i32 10
  store i32 0, ptr %303, align 8
  %304 = getelementptr inbounds ptr, ptr %284, i64 10
  store ptr %303, ptr %304, align 8
  %305 = getelementptr inbounds %.25, ptr %283, i64 0, i32 11
  store ptr null, ptr %305, align 8
  %306 = getelementptr inbounds ptr, ptr %284, i64 11
  store ptr %305, ptr %306, align 8
  %307 = getelementptr inbounds %.25, ptr %283, i64 0, i32 12
  store i32 6, ptr %307, align 8
  %308 = getelementptr inbounds ptr, ptr %284, i64 12
  store ptr %307, ptr %308, align 8
  %309 = getelementptr inbounds %.25, ptr %283, i64 0, i32 13
  store ptr %272, ptr %309, align 8
  %310 = getelementptr inbounds ptr, ptr %284, i64 13
  store ptr %309, ptr %310, align 8
  %311 = load ptr, ptr %0, align 8
  %312 = load ptr, ptr %9, align 8
  store ptr %311, ptr %283, align 8
  call void %312(ptr %311, ptr nonnull @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr nonnull %284)
  br label %498

313:                                              ; preds = %1
  br i1 %134, label %314, label %410

314:                                              ; preds = %313
  store ptr @main_kernel_3_blob_gpu.binary, ptr %135, align 8
  %315 = alloca %.26, align 8
  %316 = alloca [2 x ptr], align 8
  store i64 512, ptr %315, align 8
  store ptr %315, ptr %316, align 8
  %317 = getelementptr inbounds %.26, ptr %315, i64 0, i32 1
  store ptr %133, ptr %317, align 8
  %318 = getelementptr inbounds ptr, ptr %316, i64 1
  store ptr %317, ptr %318, align 8
  %319 = alloca %.27, align 8
  %320 = alloca [14 x ptr], align 8
  store ptr %319, ptr %320, align 8
  %321 = getelementptr inbounds %.27, ptr %319, i64 0, i32 1
  store ptr %135, ptr %321, align 8
  %322 = getelementptr inbounds ptr, ptr %320, i64 1
  store ptr %321, ptr %322, align 8
  %323 = getelementptr inbounds %.27, ptr %319, i64 0, i32 2
  store i64 1, ptr %323, align 8
  %324 = getelementptr inbounds ptr, ptr %320, i64 2
  store ptr %323, ptr %324, align 8
  %325 = getelementptr inbounds %.27, ptr %319, i64 0, i32 3
  store ptr @main_kernel_3_main_kColReduction_reduce__6_1_0___thread_tile_h32_kernel_name, ptr %325, align 8
  %326 = getelementptr inbounds ptr, ptr %320, i64 3
  store ptr %325, ptr %326, align 8
  %327 = getelementptr inbounds %.27, ptr %319, i64 0, i32 4
  store i64 1, ptr %327, align 8
  %328 = getelementptr inbounds ptr, ptr %320, i64 4
  store ptr %327, ptr %328, align 8
  %329 = getelementptr inbounds %.27, ptr %319, i64 0, i32 5
  store i64 1, ptr %329, align 8
  %330 = getelementptr inbounds ptr, ptr %320, i64 5
  store ptr %329, ptr %330, align 8
  %331 = getelementptr inbounds %.27, ptr %319, i64 0, i32 6
  store i64 1, ptr %331, align 8
  %332 = getelementptr inbounds ptr, ptr %320, i64 6
  store ptr %331, ptr %332, align 8
  %333 = getelementptr inbounds %.27, ptr %319, i64 0, i32 7
  store i64 512, ptr %333, align 8
  %334 = getelementptr inbounds ptr, ptr %320, i64 7
  store ptr %333, ptr %334, align 8
  %335 = getelementptr inbounds %.27, ptr %319, i64 0, i32 8
  store i64 1, ptr %335, align 8
  %336 = getelementptr inbounds ptr, ptr %320, i64 8
  store ptr %335, ptr %336, align 8
  %337 = getelementptr inbounds %.27, ptr %319, i64 0, i32 9
  store i64 1, ptr %337, align 8
  %338 = getelementptr inbounds ptr, ptr %320, i64 9
  store ptr %337, ptr %338, align 8
  %339 = getelementptr inbounds %.27, ptr %319, i64 0, i32 10
  store i32 0, ptr %339, align 8
  %340 = getelementptr inbounds ptr, ptr %320, i64 10
  store ptr %339, ptr %340, align 8
  %341 = getelementptr inbounds %.27, ptr %319, i64 0, i32 11
  store ptr null, ptr %341, align 8
  %342 = getelementptr inbounds ptr, ptr %320, i64 11
  store ptr %341, ptr %342, align 8
  %343 = getelementptr inbounds %.27, ptr %319, i64 0, i32 12
  store i32 2, ptr %343, align 8
  %344 = getelementptr inbounds ptr, ptr %320, i64 12
  store ptr %343, ptr %344, align 8
  %345 = getelementptr inbounds %.27, ptr %319, i64 0, i32 13
  store ptr %316, ptr %345, align 8
  %346 = getelementptr inbounds ptr, ptr %320, i64 13
  store ptr %345, ptr %346, align 8
  %347 = load ptr, ptr %0, align 8
  %348 = load ptr, ptr %9, align 8
  store ptr %347, ptr %319, align 8
  call void %348(ptr %347, ptr nonnull @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr nonnull %320)
  %349 = icmp eq i32 %120, 0
  %350 = shl nsw i64 %121, 4
  %351 = add nsw i64 %350, -16
  %352 = and i64 %351, -512
  %353 = add nsw i64 %352, 512
  %354 = select i1 %349, i64 0, i64 %353
  %355 = icmp slt i64 %354, 1
  %356 = sub nsw i64 0, %354
  %357 = add nsw i64 %354, -1
  %358 = select i1 %355, i64 %356, i64 %357
  %359 = sdiv i64 %358, 512
  %360 = sub nsw i64 0, %359
  %361 = add nsw i64 %359, 1
  %362 = select i1 %355, i64 %360, i64 %361
  %363 = alloca ptr, align 8
  store ptr @main_kernel_4_blob_gpu.binary, ptr %363, align 8
  %364 = alloca %.28, align 8
  %365 = alloca [8 x ptr], align 8
  store i64 %.fca.3.0.load, ptr %364, align 8
  store ptr %364, ptr %365, align 8
  %366 = getelementptr inbounds %.28, ptr %364, i64 0, i32 1
  store i64 %.fca.3.0.load11, ptr %366, align 8
  %367 = getelementptr inbounds ptr, ptr %365, i64 1
  store ptr %366, ptr %367, align 8
  %368 = getelementptr inbounds %.28, ptr %364, i64 0, i32 2
  store i64 512, ptr %368, align 8
  %369 = getelementptr inbounds ptr, ptr %365, i64 2
  store ptr %368, ptr %369, align 8
  %370 = getelementptr inbounds %.28, ptr %364, i64 0, i32 3
  store i64 %354, ptr %370, align 8
  %371 = getelementptr inbounds ptr, ptr %365, i64 3
  store ptr %370, ptr %371, align 8
  %372 = getelementptr inbounds %.28, ptr %364, i64 0, i32 4
  store i64 %121, ptr %372, align 8
  %373 = getelementptr inbounds ptr, ptr %365, i64 4
  store ptr %372, ptr %373, align 8
  %374 = getelementptr inbounds %.28, ptr %364, i64 0, i32 5
  store ptr %29, ptr %374, align 8
  %375 = getelementptr inbounds ptr, ptr %365, i64 5
  store ptr %374, ptr %375, align 8
  %376 = getelementptr inbounds %.28, ptr %364, i64 0, i32 6
  store ptr %78, ptr %376, align 8
  %377 = getelementptr inbounds ptr, ptr %365, i64 6
  store ptr %376, ptr %377, align 8
  %378 = getelementptr inbounds %.28, ptr %364, i64 0, i32 7
  store ptr %133, ptr %378, align 8
  %379 = getelementptr inbounds ptr, ptr %365, i64 7
  store ptr %378, ptr %379, align 8
  %380 = alloca %.29, align 8
  %381 = alloca [14 x ptr], align 8
  store ptr %380, ptr %381, align 8
  %382 = getelementptr inbounds %.29, ptr %380, i64 0, i32 1
  store ptr %363, ptr %382, align 8
  %383 = getelementptr inbounds ptr, ptr %381, i64 1
  store ptr %382, ptr %383, align 8
  %384 = getelementptr inbounds %.29, ptr %380, i64 0, i32 2
  store i64 1, ptr %384, align 8
  %385 = getelementptr inbounds ptr, ptr %381, i64 2
  store ptr %384, ptr %385, align 8
  %386 = getelementptr inbounds %.29, ptr %380, i64 0, i32 3
  store ptr @main_kernel_4_main_kColReduction_reduce__6_1_0___thread_tile_h32_1_kernel_name, ptr %386, align 8
  %387 = getelementptr inbounds ptr, ptr %381, i64 3
  store ptr %386, ptr %387, align 8
  %388 = getelementptr inbounds %.29, ptr %380, i64 0, i32 4
  store i64 %362, ptr %388, align 8
  %389 = getelementptr inbounds ptr, ptr %381, i64 4
  store ptr %388, ptr %389, align 8
  %390 = getelementptr inbounds %.29, ptr %380, i64 0, i32 5
  store i64 1, ptr %390, align 8
  %391 = getelementptr inbounds ptr, ptr %381, i64 5
  store ptr %390, ptr %391, align 8
  %392 = getelementptr inbounds %.29, ptr %380, i64 0, i32 6
  store i64 1, ptr %392, align 8
  %393 = getelementptr inbounds ptr, ptr %381, i64 6
  store ptr %392, ptr %393, align 8
  %394 = getelementptr inbounds %.29, ptr %380, i64 0, i32 7
  store i64 512, ptr %394, align 8
  %395 = getelementptr inbounds ptr, ptr %381, i64 7
  store ptr %394, ptr %395, align 8
  %396 = getelementptr inbounds %.29, ptr %380, i64 0, i32 8
  store i64 1, ptr %396, align 8
  %397 = getelementptr inbounds ptr, ptr %381, i64 8
  store ptr %396, ptr %397, align 8
  %398 = getelementptr inbounds %.29, ptr %380, i64 0, i32 9
  store i64 1, ptr %398, align 8
  %399 = getelementptr inbounds ptr, ptr %381, i64 9
  store ptr %398, ptr %399, align 8
  %400 = getelementptr inbounds %.29, ptr %380, i64 0, i32 10
  store i32 0, ptr %400, align 8
  %401 = getelementptr inbounds ptr, ptr %381, i64 10
  store ptr %400, ptr %401, align 8
  %402 = getelementptr inbounds %.29, ptr %380, i64 0, i32 11
  store ptr null, ptr %402, align 8
  %403 = getelementptr inbounds ptr, ptr %381, i64 11
  store ptr %402, ptr %403, align 8
  %404 = getelementptr inbounds %.29, ptr %380, i64 0, i32 12
  store i32 8, ptr %404, align 8
  %405 = getelementptr inbounds ptr, ptr %381, i64 12
  store ptr %404, ptr %405, align 8
  %406 = getelementptr inbounds %.29, ptr %380, i64 0, i32 13
  store ptr %365, ptr %406, align 8
  %407 = getelementptr inbounds ptr, ptr %381, i64 13
  store ptr %406, ptr %407, align 8
  %408 = load ptr, ptr %0, align 8
  %409 = load ptr, ptr %9, align 8
  store ptr %408, ptr %380, align 8
  call void %409(ptr %408, ptr nonnull @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr nonnull %381)
  br label %498

410:                                              ; preds = %313
  store ptr @main_kernel_5_blob_gpu.binary, ptr %135, align 8
  %411 = alloca %.30, align 8
  %412 = alloca [2 x ptr], align 8
  store i64 256, ptr %411, align 8
  store ptr %411, ptr %412, align 8
  %413 = getelementptr inbounds %.30, ptr %411, i64 0, i32 1
  store ptr %133, ptr %413, align 8
  %414 = getelementptr inbounds ptr, ptr %412, i64 1
  store ptr %413, ptr %414, align 8
  %415 = alloca %.31, align 8
  %416 = alloca [14 x ptr], align 8
  store ptr %415, ptr %416, align 8
  %417 = getelementptr inbounds %.31, ptr %415, i64 0, i32 1
  store ptr %135, ptr %417, align 8
  %418 = getelementptr inbounds ptr, ptr %416, i64 1
  store ptr %417, ptr %418, align 8
  %419 = getelementptr inbounds %.31, ptr %415, i64 0, i32 2
  store i64 1, ptr %419, align 8
  %420 = getelementptr inbounds ptr, ptr %416, i64 2
  store ptr %419, ptr %420, align 8
  %421 = getelementptr inbounds %.31, ptr %415, i64 0, i32 3
  store ptr @main_kernel_5_main_kColReduction_reduce__6_1_0___block_tile_h64_kernel_name, ptr %421, align 8
  %422 = getelementptr inbounds ptr, ptr %416, i64 3
  store ptr %421, ptr %422, align 8
  %423 = getelementptr inbounds %.31, ptr %415, i64 0, i32 4
  store i64 1, ptr %423, align 8
  %424 = getelementptr inbounds ptr, ptr %416, i64 4
  store ptr %423, ptr %424, align 8
  %425 = getelementptr inbounds %.31, ptr %415, i64 0, i32 5
  store i64 1, ptr %425, align 8
  %426 = getelementptr inbounds ptr, ptr %416, i64 5
  store ptr %425, ptr %426, align 8
  %427 = getelementptr inbounds %.31, ptr %415, i64 0, i32 6
  store i64 1, ptr %427, align 8
  %428 = getelementptr inbounds ptr, ptr %416, i64 6
  store ptr %427, ptr %428, align 8
  %429 = getelementptr inbounds %.31, ptr %415, i64 0, i32 7
  store i64 256, ptr %429, align 8
  %430 = getelementptr inbounds ptr, ptr %416, i64 7
  store ptr %429, ptr %430, align 8
  %431 = getelementptr inbounds %.31, ptr %415, i64 0, i32 8
  store i64 1, ptr %431, align 8
  %432 = getelementptr inbounds ptr, ptr %416, i64 8
  store ptr %431, ptr %432, align 8
  %433 = getelementptr inbounds %.31, ptr %415, i64 0, i32 9
  store i64 1, ptr %433, align 8
  %434 = getelementptr inbounds ptr, ptr %416, i64 9
  store ptr %433, ptr %434, align 8
  %435 = getelementptr inbounds %.31, ptr %415, i64 0, i32 10
  store i32 0, ptr %435, align 8
  %436 = getelementptr inbounds ptr, ptr %416, i64 10
  store ptr %435, ptr %436, align 8
  %437 = getelementptr inbounds %.31, ptr %415, i64 0, i32 11
  store ptr null, ptr %437, align 8
  %438 = getelementptr inbounds ptr, ptr %416, i64 11
  store ptr %437, ptr %438, align 8
  %439 = getelementptr inbounds %.31, ptr %415, i64 0, i32 12
  store i32 2, ptr %439, align 8
  %440 = getelementptr inbounds ptr, ptr %416, i64 12
  store ptr %439, ptr %440, align 8
  %441 = getelementptr inbounds %.31, ptr %415, i64 0, i32 13
  store ptr %412, ptr %441, align 8
  %442 = getelementptr inbounds ptr, ptr %416, i64 13
  store ptr %441, ptr %442, align 8
  %443 = load ptr, ptr %0, align 8
  %444 = load ptr, ptr %9, align 8
  store ptr %443, ptr %415, align 8
  call void %444(ptr %443, ptr nonnull @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr nonnull %416)
  %445 = add nsw i64 %121, -1
  %446 = lshr i64 %445, 1
  %447 = and i64 %446, 9223372036854775552
  %448 = add nuw i64 %447, 256
  %449 = lshr i64 %445, 9
  %450 = add nuw nsw i64 %449, 1
  %451 = alloca ptr, align 8
  store ptr @main_kernel_6_blob_gpu.binary, ptr %451, align 8
  %452 = alloca %.32, align 8
  %453 = alloca [8 x ptr], align 8
  store i64 %.fca.3.0.load, ptr %452, align 8
  store ptr %452, ptr %453, align 8
  %454 = getelementptr inbounds %.32, ptr %452, i64 0, i32 1
  store i64 %.fca.3.0.load11, ptr %454, align 8
  %455 = getelementptr inbounds ptr, ptr %453, i64 1
  store ptr %454, ptr %455, align 8
  %456 = getelementptr inbounds %.32, ptr %452, i64 0, i32 2
  store i64 256, ptr %456, align 8
  %457 = getelementptr inbounds ptr, ptr %453, i64 2
  store ptr %456, ptr %457, align 8
  %458 = getelementptr inbounds %.32, ptr %452, i64 0, i32 3
  store i64 %448, ptr %458, align 8
  %459 = getelementptr inbounds ptr, ptr %453, i64 3
  store ptr %458, ptr %459, align 8
  %460 = getelementptr inbounds %.32, ptr %452, i64 0, i32 4
  store i64 %121, ptr %460, align 8
  %461 = getelementptr inbounds ptr, ptr %453, i64 4
  store ptr %460, ptr %461, align 8
  %462 = getelementptr inbounds %.32, ptr %452, i64 0, i32 5
  store ptr %29, ptr %462, align 8
  %463 = getelementptr inbounds ptr, ptr %453, i64 5
  store ptr %462, ptr %463, align 8
  %464 = getelementptr inbounds %.32, ptr %452, i64 0, i32 6
  store ptr %78, ptr %464, align 8
  %465 = getelementptr inbounds ptr, ptr %453, i64 6
  store ptr %464, ptr %465, align 8
  %466 = getelementptr inbounds %.32, ptr %452, i64 0, i32 7
  store ptr %133, ptr %466, align 8
  %467 = getelementptr inbounds ptr, ptr %453, i64 7
  store ptr %466, ptr %467, align 8
  %468 = alloca %.33, align 8
  %469 = alloca [14 x ptr], align 8
  store ptr %468, ptr %469, align 8
  %470 = getelementptr inbounds %.33, ptr %468, i64 0, i32 1
  store ptr %451, ptr %470, align 8
  %471 = getelementptr inbounds ptr, ptr %469, i64 1
  store ptr %470, ptr %471, align 8
  %472 = getelementptr inbounds %.33, ptr %468, i64 0, i32 2
  store i64 1, ptr %472, align 8
  %473 = getelementptr inbounds ptr, ptr %469, i64 2
  store ptr %472, ptr %473, align 8
  %474 = getelementptr inbounds %.33, ptr %468, i64 0, i32 3
  store ptr @main_kernel_6_main_kColReduction_reduce__6_1_0___block_tile_h64_1_kernel_name, ptr %474, align 8
  %475 = getelementptr inbounds ptr, ptr %469, i64 3
  store ptr %474, ptr %475, align 8
  %476 = getelementptr inbounds %.33, ptr %468, i64 0, i32 4
  store i64 %450, ptr %476, align 8
  %477 = getelementptr inbounds ptr, ptr %469, i64 4
  store ptr %476, ptr %477, align 8
  %478 = getelementptr inbounds %.33, ptr %468, i64 0, i32 5
  store i64 1, ptr %478, align 8
  %479 = getelementptr inbounds ptr, ptr %469, i64 5
  store ptr %478, ptr %479, align 8
  %480 = getelementptr inbounds %.33, ptr %468, i64 0, i32 6
  store i64 1, ptr %480, align 8
  %481 = getelementptr inbounds ptr, ptr %469, i64 6
  store ptr %480, ptr %481, align 8
  %482 = getelementptr inbounds %.33, ptr %468, i64 0, i32 7
  store i64 256, ptr %482, align 8
  %483 = getelementptr inbounds ptr, ptr %469, i64 7
  store ptr %482, ptr %483, align 8
  %484 = getelementptr inbounds %.33, ptr %468, i64 0, i32 8
  store i64 1, ptr %484, align 8
  %485 = getelementptr inbounds ptr, ptr %469, i64 8
  store ptr %484, ptr %485, align 8
  %486 = getelementptr inbounds %.33, ptr %468, i64 0, i32 9
  store i64 1, ptr %486, align 8
  %487 = getelementptr inbounds ptr, ptr %469, i64 9
  store ptr %486, ptr %487, align 8
  %488 = getelementptr inbounds %.33, ptr %468, i64 0, i32 10
  store i32 0, ptr %488, align 8
  %489 = getelementptr inbounds ptr, ptr %469, i64 10
  store ptr %488, ptr %489, align 8
  %490 = getelementptr inbounds %.33, ptr %468, i64 0, i32 11
  store ptr null, ptr %490, align 8
  %491 = getelementptr inbounds ptr, ptr %469, i64 11
  store ptr %490, ptr %491, align 8
  %492 = getelementptr inbounds %.33, ptr %468, i64 0, i32 12
  store i32 8, ptr %492, align 8
  %493 = getelementptr inbounds ptr, ptr %469, i64 12
  store ptr %492, ptr %493, align 8
  %494 = getelementptr inbounds %.33, ptr %468, i64 0, i32 13
  store ptr %453, ptr %494, align 8
  %495 = getelementptr inbounds ptr, ptr %469, i64 13
  store ptr %494, ptr %495, align 8
  %496 = load ptr, ptr %0, align 8
  %497 = load ptr, ptr %9, align 8
  store ptr %496, ptr %468, align 8
  call void %497(ptr %496, ptr nonnull @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr nonnull %469)
  br label %498

498:                                              ; preds = %137, %229, %314, %410
  %499 = alloca %.13, align 8
  %500 = alloca [2 x ptr], align 8
  store ptr %499, ptr %500, align 8
  %501 = getelementptr inbounds %.13, ptr %499, i64 0, i32 1
  store ptr %78, ptr %501, align 8
  %502 = getelementptr inbounds ptr, ptr %500, i64 1
  store ptr %501, ptr %502, align 8
  %503 = load ptr, ptr %0, align 8
  %504 = load ptr, ptr %9, align 8
  store ptr %503, ptr %499, align 8
  call void %504(ptr %503, ptr nonnull @dealloc___gpu___pvoid_pvoid___void, ptr nonnull %500)
  %505 = alloca %.14, align 8
  %506 = alloca [2 x ptr], align 8
  store ptr %505, ptr %506, align 8
  %507 = getelementptr inbounds %.14, ptr %505, i64 0, i32 1
  store ptr %29, ptr %507, align 8
  %508 = getelementptr inbounds ptr, ptr %506, i64 1
  store ptr %507, ptr %508, align 8
  %509 = load ptr, ptr %0, align 8
  %510 = load ptr, ptr %9, align 8
  store ptr %509, ptr %505, align 8
  call void %510(ptr %509, ptr nonnull @dealloc___gpu___pvoid_pvoid___void, ptr nonnull %506)
  %511 = alloca i64, align 8
  %512 = alloca %.15, align 8
  %513 = alloca [13 x ptr], align 8
  store ptr %512, ptr %513, align 8
  %514 = getelementptr inbounds %.15, ptr %512, i64 0, i32 1
  store ptr null, ptr %514, align 8
  %515 = getelementptr inbounds ptr, ptr %513, i64 1
  store ptr %514, ptr %515, align 8
  %516 = getelementptr inbounds %.15, ptr %512, i64 0, i32 2
  store ptr %133, ptr %516, align 8
  %517 = getelementptr inbounds ptr, ptr %513, i64 2
  store ptr %516, ptr %517, align 8
  %518 = getelementptr inbounds %.15, ptr %512, i64 0, i32 3
  store ptr %133, ptr %518, align 8
  %519 = getelementptr inbounds ptr, ptr %513, i64 3
  store ptr %518, ptr %519, align 8
  %520 = getelementptr inbounds %.15, ptr %512, i64 0, i32 4
  store i64 0, ptr %520, align 8
  %521 = getelementptr inbounds ptr, ptr %513, i64 4
  store ptr %520, ptr %521, align 8
  %522 = getelementptr inbounds %.15, ptr %512, i64 0, i32 5
  store i64 1, ptr %522, align 8
  %523 = getelementptr inbounds ptr, ptr %513, i64 5
  store ptr %522, ptr %523, align 8
  %524 = getelementptr inbounds %.15, ptr %512, i64 0, i32 6
  store i64 1, ptr %524, align 8
  %525 = getelementptr inbounds ptr, ptr %513, i64 6
  store ptr %524, ptr %525, align 8
  %526 = getelementptr inbounds %.15, ptr %512, i64 0, i32 7
  store ptr %511, ptr %526, align 8
  %527 = getelementptr inbounds ptr, ptr %513, i64 7
  store ptr %526, ptr %527, align 8
  %528 = getelementptr inbounds %.15, ptr %512, i64 0, i32 8
  store ptr %511, ptr %528, align 8
  %529 = getelementptr inbounds ptr, ptr %513, i64 8
  store ptr %528, ptr %529, align 8
  %530 = getelementptr inbounds %.15, ptr %512, i64 0, i32 9
  store i64 0, ptr %530, align 8
  %531 = getelementptr inbounds ptr, ptr %513, i64 9
  store ptr %530, ptr %531, align 8
  %532 = getelementptr inbounds %.15, ptr %512, i64 0, i32 10
  store i64 0, ptr %532, align 8
  %533 = getelementptr inbounds ptr, ptr %513, i64 10
  store ptr %532, ptr %533, align 8
  %534 = getelementptr inbounds %.15, ptr %512, i64 0, i32 11
  store i64 1, ptr %534, align 8
  %535 = getelementptr inbounds ptr, ptr %513, i64 11
  store ptr %534, ptr %535, align 8
  %536 = getelementptr inbounds %.15, ptr %512, i64 0, i32 12
  %537 = getelementptr inbounds ptr, ptr %513, i64 12
  store ptr %536, ptr %537, align 8
  %538 = load ptr, ptr %0, align 8
  %539 = load ptr, ptr %9, align 8
  store ptr %538, ptr %512, align 8
  call void %539(ptr %538, ptr nonnull @inc_ref___gpu___pvoid_pvoid_m1df32_m1di64___m0df32, ptr nonnull %513)
  %.unpack = load ptr, ptr %536, align 8
  %.elt23 = getelementptr inbounds %.15, ptr %512, i64 0, i32 12, i32 1
  %.unpack24 = load ptr, ptr %.elt23, align 8
  %540 = alloca %.16, align 8
  %541 = alloca [2 x ptr], align 8
  store ptr %540, ptr %541, align 8
  %542 = getelementptr inbounds %.16, ptr %540, i64 0, i32 1
  store ptr %133, ptr %542, align 8
  %543 = getelementptr inbounds ptr, ptr %541, i64 1
  store ptr %542, ptr %543, align 8
  %544 = load ptr, ptr %0, align 8
  %545 = load ptr, ptr %9, align 8
  store ptr %544, ptr %540, align 8
  call void %545(ptr %544, ptr nonnull @dealloc___gpu___pvoid_pvoid___void, ptr nonnull %541)
  %546 = alloca %.17, align 8
  %547 = alloca [3 x ptr], align 8
  store ptr %546, ptr %547, align 8
  %548 = getelementptr inbounds %.17, ptr %546, i64 0, i32 1
  store i64 4, ptr %548, align 8
  %549 = getelementptr inbounds ptr, ptr %547, i64 1
  store ptr %548, ptr %549, align 8
  %550 = getelementptr inbounds %.17, ptr %546, i64 0, i32 2
  %551 = getelementptr inbounds ptr, ptr %547, i64 2
  store ptr %550, ptr %551, align 8
  %552 = load ptr, ptr %0, align 8
  %553 = load ptr, ptr %9, align 8
  store ptr %552, ptr %546, align 8
  call void %553(ptr %552, ptr nonnull @alloc___cpu___pvoid_i64___pvoid, ptr nonnull %547)
  %554 = load ptr, ptr %550, align 8
  %555 = alloca %.18, align 8
  %556 = alloca [8 x ptr], align 8
  store ptr %555, ptr %556, align 8
  %557 = getelementptr inbounds %.18, ptr %555, i64 0, i32 1
  store ptr null, ptr %557, align 8
  %558 = getelementptr inbounds ptr, ptr %556, i64 1
  store ptr %557, ptr %558, align 8
  %559 = getelementptr inbounds %.18, ptr %555, i64 0, i32 2
  store ptr %.unpack, ptr %559, align 8
  %560 = getelementptr inbounds ptr, ptr %556, i64 2
  store ptr %559, ptr %560, align 8
  %561 = getelementptr inbounds %.18, ptr %555, i64 0, i32 3
  store ptr %.unpack24, ptr %561, align 8
  %562 = getelementptr inbounds ptr, ptr %556, i64 3
  store ptr %561, ptr %562, align 8
  %563 = getelementptr inbounds %.18, ptr %555, i64 0, i32 4
  store i64 0, ptr %563, align 8
  %564 = getelementptr inbounds ptr, ptr %556, i64 4
  store ptr %563, ptr %564, align 8
  %565 = getelementptr inbounds %.18, ptr %555, i64 0, i32 5
  store ptr %554, ptr %565, align 8
  %566 = getelementptr inbounds ptr, ptr %556, i64 5
  store ptr %565, ptr %566, align 8
  %567 = getelementptr inbounds %.18, ptr %555, i64 0, i32 6
  store ptr %554, ptr %567, align 8
  %568 = getelementptr inbounds ptr, ptr %556, i64 6
  store ptr %567, ptr %568, align 8
  %569 = getelementptr inbounds %.18, ptr %555, i64 0, i32 7
  store i64 0, ptr %569, align 8
  %570 = getelementptr inbounds ptr, ptr %556, i64 7
  store ptr %569, ptr %570, align 8
  %571 = load ptr, ptr %0, align 8
  %572 = load ptr, ptr %9, align 8
  store ptr %571, ptr %555, align 8
  call void %572(ptr %571, ptr nonnull @d2h___gpu___pvoid_pvoid_m0df32_m0df32___void, ptr nonnull %556)
  %573 = alloca %.19, align 8
  %574 = alloca [2 x ptr], align 8
  store ptr %573, ptr %574, align 8
  %575 = getelementptr inbounds %.19, ptr %573, i64 0, i32 1
  store ptr null, ptr %575, align 8
  %576 = getelementptr inbounds ptr, ptr %574, i64 1
  store ptr %575, ptr %576, align 8
  %577 = load ptr, ptr %0, align 8
  %578 = load ptr, ptr %9, align 8
  store ptr %577, ptr %573, align 8
  call void %578(ptr %577, ptr nonnull @sync_on_stream___gpu___pvoid_pvoid___void, ptr nonnull %574)
  %579 = alloca %.20, align 8
  %580 = alloca [2 x ptr], align 8
  store ptr %579, ptr %580, align 8
  %581 = getelementptr inbounds %.20, ptr %579, i64 0, i32 1
  store ptr %.unpack, ptr %581, align 8
  %582 = getelementptr inbounds ptr, ptr %580, i64 1
  store ptr %581, ptr %582, align 8
  %583 = load ptr, ptr %0, align 8
  %584 = load ptr, ptr %9, align 8
  store ptr %583, ptr %579, align 8
  call void %584(ptr %583, ptr nonnull @dealloc___gpu___pvoid_pvoid___void, ptr nonnull %580)
  %585 = alloca %.21, align 8
  %586 = alloca [5 x ptr], align 8
  store ptr %585, ptr %586, align 8
  %587 = getelementptr inbounds %.21, ptr %585, i64 0, i32 1
  store i64 0, ptr %587, align 8
  %588 = getelementptr inbounds ptr, ptr %586, i64 1
  store ptr %587, ptr %588, align 8
  %589 = getelementptr inbounds %.21, ptr %585, i64 0, i32 2
  store ptr %554, ptr %589, align 8
  %590 = getelementptr inbounds ptr, ptr %586, i64 2
  store ptr %589, ptr %590, align 8
  %591 = getelementptr inbounds %.21, ptr %585, i64 0, i32 3
  store ptr %554, ptr %591, align 8
  %592 = getelementptr inbounds ptr, ptr %586, i64 3
  store ptr %591, ptr %592, align 8
  %593 = getelementptr inbounds %.21, ptr %585, i64 0, i32 4
  store i64 0, ptr %593, align 8
  %594 = getelementptr inbounds ptr, ptr %586, i64 4
  store ptr %593, ptr %594, align 8
  %595 = load ptr, ptr %0, align 8
  %596 = load ptr, ptr %9, align 8
  store ptr %595, ptr %585, align 8
  call void %596(ptr %595, ptr nonnull @ral_send_output___cpu___pvoid_i64_m0df32___void, ptr nonnull %586)
  ret void
}

[DISC] LowerLLVMToBinary takes: 7.463800e-02 s.
object file to shared library command: gcc --shared -o ./module.so ./module.so.o
save shared lib file to : ./module.so
[DISC] BinaryStrToSharedLibrary takes: 1.587000e-02 s.
[DISC] LowerHLOToSharedLibrary takes: 1.494193e+00 s.
